---
title: "Large scale processing"
---

Processing of larger areas extending to a global scale is one of the more challenging tasks in earth observation, 
but certainly one that this platform aims to tackle. In this page we describe one of the best practice based on the example of [processing a croptype map for all 27 countries in the European Union](https://github.com/openEOPlatform/openeo-classification){target="_blank"}. We do recommend you to reaching out on the forum or helpdesk regarding your particular case, as workflows can vary, and adequate processing resources may require some advanced planning.


![EU27 croptype map, processed on openEO](https://raw.githubusercontent.com/openEOPlatform/openeo-classification/60aa7a869f9000b1795afe2c9dde0d7977bcdbc6/docs/full_europe.png)

The approach described here is based on local files to track the production. This is a low-cost approach that does not require special IT knowledge but comes with some risks, such as losing your local files. A more robust approach for production-grade projects would typically rely on some sort of database or STAC catalogue service to monitor processing. Such a setup is, however quite similar in many aspects.

The basic strategy for processing large areas is to split them up into smaller areas, usually according to a regular tile grid. Splitting reduces the size of the area that needs to be processed by one batch job and avoids running into all kinds of limitations. For instance, when processing a specific projection, you anyway have to stay within the bounds of that projection. Also, the output file size of a job often becomes impractical when working over huge areas. Or you will hit bottlenecks in the backend implementation that does not occur for normally sized jobs. Also, when a smaller job fails or requires reprocessing, the cost will be smaller.

## Relevant openEO features

We want to highlight a few key elements that made us choose openEO for large-scale processing:

- [Performance & scalability](https://openeo.org/documentation/1.0/developers/backends/performance.html){target="_blank"}
- STAC metadata is automatically generated for you, ensuring that your output is ready for dissemination without requiring you to become a metadata expert.
- Where relevant FAIR principles are taken into account automatically, such as providing provenance information.
- Cloud-optimized file formats are generated by default.
- Processing can be distributed over multiple backends.


## Preparation

The concept involves initially generating and persistently storing a list of tiles to be produced, including all the necessary attributes for each specific tile. This approach provides a comprehensive visual overview of the processing that will take place.

Having job parameters in a file is also useful for debugging afterwards. Determining parameters at runtime means you don't 
have absolute certainty over the value of a specific argument, as there may be bugs in your code.

## Prepare tiling grid

The tiling grid choice depends on your preferred projection system, which, in turn, is determined by your area of interest. For Europe, you can use the EPSG:3035 projection, while for global processing, considering different projections in accordance with UTM zones may be preferable.

The size of tiles in your grid is also important and often ranges from 20km to 100km. For relatively light workflows, a 100km grid can work well, while for more demanding cases, a 20km grid is better. In our example, we chose to work with 20km tiles because the workflow was quite demanding. A smaller tile size can also result in less unneeded processing when your target area has an irregular shape, like most countries and continents.

A couple of basic grids can be found here: 
[https://artifactory.vgt.vito.be/ui/repos/tree/General/auxdata-public/grids](https://artifactory.vgt.vito.be/ui/repos/tree/General/auxdata-public/grids){target="blank"}

The images below illustrate the overlap in the UTM grids versus a regular LAEA grid.


UTM 100km             |  LAEA 100km
:-------------------------:|:-------------------------:
![](https://user-images.githubusercontent.com/5937096/231963581-1c51a512-c240-4d23-b557-30a3577c9027.png)|  ![](https://user-images.githubusercontent.com/5937096/231963750-562b921c-7b5b-4ec1-86ca-cf1fd75e625d.png)


A grid can be masked based on the countries we want to load, the following script shows an example:

```python
import geopandas as gpd
europe = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
europe = europe[europe.continent=="Europe"]
df = gpd.read_file("https://artifactory.vgt.vito.be/auxdata-public/grids/LAEA-20km.gpkg",mask=europe)
```

## Prepare job attributes

In addition to the tiling grid, we recommend determining other necessary properties for your processing jobs in advance. This enables a thorough review of these properties before initiating the processing. Examples includes basic elements such as job titles or tile-specific processing parameters, as well as attributes to determine the processing order.

In this step, you may also want to make sure to determine the correct tile extent in the coordinate system of your tile grid. Providing exact coordinates in the right projection is necessary to ensure pixel-perfect alignment of your tiles.

## Tuning your processing job

Before kicking off large processing, you want to be very sure that the correct output is generated and that you have sufficient credits and resources available to finish your job in time. This can be done by simply running various jobs and using the statistics reported in the metadata to determine average parameters. (The map production section below shows a way to collect these parameters in a CSV.)

For instance, for the case of processing the EU27 croptype map, consisting of ~11000 20km tiles, we made the following calculations up front:

- The average runtime was 30 minutes, which means that it would take ~15 days of continuous processing with 15 parallel jobs.
- The average cost was below 100 credits, so we would be able to process with a budget of 1100000 credits.

To achieve these numbers, we did have to optimize batch job settings and the overall workflow to reduce resource usage.

A common bottleneck to parallelization is memory consumption, and it can be useful to know the maximum memory allocation on a single machine in your backend of choice. For instance, in a cloud environment with 16GB per machine and 4 CPUs, using slightly less than 4GB per worker is efficient as you can fit 4 parallel workers on a single VM while requiring 6GB would fit only 2 workers and leave about 4GB unused.

In our example, we used the openEO backend running on the Copernicus dataspace, which is based on Geotrellis, the execution options are documented [here](https://documentation.dataspace.copernicus.eu/APIs/openEO/job_config.html#customizing-batch-job-resources).

## Starting map production

The openEO Python client provides a useful tool to run multiple processing jobs in [multiple backends](https://open-eo.github.io/openeo-python-client/cookbook/job_manager.html){target="_blank"}.

It takes a GeoJSON corresponding to your tile grid and job properties per tile and triggers a function provided by you whenever a new job needs to be created. You can configure multiple backends and set the number of parallel jobs per backend. 

Similarly, it also takes care of error handling and can be considered more resilient compared to writing a simple loop yourself.

This script uses a CSV file to track your jobs, and whenever it is interrupted, it can simply resume from that CSV file, making it tolerant to failure.

![Tracking jobs by CSV](https://user-images.githubusercontent.com/5937096/231968590-f0f0b415-453c-4ab7-9502-82eab795a84e.png)


## Errors during production

It's common for some tasks to run into issues during production, which is okay if it doesn't happen too frequently. If a task fails, take a quick look at the error logs. If there's no obvious reason, a simple retry might do the trick. Sometimes, you might need to allocate more memory. 

We also see a limited number of cases where issues in the underlying product archive cause failures or artifacts. These are harder to resolve and may require interaction with the backend.
