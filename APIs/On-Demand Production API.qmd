---
title: On-Demand Production API
---

On-Demand Production API provides access to two sets of data processing workflows: 

* production of the Sentinel-1/2/3 products from Level-0 data to Level-1 and Level-2. The service allows generation of products using different versions of the processors and parameters than the ones used by the Production Service.
* processing of additional products including Sentinel-1 backscatter and interferometric coherence.

Both production and processing workflows are available through the OData API described below, and the [Data Workspace](https://dataspace.copernicus.eu/analyse/data-workspace){target="blank"} which provided a graphical user interface to the service.  

Basic interaction with the On-Demand Production service is following: 

* user sends a request including the name of the workflow (the processor to be executed), the identifier of the reference product (product from which the processing should start or product which should be reprocessed) and workflow options (parameters) specific to the selected workflow.
* system runs the requested workflows and the result is made available for retrieval.

## On-Demand Production

The production of the Copernicus products is done using the recent IPF (Instrument Processing Facility) processor sets, used by the Production Services.
Users can request production of any Sentinel-2 or Sentinel-3 product using the existing output product as reference.  
Using the metadata of the reference product, the service identifies the necessary Level-0 and Auxiliary products to run the IPF.  
The input products are retrieved from the Copernicus LTA (Long Term Archive) and submitted to a processing engine. 
The output of the production is stored in the temporary data storage and available to download for a period of 14 days.  

The production workflows are available for Sentinel-1, Sentinel-2 and Sentinel-3.  

Check the examples below to learn how to submit a production order.

### Sentinel-1

All Sentinel-1 workflows start from Level-0 RAW products and require Level-0 product as input reference.  

**The available workflows** 

|Workflow name|Input product type|Output product type|
|--|--|--|
|Sentinel-1-L0-EW_GRDM_1S|EW_RAW__0|EW_GRDM_1S|
|Sentinel-1-L0-EW_OCN__2S|EW_RAW__0|EW_OCN__2S|
|Sentinel-1-L0-EW_SLC__1S|EW_RAW__0|EW_SLC__1S|
|Sentinel-1-L0-IW_GRDH_1S|IW_RAW__0|IW_GRDH_1S|
|Sentinel-1-L0-IW_OCN__2S|IW_RAW__0|IW_OCN__2S|
|Sentinel-1-L0-IW_SLC__1S|IW_RAW__0|IW_SLC__1S|
|Sentinel-1-L0-S1_GRDH_1S|S1_RAW__0|S1_GRDH_1S|
|Sentinel-1-L0-S1_OCN__2S|S1_RAW__0|S1_OCN_2S|
|Sentinel-1-L0-S1_SLC__1S|S1_RAW__0|S1_SLC__1S|
|Sentinel-1-L0-S2_GRDH_1S|S2_RAW__0|S2_GRDH_1S|
|Sentinel-1-L0-S2_OCN__2S|S2_RAW__0|S2_OCN__2S|
|Sentinel-1-L0-S2_SLC__1S|S2_RAW__0|S2_SLC__1S|
|Sentinel-1-L0-S3_GRDH_1S|S3_RAW__0|S3_GRDH_1S|
|Sentinel-1-L0-S3_OCN__2S|S3_RAW__0|S3_OCN__2S|
|Sentinel-1-L0-S3_SLC__1S|S3_RAW__0|S3_SLC__1S|
|Sentinel-1-L0-S4_GRDH_1S|S4_RAW__0|S4_GRDH_1S|
|Sentinel-1-L0-S4_OCN__2S|S4_RAW__0|S4_OCN__2S|
|Sentinel-1-L0-S4_SLC__1S|S4_RAW__0|S4_SLC__1S|
|Sentinel-1-L0-S5_GRDH_1S|S5_RAW__0|S5_GRDH_1S|
|Sentinel-1-L0-S5_OCN__2S|S5_RAW__0|S5_OCN__2S|
|Sentinel-1-L0-S5_SLC__1S|S5_RAW__0|S5_SLC__1S|
|Sentinel-1-L0-S6_GRDH_1S|S6_RAW__0|S6_GRDH_1S|
|Sentinel-1-L0-S6_OCN__2S|S6_RAW__0|S6_OCN__2S|
|Sentinel-1-L0-S6_SLC__1S|S6_RAW__0|S6_SLC__1S|
|Sentinel-1-L0-WV_OCN__2S|WV_RAW__0|WV_OCN__2S|
|Sentinel-1-L0-WV_SLC__1S|WV_RAW__0|WV_SLC__1S|

**Workflow options for Sentinel-1** 

|Option name|Description|Values|
|--|--|--|
|DEM|Identifier of the digital elevation model to use for processing.|"coarse", "getasse30v1", "getasse30v2"|
|output_storage|Location of output product.|“TEMPORARY”|

### Sentinel-2

All Sentinel-2 workflows start from Level-0 products and require Level-1/Level-2 (output) product as input reference.
For Sentinel-2-L1-S2MSI2A the input is searched in the catalogue and it is possible that the baseline of the found product is different from the baseline provided in WorkflowOptions.

**The available workflows** 

|Workflow name|Input product type|Output product type|
|--|--|--|
|Sentinel-2-L0-S2MSI1C|S2MSI1C|S2MSI1C|
|Sentinel-2-L0-S2MSI2A|S2MSI2A|S2MSI2A|
|Sentinel-2-L1-S2MSI2A|S2MSI2A|S2MSI2A|

**Workflow options for Sentinel-2** 

|Option name|Description|Values|
|--|--|--|
|Baseline|Version number of the processor and static auxiliary files.|dependent on the specific workflow|
|Baseline Collection|Compatible Baseline group.|dependent on the specific workflow|
|output_storage|Location of output product.|“TEMPORARY”|

### Sentinel-3

The Sentinel-3 workflows run from Level-0 products and require Level-1/Level-2 (output) product as input reference.

**The available workflows** 

|Workflow name|Input product type|Output product type|
|--|--|--|
|Sentinel-3-L0-OL_1_EFR|OL_0_EFR___|OL_1_EFR___|
|Sentinel-3-L0-OL_1_ERR|OL_0_EFR___|OL_1_ERR___|
|Sentinel-3-L0-OL_2_LFR|OL_0_EFR___|OL_2_LFR___|
|Sentinel-3-L0-OL_2_LRR|OL_0_EFR___|OL_2_LRR___|
|Sentinel-3-L0-SL_1_RBT|SL_0_SLT___|SL_1_RBT___|
|Sentinel-3-L0-SL_2_FRP|SL_0_SLT___|SL_2_FRP___|
|Sentinel-3-L0-SL_2_LST|SL_0_SLT___|SL_2_LST___|
|Sentinel-3-L0-SR_1_SRA|SR_0_SRA___|SR_1_SRA___|
|Sentinel-3-L0-SR_2_LAN|SR_0_SRA\_\_\_,MW_0_MWR___|SR_2_LAN___|
|Sentinel-3-L0-SY_2_AOD|OL_0_EFR\_\_\_,SL_0_SLT___|SY_2_AOD___|
|Sentinel-3-L0-SY_2_SYN|OL_0_EFR\_\_\_,SL_0_SLT___|SY_2_SYN___|
|Sentinel-3-L0-SY_2_VG1|OL_1_EFR\_\_\_,SL_1_RBT___|SY_2_VG1___|
|Sentinel-3-L0-SR_2_LAN_LI|SR_0_SRA\_\_\_,MW_0_MWR___|SR_2_LAN_LI|
|Sentinel-3-L0-SR_2_LAN_HY|SR_0_SRA\_\_\_,MW_0_MWR___|SR_2_LAN_HY|
|Sentinel-3-L0-SR_2_LAN_SI|SR_0_SRA\_\_\_,MW_0_MWR___|SR_2_LAN_SI|

**Workflow options for Sentinel-3** 

|Option name|Description|Values|
|--|--|--|
|Baseline|Version number of the processor and static auxiliary files.|dependent on the specific workflow|
|Mode|Timeliness condition : NTC (Non-Time Critical).|“NTC”|
|output_storage|Location of output product.|“TEMPORARY”|


## On-Demand Processing 

On-demand processing capability for CARD-BS and CARD-COH6/12 is available in the Copernicus Data Space Ecosystem. This service is offered via a limited pool of resources, shared across all users, and can be used to process the data free of charge. This is suitable for users who need to process smaller batches of products. There is no guarantee that processing will be completed in a specific time. For commercial use, the [price list](https://creodias.eu/pricing/eo-data-hub-services/){target="blank"} is available in the CREODIAS portal. The service is available via an On-Demand Processing API and Data Workspace.

## OnDemand Production API with OData interface

The OnDemand Processing API allows the users to interact with the service to issue and control the orders. It provides functionalities like creation, update, cancellation, pausing and monitoring of orders. This allows the users to have better control over the workflow execution process.
The API is based on the Production Interface Delivery Point ICD, extended to provide more flexibility and additional functionality.

## General information

This documentation provides an overview of the OnDemand Processing (ODP) API based on the Open Data Protocol (OData) standard. The ODP API complements the Catalogue API used for accessing data and metadata from the Copernicus data catalogue.  
Access to the API is limited by the Authentication service. Quotas are assigned according to the user typology and include limits on the number of concurrent orders and available processing workflows.  
The API allows the discovery of all available workflows that can be run in the Copernicus Data Space Ecosystem platform, indicating which data types can be processed and what the available parameters and output modes are.

### API Endpoint

The **ODP API endpoint is [https://odp.dataspace.copernicus.eu/odata/v1](https://odp.dataspace.copernicus.eu/odata/v1){target="blank"}**.

**OpenAPI documentation is located at [https://odp.dataspace.copernicus.eu/odata/docs](https://odp.dataspace.copernicus.eu/odata/docs){target="blank"}**

### API Operations

The ODP API supports the following operations: 

* __GET__ - This operation retrieves data and metadata from the ODP. The GET operation supports various query options to filter, order, and limit the data retrieved
* __POST__ - This operation creates new entities in the ODP. The POST operation requires a payload in JSON format that specifies the properties of the new entity
* __PATCH__ - This operation is used to update existing entities in the ODP. The PATCH operation requires a JSON payload that specifies the entity's properties to update
* __DELETE__ - This operation deletes existing entities from the ODP. The DELETE operation requires the URL of the entity to be deleted 

### API Resources

The ODP API provides access to the following resources:  

* _Workflow_ - predefined processor which creates a single output product or series of products based on the input parameters provided by the user. Typical inputs are the name of the source product and parameters specific to the processing chain
* _Production Order_ - request for production using a Workflow chosen by the user
* _Batch Order_ - request to produce multiple products using a chosen Workflow
* _Order Item_ - single processing job within and a Production Order or Batch Order

### Authentication

:::{.callout-important}
To access the ODP API, you need an authorization token, as only authorized users are allowed to interact with the processing service.  
:::

To get the token, you can use the following script:

```bash
export KEYCLOAK_TOKEN=$(curl -d 'client_id=cdse-public' \
-d 'username=<LOGIN>' \
-d 'password=<PASSWORD>' \
-d 'grant_type=password' \
'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' | \
python -m json.tool | grep "access_token" | awk -F\" '{print $4}')
```

Once you have your token, you can execute a request to the API, including the token in the request header. For example, to list available Workflows, you can use the following command:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" 'https://odp.dataspace.copernicus.eu/odata/v1/Workflows'
```

More information on the tokens and authentication can be found here: [https://documentation.dataspace.copernicus.eu/APIs/OData.html#product-download](https://documentation.dataspace.copernicus.eu/APIs/OData.html#product-download){target="blank"}

## Querying the API

### Workflows
#### Listing available Workflows
To list all processing Workflows available to the user:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/Workflows'
```

To search for specific Workflows you can use filters on the attributes. To find workflow named “coh”:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/Workflows?$filter=Name%20eq%20coh'
```

In a similar way to find all Workflows suitable for processing Sentinel-1 SLC products:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/Workflows?$filter=contains(InputProductType,'SLC')'
```

Details of the Workflow in the response list the parameters which are needed to create a new Production Order:

```json
{
    "Id": "47",
    "Name": "card_coh12_public",
    "DisplayName": "Sentinel-1: Coherence (12 days)",
    "Documentation": null,
    "Description": "The Sentinel-1 CARD COH12 (Copernicus Analysis Ready
      Data Coherence) processor generates a Sentinel-1 Level 2 product describing 
      the coherence of a pair of images - 12 days apart. 
      Concurrently, a terrain-correction (using DEM) is performed. This processor 
      provided by the Joint Research Centre is based on a GPT graph that can be run 
      with ESA SNAP.",
    "InputProductType": "SLC",
    "InputProductTypes": [
        "SLC",
        "S1_SLC__1S",
        "S2_SLC__1S",
        "S3_SLC__1S",
        "S4_SLC__1S",
        "S5_SLC__1S",
        "S6_SLC__1S",
        "IW_SLC__1S",
        "EW_SLC__1S",
        "WV_SLC__1S"
    ],
    "OutputProductType": "CARD-COH12",
    "WorkflowVersion": "3.1.0"
}
```

### Production Orders

#### Create a new Production Order
To submit a new processing job, you need to use the POST method and send the parameters as a JSON message according to the requirements of a specific Workflows:

```bash
curl -X POST -H 'Content-Type:application/json' \
-H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder/OData.CSC.Order' \
-d '<json_message>'
```

Production orders accept the following general parameters: 

* _WorkflowName_ - the  identifier of the workflow – “Name” attribute in the /Workflows response
* _InputProductReference_ - information about the input data to be used by the processor
  * _Reference_ - identifier of a single input product or multiple products (example – mosaicking processors)
  * _ContentDate_ - time period which should be used by the Workflows (example - time-series based processors)
* _WorkflowOptions_ - parameters specific to the Workflows (example – DEM version, cloud coverage)
* _Priority_ - priority of the order in the users job queue. Orders with higher priority will be processed first
* _NotificationEndpoint_ - (not used) URL of the endpoint which should receive the information once the order is completed (done or failed)
* _Name_ - non-unique name for the order to help identify the orders 

The structure of the JSON message:

```json
{
  "WorkflowName": "string",
  "InputProductReference": {
    "Reference": "string",
    "ContentDate": {
      "Start": "YYYY-MM-DDTHH:mm:ss.SSSZ",
      "End": "YYYY-MM-DDTHH:mm:ss.SSSZ"
    }
  },
  "WorkflowOptions": [
    {
      "Name": "string",
      "Value": "string"
    }
  ],
  "Priority": 0,
  "NotificationEndpoint": "string",
  "Name": "string"
}
```

Example: to submit an order for the Sentinel-1 CARD Backscatter product:

```bash
curl -X POST -H 'Content-Type:application/json' \
-H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder/OData.CSC.Order' \
-d '{ \
  "WorkflowName": "card_bs",
  "InputProductReference": {
    "Reference": "S1A_IW_GRDH_1SDV_20230404T162838_20230404T162903_047949_05C333_B4FC.SAFE"
  },
  "Priority": 1,
  "Name": "card_bs_order_1"
}'
```

Sample response after successful submission of the order:

```json
{
  "@odata.context": "$metadata#OData.CSC.Order",
  "value": {
    "Id": "9999999",
    "Status": "queued",
    "StatusMessage": "queued",
    "SubmissionDate": "2023-04-05T10:03:25.474Z",
    "Name": "S1A_IW_GRDH_1SDV_20230404T162838_20230404T162903_047949_05C333_B4FC.SAFE",
    "EstimatedDate": "2023-04-05T10:33:16.161Z",
    "InputProductReference": {
      "Reference": S1A_IW_GRDH_1SDV_20230404T162838_20230404T162903_047949_05C333_B4FC.SAFE",
      "ContentDate": null
    },
    "WorkflowOptions": [],
    "WorkflowName": "card_bs",
    "WorkflowId": null,
    "Priority": 1
  }
}
```

#### Check list of Production Orders
To list all Production Orders requested by the user:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders'
```

When looking for completed orders for a specific processor:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders?$filter=WorkflowName%20eq%20coh%20and%20Status%20eq%20completed'
```

#### Check the status of a single Production Order
To check details of the single order using the order Id:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders(<order_id>)'
```

#### Cancel a Production Order
To cancel an existing order:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(<order_id>)/OData.CSC.Cancel'
```

The orders that are in the queue and have not yet been processed will be removed instantly. For the orders in processing, the Order Items (single item within a Production Order) being processed will complete but the remaining part of the Order will be cancelled.  

#### Display details of the result
The order generates a new product which can be downloaded from the public repository or private storage. To check the details of the result:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(<order_id>)/Product'
```

#### Download the result
Once the order is successfully processed, the status changes to complete, and the result is ready for download. The user may choose to instruct the service to put the results in a specified location (mandatory if custom parameters have been passed to the Workflow), and standard results (for Workflows like CARD-BS or CARD-COH12) are stored in the CDSE public repository and can be retrieved through the API.  
To download the result:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(<order_id>)/Product/$value' \
-o result.zip
```

### Batch Orders
#### Create a new Batch Order
In a way similar to a single Production Order, you can request processing of multiple input products as a Batch Order. The Batch Order will run a selected Workflow with the same parameters for all inputs and output the results to the same location. Using Batch Orders makes it easier to process time series or large AOIs.  
Batch Order endpoint uses the same mechanism as described for the Production Order:

```bash
curl -X POST -H 'Content-Type:application/json' \
-H "Authorization: Bearer $KEYCLOAK_TOKEN" \ 
'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder/OData.CSC.Order' \
-d '<json_message>'
```

Batch Orders accept the following general parameters: 

* _WorkflowName_ - the  identifier of the workflow – “Name” attribute in the /Workflows response
* _IdentifierList_ - information about the input data to be used by the processor - identifiers of products
* _WorkflowOptions_ - parameters specific to the Workflows (example – DEM version, cloud coverage)
* _Priority_ - priority of the order in the users job queue. Orders with higher priority will be processed first
* _Callback_ - (not used) URL of the endpoint which should receive the information once the order is completed (done or failed)
* _Name_ - non-unique name for the order to help identify the orders
* _BatchSize_ - maximum number of items in the batch
* _BatchVolume_ - maximum size of output data 

The structure of the JSON message:

```json
{
  "Name": "string",
  "Priority": 0,
  "WorkflowName": "string",
  "Callback": "string",
  "WorkflowOptions": [
    {
      "Name": "string",
      "Value": "string"
    }
  ],
  "IdentifierList": [
    "string"
  ],
  "BatchSize": 0,
  "BatchVolume": 0
}
```

#### Check list of Batch Orders
To list all Production Orders requested by the user:

```bash
curl -X POST -H 'Content-Type:application/json' \
-H "Authorization: Bearer $KEYCLOAK_TOKEN" \ 
'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder'
```

When looking for batch orders for a specific processor (in example 'coh'):

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder?$filter=WorkflowName%20eq%20%coh'
```

#### Check the status of a single Batch Order
To check details of the single order using the order Id:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder(<batch_order_id>)'
```

#### List products generated in a Batch Order
When the batch order is processed, for each input product an output is generated. To list the output of a batch:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder(<batch_order_id>)/Products'
```

#### Display details of the results
Each item in the batch is an individual Production Order. To check the details of the single result:

```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder(<batch_order_id>)/Product(<order_id>)'
```

## Usage examples
The examples below show how to use the OnDemand Production API to order production of Sentinel data packages. The requests to the API endpoint can be made using standard Linux CLI tools like curl and wget, or by other programmatic methods.  
More information and examples of using the OData APIs of the Copernicus Dataspace Ecosystem can be found above and in the [Catalogue API documentation](https://documentation.dataspace.copernicus.eu/APIs/OData.html){target="blank"}

### On-Demand Production
#### Ordering Sentinel-1 Level-1 IW_GRDH_1S using Production Order
The production of Sentinel-1 GRD products starts from Level-0 RAW product. As the Sentinel-1 products require single input product, the workflow can use Level-0 product as input reference.  
In the following example we will order production of GRD product covering Brussels on 12th of May, 2024.
First the required input Level-0 products needs to be identified using the CDSE catalogue:
```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=((ContentDate/Start%20ge%202024-05-12T00:00:00.000Z%20and%20ContentDate/Start%20le%202024-05-12T23:59:59.999Z)%20and%20(Online%20eq%20true)%20and%20(OData.CSC.Intersects(Footprint=geography%27SRID=4326;POLYGON%20((4.313869%2050.796245,%204.437046%2050.796245,%204.437046%2050.913716,%204.313869%2050.913716,%204.313869%2050.796245))%27))%20and%20(((((Collection/Name%20eq%20%27SENTINEL-1%27)%20and%20(((Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27productType%27%20and%20i0/Value%20eq%20%27RAW%27)))))))))&$expand=Attributes&$expand=Assets&$orderby=ContentDate/Start%20asc&$top=1'
```
The query breaks down to the following filters: 

* _(ContentDate/Start ge 2024-05-12T00:00:00.000Z and ContentDate/Start le 2024-05-12T23:59:59.999Z)_ - observation starting on 12/05/2024
* _(Online eq true)_ - the product is online (ready to use) in the CDSE archive
* _(OData.CSC.Intersects(Footprint=geography'SRID=4326;POLYGON ((4.313869 50.796245, 4.437046 50.796245, 4.437046 50.913716, 4.313869 50.913716, 4.313869 50.796245))'))_ - product footprint intersects with the Brussels metropolitan area
* _(Collection/Name eq 'SENTINEL-1')_ - we are looking for Sentinel-1 data
* _(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name eq 'productType' and i0/Value eq 'RAW'))_ - the "RAW" product type is a group covers all Level-0 product types
* _$top=1_ - results in a single product
  
The result is _S1A\_IW\_RAW\_\_0SDV\_20240512T055035\_20240512T055108\_053834\_068ADC\_5C24.SAFE_ which will be used as input to the production order.  
  
The next step is preparation of the body of the request:  
```json
{
	"Name":"S1 IW_GRDH_1S getasse30v1 1",
	"WorkflowName":"Sentinel-1-L0-IW_GRDH_1S",
	"InputProductReference": {
		"Reference": "S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE"
		},
	"Priority": 1,
	"WorkflowOptions":[
		{
		"Name":"Dem","Value":"getasse30v1"
		},
		{
		"Name":"output_storage","Value":"TEMPORARY"
		}
	]
}
```
and save it in a file called *odp_request.json* (example name).  

The request contents break down to the following: 

* _"Name":"S1 IW_GRDH_1S getasse30v1"_ - user provided string identifying the order
* _"WorkflowName":"Sentinel-1-L0-IW\_GRDH\_1S"_ - identifier of the workflow which will be used to process input data
* _"Name":"Dem","Value":"getasse30v1"_ - GETASSE30 elevation model selected
* _"Name":"output_storage","Value":"TEMPORARY"_ - the data will be stored in temporary storage (available for download for 14 days)
* _"IdentifierList":["S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE"]_ - Level-0 RAW product used as input to the processor

The body of the request needs to be sent in a call to ODP API:
```bash
curl -X POST -H 'Content-Type:application/json' -H "Authorization: Bearer $KEYCLOAK_TOKEN" -d @odp_request.json 'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder/OData.CSC.Order'
```
As a result new order should be created. The status of the order can be checked with the following request:
```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" 'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders?$filter=Name%20eq%20%27S1%20IW_GRDH_1S%20getasse30v1%27'
```
The API response includes the Status and the ID:
```json
{
  "@odata.context": "$metadata#ProductionOrder/$entity",
  "value": [
    {
      "Id": "2322968",
      "Status": "completed",
      "StatusMessage": "requested output product is available",
      "SubmissionDate": "2024-05-21T05:34:52.840Z",
      "Name": "S1 IW_GRDH_1S getasse30v1",
      "EstimatedDate": "2024-05-21T08:33:31.103Z",
      "InputProductReference": {
        "Reference": "S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE",
        "ContentDate": null
      },
...
```
Key information in the response: 

* _"Id": "2322968"_ - the identifier (ID) of the order
* _"Status": "completed"_ - status of the processing (queued, in progress, completed, failed)
* _"StatusMessage": "requested output product is available"_ - detailed information about the processing status. In case the production fails, the cause of error is provided

Once the order is in status "Completed" the result can be downloaded using the order ID:
```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(2322968)/Product/$value' -o result.zip
```
The output will be saved as *result.zip* in the local folder.
It contains the output product stored as compressed SAFE archive.
```bash
unzip -l result.zip
Archive:  result.zip
  Length      Date    Time    Name
---------  ---------- -----   ----
1771670525  2024-05-21 15:46   S1A_IW_GRDH_1SDV_20240512T055039_20240512T055104_053834_068ADC_A2BD.SAFE.zip
---------                     -------
1771670525                     1 file
```
#### Ordering Sentinel-3 Level-2 SL_2_LST using Production Order
The production of **Sentinel-2** and **Sentinel-3** products starts from a **set of Level-0** products chosen by the selection rules.  

:::{.callout-important}
Differently than in case of Sentinel-1, the **reference input is an existing higher level product**. The workflow based on the reference will identify required Level-0 inputs and produces an equivalent of the reference product using the chosen processor version and workflow options.
:::

In this example we will order production of  product covering Brussels on 12th of May, 2024.  
When choosing Sentinel-3 product it is important to select NTC (non-time critical) timeliness as the workflow does not generate NRT or STC products. Providing NRT or STC product as input reference will result in an error.  

First the required input Level-2 reference products needs to be identified using the CDSE catalogue:
```bash
curl -H "Authorization: Bearer $KEYCLOAK_TOKEN" \
'https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=((ContentDate/Start%20ge%202024-05-12T00:00:00.000Z%20and%20ContentDate/Start%20le%202024-05-12T23:59:59.999Z)%20and%20(Online%20eq%20true)%20and%20(OData.CSC.Intersects(Footprint=geography%27SRID=4326;POLYGON%20((4.313869%2050.796245,%204.437046%2050.796245,%204.437046%2050.913716,%204.313869%2050.913716,%204.313869%2050.796245))%27))%20and%20((Collection/Name%20eq%20%27SENTINEL-3%27)%20and%20(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27productType%27%20and%20i0/Value%20eq%20%27SL_2_LST___%27)))%20and%20(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27timeliness%27%20and%20i0/Value%20eq%20%27NT%27)))&$expand=Attributes&$expand=Assets&$orderby=ContentDate/Start%20asc&$top=1'
```
Filtering parameters specific to this example (standard parameters as in Sentinel-1 example): 

* _(Collection/Name%20eq%20%27SENTINEL-3%27)_ - we are looking for Sentinel-3 data
* _(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27productType%27%20and%20i0/Value%20eq%20%27SL\_2\_LST___%27))_ - product type SL_2_LST___
* _(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27timeliness%27%20and%20i0/Value%20eq%20%27NT%27))_ - product timeliness non-time critical (NTC)

The result is  *S3B\_SL\_2\_LST\_\_\_\_20240512T093034\_20240512T093334\_20240513T031156\_0180\_093\_036\_2160\_PS2\_O\_NT\_004.SEN3* which will be used as input to the production order.  

The next step is preparation of the body of the request:  
```json
{
	"Name":"S3 SL_2_LST 1",
	"WorkflowName":"Sentinel-3-L0-SL_2_LST",
	"InputProductReference": {
		"Reference": "S3B_SL_2_LST____20240512T093034_20240512T093334_20240513T031156_0180_093_036_2160_PS2_O_NT_004.SEN3"
		},
	"Priority": 1,
	"WorkflowOptions":[
		{
		"Name":"Baseline","Value":"3.27"
		},
		{
		"Name":"output_storage","Value":"TEMPORARY"
		}
	]
}
```
and save it in a file called *odp_request.json* (example name).

The additional workflow option used in this example: 

* _"Name":"Baseline","Value":"3.27"_ - selects specific baseline (version of the processor)

Further steps to submit, check the status of the order and download results are the same as for the Sentinel-1 example.  
