[
  {
    "objectID": "Applications/AlgorithmPlaza.html",
    "href": "Applications/AlgorithmPlaza.html",
    "title": "openEO Algorithm Plaza",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem offers an openEO algorithm plaza to share openEO workflows for Earth Observation (EO) data processing. The openEO Algorithm Plaza is a marketplace where various EO algorithms are shared with openEO process graphs.\nTherefore, the plaza enhances algorithm reusability, which is a cornerstone of the FAIR principles. Assuming familiarity with EO data and openEO concepts, this documentation section is a beginners guide for sharing algorithms.\nThe hosted algorithms can be used within the platform through user interfaces or APIs. Users can seamlessly onboard their algorithms on the openEO Algorithm Plaza for further exposure to a large audience."
  },
  {
    "objectID": "Applications/AlgorithmPlaza.html#overview",
    "href": "Applications/AlgorithmPlaza.html#overview",
    "title": "openEO Algorithm Plaza",
    "section": "Overview",
    "text": "Overview\nThe marketplace showcases a diverse catalogue of EO services from various organisations. These services are classified based on their maturity levels, which indicates the quality and reliability of the service.\n\nEach service has its dedicated page providing detailed information, including the methodology, expected results, and service execution instructions."
  },
  {
    "objectID": "Applications/AlgorithmPlaza.html#what-is-a-service",
    "href": "Applications/AlgorithmPlaza.html#what-is-a-service",
    "title": "openEO Algorithm Plaza",
    "section": "What is a service?",
    "text": "What is a service?\nThe openEO Algorithm Plaza offers a wide range of EO workflows as services. These services can range from simple computations such as the Normalized Difference Vegetation Index (NDVI) to more complex algorithms.\nIn addition to providing access to the available services, the marketplace also supports users in showcasing their algorithms. To register an algorithm on the marketplace, it must be developed as an openEO UDP. For more information on openEO user-defined processes, visit the openEO UDP.\nOnce an algorithm is exposed as a service, other users can readily access and use it for their workflows."
  },
  {
    "objectID": "Applications/AlgorithmPlaza.html#service-maturity-levels",
    "href": "Applications/AlgorithmPlaza.html#service-maturity-levels",
    "title": "openEO Algorithm Plaza",
    "section": "Service maturity levels",
    "text": "Service maturity levels\nThe platform assigns maturity levels to maintain quality control across the services available in the openEO algorithm plaza. These levels indicate the expected performance and reliability of the services for end users:\n\nValidation of the result ensures the accuracy and reliability of the outcomes.\nStability shows how well the service performs under different conditions.\nScalability informs users on how increasing amounts of data or users affect the algorithm’s performance.\nDocumentation provides detailed information about the service, including its purpose, inputs, outputs, and how to use it.\n\nThese criteria classify services into five maturity levels: Prototype, Incubating, Verified, Validated, and Operational.\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\nPrototype\nService is provided ‘as-is’, with a short description and possibly a reference to what is implemented.\n\n\nIncubating\nThe service is documented with example requests (sets of parameters), the corresponding output, and the resources required to generate that output.\n\n\nVerified\nThe service is labelled verified based on its software readiness and verification that validation reports are not required.\n\n\nValidated\nThe service is labelled validated when the validation reports and software readiness are verified by the openEO Algorithm Plaza team.\n\n\nOperational\nThe service is fit for larger-scale production and integration in operational systems. Rules for estimating resource usage are available, or a unit cost is provided. (€ per hectare, € per request, etc.)\n\n\n\n\nDetailed descriptions of the criteria for each maturity level are explained here.\n\nFor more information on managing services, please refer to the Manage your services guide.\nOnce familiar with the concepts used in the openEO Algorithm Plaza, individuals can start to explore the platform."
  },
  {
    "objectID": "Applications/AlgorithmPlaza.html#interested-in-using-or-publishing-services",
    "href": "Applications/AlgorithmPlaza.html#interested-in-using-or-publishing-services",
    "title": "openEO Algorithm Plaza",
    "section": "Interested in using or publishing services?",
    "text": "Interested in using or publishing services?\nUsers can either use an existing service or publish their algorithms. Below is a non-extensive list of considerations to take when sharing a service.\n\nManaging your account\nExecuting the available services and features within the openEO Algorithm Plaza, requires logging in.\nNew users can register for one.\n\nStep 1: Manage your Profile\nFor further details on updating profile settings, users can click on the avatar in the top right corner of the openEO Algorithm Plaza portal. This action redirects users to a page with options such as Overview, Team, and Organization in a sub-navigation menu.\n\nThe Overview section presents profile details such as name, email, and affiliated organization. These details can be updated by clicking the MANAGE button. Meanwhile, in the Team sub-menu, a list of all organization members is displayed. This list can also be accessed by clicking the VIEW TEAM button at the bottom left of the overview sub-menu. Further information on team management can be found here. Lastly, within Organisation sub-menu, organization information such as name, email address, website, VAT number, and more can be viewed and modified.\n\n\nStep 2: Manage your Organisation\nEach registered user acts as an individual organisation; profile details can be modified, as discussed earlier in Step 1. However, step 2 could be helpful when working with multiple individuals or being part of multiple organisations. Organisations allow collaboration with other users, facilitating the sharing of billing and service accounts. For more information on managing the organisation, please refer to the organisation’s documentation of the openEO Algorithm Plaza.\n\n\nStep 3: Check your Credits\nUsing any openEO processes, including those offered as services in this marketplace, consumes a certain amount of credits. Notably, these credits are shared among the organisation’s members. Credits are consumed whenever a service or a supported openEO process is used.\nThis marketplace simplifies credit management, allowing users to monitor their accounts. The credits can be monitored under the Billing section. Moreover, every user is provided with 10 000(¹) free openEO credits each month to execute multiple services.\n(¹)Temporary Boost of monthly openEO Credits to 10 000. Read our news item published on September 3, 2024.\n\nFor more information on credit usage, please refer to the credit usage page.\n\n\n\nExecuting a services\nOnce sufficient credits are available, users can start using the services. Clicking on any of these services redirects users to the service details page. Here, information about the service, including a general description and execution instructions, can be found. For more information on executing a service, please refer to the Execute a service page.\n\n\n\nPublishing a services\nEvery user has the choice to onboard their services as an individual or as part of a group organisation.\n\nPublish a algorithm\nTo publish a service on the marketplace, the algorithm must be developed using openEO. This ensures that users can fully use the plaza’s features, including reporting and the ability to execute services directly through the web editor.\nFor more information on how to publish a service, please refer to the Publish a service page.\n\n\nManage a service\nManaging services in this marketplace is a simple process. Users can edit or delete services, and hide or display them in the plaza catalogue. Please refer to the manage your service page for detailed instructions on managing a service."
  },
  {
    "objectID": "Applications/AlgorithmPlaza.html#support",
    "href": "Applications/AlgorithmPlaza.html#support",
    "title": "openEO Algorithm Plaza",
    "section": "Support",
    "text": "Support\nPlease contact the support team for further assistance creating a ticket."
  },
  {
    "objectID": "Applications/Catalogue-csv.html",
    "href": "Applications/Catalogue-csv.html",
    "title": "Catalogue CSV",
    "section": "",
    "text": "This webpage enables users to access and download Sentinel-1, Sentinel-2, Sentinel-3 and Sentinel-5P product lists in csv format.\n\nCSV Copernicus Catalogue\nDiscover the webpage via CSV Copernicus Catalogue."
  },
  {
    "objectID": "Applications/PlazaDetails/PublishService.html",
    "href": "Applications/PlazaDetails/PublishService.html",
    "title": "Publish a Service",
    "section": "",
    "text": "The openEO Algorithm Plaza provides a self-service onboarding wizard that enables developers to publish their services effortlessly. It assigns a service maturity label to each onboarded service.\nDevelopers can publish their algorithms as services on openEO Algorithm Plaza. These algorithms must be implemented as openEO User Defined Processes, focusing on Earth Observation solutions and utilizing openEO standards. When set to public, the service is made available to all external users.\nDevelopers are directed to the described page when “Services” is selected from the sub-menu options under the “Dashboard”. If no services have been published yet, an option to “REGISTER YOUR FIRST SERVICE” is displayed. However, when a service has already been published within the organization, a list of the relevant registered services is shown.\nDevelopers can click on the avatar or the service name to load its detail page. Additionally, clicking the REGISTER NEW SERVICE CTA button allows users to add/create a new service."
  },
  {
    "objectID": "Applications/PlazaDetails/PublishService.html#develop-an-openeo-algorithm",
    "href": "Applications/PlazaDetails/PublishService.html#develop-an-openeo-algorithm",
    "title": "Publish a Service",
    "section": "Develop an openEO algorithm",
    "text": "Develop an openEO algorithm\nIn openEO, a ‘datacube’ concept is used, which hides a lot of the complexity when working with huge EO data archives. It provides full archive access to the most popular datasets. To integrate existing code, the concept of ‘User Defined Functions’ (UDFs) can be used. Furthermore, Parallelization and scalability are taken care of.\n\nWorking with openEO\nTo get familiar with openEO, it is recommended to begin with the basic introduction of openEO. Developers can then share their algorithms as services on the openEO Algorithm Plaza. For deploying an openEO algorithm as a service, developers can utilize the ‘user defined process’ functionality."
  },
  {
    "objectID": "Applications/PlazaDetails/PublishService.html#register-and-publish-your-service",
    "href": "Applications/PlazaDetails/PublishService.html#register-and-publish-your-service",
    "title": "Publish a Service",
    "section": "Register and publish your service",
    "text": "Register and publish your service\nOnce an algorithm is ready, a developer can register and publish it.\nClicking the Register your first service button presents a wizard to enter the necessary information regarding the service. The following sections provide a guide through the publishing process:\n\nStep 1: Register your service\nThe first step involves selecting the type of service to publish. Currently, only those services integrated with openEO as an orchestrator are supported.\n\n\n\nStep 2: Input general information\nThe following table outlines the basic information required for the service at this step:\n\n\n\n\n\n\n\n\nField\nRequired\nDescription\n\n\n\n\nService name\nYes\nTitle of the service to be displayed in the catalogue.\n\n\nSummary\nNo\nShort description of the service.\n\n\nDescription\nNo\nThe service description starts\n\n\nwith an overview and a brief methodology. It is helpful to list required parameters and include examples and output images for user guidance.\n\n\n\n\nAvatar\nNo\nURL to an image that can be used as an avatar of your service.\n\n\n\n\n\nStep 3: Additional Sections\nIn addition to the basic information mentioned above, a developer can provide the necessary details about their service. The following sections are available:\n\nParameters: A list of all the parameters that should be provided to the service to execute it. Their name, type, description, and default value are specified.\nUsage example(Python code): A Python code example to demonstrate how to use the service. The example should include the input parameters and the expected output.\nResults: Service results and it is supported output format.\nCost Estimation: Estimating the resource consumption and time required to run the service for a given input.\nReferences: A list of references to publications, websites, or other resources relevant to the service.\n\n\n\nStep 4: Add labels\nMultiple labels can be added for a service to help users find the service within the platform. These labels serve as filters within the marketplace and indicate its category.\n\n\nStep 5: Select service visibility\nThe service can be designated as either public or private. If set to public, it will be visible to all users in the openEO Algorithm Plaza. However, only the developer and organization members have access to the service if set to private.\nClick on REGISTER SERVICE to finish the basic registration. Finishing the basic registration enables the developer to provide more details on the service by either clicking the NEXT button or REMOVE to delete the service or even BACK TO SERVICE to exit the editing of the service.\n\n\nStep 6: Add Media Files and Links\nImages uploaded in the Media Files and Links section are displayed as the avatar on the catalogue. Additionally, multiple URLs can be specified and shown in the detailed information of the service.\n\n\nStep 7: Add openEO Settings\nIn the openEO Settings the openEO namespace and service ID should be filled in Entering this information enables the Access Service button, allowing visitors of openEO Algorithm Plaza to execute the service through the openEO Web Editor. The required information is represented in the following table:\n\n\n\nField\nRequired\nDescription\n\n\n\n\nNamespace\nYes\nNamespace of openEO service. When the service was created through a User Defined Process (UDP), the namespace is formatted as u:&lt;publisher username/id&gt;. This information can be extracted from the public URL when creating and sharing the UDP through openEO.\n\n\nService name\nYes\nName of the service as shared within openEO. For a User Defined Process (UDP), the service name corresponds with the ID of the service.\n\n\n\nCongratulations! The service has been successfully published in the openEO Algorithm Plaza. It can now be viewed and shared with others. Furthermore, it is recommended that developers go through the Manage a Service documentation to learn how to manage the service."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageOrg.html",
    "href": "Applications/PlazaDetails/ManageOrg.html",
    "title": "Manage your organization",
    "section": "",
    "text": "Assuming that the user has registered in the Copernicus Data Space Ecosystem and has access to the openEO Algorithm Plaza, a personal organization will be created under the profile.\nThe Organizations are core elements of the openEO Algorithm Plaza, as they are the entities that relate users, services, openEO credits, and more. While organisations can encompass multiple users, an individual can be viewed as an organisation. This organisational concept allows users to manage shared services and distribute credits accordingly. Furthermore, the organisations can be tailored to suit specific requirements, whether for project collaborators, a particular team, or at the organisational level."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageOrg.html#provide-your-organisation-details",
    "href": "Applications/PlazaDetails/ManageOrg.html#provide-your-organisation-details",
    "title": "Manage your organization",
    "section": "Provide your organisation details",
    "text": "Provide your organisation details\nThe organisation name is displayed on the profile page within the openEO Algorithm Plaza under the linked organisation section. Additionally, the “Organization” option in the sub-navigation provides access to the Organization page. Here, users can view and edit the details of an organisation details, which includes:\n\nOrganisation name (mandatory)\nOrganisation Identity registration (optional)\nOrganisation Avatar/logo URL (optional)\nOrganisation description (optional)\nOrganisation website (optional)\nTerms of use URL (optional) and other “useful links”, e.g., Terms of Service, Privacy, YouTube, and Support URLs\nUpdate button, disabled by default"
  },
  {
    "objectID": "Applications/PlazaDetails/ManageOrg.html#invite-team-members",
    "href": "Applications/PlazaDetails/ManageOrg.html#invite-team-members",
    "title": "Manage your organization",
    "section": "Invite Team members",
    "text": "Invite Team members\nA key feature of this platform is its ability to invite contributors, friends, or co-workers to join a shared organization. New members can be invited by clicking on the INVITE MEMBER button in the Team sub-menu of the profile. This action will open a form requesting additional information to add a new user to the organization. This block contains the following fields:\n\nEmail address (Mandatory)\nRole dropdown (Organisation owner or Developer).\nSEND button\n\nThe form should also disappear after successful submission.\n\n\n\nInvite member\n\n\nWhen clicking on the SEND button, a message confirming that the invitation has been sent will pop up at the top of the page. The invitee will receive an email with a link to accept the invitation.\n\n\n\nOnce the invitation is accepted, the user is part of the organisation. Thus, they can access the organisation resources."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageOrg.html#accept-the-invitation",
    "href": "Applications/PlazaDetails/ManageOrg.html#accept-the-invitation",
    "title": "Manage your organization",
    "section": "Accept the invitation",
    "text": "Accept the invitation\nAn invitation from the organisation owner or admin is required to join an existing organisation. As mentioned earlier, an invitation can be sent to any user within the platform. Upon receiving an email with the invitation link, it is recommended that the user sign in first and then accept the invitation by confirming the link. Clicking on the confirmation link leads to the following screen:\n\n\n\nUpon clicking the ACCEPT INVITATION button, a message confirming successful acceptance of the invitation appears at the top of the screen."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageOrg.html#switch-between-organisation",
    "href": "Applications/PlazaDetails/ManageOrg.html#switch-between-organisation",
    "title": "Manage your organization",
    "section": "Switch between organisation",
    "text": "Switch between organisation\nOnce the invitation to join the organisation has been accepted, users can find the organisation listed under the Linked Organisation dropdown menu on the profile page. Select the new organisation from the dropdown menu and click the SWITCH button to switch to the new organisation.\n\n\n\nOn successful switching, the user can find a list of all team members and their roles. However, please note that users with a Developer permissions cannot invite new members to the organisation. Only the Organisation Owner has permission to invite new members."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageService.html",
    "href": "Applications/PlazaDetails/ManageService.html",
    "title": "Manage your services",
    "section": "",
    "text": "In this section, we aim to address the key aspects of managing services on the openEO Algorithm Plaza."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageService.html#upgrade-your-service",
    "href": "Applications/PlazaDetails/ManageService.html#upgrade-your-service",
    "title": "Manage your services",
    "section": "Upgrade your Service",
    "text": "Upgrade your Service\nEach service is given a maturity level based on the quality of the service. To upgrade the level of service, the developer should improve their services and documentation so that they meet the criteria for the desired levels. Then a request can be made at our help center for upgrading the service."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageService.html#changing-a-service-visibility-to-private",
    "href": "Applications/PlazaDetails/ManageService.html#changing-a-service-visibility-to-private",
    "title": "Manage your services",
    "section": "Changing a service visibility to private",
    "text": "Changing a service visibility to private\nChanging the visibility of a service to private, ensures that the service is not visible in the openEO Algorithm Plaza. This can be useful for fixing bugs, developing, and testing before publishing it to the marketplace.\nTo change the visibility of a service, scroll down in the details section and set the visibility to private."
  },
  {
    "objectID": "Applications/PlazaDetails/ManageService.html#remove-a-service",
    "href": "Applications/PlazaDetails/ManageService.html#remove-a-service",
    "title": "Manage your services",
    "section": "Remove a service",
    "text": "Remove a service\nTo remove a service from the openEO Algorithm Plaza, users can follow the steps below:\n\nNavigate to the service details page by clicking on the service card from the Services page.\nScroll down to the bottom of the service details page.\nClick on the REMOVE button.\nConfirm the deletion by clicking YES in the popup window.\n\nIt is important to note that removing the service from the marketplace does not remove it from the orchestrators. Users are still able to execute the service through the orchestrators. To remove the service from the orchestrators, please follow the instructions below.\n\nRemoving a service in the orchestrators\nRemoving a service from the orchestrators deactivates its use. openEO provides two ways to remove a service (also known as a user-defined process in openEO):\n\nUsing the OpenEO APIThe process_graphs endpoint allows users to remove a service based on its ID. More information is available in the official API documentation.\nUsing the openEO Python ClientThe openEO Python Client provides a delete function that can be used for any User Defined Process the authenticated user manages. More information is available in the official Python Client documentation."
  },
  {
    "objectID": "Applications/Browser.html",
    "href": "Applications/Browser.html",
    "title": "About the Browser",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem Browser serves as a central hub for accessing, exploring and utilizing the wealth of Earth observation and environmental data provided by the Copernicus Sentinel constellations, contributing missions, Auxiliary engineering data, on-demand data and more (Check out the documentation on Data for more details) . Based on Sentinel Hub’s EO Browser, users can visualise, compare and analyse and download all this data for a variety of applications, from environmental monitoring and disaster management to urban planning and agriculture. You can access the Browser at:\nhttps://dataspace.copernicus.eu/browser/\nCurrently you need a free account to use the Browser. New user? Register an account by following the steps here. Remember to save your login credentials for the next time you want to log in to the Browser. The Copernicus Browser is also available in multiple languages.\nFig 1: Browser start screen\nThe Browser window is divided into three parts:"
  },
  {
    "objectID": "Applications/Browser.html#visualization",
    "href": "Applications/Browser.html#visualization",
    "title": "About the Browser",
    "section": "Visualization",
    "text": "Visualization\nYou can find the VISUALIZE tab in the upper left corner of the sidebar (selected by default). The VISUALIZE tab will allow you to easily visualize satellite imagery on the map. Change or modify your visualization with just a few clicks.\n\nVisualizing data\nIn order to visualize data on the map, you need to zoom in to your area of interest. You can do this either with the mouse wheel or with the location search in the upper right corner.\nLet’s try to visualize the latest Sentinel-2 L2A imagery over Italy.\n\nEither zoom to Italy with the mouse wheel or type Italy in the search box in the upper right corner.\nIn the sidebar, a maximum cloud coverage of 30% and the product type Sentinel-2 L2A are already preselected. To visualize the latest available data with cloud coverage below 30% click on the Show latest date button.\n\n\nFig 2: VISUALIZE tab with show latest date button and Sentinel-2 L2A collection highlighted\nYou can now see the latest data over Italy on the map. Depending on the latest data available you will see data from one or more orbits (stripes of images on the map).\n\nModifying and Changing a Visualization\nIf you want to improve how the data is displayed on the map, you can modify the visualization by clicking on Show effects and advanced options at the bottom of the sidebar. Change the Gain/Gamma values, the values of the R/G/B colour channels, specify which sampling method is used for the visualization (Layer default, Bilinear, Bicubic, Nearest) or click on Reset to reset all changes made. To return to the visualization layers overview, click on Show visualizations.\nTo visualize different Sentinel-2 band combinations, either use one of the prepared options from the list of layers (e.g., NDVI for the Normalized Difference Vegetation Index using the Sentinel bands B4 and B8) or click Custom at the bottom of the layers list.\n\nFig 3: Custom Layers option with Composite Index and Custom script highlighted\nHere you can create a custom R/G/B composite or Index (band ratio, normalized difference index) by dragging and dropping the Sentinel-2 bands into the appropriate circles or use the Custom script functionality to insert a piece of JavaScript code.\n\n\nChanging Configurations\nYou can change the configuration to create your own layers and visualizations. A configuration instance acts as a separate WMS/WMTS/WFS/WCS service and each can be configured to provide a certain set of layers with different settings. It is therefore possible to create multiple configuration instances, each providing a different set of layers for different needs. The configuration instances can contain any number of layers that can be configured with the settings defined above, e.g., cloud coverage, time range, etc. Each visualizations is based on a predefined visualization option or a custom script. The configuration instance itself also has some global settings for default values on all layers, suach as image quality.\nYou can create your own configuration by accessing the Dashboard via the drop-down menu under your username or directly here. In the panel on the left you will find “Configuration Utility” where you can create, modify and delete your instances. Once you have created a configuration, you can open it in the Copernicus Browser. Alternatively, you can change the configurations directly in the Copernicus Browser by selecting your configuration from the drop-down menu as shown in the figure.\n\nFig 4: Changing the Configuration in Copernicus Browser\n\n\nChanging the Data Collection\nYou can switch visualizing between different data collections by clicking on the arrow next to the Data Collections section in the Visualization tab. Once you click on the arrow as seen in Fig. 4, you will be able to see a drop-down menu with a list of the satellite data that is available. Let us try to visualize Sentinel-2 Quarterly Mosaics data of the same location and date as that of the Sentinel-2 data in Visualizing data section.\n\nClick on the drop-down arrow on the right next to Pins icon.\nClick on the drop-down arrow next to Sentinel-2 and select Sentinel-2 Mosaics. You will get two options to choose from because there are Quarterly and Annual Mosaics available. Choose Sentinel-2 Quarterly Mosaics.\nYou will get two options to choose from because there are Quarterly and Annual Mosaics available. Choose Sentinel-2 Quarterly Mosaics to visualise the Sentinel-2 cloudless mosaics created for each quarter of 2023.\n\nYou can now see the latest Sentinel-2 Quarterly Mosaic.\nAt the moment, the Data Collections available for visualization are Sentinel-1 (SW, IW and EW mode), Sentinel-2 (L1C and L2A), Sentinel-2 Mosaics (Quarterly Mosaics for 2023 and WorldCover Annual Cloudless Mosaics for 2020 and 2021), Sentinel-3 (OLCI Level-1 EFT, SLSTR Level-1 RBT), Sentinel-5P and two Digital Elevation Models (Copernicus 30, Copernicus 90).\n\nFig 5: Changing Data Collection from Visualization tab directly\n\n\nComparing Visualizations\nTo compare two (or more) visualizations you must add them to the compare panel. You can add a visualization to the compare panel by clicking on the Add to compare button in each visualization layer (see Fig. 4). When you have added all the layers you want to compare to the compare panel, you can switch to it by clicking on the compare icon (  ). In the compare panel you can choose between a Split and an Opacity mode. With the Split mode you can compare two images side by side. With the Opacity mode you can compare two (or more) visualizations on top of each other.\n\nFig 6: Add to compare and compare icon\n\n\nSaving Pins\nTo save a visualization for future viewing, you can save it as a pin by clicking on ( ) next to the Layer name and clicking on Add to Pins. You can find the saved pins by clicking on the ( ) icon. If you wish to compare saved pins, you can add them to the compare panel as explained in the previous section. If you have multiple pins saved and want to compare them altogether, you can directly go to the compare panel and add all the pins to compare by clicking on ( ). Another feature of the Browser is that you can export pins as a JSON file and import previously exported pins as well.\n\nFig 7: Add to pins and Pins icon\n\n\n\nProduct Search for Current Visualization\nWhen you are visualizing data (chapter Visualizing data), you can easily find the products associated with the data you see on the map. The product allows you to inspect the full metadata and easily download the raw data. To find connected products, just click the Find products for current view button in the sidebar (under the Show latest date button).\n\nFig 8: Find products for current view button position in the sidebar.\n\n\n3D Visualization\nWith the 3D visualization tool, users can also visualize the terrain. To obtain a 3D visualization, you need to first select a layer to view and then click on the  icon. You can move forward, backward, left, or right by right clicking on the pan console (labelled 1 in the red box in Fig. 8) and rotate around a point by right clicking on the camera console (labelled 2 in the red box in Fig. 8). The viewing angle can be adjusted by scaling vertically and panning in all directions. You can further explore the area by adjusting the sun projected shadows and the shading parameters of the scene in the settings (labelled as box 3 and 4 respectively in Fig. 8). This 3D view can also be downloaded as a PNG or JPEG file. Let us try visualizing Mont Blanc, the highest peak in the Alps.\n\nFollow the steps mentioned in Visualizing data chapter to visualize Mont Blanc and select the “True Color” visualization.\nClick on the  icon placed at the right of the screen.\nYou can navigate around the visualization either with your mouse, keyboard or directly on the map by following the instructions mentioned in the “Help” section (click on the ( ) icon).\nClick on the Settings icon ( ). Set the Vertical terrain scaling to 150% by moving the slider.\nTo adjust shadows, click on the Parameters next to Sun projected shadows toggle switch.\nTo adjust Shading parameters, click on Edit and modify the Ambient factor, Diffuse factor, Specular factor, and Specular power.\nYou can Reset values at any point to return to the default settings.\n\n\n\n\n3D visualization\n\n\nFig 9: 3D visualization in the Browser with pop-up Settings windows on the right"
  },
  {
    "objectID": "Applications/Browser.html#product-search",
    "href": "Applications/Browser.html#product-search",
    "title": "About the Browser",
    "section": "Product Search",
    "text": "Product Search\nWith the product search you can find products from Sentinel missions (Sentinel-1, Sentinel-2, Sentinel-3, Sentinel-5p) and the sensors on board these satellites (C-SAR, MSI, OLCI, SRAL, SLSTR, SYNERGY). You can also find engineering and auxiliary data for all Sentinels (including Sentinel-6). You can explore the metadata for each of those products, download the raw data or visualize the data on the map (at present, Sentinel-1 GRD, Sentinel-2 L1C and L2A, Sentinel-3 OLCI L1B and SLSTR L1B, Sentinel-5P L2 and DEM data can be visualized).\nThe SEARCH tab is located in the sidebar next to the VISUALIZE tab (see Fig. 9).\n\nFig 10: SEARCH tab with different Data Sources, Time range and Search button\n\nHow to find a Product\nTo find products you can either use the keyword search (text input) or select one or more data sources using the checkboxes. To find products for a specific time range only, set the from/to date in the date input boxes. For example, let us find the latest Sentinel-2 L2A image over Italy for the beginning of 2023.\n\nZoom in on Italy on the map with the scroll wheel of your mouse.\nSelect Sentinel-2 &gt; MSI (selected by default) &gt; L2A.\nSet the Time Range to reflect two weeks (e.g., 2023-01-02, 2023-01-16)\nPress the Search button\n\n Fig 11: SEARCH tab with L2A collection selected and map centred on Rome (Italy)\nYou will now see the first 50 search results for your search settings (Sentinel L2A data over Italy for a time range of 2 weeks) in the sidebar and on the map. To load the next 50 results, click on the Load more button at the end of the list in the sidebar. You can view the metadata of a product in the sidebar or by selecting a product on the map. In both cases you can:\n\nDirectly view the basic metadata (preview image (available for most Sentinel-2 L1C, L2A, Sentinel-3 SLSTR and Sentinel-3 OLCI products), name, mission, instrument, acquisition time)\nView the full metadata by clicking on the product info button ( ) in the results (full metadata)\n\n\nAdditional Filters\nTo get more suitable results, you can also select or choose additional filters as shown in Figure 11. 1. Select the Data Source and the appropriate instrument/ processing level. 2. Click on the Filter button and set the filtering parameters. 3. Press the Search button. Here, you can choose various parameters depending on the chosen Data Source. For example, you can see the filter parameters for Sentinel-1 in figure below, letting you filter the results based on satellite platform, orbit direction, relative orbit number, acquisition mode, Beam ID and polarization.\n\nFig 12: Data filters and parameters\n\n\nVisualize the search result\nOnce you have found a product, you can visualize the results in two ways: either by directly selecting the viszualize button (  ) in the sidebar or by selecting the visualize button in the results panel on the map. You can open the results panel by clicking on one of the displayed tile footprints on the map.\n\n\n\nSearch tab\n\n\nFig 13: Product metadata and visualize button\n\n\n\nHow to download a Product\nWhen you have found a product (see How to find a Product) that you would like to download, you can do so by clicking click on the download icon (  ) for the desired product in the results (in the sidebar or in the results panel on the map after selecting a product). After you click the button, a progress bar will appear below the product to indicate the status of your download. If you have started a download by mistake, you can cancel it by clicking on the “x” below the download button.\nYou can continue to use the app as normal while a product is being downloaded.\n\nFig 14: Product download (in progress) with Download product and cancel button highlighted\n\nDownload single files\nA “product” refers to a directory containing a collection of information such as metadata, product information, the satellite image and quality data, auxiliary data and more. If downloading all of this data is not of your interest, you can opt to download single files within this directory but clicking on the single file download icon (). Depending on the product you are trying to download, you will get a list of files within the directory and the  icon next to them to download each file individually.\n\n\n\nAdd Product to Workspace\nTo avoid downloading all the products, you can add the product to your workspace and give as input to your workflow on the Copernicus Dataspace Ecosystem directly. The main objective of the workspace is to:\n\nFurther process single products (with various options to select the algorithm/processing required for your application).\nKeep track of the processed products (status, finished/queued products)\n\nTo do this, click on the Add to Workspace icon () appearing in your search result. The workspace can be accessed by clicking on the dropdown menu under your username or directly here.\n\nFig 15: User dropdown menu to access Workspace."
  },
  {
    "objectID": "Applications/Browser.html#tools",
    "href": "Applications/Browser.html#tools",
    "title": "About the Browser",
    "section": "Tools",
    "text": "Tools\nThe Browser has several tools to help you better understand the data on the map and prepare it for sharing with others. These tools can be found in the upper right corner of the Browser. They can help you select the Area of Interest, measure, download the image, create a timelapse if you want to observe the area over a longer period of time, or analyse the statistics of an index (e.g., the NDVI).\n\nArea/Point of Interest\nUse the Area of Interest (AOI) tool to draw a rectangular or polygonal area of interest by clicking on the  icon in the upper right corner of the browser. You can also upload a KML/KMZ, GPX, WKT (in EPSG:4326) or GEOJSON/JSON file to create an AOI.\nUse the  icon to mark a location and re-centre to the Point of Interest(POI)\nOnce you have selected the AOI, depending on the type of data you are looking at, you can view the spectral signature of the region using the Spectral Explorer() or, in case of indices, look at the change in value over time using the Statistical Info () feature.\n\n\nMeasure\nYou can use the Measure tool by clicking on the  icon to get the distance and area measurements. To measure the distance between two points, simply click on the start and end points on the map, to measure the area, draw a polygon (areas can also be measured using the AOI drawing, as described in Area/Point of Interest).\n\n\nImage Download\nThere are three different download options. You can switch between the options using the tabs at the top of the pop-up window. Each option contains a preview of the data at the bottom. When you are satisfied with your download settings, you will find the  button below the preview:\n\nBasic\n\nYou can use the Show Captions toggle switch to add data source, date, zoom scale and branding information to the exported images.\nYou can also use the Add Map Overlays toggle switch to add place labels, streets and political boundaries to the image or the Show Legend toggle switch to add the legend data.\nYou can use the Crop to AOI toggle switch to crop the image to the bounds of area of interest, if drawn previously.\nIf you want to download the entire image but highlight the AOI, it can be done by enabling the Draw AOI Geometry.\nUse the textbox to add a short description to the exported image.\nChoose between two image formats (JPG, PNG).\nA preview of the image that will be downloaded is displayed under Preview. Previews are available only when you zoom in enough.\n\nAnalytical\n\nAfter preparing the data for download, click the  button to download the image in JPG, PNG, KMZ or GeoTIFF format.\nChoose between different image formats, resolutions and coordinate systems before downloading the image. You can also attach a logo.\nIn the Analytical panel, you can select multiple layers (Visualized/Raw) and download them all in a single ZIP file.\n\nHigh-res print\n\nPrepare the selected visual for high-resolution printing by manually selecting a format, size and DPI. Add captions, legends and descriptions as needed.\n\n\n\n\nTimelapse\nTimelapses are a very popular and useful tool to show how a certain location on Earth changed through time. Using the timelapse tool you can create your own visualization of changes through time and export it as .GIF or .MPEG4 to share it with others online. Let’s create a timelapse of the deforestation in the Brazil from 2018 – 2022.\n\nGo to: https://sentinelshare.page.link/osH4\nClick on the timelapse icon (  ) and click on the play button in the middle of the screen. This opens a pop-up window to create a timelapse.\nChange the settings on the left side to:\n\nDates 2018-01-01 – 2022-12-31\nSelect 1 image per: month Alternatively, you can select only certain months in a year using the filter by months option. Click on Search to see all the results.\n\nIn the Visualizations set the Min. tile coverage to 100% and the Max. cloud coverage to 2% and manually deselect the images from the 2022-05-30 (slightly cloudy) and the 2022-09-07 (blurry).\nOnce you have the list of images you want to display in the timelapse, select the speed, and transition to prepare your timelapse.\nClick on the play button to check the result and download the animation as a GIF-file using the Download button for further use online/offline.\n\n\n\n\nTimelapse\n\n\nFig 16: Browser timelapse tool with settings highlighted\n\n\nStatistical Analysis\nThere are quite a lot of statistical analyses that can be done in the Browser itself. Depending on the type of data you are looking at, you can look at the distribution of the pixel values, view the spectral signature of the region using the Spectral Explorer() or, in case of indices, look at its change over time using the Statistical Info () feature. #### Histogram\nWith the Histogram tool you can display statistical data (the distribution of values) for specific layers by clicking on the  icon. The histogram is calculated for the data within your AOI, if defined or otherwise for the whole screen. This tool currently only works for index layers (e.g., the NDVI).\n\nFig. 17: Example of a distribution plot of NDVI values\n\nSpectral Explorer\nThe Spectral Explorer analyses the various bands of the multi-spectral imagery to extract the spectral signature and helps to identify the scene in the region of interest. You can follow these steps to see a simple example of this feature. Note that this feature is only available for Sentinel-2 imagery at the moment.\n\nGo to the location and draw a bounding box in the open water like shown in the figure.\nClick on the Spectral Explorer icon () within the AOI tool.\n\n\nFig. 18: Screenshot of steps to follow to see an example of spectral signature of open water.\nBy doing this, you can see a graph pop up with the spectral signature (associated with specific chemical composition) averaged over all the pixels within this box. This helps to compare the spectral signature of the scene (in light green) with other known signatures.\n\nFig. 19: Example of spectral signatures plotted in comparison to other known signatures labelled in the bottom of the graph.\n\n\nTime Series\nWith the Statistical Info () feature within the AOI tool, you can see how the value of an index has changed in time. To see this, it is necessary to choose a visualisation which give an index as output, (e.g., NDVI) or any single band. You can follow these steps to see an example Time Series of the NDVI of a single agricultural field.\n\nSelect the Normalized Difference Vegetation Index (NDVI) layer to visualise the scene.\nOutline an agriculutral field with the AOI tool to select the area you want to analyse the changes in.\n\n\nFig. 20: Screenshot of steps to follow to see an example of time series of NDVI over an agricultural field.\n\nSelect the Statistical Info () chart icon within the AOI tools.\nYou can choose the time over which you want to see the changes at the top of the graph that pops up. Due to the clouds being in the way and distorting the NDVI considerably (cloud NDVI values are low), the growth curve isn’t as orderly as one would have hoped. So you can also adjust the cloud cover to visualise the changes without distortion.\n\n\nFig. 21: Example of Time Series plotted for an agricultural field over a period of 1 year with 0% cloud cover."
  },
  {
    "objectID": "Applications/WebEditor.html",
    "href": "Applications/WebEditor.html",
    "title": "openEO Web Editor",
    "section": "",
    "text": "The openEO Web Editor is a web-based graphical user interface (GUI) designed to interact with the openEO API visually. It offers a user-friendly platform for performing various Earth Observation data processing tasks. These tasks include querying available data, defining processing workflows, executing processes, and visualizing results.\nUsers can create intricate processing chains by linking different processing steps as building blocks. The interface allows for specifying parameters and input data for each step, facilitating the creation of customized and detailed processing workflows.\nIn other words, the openEO Web Editor can act as a simple interface for:\nThe openEO Web Editor can be accessed via https://openeo.dataspace.copernicus.eu/. Even without logging in, users can explore information on available collections, processes, User Defined Functions(UDF) Runtimes, and the options for exporting files. Additionally, users can create openEO process graphs; however, logging in is necessary to run them."
  },
  {
    "objectID": "Applications/WebEditor.html#getting-started",
    "href": "Applications/WebEditor.html#getting-started",
    "title": "openEO Web Editor",
    "section": "Getting Started",
    "text": "Getting Started\nUpon initial access to the provided link, users are presented with the following screen, which is further explained below in reference to the given numbering:\n\n\nService Offering\nThe sidebar allows users to navigate the available collections, processes, UDF Runtimes and Export file formats. A search feature at the top of the sidebar allows for direct searching.\nWithin the Collections section, users can access a comprehensive list of data collections available in the backend through openEO. Clicking on any of these collections will bring up a detailed metadata window.\nUnder the Processes section, users can find a comprehensive list of openEO processes designed explicitly for Earth Observation processing. These processes operate on individual values within an array, accepting and returning a single value.\nThe UDF Runtimes section provides information on the available environments or platforms where User Defined Functions (UDFs) can be executed. Currently, the Python runtime is available during this stage of development.\nIn the Export File Formats section, users are guided on the supported output formats within openEO. Clicking on each format provides a detailed summary of its associated parameters.\nHelp\nThe Help icon at the top of the screen provides a short tour of the main sections of the editor.\nWizard\nThe Wizard is an experimental feature designed to simplify the creation of openEO processes for common use cases.\nServer\nThe Server icon will pop up a window giving the user detailed information on the server used for processing the created processes.\nGuest\nWhen a user clicks on Guest, the dropdown will provide an option to log in. The profile will be updated with the username when logged in.\nFeatures\nThe basic functionalities that can be handy when creating the processes in openEO Web Editor are available in this row. These functionalities include creating a new script, importing processes from external sources, exporting in another programming language, validating processes on the server side, editing process metadata, adding parameters, etc.\nProcess Editor\nThe Process Editor features both “Visual Model” and “Code” modes. In “Visual Model,” users can create processing chains by adding collections and processes and then connecting them. This mode simplifies the creation of workflows using a graphical interface. The “Code” tab lets users view the generated JSON workflow process as a process graph. This provides a detailed view of the workflow structure in a machine-readable format.\nThe area on the right will later be used for previewing collections or inspecting the results of batch jobs, web services or other computations. It will also display log messages for a batch job.\nLog in\nAs previously mentioned, it is necessary to log in to interact with the server. As demonstrated below, a new window will appear when attempting to log in. While other options are sometimes available, the “Copernicus Data Space Ecosystem” is recommended authentication. For further information regarding various authentication methods or to seek assistance, click on the “help” option at the top or contact us."
  },
  {
    "objectID": "Applications/WebEditor.html#create-a-workflow",
    "href": "Applications/WebEditor.html#create-a-workflow",
    "title": "openEO Web Editor",
    "section": "Create a workflow",
    "text": "Create a workflow\nUsers can build their models using a simple drag-and-drop method based on their applications. Some processes may necessitate input parameters, which must be carefully considered. As an illustration, we present a simple case of creating a workflow to calculate NDVI using the Sentinel 2 L2A collection. Three main steps in using openEO for Earth Observation data processing are shown below:\n\nLoad Collection\n\nThe first step is to search for the required collections for analysis. Therefore, verifying that the necessary collections exist within the openEO database is important. This can be done by exploring or searching the list of available collections from the sidebar of the interface. In our example, we want to calculate the Normalized Difference Vegetation Index (NDVI), so the Sentinel 2 L2A collection will be used. So, the next step is to drag and drop it into the Process Editor for further operations.\n\n\n\nOnce a collection is loaded, several parameters must be specified, including the area of interest, temporal extent, and selection of bands. Clicking on load_collection opens a window where these parameters can be defined for subsequent processing.     Users can choose between generating a bounding box or importing their spatial extent by dragging and dropping GeoJSON or KML files onto the map view to specify the area of interest. While this example demonstrates the use of a bounding box, it is suggested to experiment with a suitably compact area for testing purposes.\n\n\nAnother parameter to consider is the temporal extent, which allows users to restrict the loaded data to a specified time window. It is encouraged to choose a timeframe of 1-2 weeks for testing purposes.\n\nIn our case, for NDVI calculation, we have specifically chosen the Red band (B04) and the Near-Infrared (NIR) band (B08).\n\nApply Processes\nThe next step involves applying essential openEO processes, ranging from straightforward mathematical operations to more complex tasks.\nIn this task, we will use a specific openEO process called reduce_dimension two times to simplify our data cube. First, we deal with the various bands and then reduce the temporal dimension.\n\n\nIn the initial process, the reduce_dimension algorithm is utilised to reduce the band dimension after executing a series of addition, subtraction, and division operations necessary for the NDVI calculation.\n\nFollowing this, with the same reduce_dimension process, we eliminate the temporal dimension by selecting the maximum value using the max function.\n\n\nSelect a format\n\nThe user needs to select the output format as a final step in the workflow creation.\n\nSince this workflow eliminated the temporal dimension, we can keep things simple and save the result as a GeoTiff."
  },
  {
    "objectID": "Applications/WebEditor.html#execute-the-workflow",
    "href": "Applications/WebEditor.html#execute-the-workflow",
    "title": "openEO Web Editor",
    "section": "Execute the workflow",
    "text": "Execute the workflow\nThe final step involves running the created workflow to complete the data analysis process. This can be done in two ways: synchronously or through batch job-based method. The synchronous method allows the user to download the data directly. In contrast, the batch job-based method enables the user to run the process as a batch. The choice of method depends on the user’s preference and the dataset size.\n\nIn the figure above, the red box includes the two methods possible for running the process. In this example, we used the synchronous method by directly clicking on Run now, which popped up a box in the bottom right corner.\nOnce the execution process is completed, the result is automatically saved locally. It can also be visualised in the parallel window, as shown in the image below:\n\nFurthermore, if a Batch Job is created, its actions can be monitored from the same window."
  },
  {
    "objectID": "Applications/WebEditor.html#data-download-using-wizard",
    "href": "Applications/WebEditor.html#data-download-using-wizard",
    "title": "openEO Web Editor",
    "section": "Data Download using Wizard",
    "text": "Data Download using Wizard\nThe Wizard feature at the top navigation bar allows easy creation of process graphs for basic use cases without dragging and dropping processes as done earlier.\n\n\n\n\n\n\nWarning\n\n\n\nThis feature is experimental and it is not guaranteed that the generated process graphs are fully functional.\n\n\nAt the time of this documentation release, two cases are available through the Wizard feature: direct downloading of data for a selected area and downloading computed spectral indices as shown in the figure below:\n\nUsers can select their preferred task and click the Next button to proceed. The following window will allow the user to define the area of interest, temporal extent, and required collection. This method makes the task considerably more straightforward than the previously mentioned workflow creation approach."
  },
  {
    "objectID": "Applications/WebEditor.html#monitor-the-workflow",
    "href": "Applications/WebEditor.html#monitor-the-workflow",
    "title": "openEO Web Editor",
    "section": "Monitor the workflow",
    "text": "Monitor the workflow\nWhen a batch job is created, openEO’s logs feature can help with monitoring and debugging of workflows. The backend typically uses this to dump information during data processing that may be relevant for the user (e.g. warnings, resource stats, …). Job logs can be visualized programmatically and in the web editor like many other processes.\nAs shown in the following figure, users can access the job logs by clicking on the icon next to the job ID at the bottom of the interface.\n\nWhen clicking on it, a pane emerges on the right, as shown below:\n\nMoreover, users can utilize openEO processes like inspect. This allows logging of custom information to be displayed in the job logs."
  },
  {
    "objectID": "Applications/JupyterHub.html",
    "href": "Applications/JupyterHub.html",
    "title": "JupyterLab",
    "section": "",
    "text": "JupyterLab is a user-friendly tool for working with data and code. Users can customize it by adding extra features and connecting it with other software packages. This makes organizing work and interacting with data more manageable. For more detailed information, please visit the JupyterLab documentation.\nWithin the Copernicus Data Space ecosystem, a free JupyterLab service is offered. It allows users to access and analyze Earth observation data effectively."
  },
  {
    "objectID": "Applications/JupyterHub.html#notebooks",
    "href": "Applications/JupyterHub.html#notebooks",
    "title": "JupyterLab",
    "section": "Notebooks",
    "text": "Notebooks\nWithin JupyterLab, Python notebooks can be used for those who want to prototype their EO data processing programmatically. Additionally, example notebooks are available to help users get started with their analysis.\nAccess to this service is available by clicking in: https://jupyterhub.dataspace.copernicus.eu/\n\nUpon clicking the “ACCESS JUPYTERLAB” button, users are directed to the login window. Please use the Copernicus Data Space Ecosystem credentials to log in."
  },
  {
    "objectID": "Applications/JupyterHub.html#server-options",
    "href": "Applications/JupyterHub.html#server-options",
    "title": "JupyterLab",
    "section": "Server Options",
    "text": "Server Options\nOnce logged into JupyterLab, users are presented with three Jupyter instance flavours: Small, Medium, and Large. The instance size determines the allocation of resources such as CPU cores and memory to the notebook kernels. All flavours are capable of handling simple tasks and running all the samples provided in the samples folder. For fair resource usage, it is suggested to start with the “Small” flavour. Consider upgrading to a larger server to ensure smooth performance when analysing larger regions or conducting memory-intensive processing."
  },
  {
    "objectID": "Applications/JupyterHub.html#jupyterlab-user-interface",
    "href": "Applications/JupyterHub.html#jupyterlab-user-interface",
    "title": "JupyterLab",
    "section": "JupyterLab User Interface",
    "text": "JupyterLab User Interface\nAfter signing in, users will see a launcher with Python environments such as Python 3, Geo Science, OpenEO, and Sentinel Hub. Each environment is equipped with specific Python packages tailored to various requirements. Users can run their code in a notebook or a console, depending on their preference. Additionally, options are provided to create text files, markdown files, or Python files, allowing users to work with different types of documents as needed.\n\nFor a more detailed overview of the JupyterLab user interface, refer to the datacamp tutorial."
  },
  {
    "objectID": "Applications/JupyterHub.html#creating-and-managing-notebooks",
    "href": "Applications/JupyterHub.html#creating-and-managing-notebooks",
    "title": "JupyterLab",
    "section": "Creating and Managing Notebooks",
    "text": "Creating and Managing Notebooks\nBy default, sample notebooks are located in the “samples” folder.\nTo create a notebook in JupyterLab, users can select their desired kernel from the Notebook section in the launcher. Clicking on the kernel of choice will generate a new notebook named “Untitled.ipynb”."
  },
  {
    "objectID": "Applications/JupyterHub.html#installing-additional-packages",
    "href": "Applications/JupyterHub.html#installing-additional-packages",
    "title": "JupyterLab",
    "section": "Installing additional packages",
    "text": "Installing additional packages\nAdditional Python packages can be installed if needed. This can be accomplished either from the Notebook Terminal, accessible in the Launcher tab, or directly within a Notebook cell, as shown below:\n\nTerminalNotebook\n\n\npip install required_package\n\n\n!pip install required_package"
  },
  {
    "objectID": "Applications/JupyterHub.html#collaboration-and-sharing",
    "href": "Applications/JupyterHub.html#collaboration-and-sharing",
    "title": "JupyterLab",
    "section": "Collaboration and Sharing",
    "text": "Collaboration and Sharing\nJupyterLab facilitates collaborative work by allowing multiple users to edit and view the same notebooks or projects in real time. It also offers various sharing options, including exporting notebooks to different formats (HTML, PDF, Markdown) and publishing them on platforms like GitHub or JupyterHub. These features empower users to easily share their work, communicate findings, and collaborate with a broader audience, promoting efficient collaboration and seamless knowledge dissemination."
  },
  {
    "objectID": "Applications/JupyterHub.html#storage",
    "href": "Applications/JupyterHub.html#storage",
    "title": "JupyterLab",
    "section": "Storage",
    "text": "Storage\nWhen starting the notebook, in the file navigation pane (the sidebar - #2), two folders will be visible:\n\nmystorage\nsamples\n\n\nAdditional folders or files created by the user outside storage will be deleted after the session ends.\n\n\nmystorage\nEach JupyterLab instance includes 10GB of storage provided by CloudFerro’s cloud infrastructure. This storage can be used to save notebooks between sessions, store results, or upload data files.\n The storage area remains intact when the user logs out or when the Jupyter kernel shuts down. It is preserved for up to 15 days from the last login. Users receive a notification prompting them to log back into JupyterLab to reset the timer and ensure their data remains preserved. If no action is taken within this period, the files will be deleted from CloudFerro cloud storage after 15 days.\n\n\n\n\n\n\nMissing folders?\n\n\n\n\n\nSuppose the mystorage folder was lost due to inactivity for several weeks. In that case, the user can request restoration from the support team (though it is not guaranteed). If the folder disappeared while still active in the past few days, try restarting the server by going to File -&gt; Hub Control Panel -&gt; Stop My Server, then Start My Server. If the issue persists, reach out to our support team.\n\n\n\n\n\nsamples\nThese folders are recreated with every start of the Jupyter kernel. The samples folder is always up-to-date with the latest version of notebooks. If changes are made to the samples, please ensure to save the updated notebook in the mystorage area to ensure it can be accessed when returning after a period of inactivity."
  },
  {
    "objectID": "Applications/JupyterHub.html#session-persistence",
    "href": "Applications/JupyterHub.html#session-persistence",
    "title": "JupyterLab",
    "section": "Session persistence",
    "text": "Session persistence\nThe Jupyterlab instance remains running as long as there is interaction, and it is kept alive for an additional 8 hours after the last activity. After this period, the instance will shut off, requiring relaunching during the next login.\nAll data saved in mystorage is preserved, but all other files will be deleted. Additionally, any packages (dependencies) installed using pip must be reinstalled."
  },
  {
    "objectID": "Applications/JupyterHub.html#resources-and-references",
    "href": "Applications/JupyterHub.html#resources-and-references",
    "title": "JupyterLab",
    "section": "Resources and References:",
    "text": "Resources and References:\nHere are some helpful links for effectively using JupyterLab in the Copernicus Data Space Ecosystem:\n\nJupyterLab Documentation\nNotebook Documentation\nBasic Notebook Tutorials"
  },
  {
    "objectID": "Support.html",
    "href": "Support.html",
    "title": "Support",
    "section": "",
    "text": "If you don’t find answer to your questions in the documentation portal, this page describes how to ask for support.\n\n\nImportant to know is that only users with a Copernicus Data Space Ecosystem account can ask for support. If you don’t have one yet, you can register here. If you have an issue with registering or you want to deregister, please contact us directly.\n\n\n\nNavigate to the following website.\nIn case you’re not logged in, click on LOGIN.\n\nYou will now get the Copernicus Data Space Ecosystems login form.\n\nEnter your credentials and click LOG IN.\n\n\n\nOnce you have logged in you should see this window, click SUBMIT A REQUEST.\n\nThe form used to create tickets should now appear.\n\nFrom the dropdown select what the question is about.\nEnter the subject.\nDescribe your problem in detail in the field Description.\nYou can also upload attachments such as screenshots in the Attachments section.\nOnce you’ve finished, click SUBMIT.\nYour ticket should now be submitted.\n\nYou can see its status here. You can also post additional comments and attachments.\n\n\n\nAfter logging in (as described in Step 1), you can see the status of your requests under your account. Select Requests from the drop-down.\n\nYou will now see all your requests.\n\nIf you can’t see your request here, make sure that Status “Any” is selected from the drop-down.\nYou should now see your request."
  },
  {
    "objectID": "Support.html#prerequisites",
    "href": "Support.html#prerequisites",
    "title": "Support",
    "section": "",
    "text": "Important to know is that only users with a Copernicus Data Space Ecosystem account can ask for support. If you don’t have one yet, you can register here. If you have an issue with registering or you want to deregister, please contact us directly."
  },
  {
    "objectID": "Support.html#step-1-navigate-to-the-help-center",
    "href": "Support.html#step-1-navigate-to-the-help-center",
    "title": "Support",
    "section": "",
    "text": "Navigate to the following website.\nIn case you’re not logged in, click on LOGIN.\n\nYou will now get the Copernicus Data Space Ecosystems login form.\n\nEnter your credentials and click LOG IN."
  },
  {
    "objectID": "Support.html#step-2-submit-a-request",
    "href": "Support.html#step-2-submit-a-request",
    "title": "Support",
    "section": "",
    "text": "Once you have logged in you should see this window, click SUBMIT A REQUEST.\n\nThe form used to create tickets should now appear.\n\nFrom the dropdown select what the question is about.\nEnter the subject.\nDescribe your problem in detail in the field Description.\nYou can also upload attachments such as screenshots in the Attachments section.\nOnce you’ve finished, click SUBMIT.\nYour ticket should now be submitted.\n\nYou can see its status here. You can also post additional comments and attachments."
  },
  {
    "objectID": "Support.html#accessing-your-submitted-requests",
    "href": "Support.html#accessing-your-submitted-requests",
    "title": "Support",
    "section": "",
    "text": "After logging in (as described in Step 1), you can see the status of your requests under your account. Select Requests from the drop-down.\n\nYou will now see all your requests.\n\nIf you can’t see your request here, make sure that Status “Any” is selected from the drop-down.\nYou should now see your request."
  },
  {
    "objectID": "Quotas.html#copernicus-general-users",
    "href": "Quotas.html#copernicus-general-users",
    "title": "Quotas and Limitations",
    "section": "Copernicus General Users",
    "text": "Copernicus General Users\n\n\n\n\n\n\nLimitations\n\n\nS3, OData, STAC\n\n\nData Workspace API\n\n\nopenEO API / Algorithm plaza\n\n\nSentinel Hub APIs⁸\n\n\nDirect HTTP access to COGs\n\n\n\n\n\n\nRequests per month\n\n\n-\n\n\n-\n\n\n-\n\n\n10 000\n\n\n50 000⁷ ⁹\n\n\n\n\nRequests per minute\n\n\n2000¹⁴\n\n\n-\n\n\n12¹¹ ¹³\n\n\n300\n\n\n-\n\n\n\n\nProcessing units (PU) per month\n\n\n-\n\n\n-\n\n\n-\n\n\n10 000\n\n\n-\n\n\n\n\nProcessing units (PU) per minute\n\n\n-\n\n\n-\n\n\n-\n\n\n300\n\n\n-\n\n\n\n\nBandwidth limit per connection (MB/s) (IAD¹)\n\n\n20\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nNumber of concurrent connections limit (IAD¹)\n\n\n4\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nMonthly transfer limit (TB) (IAD¹)¹⁰\n\n\n12\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nNumber of concurrent orders limit (DAD²)\n\n\n-\n\n\n1\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nMonthly transfer limit (TB) (DAD²)\n\n\n-\n\n\n0,1\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nProcessed Products per month\n\n\n-\n\n\n25\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nConcurrent Processing\n\n\n-\n\n\n2¹²\n\n\n2\n\n\n-\n\n\n-\n\n\n\n\nNumber of active sessions³\n\n\n100\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nA token stays active for⁴\n\n\n10 minutes\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nA token can be refreshed in⁵\n\n\n60 minutes\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nNumber of products that be accessed with one token⁶\n\n\nNo limits\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\nCredits per month\n\n\n-\n\n\n-\n\n\n10 000¹⁵\n\n\n-\n\n\n-\n\n\n\n\nConcurrent API requests\n\n\n-\n\n\n-\n\n\n2\n\n\n-\n\n\n-\n\n\n\n\n\n\n ¹ IAD: Immediately Available Data.  ² DAD: Deferred Available Data (known as Offline data). It is not possible to order DAD by using OData,STAC or S3, Copernicus Browser or any of the Sentinel Hub APIs. DAD (Offline data) can only be ordered by using the Data Workspace. Only after ordering, it can be downloaded from the catalogue by using the download APIs.  ³ This includes, among others, a newly generated token and logging in to the user panel. Please refer to: Device Activity and see the number of signed in devices - It is not possible to increase this number to a bigger value than 100, with a paid plan. Each session starts when the user generates a new token and use it for the downloading process. One session ends when the token reaches its expiry time (a token stays active for 10 min).  ⁴ After reaching this limit, the Access Token must either be refreshed by using the Refresh Token or be re-generated.  ⁵ Anytime within 60 minutes after the access token is generated.  ⁶ As long as any other limit(s) are not breached.  ⁷ Extensions are available via CREODIAS offering.  ⁸ Similar principles apply for all SentinelHub APIs while the differences and details are covered in the Processing Unit section of our documentation. Note that there are APIs that are not available to Copernicus General Users such as Sentinel Hub Batch Processing API.  ⁹ Technical limitation may be applied to maintain platform stability.  ¹⁰ After reaching this monthly transfer limit, the maximum bandwidth drops to 1MB/s and the number of concurrent connections drops to 1.  ¹¹ Only enabled for openEO synchronous execution requests (POST /result) and batch execution requests (POST /{job_id}/result).  ¹² The maximum number of simultaneous production orders running in parallel, i.e. the orders in “In progress” status.  ¹³ 1 request per 5 seconds.  ¹⁴ Only applies to S3.  ¹⁵ Temporary Boost of monthly openEO Credits to 10 000. Read our news item published on September 3, 2024."
  },
  {
    "objectID": "404-not-found.html",
    "href": "404-not-found.html",
    "title": "Page not found",
    "section": "",
    "text": "Page not found"
  },
  {
    "objectID": "notebook-samples/openeo/Sentinel_3.html",
    "href": "notebook-samples/openeo/Sentinel_3.html",
    "title": "Sentinel-3",
    "section": "",
    "text": "This notebook explores working with Sentinel-3 data."
  },
  {
    "objectID": "notebook-samples/openeo/Sentinel_3.html#sentinel-3-olci",
    "href": "notebook-samples/openeo/Sentinel_3.html#sentinel-3-olci",
    "title": "Sentinel-3",
    "section": "Sentinel-3 OLCI",
    "text": "Sentinel-3 OLCI\nhttps://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-3-olci\nThe OLCI dataset provided by Sentinelhub is based on the level-1b products. These products are provided in “instrument” projection rather than being projected into a ground-based reference system. Hence, these products do not come with a ‘native’ reference system. The openEO collections are currently configured to use EPSG:4326 unprojected coordinates, with a resolution set to a fixed value that tries to approximate the native 300m ground resolution.\n\nimport openeo\nimport xarray\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nconn = openeo.connect(\"openeo.dataspace.copernicus.eu\")\nconn.authenticate_oidc()\n\nVisit https://identity.dataspace.copernicus.eu/auth/realms/CDSE/device?user_code=QPMD-IMEL 📋 to authenticate.\n\n\n[####################################-] Authorized successfully\n\n\nAuthenticated using device code flow.\n\n\n&lt;Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.1/' with OidcBearerAuth&gt;\n\n\n\nLoad the collection\n\nconn.describe_collection(\"SENTINEL3_OLCI_L1B\")\n\n\n    \n    \n        \n    \n    \n\n\n\nbbox = {\"west\": 27.564697, \"south\": 34.764179, \"east\": 33.002930, \"north\": 37.387617}\nsentinel3 = conn.load_collection(\n    \"SENTINEL3_OLCI_L1B\",\n    spatial_extent=bbox,\n    temporal_extent=[\"2021-07-30\", \"2021-07-30\"],\n    bands=[\"B08\", \"B06\", \"B04\"],\n)\n\nLet’s download this slice of data in netCDF format to give it an initial inspection.\n\nsentinel3.download(\"sentinel3.nc\")\n\nQuick visualisation of the output\n\nds = xarray.load_dataset(\"sentinel3.nc\")\n\n# Convert xarray DataSet to a (bands, t, x, y) DataArray\ndata = ds[[\"B08\", \"B06\", \"B04\"]].to_array(dim=\"bands\")\n\n\nfig, (axrgb, axhist) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\n# plot the data\ndata[{\"t\": 0}].plot.imshow(ax=axrgb)\n\n# Plot the data histogram\ndata.plot.hist(bins=50, ax=axhist, histtype=\"stepfilled\")\n\nplt.show()"
  },
  {
    "objectID": "notebook-samples/openeo/NO2Covid.html",
    "href": "notebook-samples/openeo/NO2Covid.html",
    "title": "NO2 emission and COVID Lockdown effects",
    "section": "",
    "text": "The COVID-19 pandemic has had a significant effect on reducing both human and industrial activities, leading to certain positive outcomes such as a decrease in air pollutants, as discussed in this paper.\nTherefore, in this notebook, we aim to explore the trend in air pollution, focusing on the NO2 product of the Sentinel 5P collection. We will simply, compare the situation during and after the onset of COVID-19 lockdown in the Delhi region of India.\nFurther information on Sentinel-5P can be found the following links:\n\nhttps://documentation.dataspace.copernicus.eu/Data/SentinelMissions/Sentinel5P.html\nhttps://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-5P/Sentinel-5P_brings_air_pollution_into_focus\n\n\nimport openeo\n\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n\n1. Create a datacube for period during COVID lockdowns\n\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [77.11, 28.69],\n            [77.11, 28.56],\n            [77.29, 28.56],\n            [77.29, 28.69],\n            [77.11, 28.69],\n        ]\n    ],\n}\n\n\ns5covid = connection.load_collection(\n    \"SENTINEL_5P_L2\",\n    temporal_extent=[\"2020-06-01\", \"2021-06-30\"],\n    spatial_extent={\"west\": 77.11, \"south\": 28.56, \"east\": 77.29, \"north\": 28.69},\n    bands=[\"NO2\"],\n)\n\n\n# Now aggregate by day to avoid having multiple data per day\ns5covid = s5covid.aggregate_temporal_period(reducer=\"mean\", period=\"day\")\n\n# let's create a spatial aggregation to generate mean timeseries data\ns5covid = s5covid.aggregate_spatial(reducer=\"mean\", geometries=aoi)\n\n\n\n2. Let’s repeat the same process for Post Covid Situation\n\n# Create a datacube for period after COVID lockdowns\n\ns5post = connection.load_collection(\n    \"SENTINEL_5P_L2\",\n    temporal_extent=[\"2022-06-01\", \"2023-06-30\"],\n    spatial_extent={\"west\": 77.11, \"south\": 28.56, \"east\": 77.29, \"north\": 28.69},\n    bands=[\"NO2\"],\n)\n\n# Now aggregate by day to avoid having multiple data per day\ns5post = s5post.aggregate_temporal_period(reducer=\"mean\", period=\"day\")\n\n# Now create a spatial aggregation to generate mean timeseries data\ns5post = s5post.aggregate_spatial(reducer=\"mean\", geometries=aoi)\n\nFinally we execute them as batch jobs and download the results as netCDF. They are further plotted as shown below to study the data.\n\njob = s5covid.execute_batch(title=\"NO2 during Covid\", outputfile=\"during_covid.nc\")\n\n0:00:00 Job 'j-240115e1f4dc4beea1c133d6498ee198': send 'start'\n0:02:06 Job 'j-240115e1f4dc4beea1c133d6498ee198': running (progress N/A)\n0:02:12 Job 'j-240115e1f4dc4beea1c133d6498ee198': running (progress N/A)\n0:02:18 Job 'j-240115e1f4dc4beea1c133d6498ee198': running (progress N/A)\n0:04:08 Job 'j-240115e1f4dc4beea1c133d6498ee198': running (progress N/A)\n0:04:18 Job 'j-240115e1f4dc4beea1c133d6498ee198': running (progress N/A)\n0:06:00 Job 'j-240115e1f4dc4beea1c133d6498ee198': running (progress N/A)\n0:09:46 Job 'j-240115e1f4dc4beea1c133d6498ee198': finished (progress N/A)\n\n\n\njob = s5post.execute_batch(title=\"NO2 Post-Covid\", outputfile=\"post_covid.nc\")\n\n0:00:00 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': send 'start'\n0:01:55 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': created (progress N/A)\n0:02:01 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': created (progress N/A)\n0:03:30 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:03:48 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:03:58 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:04:11 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:04:26 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:04:45 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:05:19 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:05:49 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': running (progress N/A)\n0:07:54 Job 'j-240115fb53df44bea28bb7e8dcc1e4cd': finished (progress N/A)\n\n\n\n\nLet’s plot the result\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\n\n# load the results\nduringdata = xr.load_dataset(\"during_covid.nc\")\npostdata = xr.load_dataset(\"post_covid.nc\")\n\nLet’s calculate the mean along-time dimension for the window size of 30days.\n\nduringdata = duringdata.rolling(t=30).mean()\npostdata = postdata.rolling(t=30).mean()\n\n\nfig, ax1 = plt.subplots(dpi=100)\n(line1,) = ax1.plot(\n    duringdata.t, duringdata.NO2.to_numpy().flatten(), color=\"r\", label=\"During COVID\"\n)\nax1.set_xlabel(\"During COVID\")\nax1.set_ylabel(\"NO2 Level\")\nax1.xaxis.label.set_color(\"r\")\nax1.tick_params(axis=\"x\", colors=\"r\")\n\nax2 = ax1.twiny()\n(line2,) = ax2.plot(\n    postdata.t, postdata.NO2.to_numpy().flatten(), color=\"g\", label=\"Post COVID\"\n)\nax2.set_xlabel(\"Post COVID\")\n# Combine legends from both axes\nlines = [line1, line2]\nlabels = [line.get_label() for line in lines]\nax1.legend(lines, labels, loc=\"upper left\")\nax2.xaxis.label.set_color(\"g\")\nax2.tick_params(axis=\"x\", colors=\"g\")\n\n\n\n\nThe red line in the plot shows the NO2 concentration during the COVID lockdown period in a selected area of Delhi, India, and the green line represents the post-COVID period during similar months. There was a minor decline in NO2 levels during the COVID lockdown periods, showing that the air pollution (Specifically NO2) was less during the lockdown, and it increased to higher levels as life went back to normal. It also shows that the months between November and April have higher concentrations of air pollutants in Delhi compared to May and September.\nThus, taking this notebook as a reference case, further scenarios can be investigated using Sentinel 5P data within the Copernicus Data Space Ecosystem, such as PM2.5 concentration, ozone layer depletion, SO2 concentration, etc."
  },
  {
    "objectID": "notebook-samples/openeo/Whittaker.html",
    "href": "notebook-samples/openeo/Whittaker.html",
    "title": "Creating a smoothed dataset using Whittaker",
    "section": "",
    "text": "In this notebook, we use Whittaker algorithm is available in the FuseTS toolbox as a user-defined-function (UDF) to create a smoothed time series. It employs a discrete penalized least squares algorithm that fits a smooth series, denoted as z, to the original data series, denoted as y.\nPlease note that FuseTS library used here is compatible with Python 3.8 - 3.10.\n\nimport itertools\nimport warnings\nfrom datetime import datetime, timedelta\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport openeo\nimport pandas as pd\nimport xarray\nfrom ipyleaflet import GeoJSON, Map, basemaps\nfrom openeo.processes import eq\nfrom openeo.rest.conversions import timeseries_json_to_pandas\n\nfrom fusets.whittaker import whittaker\n\nwarnings.filterwarnings(\"ignore\")\n\nThe first step is to connect to an openEO backend and authenticate with the Copernicus Dataspace Ecosystem’s credentials.\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nNext we define the area of interest, in this case an extent, for which we would like to fetch time series data.\n\nyear = 2019\nspat_ext = {\n    \"coordinates\": [\n        [\n            [-4.875091217039325, 41.77290587433312],\n            [-4.872773788450457, 41.77290587433312],\n            [-4.872773788450457, 41.77450614847532],\n            [-4.875091217039325, 41.77450614847532],\n            [-4.875091217039325, 41.77290587433312],\n        ]\n    ],\n    \"type\": \"Polygon\",\n}\ntemp_ext = [f\"{year}-01-01\", f\"{year}-12-30\"]\n\n\ncenter = np.mean(spat_ext[\"coordinates\"][0], axis=0).tolist()[::-1]\nzoom = 16\n\nm = Map(basemap=basemaps.Esri.WorldImagery, center=center, zoom=zoom)\ng = GeoJSON(\n    data=spat_ext,\n    style={\n        \"color\": \"red\",\n        \"opacity\": 1,\n        \"weight\": 1.9,\n        \"dashArray\": \"9\",\n        \"fillOpacity\": 0.5,\n    },\n)\nm.add(g)\nm\n\n\n\n\nWe will be working with with the rapeseed from 2019, located in the Nothern Spain.\nWe will create an openEO process to calculate the NDVI time series for our area of interest. First we begin by using the SENTINEL2_L2A collection, and apply a Sen2Cor cloud masking algorithm to remove any interfering clouds before calculating the NDVI values.\n\ns2 = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent=spat_ext,\n    temporal_extent=temp_ext,\n    bands=[\"B04\", \"B08\", \"SCL\"],\n)\ns2 = s2.process(\"mask_scl_dilation\", data=s2, scl_band_name=\"SCL\")\ns2 = s2.mask_polygon(spat_ext)\nndvi_cube = s2.ndvi(red=\"B04\", nir=\"B08\", target_band=\"NDVI\")\n\nNow that we have calculated the NDVI time series for our area of interest, we can request openEO to download the result to our local storage. This will allow us to access the file and use it for further analysis in this notebook.\n\nndvi_output_file = \"raw_s2_ndvi_field.nc\"\n\n# batch job\n\nndvi_job = ndvi_cube.execute_batch(ndvi_output_file, title=f\"FUSETS-Raw NDVI\")\n\n# load the dataset and check it's structure\nraw_ndvi_ds = xarray.load_dataset(ndvi_output_file)\nraw_ndvi_ds\n\n0:00:00 Job 'j-2310308bb89149cf8106aabb55eba553': send 'start'\n0:00:12 Job 'j-2310308bb89149cf8106aabb55eba553': created (progress N/A)\n0:00:18 Job 'j-2310308bb89149cf8106aabb55eba553': created (progress N/A)\n0:00:24 Job 'j-2310308bb89149cf8106aabb55eba553': created (progress N/A)\n0:00:33 Job 'j-2310308bb89149cf8106aabb55eba553': created (progress N/A)\n0:00:43 Job 'j-2310308bb89149cf8106aabb55eba553': created (progress N/A)\n0:00:56 Job 'j-2310308bb89149cf8106aabb55eba553': created (progress N/A)\n0:01:12 Job 'j-2310308bb89149cf8106aabb55eba553': running (progress N/A)\n0:01:32 Job 'j-2310308bb89149cf8106aabb55eba553': running (progress N/A)\n0:01:56 Job 'j-2310308bb89149cf8106aabb55eba553': running (progress N/A)\n0:02:27 Job 'j-2310308bb89149cf8106aabb55eba553': finished (progress N/A)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (t: 31, x: 21, y: 19)\nCoordinates:\n  * t        (t) datetime64[ns] 2019-01-27 2019-02-11 ... 2019-12-18 2019-12-28\n  * x        (x) float64 3.442e+05 3.442e+05 3.442e+05 ... 3.443e+05 3.444e+05\n  * y        (y) float64 4.626e+06 4.626e+06 4.626e+06 ... 4.626e+06 4.626e+06\nData variables:\n    crs      |S1 b''\n    B04      (t, y, x) float32 nan 1.212e+03 1.226e+03 1.236e+03 ... nan nan nan\n    B08      (t, y, x) float32 nan 1.934e+03 1.964e+03 1.982e+03 ... nan nan nan\n    SCL      (t, y, x) float32 nan 5.0 5.0 5.0 5.0 5.0 ... nan nan nan nan nan\n    NDVI     (t, y, x) float32 nan 0.2295 0.2313 0.2318 ... nan nan nan nan\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platform - Geotrellis backend: 0.18.0a1\n    description:  \n    title:        xarray.DatasetDimensions:t: 31x: 21y: 19Coordinates: (3)t(t)datetime64[ns]2019-01-27 ... 2019-12-28standard_name :tlong_name :taxis :Tarray(['2019-01-27T00:00:00.000000000', '2019-02-11T00:00:00.000000000',\n       '2019-02-21T00:00:00.000000000', '2019-02-26T00:00:00.000000000',\n       '2019-03-03T00:00:00.000000000', '2019-03-13T00:00:00.000000000',\n       '2019-03-23T00:00:00.000000000', '2019-03-28T00:00:00.000000000',\n       '2019-04-07T00:00:00.000000000', '2019-04-12T00:00:00.000000000',\n       '2019-04-27T00:00:00.000000000', '2019-05-02T00:00:00.000000000',\n       '2019-05-27T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-06-26T00:00:00.000000000', '2019-07-01T00:00:00.000000000',\n       '2019-07-11T00:00:00.000000000', '2019-07-16T00:00:00.000000000',\n       '2019-07-21T00:00:00.000000000', '2019-08-05T00:00:00.000000000',\n       '2019-08-15T00:00:00.000000000', '2019-08-20T00:00:00.000000000',\n       '2019-08-30T00:00:00.000000000', '2019-09-04T00:00:00.000000000',\n       '2019-09-19T00:00:00.000000000', '2019-09-29T00:00:00.000000000',\n       '2019-10-09T00:00:00.000000000', '2019-11-18T00:00:00.000000000',\n       '2019-11-23T00:00:00.000000000', '2019-12-18T00:00:00.000000000',\n       '2019-12-28T00:00:00.000000000'], dtype='datetime64[ns]')x(x)float643.442e+05 3.442e+05 ... 3.444e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([344155., 344165., 344175., 344185., 344195., 344205., 344215., 344225.,\n       344235., 344245., 344255., 344265., 344275., 344285., 344295., 344305.,\n       344315., 344325., 344335., 344345., 344355.])y(y)float644.626e+06 4.626e+06 ... 4.626e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([4626435., 4626425., 4626415., 4626405., 4626395., 4626385., 4626375.,\n       4626365., 4626355., 4626345., 4626335., 4626325., 4626315., 4626305.,\n       4626295., 4626285., 4626275., 4626265., 4626255.])Data variables: (5)crs()|S1b''crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", -3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32630\"]]spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", -3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32630\"]]array(b'', dtype='|S1')B04(t, y, x)float32nan 1.212e+03 1.226e+03 ... nan nanlong_name :B04units :grid_mapping :crsarray([[[  nan, 1212., 1226., ..., 1064., 1040.,   nan],\n        [  nan, 1214., 1224., ..., 1096., 1054.,   nan],\n        [  nan, 1232., 1222., ..., 1082., 1052.,   nan],\n        ...,\n        [  nan, 1230., 1248., ..., 1186., 1142.,   nan],\n        [  nan, 1262., 1262., ..., 1148., 1142.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 1272., 1258., ..., 1102., 1060.,   nan],\n        [  nan, 1238., 1216., ..., 1094., 1052.,   nan],\n        [  nan, 1266., 1256., ..., 1112., 1096.,   nan],\n        ...,\n        [  nan, 1236., 1202., ..., 1188., 1164.,   nan],\n        [  nan, 1240., 1202., ..., 1162., 1146.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 1262., 1300., ..., 1130., 1124.,   nan],\n        [  nan, 1288., 1300., ..., 1152., 1118.,   nan],\n        [  nan, 1324., 1294., ..., 1146., 1132.,   nan],\n        ...,\n...\n        ...,\n        [  nan, 1108., 1108., ..., 1112., 1112.,   nan],\n        [  nan, 1100., 1126., ..., 1094., 1104.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 1352., 1462., ..., 1332., 1304.,   nan],\n        [  nan, 1284., 1388., ..., 1298., 1250.,   nan],\n        [  nan, 1308., 1294., ..., 1266., 1240.,   nan],\n        ...,\n        [  nan, 1324., 1310., ..., 1334., 1384.,   nan],\n        [  nan, 1330., 1348., ..., 1312., 1412.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 1118., 1102., ..., 1013., 1021.,   nan],\n        [  nan, 1042.,  986., ..., 1030., 1002.,   nan],\n        [  nan, 1066.,  979., ..., 1030., 1015.,   nan],\n        ...,\n        [  nan, 1062., 1106., ..., 1022., 1064.,   nan],\n        [  nan, 1076., 1116., ..., 1030., 1024.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]], dtype=float32)B08(t, y, x)float32nan 1.934e+03 1.964e+03 ... nan nanlong_name :B08units :grid_mapping :crsarray([[[  nan, 1934., 1964., ..., 1916., 1852.,   nan],\n        [  nan, 1990., 2010., ..., 1846., 1754.,   nan],\n        [  nan, 2050., 2064., ..., 1784., 1680.,   nan],\n        ...,\n        [  nan, 2046., 2032., ..., 1784., 1760.,   nan],\n        [  nan, 2078., 2042., ..., 1796., 1766.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 1968., 1992., ..., 1838., 1796.,   nan],\n        [  nan, 2028., 2054., ..., 1808., 1748.,   nan],\n        [  nan, 2082., 2110., ..., 1758., 1782.,   nan],\n        ...,\n        [  nan, 1958., 1888., ..., 1786., 1830.,   nan],\n        [  nan, 1926., 1882., ..., 1802., 1816.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 2120., 2154., ..., 1946., 1898.,   nan],\n        [  nan, 2208., 2214., ..., 1910., 1908.,   nan],\n        [  nan, 2184., 2204., ..., 1908., 1932.,   nan],\n        ...,\n...\n        ...,\n        [  nan, 2008., 2084., ..., 1932., 2020.,   nan],\n        [  nan, 2082., 2080., ..., 1914., 2002.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 2862., 2850., ..., 2730., 2744.,   nan],\n        [  nan, 2820., 2878., ..., 2714., 2668.,   nan],\n        [  nan, 2890., 2972., ..., 2624., 2608.,   nan],\n        ...,\n        [  nan, 2686., 2742., ..., 2630., 2630.,   nan],\n        [  nan, 2674., 2740., ..., 2618., 2658.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan, 2382., 2340., ..., 2248., 2274.,   nan],\n        [  nan, 2252., 2398., ..., 2206., 2222.,   nan],\n        [  nan, 2314., 2424., ..., 2204., 2216.,   nan],\n        ...,\n        [  nan, 2306., 2354., ..., 2152., 2136.,   nan],\n        [  nan, 2302., 2336., ..., 2188., 2132.,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]], dtype=float32)SCL(t, y, x)float32nan 5.0 5.0 5.0 ... nan nan nan nanlong_name :SCLunits :grid_mapping :crsarray([[[nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        ...,\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        ...,\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        ...,\n...\n        ...,\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        ...,\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        ...,\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan,  5.,  5., ...,  5.,  5., nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)NDVI(t, y, x)float32nan 0.2295 0.2313 ... nan nan nanlong_name :NDVIunits :grid_mapping :crsarray([[[       nan, 0.22949778, 0.23134796, ..., 0.28590605,\n         0.28077456,        nan],\n        [       nan, 0.24219726, 0.24304268, ..., 0.25492862,\n         0.24928775,        nan],\n        [       nan, 0.24923827, 0.25623858, ..., 0.24494068,\n         0.22986823,        nan],\n        ...,\n        [       nan, 0.24908425, 0.23902439, ..., 0.2013468 ,\n         0.21295658,        nan],\n        [       nan, 0.24431138, 0.23607749, ..., 0.2201087 ,\n         0.21458046,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan]],\n\n       [[       nan, 0.21481481, 0.22584616, ..., 0.25034013,\n         0.2577031 ,        nan],\n        [       nan, 0.2418861 , 0.25626913, ..., 0.24603721,\n         0.24857143,        nan],\n        [       nan, 0.2437276 , 0.2537136 , ..., 0.2250871 ,\n         0.23835997,        nan],\n...\n        [       nan, 0.33965087, 0.3534057 , ..., 0.32694247,\n         0.31041354,        nan],\n        [       nan, 0.33566433, 0.34050882, ..., 0.33231553,\n         0.3061425 ,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan]],\n\n       [[       nan, 0.36114284, 0.3596746 , ..., 0.3787182 ,\n         0.38027313,        nan],\n        [       nan, 0.36733454, 0.4172577 , ..., 0.3634116 ,\n         0.37841192,        nan],\n        [       nan, 0.36923078, 0.42462534, ..., 0.36301795,\n         0.37171155,        nan],\n        ...,\n        [       nan, 0.36935866, 0.36069363, ..., 0.35601765,\n         0.335     ,        nan],\n        [       nan, 0.36293665, 0.35341832, ..., 0.35985085,\n         0.35107732,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan]]], dtype=float32)Indexes: (3)tPandasIndexPandasIndex(DatetimeIndex(['2019-01-27', '2019-02-11', '2019-02-21', '2019-02-26',\n               '2019-03-03', '2019-03-13', '2019-03-23', '2019-03-28',\n               '2019-04-07', '2019-04-12', '2019-04-27', '2019-05-02',\n               '2019-05-27', '2019-06-01', '2019-06-26', '2019-07-01',\n               '2019-07-11', '2019-07-16', '2019-07-21', '2019-08-05',\n               '2019-08-15', '2019-08-20', '2019-08-30', '2019-09-04',\n               '2019-09-19', '2019-09-29', '2019-10-09', '2019-11-18',\n               '2019-11-23', '2019-12-18', '2019-12-28'],\n              dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Float64Index([344155.0, 344165.0, 344175.0, 344185.0, 344195.0, 344205.0,\n              344215.0, 344225.0, 344235.0, 344245.0, 344255.0, 344265.0,\n              344275.0, 344285.0, 344295.0, 344305.0, 344315.0, 344325.0,\n              344335.0, 344345.0, 344355.0],\n             dtype='float64', name='x'))yPandasIndexPandasIndex(Float64Index([4626435.0, 4626425.0, 4626415.0, 4626405.0, 4626395.0, 4626385.0,\n              4626375.0, 4626365.0, 4626355.0, 4626345.0, 4626335.0, 4626325.0,\n              4626315.0, 4626305.0, 4626295.0, 4626285.0, 4626275.0, 4626265.0,\n              4626255.0],\n             dtype='float64', name='y'))Attributes: (4)Conventions :CF-1.9institution :openEO platform - Geotrellis backend: 0.18.0a1description :title :\n\n\nPlot the raw NDVI time series, averaged across the parcel\n\nraw_ndvi = raw_ndvi_ds.NDVI.rename({\"t\": \"time\"})\n\nfig, ax = plt.subplots(figsize=(15, 5), dpi=120)\n\nraw_ndvi.median(dim=[\"x\", \"y\"]).plot(ax=ax, marker=\"x\", label=\"Raw NDVI\")\nax.legend()\nax.grid()\n\n\n\n\n\n# Make a prediction every 5 days\n# to use the same dates as in the raw time series, just set the `prediction_period` to `None`\nsmoothed = whittaker(raw_ndvi, prediction_period=\"P5D\", smoothing_lambda=10)\n\n\nfig, ax = plt.subplots(figsize=(15, 5), dpi=120)\n\nraw_ndvi.median(dim=[\"x\", \"y\"]).plot(ax=ax, marker=\"x\", label=\"Raw NDVI\", color=\"C0\")\nsmoothed.median(dim=[\"x\", \"y\"]).plot(\n    ax=ax, marker=\"x\", label=\"Smoothed NDVI\", color=\"C1\"\n)\nax.legend()\nax.grid()"
  },
  {
    "objectID": "notebook-samples/openeo/Load_Collection.html",
    "href": "notebook-samples/openeo/Load_Collection.html",
    "title": "openEO Basics: How to load a data cube from a data collection?",
    "section": "",
    "text": "This notebook provides a detailed guide on how to load a DataCube from a data collection. Additionally, it will cover how to authenticate in order to process and download data."
  },
  {
    "objectID": "notebook-samples/openeo/Load_Collection.html#setup",
    "href": "notebook-samples/openeo/Load_Collection.html#setup",
    "title": "openEO Basics: How to load a data cube from a data collection?",
    "section": "Setup",
    "text": "Setup\nImport the openeo package and connect to the Copernicus Data Space Ecosystem openEO back-end.\n\nimport openeo\nimport xarray\nimport matplotlib.pyplot as plt\n\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\nconnection\n\n&lt;Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.1/' with NullAuth&gt;\n\n\nNote the NullAuth in the representation of the connection, which indicates that we are not logged in yet.\nThe canonical way to log in is using the authenticate_oidc() method. This might, depending on your situation, trigger an authentication procedure. Follow the instructions, if any.\n\nconnection.authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n&lt;Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.1/' with OidcBearerAuth&gt;\n\n\nNote that the connection is now authenticated now through OidcBearerAuth."
  },
  {
    "objectID": "notebook-samples/openeo/Load_Collection.html#data-loading",
    "href": "notebook-samples/openeo/Load_Collection.html#data-loading",
    "title": "openEO Basics: How to load a data cube from a data collection?",
    "section": "Data Loading",
    "text": "Data Loading\nWith our authenticated connection, we can now start loading a data collection data to build a DataCube, filtered according to specific spatio-temporal constraints:\n\ns2_cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=(\"2022-05-01\", \"2022-05-30\"),\n    spatial_extent={\n        \"west\": 3.20,\n        \"south\": 51.18,\n        \"east\": 3.25,\n        \"north\": 51.21,\n        \"crs\": \"EPSG:4326\",\n    },\n    bands=[\"B04\", \"B03\", \"B02\", \"SCL\"],\n    max_cloud_cover=50,\n)\n\nLet’s download this slice of data in netCDF format to give it an initial inspection.\n\ns2_cube.download(\"load-raw.nc\")\n\nQuick visualisation of first and last observation in the timeseries.\n\nds = xarray.load_dataset(\"load-raw.nc\")\n# Convert xarray DataSet to a (bands, t, x, y) DataArray\ndata = ds[[\"B04\", \"B03\", \"B02\"]].to_array(dim=\"bands\")\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 3), dpi=90, sharey=True)\ndata[{\"t\": 0}].plot.imshow(vmin=0, vmax=2000, ax=axes[0])\ndata[{\"t\": -1}].plot.imshow(vmin=0, vmax=2000, ax=axes[1]);\n\n\n\n\nNotice how the observation on the right suffers from clouds and cloud shadows."
  },
  {
    "objectID": "notebook-samples/openeo/Load_Collection.html#data-processing",
    "href": "notebook-samples/openeo/Load_Collection.html#data-processing",
    "title": "openEO Basics: How to load a data cube from a data collection?",
    "section": "Data Processing",
    "text": "Data Processing\nLet’s include a bit of extra data processing. We’ll build a naive composite image by taking the temporal maximum:\n\ncomposite = s2_cube.max_time()\n\nDownload this composite and visualize it:\n\ncomposite.download(\"load-composite.nc\")\n\n\nds = xarray.load_dataset(\"load-composite.nc\")\n# Convert xarray DataSet to a (bands, x, y) DataArray\ndata = ds[[\"B04\", \"B03\", \"B02\"]].to_array(dim=\"bands\")\n\nfig, ax = plt.subplots(ncols=1, figsize=(4, 4), dpi=90)\ndata.plot.imshow(vmin=0, vmax=2000, ax=ax)\n\n&lt;matplotlib.image.AxesImage at 0x7f5288567fd0&gt;\n\n\n\n\n\nNote how the clouds influence this composite: while the cloud shadows are eliminated by the max operation, the bright clouds ruin the composite image."
  },
  {
    "objectID": "notebook-samples/openeo/Load_Collection.html#cloud-masking",
    "href": "notebook-samples/openeo/Load_Collection.html#cloud-masking",
    "title": "openEO Basics: How to load a data cube from a data collection?",
    "section": "Cloud masking",
    "text": "Cloud masking\nIn general, to make the raw data more useful, we typically want remove the cloud pixels and only work with non-cloud data. It is very common for earth observation data to have separate masking layers that for instance indicate whether a pixel is covered by a (type of) cloud or not. For Sentinel-2, one such layer is the “scene classification” layer (band name “SCL”) generated by the Sen2Cor algorithm.\nWith openEO and the openEO Python client library, we can take this “SCL” band (which we already included before in the load_collection call) and apply cloud masking as follows.\nFirst we build a binary cloud mask from the SCL values 3 (cloud shadows), 8 (cloud medium probability) and 9 (cloud high probability):\n\nscl_band = s2_cube.band(\"SCL\")\ncloud_mask = (scl_band == 3) | (scl_band == 8) | (scl_band == 9)\n\n# TODO: TEMP WORKAROUND FOR OFFSET ERROR ON SCL BAND\ncloud_mask = (scl_band == 3 - 1000) | (scl_band == 8 - 1000) | (scl_band == 9 - 1000)\n\nBefore we can apply this mask to the cube we have to resample it, as the “SCL” layer has a “ground sample distance” of 20 meter, while it is 10 meter for the “B02”, “B03” and “B04” bands. We can easily do the resampling by referring directly to the data cube to mask.\n\ncloud_mask = cloud_mask.resample_cube_spatial(s2_cube)\n\nApply the cloud mask, and build the composite again:\n\ncube_masked = s2_cube.mask(cloud_mask)\n\ncomposite_masked = cube_masked.max_time()\n\nDownload the result and visualize it.\n\ncomposite_masked.download(\"load-composite-masked.nc\")\n\n\nds = xarray.load_dataset(\"load-composite-masked.nc\")\n# Convert xarray DataSet to a (bands, x, y) DataArray\ndata = ds[[\"B04\", \"B03\", \"B02\"]].to_array(dim=\"bands\")\n\nfig, ax = plt.subplots(ncols=1, figsize=(4, 4), dpi=90)\ndata.plot.imshow(vmin=0, vmax=2000, ax=ax);\n\n\n\n\nThe cloud masking clearly helped to build a better composte. Note however that there are still some artifacts due to the quality of the SCL band and our simple cloud mask."
  },
  {
    "objectID": "notebook-samples/openeo/UDP.html",
    "href": "notebook-samples/openeo/UDP.html",
    "title": "User-Defined Processes (UDP) in openEO",
    "section": "",
    "text": "openEO allows processes to be chained together in a process graph to build a particular algorithm. Often, users have specific (sub)graphs that reoccur in the same process graph or even in different process graphs or algorithms. openEO back-ends allows you to store such subgraphs as so-called “User-Defined Process” (often abbreviated as UDP), and build your library of reusable openEO building blocks.\nThis notebook provides a step-by-step guide on how to create and use a User-Defined Process, for with a Normalized Difference Water Index (NDWI) use case."
  },
  {
    "objectID": "notebook-samples/openeo/UDP.html#set-up",
    "href": "notebook-samples/openeo/UDP.html#set-up",
    "title": "User-Defined Processes (UDP) in openEO",
    "section": "Set up",
    "text": "Set up\nImport a couple of packages and establish an authenticated connection\n\nimport json\nimport openeo\nfrom openeo.api.process import Parameter\n\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\nconnection.authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n&lt;Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.1/' with OidcBearerAuth&gt;"
  },
  {
    "objectID": "notebook-samples/openeo/UDP.html#building-a-parameterized-datacube",
    "href": "notebook-samples/openeo/UDP.html#building-a-parameterized-datacube",
    "title": "User-Defined Processes (UDP) in openEO",
    "section": "Building a parameterized datacube",
    "text": "Building a parameterized datacube\nThe openEO Python client lets you define parameters as Parameter instances (from openeo.api.process subpackage). In general you have to specify at least the parameter name, a description and a schema.\n\ntemporal_extent_param = Parameter(\n    name=\"date_range\",\n    description=\"The date range to load.\",\n    schema={\"type\": \"array\", \"subtype\": \"temporal-interval\"},\n)\n\nspatial_extent_param = Parameter(\n    name=\"bbox\",\n    description=\"The bounding box to load.\",\n    schema={\"type\": \"object\", \"subtype\": \"geojson\"},\n)\n\nUse the parameters directly as arguments to load_collection to build in initial data cube with SENTINEL2_L2A data.\n\nband = [\"B03\", \"B08\"]\ncube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=temporal_extent_param,\n    spatial_extent=spatial_extent_param,\n    bands=band,\n    max_cloud_cover=80,\n)\n\nThe NDWI is a vegetation index sensitive to the water content of vegetation and is complementary to the NDVI. High NDWI values show a high water content of the vegetation. \\[ \\mathrm{NDWI} = \\frac{\\mathrm{Green} - \\mathrm{NIR}}{\\mathrm{Green} + \\mathrm{NIR}} \\]\n\ngreen = cube.band(\"B03\")\nnir = cube.band(\"B08\")\n\nndwi = (green - nir) / (green + nir)\nndwi\n\n\n    \n    \n        \n    \n    \n\n\nNow, let’s produce a temporal aggregation by taking the temporal maximum value.\n\nndwi = ndwi.max_time()"
  },
  {
    "objectID": "notebook-samples/openeo/UDP.html#store-as-user-defined-process-udp",
    "href": "notebook-samples/openeo/UDP.html#store-as-user-defined-process-udp",
    "title": "User-Defined Processes (UDP) in openEO",
    "section": "Store as User-Defined Process (UDP)",
    "text": "Store as User-Defined Process (UDP)\nWe can now store this parametarized data cube representation as a user-defined process called NDWI on the back-end.\n\nconnection.save_user_defined_process(\n    user_defined_process_id=\"NDWI\",\n    process_graph=ndwi,\n    parameters=[temporal_extent_param, spatial_extent_param],\n)"
  },
  {
    "objectID": "notebook-samples/openeo/UDP.html#use-the-udp",
    "href": "notebook-samples/openeo/UDP.html#use-the-udp",
    "title": "User-Defined Processes (UDP) in openEO",
    "section": "Use the UDP",
    "text": "Use the UDP\nNow, let’s evaluate our freshly created user-defined processes “NDWI”. We can using datacube_from_process() to create a DataCube from this process and only have to provide concrete temporal and spatial extents:\n\nndwi2022 = connection.datacube_from_process(\n    process_id=\"NDWI\",\n    date_range=[\"2022-07-19\", \"2022-07-19\"],\n    bbox={\"west\": 5.09, \"south\": 51.18, \"east\": 5.15, \"north\": 51.21},\n)\n\n\nndwi2022.download(\"ndwi2022.tiff\")\n\n\nVisualize the result\n\nimport rasterio\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = rasterio.open(\"ndwi2022.tiff\").read()\n\nfig, ax = plt.subplots(figsize=(6, 4), dpi=90)\nimg = ax.imshow(data[0], vmin=np.percentile(data, 1), vmax=np.percentile(data, 99))\nax.set_title(\"NDWI\")\nplt.colorbar(img)\n\n&lt;matplotlib.colorbar.Colorbar at 0x7f21ae661a90&gt;"
  },
  {
    "objectID": "notebook-samples/sentinelhub/EGU_notebook.html",
    "href": "notebook-samples/sentinelhub/EGU_notebook.html",
    "title": "Why it’s time to stop processing satellite imagery on your laptop",
    "section": "",
    "text": "For this comparison exercise, we run a sample workflow that mimics a typical use case in remote sensing. Based on a pair of Sentinel-2 images, we will create a simple algorithm that detects crop-harvesting events. For this purpose, we will collect Sentinel-2 imagery for two cloud-free dates about 2 months apart in 2023. For each of these dates, we will extract NDVI and identify large decreases in the index between the two dates. To reduce the likelihood of false positives due to crop senescence, we will calculate the BSI and only consider harvested areas at locations where the BSI is above an empirical threshold on the second date.\nNote: The approach is for illustrative purposes only and is not a robust method for recognising harvests. The algorithm is used here to show typical band operations.\n\n\n\nIn a first method, we will look at a “traditional” approach, which consists of the following steps:\n\ndownload the Sentinel-2 scenes covering the area of interest (AOI)\nopen the required bands (B04, B08, B11)\ncropping the bands to the AOI\nmosaic the bands\ncalculation of the NDVI and BSI indices\nexecution of band-wise operations\nfiltering the data to reduce noise\nplotting the resulting map\n\nIn the second part of the notebook, we will obtain the same results by:\n\nrunning a Sentinel Hub request that directly returns the indices of interest\nfiltering the data to reduce noise\nplotting the resulting map\n\n\n\n\nThe following table recapitulates the run times based on the two approaches.\n\n\n\n\nTraditional Workflow\nSentinel Hub Workflow\n\n\n\n\nLines of code (excluding imports)\n~450\n~202\n\n\nRuntime\n~11 minutes\n5 seconds\n\n\nVolume of data downloaded\n4.7 GB\n5.7MB\n\n\n\nThis workflow was run on the Copernicus Data Space Ecosystem JupyterLab:\n\nCPU: 4 cores\nRAM: 16GB\n\nAt the time of the Sentinel-2 scenes download an internet speed test was performed, with the following results:\n\nDownload: 96 Mbps\nUpload: 49 Mbps"
  },
  {
    "objectID": "notebook-samples/sentinelhub/EGU_notebook.html#havent-moved-to-the-cloud-yet-heres-a-good-reason-why-you-should",
    "href": "notebook-samples/sentinelhub/EGU_notebook.html#havent-moved-to-the-cloud-yet-heres-a-good-reason-why-you-should",
    "title": "Why it’s time to stop processing satellite imagery on your laptop",
    "section": "",
    "text": "For this comparison exercise, we run a sample workflow that mimics a typical use case in remote sensing. Based on a pair of Sentinel-2 images, we will create a simple algorithm that detects crop-harvesting events. For this purpose, we will collect Sentinel-2 imagery for two cloud-free dates about 2 months apart in 2023. For each of these dates, we will extract NDVI and identify large decreases in the index between the two dates. To reduce the likelihood of false positives due to crop senescence, we will calculate the BSI and only consider harvested areas at locations where the BSI is above an empirical threshold on the second date.\nNote: The approach is for illustrative purposes only and is not a robust method for recognising harvests. The algorithm is used here to show typical band operations.\n\n\n\nIn a first method, we will look at a “traditional” approach, which consists of the following steps:\n\ndownload the Sentinel-2 scenes covering the area of interest (AOI)\nopen the required bands (B04, B08, B11)\ncropping the bands to the AOI\nmosaic the bands\ncalculation of the NDVI and BSI indices\nexecution of band-wise operations\nfiltering the data to reduce noise\nplotting the resulting map\n\nIn the second part of the notebook, we will obtain the same results by:\n\nrunning a Sentinel Hub request that directly returns the indices of interest\nfiltering the data to reduce noise\nplotting the resulting map\n\n\n\n\nThe following table recapitulates the run times based on the two approaches.\n\n\n\n\nTraditional Workflow\nSentinel Hub Workflow\n\n\n\n\nLines of code (excluding imports)\n~450\n~202\n\n\nRuntime\n~11 minutes\n5 seconds\n\n\nVolume of data downloaded\n4.7 GB\n5.7MB\n\n\n\nThis workflow was run on the Copernicus Data Space Ecosystem JupyterLab:\n\nCPU: 4 cores\nRAM: 16GB\n\nAt the time of the Sentinel-2 scenes download an internet speed test was performed, with the following results:\n\nDownload: 96 Mbps\nUpload: 49 Mbps"
  },
  {
    "objectID": "notebook-samples/sentinelhub/EGU_notebook.html#library-imports",
    "href": "notebook-samples/sentinelhub/EGU_notebook.html#library-imports",
    "title": "Why it’s time to stop processing satellite imagery on your laptop",
    "section": "Library imports",
    "text": "Library imports\n\n# General utilities\nimport getpass\nimport glob\nimport tarfile\nimport warnings\nimport zipfile\nfrom pathlib import Path\n\n# Plotting\nimport folium\nimport geopandas as gpd\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Reading satellite imagery\nimport rasterio as rio\nimport requests\nfrom matplotlib import colors\nfrom rasterio.mask import mask\nfrom rasterio.merge import merge\nfrom rasterio.warp import calculate_default_transform, reproject\nfrom scipy.ndimage import morphology\n\n# Sentinel Hub services\nfrom sentinelhub import (\n    CRS,\n    DataCollection,\n    Geometry,\n    MimeType,\n    SentinelHubRequest,\n    SHConfig,\n)\nfrom shapely.geometry import shape\n\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "notebook-samples/sentinelhub/EGU_notebook.html#getting-started-setup-the-area-of-interest",
    "href": "notebook-samples/sentinelhub/EGU_notebook.html#getting-started-setup-the-area-of-interest",
    "title": "Why it’s time to stop processing satellite imagery on your laptop",
    "section": "Getting started: setup the Area of Interest",
    "text": "Getting started: setup the Area of Interest\nFor this comparison, we will extract satellite data over Volkmarsen and Bad Arolsen in the district of Waldeck-Frankenberg in the state of Hesse in north-west Germany.\n\narea_of_interest = \"data/EGU_aoi.geojson\"\n\naoi = gpd.read_file(area_of_interest)\naoi_simplified = aoi.geometry.simplify(0.001)\naoi[\"geometry\"] = aoi_simplified\naoi[\"area\"] = aoi.area\naoi.explore(\"area\", color=\"Green\", legend=False)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebook-samples/sentinelhub/xcube_on_CDSE.html",
    "href": "notebook-samples/sentinelhub/xcube_on_CDSE.html",
    "title": "Working with xcube on CDSE",
    "section": "",
    "text": "xcube is an open-source Python package and toolkit that has been developed to provide Earth observation (EO) data in an analysis-ready form to users. This is achieved by carefully converting EO data sources into self-contained data cubes (xarray.Datasets).\nThis notebook shows how to: * Access Sentinel Hub with xcube on CDSE * Effectively mask cube data * Develop and run a Python function to compute a new variable that is automatically utilizing multiple threads * Compute Time-series, means, anomalies of a variable\nPlease, refer to the xcube documentation for further information. The xcube package is developed and maintained by Brockmann Consult GmbH and contributors.\n\n\nPrerequisites\n\nInstall the xcube-sh data store plugin to allow accessing Setinel Hub on CDSE\nExecute the following line in the Terminal to install the xcube store for Sentinel Hub into the respective environment:\n$ mamba install xcube-sh -n sentinelhub\nThis step could be omitted if xcube-sh is installed into the environment by default.\n\n\n# xcube imports\nfrom xcube.core.store import new_data_store\nfrom xcube.core.compute import compute_cube\nfrom xcube.core.maskset import MaskSet\n\n# Various utilities\nfrom sentinelhub import SHConfig\n\n\n%matplotlib inline\n\n\n\n\nCredentials\nLoad client_id and client_secret from user’s SHconfig to create a store instance. Note that the credentials they may also be inserted manually here. In a future release of xcube-sh, the endpoint urls may be integrated into a CDSE profile so that users do not have to provide them as parameters here.\nThe creation of a user’s SHconfig is also shown in a Jupyter Notebook under the path: samples/sentinelhub/introduction_to_SH_APIs.ipynb\n\n# Only run this cell if you have not created a configuration.\n\nimport getpass\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n# config.save(\"xcube\")\n\n\n\n\nCreate store instance and get an overview over the available products\nTo be able to access data records with xcube, so-called store instances must be created. These create connections either to the local file system, S3 buckets or various data portals (e.g. Sentinel Hub, CMEMS, CCI etc.). An overview of all stores and their properties can be found here.\n\n# config = SHConfig(\"xcube\")\n\n\nstore = new_data_store(\n    \"sentinelhub\",\n    client_id=config.sh_client_id,\n    client_secret=config.sh_client_secret,\n    instance_url=config.sh_base_url,\n    oauth2_url=config.sh_token_url.rsplit(\"/\", maxsplit=1)[0],\n)\n\nThe following datasets are available through the sentinelhub store:\n\nstore.list_data_ids()\n\n['S2L1C', 'S3OLCI', 'S3SLSTR', 'S1GRD', 'S2L2A', 'S5PL2']\n\n\n\n\n\nStudy area\nFor this demo, we are focussing on the small lake Selenter See near Kiel, Northern Germany (Baltic Sea):\n\nx1 = 10.37  # degree\ny1 = 54.28  # degree\nx2 = 10.52  # degree\ny2 = 54.33  # degree\n\nbbox = x1, y1, x2, y2\n\nLater in this NB we are going to compute some indexes from atmospherically corrected bands B04, B05, B06, B11 of Sentinel-2 (S2L2A). Our time range covers two and a half months of the summer 2018: 2018-05-14 to 2018-07-31\nThe desired resolution is 20 meters per pixel.\n\nspatial_res = 0.00018  # = 20.038 meters in degree\n\n\n\n\nAccess data from sentinelhub store\nGet an insight to the data product you are interested in. In the open_params_schema all parameters that can be used to specify the search for a product are listed. Including a list of variables, that are required for the search.\n\nstore.describe_data(\"S2L2A\")\n\n&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7fb8757c4790&gt;\n\n\n\ncube = store.open_data(\n    \"S2L2A\",\n    variable_names=[\"B04\", \"B05\", \"B06\", \"B11\", \"SCL\", \"CLD\"],\n    bbox=bbox,\n    spatial_res=spatial_res,\n    time_range=[\"2019-07-21\", \"2019-09-21\"],\n    time_period=\"4D\",\n    tile_size=[512, 512],\n)\ncube\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 16, lat: 278, lon: 1024, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 54.33 54.33 54.33 54.33 ... 54.28 54.28 54.28 54.28\n  * lon        (lon) float64 10.37 10.37 10.37 10.37 ... 10.55 10.55 10.55 10.55\n  * time       (time) datetime64[ns] 2019-07-23 2019-07-27 ... 2019-09-21\n    time_bnds  (time, bnds) datetime64[ns] dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    B04        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    B05        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    B06        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    B11        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    CLD        (time, lat, lon) uint8 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    SCL        (time, lat, lon) uint8 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S2L2A Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-05-13T14:42:05.877516\n    time_coverage_start:       2019-07-21T00:00:00+00:00\n    time_coverage_end:         2019-09-23T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P4DT0H0M0S\n    geospatial_lon_min:        10.37\n    geospatial_lat_min:        54.28\n    geospatial_lon_max:        10.554319999999999\n    geospatial_lat_max:        54.330040000000004\n    processing_level:          L2Axarray.DatasetDimensions:time: 16lat: 278lon: 1024bnds: 2Coordinates: (4)lat(lat)float6454.33 54.33 54.33 ... 54.28 54.28units :decimal_degreeslong_name :latitudestandard_name :latitudearray([54.32995, 54.32977, 54.32959, ..., 54.28045, 54.28027, 54.28009])lon(lon)float6410.37 10.37 10.37 ... 10.55 10.55units :decimal_degreeslong_name :longitudestandard_name :longitudearray([10.37009, 10.37027, 10.37045, ..., 10.55387, 10.55405, 10.55423])time(time)datetime64[ns]2019-07-23 ... 2019-09-21standard_name :timebounds :time_bndsarray(['2019-07-23T00:00:00.000000000', '2019-07-27T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-04T00:00:00.000000000',\n       '2019-08-08T00:00:00.000000000', '2019-08-12T00:00:00.000000000',\n       '2019-08-16T00:00:00.000000000', '2019-08-20T00:00:00.000000000',\n       '2019-08-24T00:00:00.000000000', '2019-08-28T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-09-05T00:00:00.000000000',\n       '2019-09-09T00:00:00.000000000', '2019-09-13T00:00:00.000000000',\n       '2019-09-17T00:00:00.000000000', '2019-09-21T00:00:00.000000000'],\n      dtype='datetime64[ns]')time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;standard_name :time\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n256 B\n256 B\n\n\nShape\n(16, 2)\n(16, 2)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\nData variables: (6)\n\n\n\n\n\nB04\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n664.75\n\nwavelength_a :\n\n664.6\n\nwavelength_b :\n\n664.9\n\nbandwidth :\n\n31.0\n\nbandwidth_a :\n\n31\n\nbandwidth_b :\n\n31\n\nresolution :\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB05\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n703.95\n\nwavelength_a :\n\n704.1\n\nwavelength_b :\n\n703.8\n\nbandwidth :\n\n15.5\n\nbandwidth_a :\n\n15\n\nbandwidth_b :\n\n16\n\nresolution :\n\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB06\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n739.8\n\nwavelength_a :\n\n740.5\n\nwavelength_b :\n\n739.1\n\nbandwidth :\n\n15.0\n\nbandwidth_a :\n\n15\n\nbandwidth_b :\n\n15\n\nresolution :\n\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB11\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n1612.05\n\nwavelength_a :\n\n1613.7\n\nwavelength_b :\n\n1610.4\n\nbandwidth :\n\n92.5\n\nbandwidth_a :\n\n91\n\nbandwidth_b :\n\n94\n\nresolution :\n\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nCLD\n\n\n(time, lat, lon)\n\n\nuint8\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nUINT8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.34 MiB\n139.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 2 graph layers\n\n\nData type\nuint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nSCL\n\n\n(time, lat, lon)\n\n\nuint8\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nUINT8\n\nflag_values :\n\n0,1,2,3,4,5,6,7,8,9,10,11\n\nflag_meanings :\n\nno_data saturated_or_defective dark_area_pixels cloud_shadows vegetation bare_soils water clouds_low_probability_or_unclassified clouds_medium_probability clouds_high_probability cirrus snow_or_ice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.34 MiB\n139.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 2 graph layers\n\n\nData type\nuint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([54.329950000000004,           54.32977,           54.32959,\n                 54.32941,           54.32923,           54.32905,\n                 54.32887,           54.32869,           54.32851,\n                 54.32833,\n       ...\n       54.281710000000004, 54.281530000000004,           54.28135,\n                 54.28117,           54.28099,           54.28081,\n                 54.28063,           54.28045,           54.28027,\n                 54.28009],\n      dtype='float64', name='lat', length=278))lonPandasIndexPandasIndex(Index([          10.37009,           10.37027,           10.37045,\n                 10.37063, 10.370809999999999, 10.370989999999999,\n                 10.37117,           10.37135,           10.37153,\n                 10.37171,\n       ...\n       10.552609999999998, 10.552789999999998, 10.552969999999998,\n       10.553149999999999, 10.553329999999999,           10.55351,\n       10.553689999999998, 10.553869999999998, 10.554049999999998,\n       10.554229999999999],\n      dtype='float64', name='lon', length=1024))timePandasIndexPandasIndex(DatetimeIndex(['2019-07-23', '2019-07-27', '2019-07-31', '2019-08-04',\n               '2019-08-08', '2019-08-12', '2019-08-16', '2019-08-20',\n               '2019-08-24', '2019-08-28', '2019-09-01', '2019-09-05',\n               '2019-09-09', '2019-09-13', '2019-09-17', '2019-09-21'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (13)Conventions :CF-1.7title :S2L2A Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S2L2A', 'band_names': ['B04', 'B05', 'B06', 'B11', 'SCL', 'CLD'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 278], 'bbox': [10.37, 54.28, 10.554319999999999, 54.330040000000004], 'spatial_res': 0.00018, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2019-07-21T00:00:00+00:00', '2019-09-21T00:00:00+00:00'], 'time_period': '4 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-05-13T14:42:05.877516time_coverage_start :2019-07-21T00:00:00+00:00time_coverage_end :2019-09-23T00:00:00+00:00time_coverage_duration :P64DT0H0M0Stime_coverage_resolution :P4DT0H0M0Sgeospatial_lon_min :10.37geospatial_lat_min :54.28geospatial_lon_max :10.554319999999999geospatial_lat_max :54.330040000000004processing_level :L2A\n\n\n\n\n\nMasking\nThe band SCL provides scene classification flags. Because this “band” has CF-compliant flag encodings in its metadata attributes, we can interpret them correctly:\n\nscene_classif = MaskSet(cube.SCL)\nscene_classif\n\n\n\n\n\n\nFlag name\nMask\nValue\n\n\nno_data\nNone\n0\n\n\nsaturated_or_defective\nNone\n1\n\n\ndark_area_pixels\nNone\n2\n\n\ncloud_shadows\nNone\n3\n\n\nvegetation\nNone\n4\n\n\nbare_soils\nNone\n5\n\n\nwater\nNone\n6\n\n\nclouds_low_probability_or_unclassified\nNone\n7\n\n\nclouds_medium_probability\nNone\n8\n\n\nclouds_high_probability\nNone\n9\n\n\ncirrus\nNone\n10\n\n\nsnow_or_ice\nNone\n11\n\n\n\n\n\n\n\nxcube mask sets also follow data cube structure:\n\nscene_classif.cirrus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'cirrus' (time: 16, lat: 278, lon: 1024)&gt;\ndask.array&lt;where, shape=(16, 278, 1024), dtype=uint8, chunksize=(1, 278, 512), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float64 54.33 54.33 54.33 54.33 ... 54.28 54.28 54.28 54.28\n  * lon      (lon) float64 10.37 10.37 10.37 10.37 ... 10.55 10.55 10.55 10.55\n  * time     (time) datetime64[ns] 2019-07-23 2019-07-27 ... 2019-09-21xarray.DataArray'cirrus'time: 16lat: 278lon: 1024dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.34 MiB\n139.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 5 graph layers\n\n\nData type\nuint8 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float6454.33 54.33 54.33 ... 54.28 54.28units :decimal_degreeslong_name :latitudestandard_name :latitudearray([54.32995, 54.32977, 54.32959, ..., 54.28045, 54.28027, 54.28009])lon(lon)float6410.37 10.37 10.37 ... 10.55 10.55units :decimal_degreeslong_name :longitudestandard_name :longitudearray([10.37009, 10.37027, 10.37045, ..., 10.55387, 10.55405, 10.55423])time(time)datetime64[ns]2019-07-23 ... 2019-09-21standard_name :timebounds :time_bndsarray(['2019-07-23T00:00:00.000000000', '2019-07-27T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-04T00:00:00.000000000',\n       '2019-08-08T00:00:00.000000000', '2019-08-12T00:00:00.000000000',\n       '2019-08-16T00:00:00.000000000', '2019-08-20T00:00:00.000000000',\n       '2019-08-24T00:00:00.000000000', '2019-08-28T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-09-05T00:00:00.000000000',\n       '2019-09-09T00:00:00.000000000', '2019-09-13T00:00:00.000000000',\n       '2019-09-17T00:00:00.000000000', '2019-09-21T00:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([54.329950000000004,           54.32977,           54.32959,\n                 54.32941,           54.32923,           54.32905,\n                 54.32887,           54.32869,           54.32851,\n                 54.32833,\n       ...\n       54.281710000000004, 54.281530000000004,           54.28135,\n                 54.28117,           54.28099,           54.28081,\n                 54.28063,           54.28045,           54.28027,\n                 54.28009],\n      dtype='float64', name='lat', length=278))lonPandasIndexPandasIndex(Index([          10.37009,           10.37027,           10.37045,\n                 10.37063, 10.370809999999999, 10.370989999999999,\n                 10.37117,           10.37135,           10.37153,\n                 10.37171,\n       ...\n       10.552609999999998, 10.552789999999998, 10.552969999999998,\n       10.553149999999999, 10.553329999999999,           10.55351,\n       10.553689999999998, 10.553869999999998, 10.554049999999998,\n       10.554229999999999],\n      dtype='float64', name='lon', length=1024))timePandasIndexPandasIndex(DatetimeIndex(['2019-07-23', '2019-07-27', '2019-07-31', '2019-08-04',\n               '2019-08-08', '2019-08-12', '2019-08-16', '2019-08-20',\n               '2019-08-24', '2019-08-28', '2019-09-01', '2019-09-05',\n               '2019-09-09', '2019-09-13', '2019-09-17', '2019-09-21'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\nscene_classif.cirrus.plot.imshow(col=\"time\", col_wrap=4, cmap=\"viridis\")\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fb8757c7490&gt;\n\n\n\n\n\nWe can use any of the SCL masks or combinations thereof to mask entire cubes. Here we create a “water cube”:\n\nwater_cube = cube.where(scene_classif.water)\nwater_cube\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 16, lat: 278, lon: 1024, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 54.33 54.33 54.33 54.33 ... 54.28 54.28 54.28 54.28\n  * lon        (lon) float64 10.37 10.37 10.37 10.37 ... 10.55 10.55 10.55 10.55\n  * time       (time) datetime64[ns] 2019-07-23 2019-07-27 ... 2019-09-21\n    time_bnds  (time, bnds) datetime64[ns] dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    B04        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    B05        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    B06        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    B11        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    CLD        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n    SCL        (time, lat, lon) float32 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S2L2A Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-05-13T14:42:05.877516\n    time_coverage_start:       2019-07-21T00:00:00+00:00\n    time_coverage_end:         2019-09-23T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P4DT0H0M0S\n    geospatial_lon_min:        10.37\n    geospatial_lat_min:        54.28\n    geospatial_lon_max:        10.554319999999999\n    geospatial_lat_max:        54.330040000000004\n    processing_level:          L2Axarray.DatasetDimensions:time: 16lat: 278lon: 1024bnds: 2Coordinates: (4)lat(lat)float6454.33 54.33 54.33 ... 54.28 54.28units :decimal_degreeslong_name :latitudestandard_name :latitudearray([54.32995, 54.32977, 54.32959, ..., 54.28045, 54.28027, 54.28009])lon(lon)float6410.37 10.37 10.37 ... 10.55 10.55units :decimal_degreeslong_name :longitudestandard_name :longitudearray([10.37009, 10.37027, 10.37045, ..., 10.55387, 10.55405, 10.55423])time(time)datetime64[ns]2019-07-23 ... 2019-09-21standard_name :timebounds :time_bndsarray(['2019-07-23T00:00:00.000000000', '2019-07-27T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-04T00:00:00.000000000',\n       '2019-08-08T00:00:00.000000000', '2019-08-12T00:00:00.000000000',\n       '2019-08-16T00:00:00.000000000', '2019-08-20T00:00:00.000000000',\n       '2019-08-24T00:00:00.000000000', '2019-08-28T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-09-05T00:00:00.000000000',\n       '2019-09-09T00:00:00.000000000', '2019-09-13T00:00:00.000000000',\n       '2019-09-17T00:00:00.000000000', '2019-09-21T00:00:00.000000000'],\n      dtype='datetime64[ns]')time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;standard_name :time\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n256 B\n256 B\n\n\nShape\n(16, 2)\n(16, 2)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\nData variables: (6)\n\n\n\n\n\nB04\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n664.75\n\nwavelength_a :\n\n664.6\n\nwavelength_b :\n\n664.9\n\nbandwidth :\n\n31.0\n\nbandwidth_a :\n\n31\n\nbandwidth_b :\n\n31\n\nresolution :\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB05\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n703.95\n\nwavelength_a :\n\n704.1\n\nwavelength_b :\n\n703.8\n\nbandwidth :\n\n15.5\n\nbandwidth_a :\n\n15\n\nbandwidth_b :\n\n16\n\nresolution :\n\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB06\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n739.8\n\nwavelength_a :\n\n740.5\n\nwavelength_b :\n\n739.1\n\nbandwidth :\n\n15.0\n\nbandwidth_a :\n\n15\n\nbandwidth_b :\n\n15\n\nresolution :\n\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB11\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nFLOAT32\n\nunits :\n\nreflectance\n\nwavelength :\n\n1612.05\n\nwavelength_a :\n\n1613.7\n\nwavelength_b :\n\n1610.4\n\nbandwidth :\n\n92.5\n\nbandwidth_a :\n\n91\n\nbandwidth_b :\n\n94\n\nresolution :\n\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nCLD\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nUINT8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nSCL\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\nsample_type :\n\nUINT8\n\nflag_values :\n\n0,1,2,3,4,5,6,7,8,9,10,11\n\nflag_meanings :\n\nno_data saturated_or_defective dark_area_pixels cloud_shadows vegetation bare_soils water clouds_low_probability_or_unclassified clouds_medium_probability clouds_high_probability cirrus snow_or_ice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.38 MiB\n556.00 kiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([54.329950000000004,           54.32977,           54.32959,\n                 54.32941,           54.32923,           54.32905,\n                 54.32887,           54.32869,           54.32851,\n                 54.32833,\n       ...\n       54.281710000000004, 54.281530000000004,           54.28135,\n                 54.28117,           54.28099,           54.28081,\n                 54.28063,           54.28045,           54.28027,\n                 54.28009],\n      dtype='float64', name='lat', length=278))lonPandasIndexPandasIndex(Index([          10.37009,           10.37027,           10.37045,\n                 10.37063, 10.370809999999999, 10.370989999999999,\n                 10.37117,           10.37135,           10.37153,\n                 10.37171,\n       ...\n       10.552609999999998, 10.552789999999998, 10.552969999999998,\n       10.553149999999999, 10.553329999999999,           10.55351,\n       10.553689999999998, 10.553869999999998, 10.554049999999998,\n       10.554229999999999],\n      dtype='float64', name='lon', length=1024))timePandasIndexPandasIndex(DatetimeIndex(['2019-07-23', '2019-07-27', '2019-07-31', '2019-08-04',\n               '2019-08-08', '2019-08-12', '2019-08-16', '2019-08-20',\n               '2019-08-24', '2019-08-28', '2019-09-01', '2019-09-05',\n               '2019-09-09', '2019-09-13', '2019-09-17', '2019-09-21'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (13)Conventions :CF-1.7title :S2L2A Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S2L2A', 'band_names': ['B04', 'B05', 'B06', 'B11', 'SCL', 'CLD'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 278], 'bbox': [10.37, 54.28, 10.554319999999999, 54.330040000000004], 'spatial_res': 0.00018, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2019-07-21T00:00:00+00:00', '2019-09-21T00:00:00+00:00'], 'time_period': '4 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-05-13T14:42:05.877516time_coverage_start :2019-07-21T00:00:00+00:00time_coverage_end :2019-09-23T00:00:00+00:00time_coverage_duration :P64DT0H0M0Stime_coverage_resolution :P4DT0H0M0Sgeospatial_lon_min :10.37geospatial_lat_min :54.28geospatial_lon_max :10.554319999999999geospatial_lat_max :54.330040000000004processing_level :L2A\n\n\n\nwater_cube.B04.plot.imshow(col=\"time\", col_wrap=4, vmin=0, vmax=0.05, cmap=\"Greys_r\")\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fb86f8a8e80&gt;\n\n\n\n\n\n\n\n\nCompute Index and generate a new cube\nWe now compute a Chlorophyll indicator called Maximum Chlorophyll Index from bands B04, B05, B06. Note, that it uses the wavelength for the bands as input parameters. The function is called for every data chunk in the cube and returns a chunk for the variable to be computed. Chunks are computed independently and in parallel.\n\ndef compute_mci(b_from, b_peek, b_to, input_params, dim_coords):\n    # The first three arguments are chunks of the three input variables we define below.\n    # You can name them as you like. They are pure 3D numpy arrays.\n\n    # The 'input_params' argument is a standard parameter that we define in the call below.\n    wlen_from = input_params[\"wlen_from\"]\n    wlen_peek = input_params[\"wlen_peek\"]\n    wlen_to = input_params[\"wlen_to\"]\n\n    # The 'dim_coords' argument is optional and provides the coordinate values for all dimension\n    # of the current chunk. We don't use it here, but for many algorithms this is important\n    # information (e.g. looking up aux data).\n    lon, lat = (dim_coords[dim] for dim in (\"lon\", \"lat\"))\n    # print('dim_coords from', lon[0], lat[0], 'to', lon[-1], lat[-1])\n\n    # You can use any popular data packages such as numpy, scipy, dask here,\n    # or we can use ML packages such as scikitlearn!\n    # For simplity, we do some very simple array math here:\n\n    f = (wlen_peek - wlen_from) / (wlen_to - wlen_from)\n    mci = (b_peek - b_from) - f * (b_to - b_from)\n\n    return mci\n\nPrepare input parameters from band attributes:\n\ninput_params = dict(\n    wlen_from=water_cube.B04.attrs[\"wavelength\"],\n    wlen_peek=water_cube.B05.attrs[\"wavelength\"],\n    wlen_to=water_cube.B06.attrs[\"wavelength\"],\n)\ninput_params\n\n{'wlen_from': 664.75, 'wlen_peek': 703.95, 'wlen_to': 739.8}\n\n\n\nmci_cube = compute_cube(\n    compute_mci,\n    water_cube,\n    input_var_names=[\"B04\", \"B05\", \"B06\"],\n    input_params=input_params,\n    output_var_name=\"mci\",\n)\nmci_cube\n\n/opt/conda/envs/sentinelhub/lib/python3.10/site-packages/xcube/core/compute.py:361: RuntimeWarning: Failed to open Zarr store with consolidated metadata, but successfully read with non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n  dataset = xr.open_zarr(store)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 278, lon: 1024, time: 16)\nCoordinates:\n  * lat      (lat) float64 54.33 54.33 54.33 54.33 ... 54.28 54.28 54.28 54.28\n  * lon      (lon) float64 10.37 10.37 10.37 10.37 ... 10.55 10.55 10.55 10.55\n  * time     (time) datetime64[ns] 2019-07-23 2019-07-27 ... 2019-09-21\nData variables:\n    mci      (time, lat, lon) float64 dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;xarray.DatasetDimensions:lat: 278lon: 1024time: 16Coordinates: (3)lat(lat)float6454.33 54.33 54.33 ... 54.28 54.28array([54.32995, 54.32977, 54.32959, ..., 54.28045, 54.28027, 54.28009])lon(lon)float6410.37 10.37 10.37 ... 10.55 10.55array([10.37009, 10.37027, 10.37045, ..., 10.55387, 10.55405, 10.55423])time(time)datetime64[ns]2019-07-23 ... 2019-09-21array(['2019-07-23T00:00:00.000000000', '2019-07-27T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-04T00:00:00.000000000',\n       '2019-08-08T00:00:00.000000000', '2019-08-12T00:00:00.000000000',\n       '2019-08-16T00:00:00.000000000', '2019-08-20T00:00:00.000000000',\n       '2019-08-24T00:00:00.000000000', '2019-08-28T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-09-05T00:00:00.000000000',\n       '2019-09-09T00:00:00.000000000', '2019-09-13T00:00:00.000000000',\n       '2019-09-17T00:00:00.000000000', '2019-09-21T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)mci(time, lat, lon)float64dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n34.75 MiB\n1.09 MiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 19 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([54.329950000000004,           54.32977,           54.32959,\n                 54.32941,           54.32923,           54.32905,\n                 54.32887,           54.32869,           54.32851,\n                 54.32833,\n       ...\n       54.281710000000004, 54.281530000000004,           54.28135,\n                 54.28117,           54.28099,           54.28081,\n                 54.28063,           54.28045,           54.28027,\n                 54.28009],\n      dtype='float64', name='lat', length=278))lonPandasIndexPandasIndex(Index([          10.37009,           10.37027,           10.37045,\n                 10.37063, 10.370809999999999, 10.370989999999999,\n                 10.37117,           10.37135,           10.37153,\n                 10.37171,\n       ...\n       10.552609999999998, 10.552789999999998, 10.552969999999998,\n       10.553149999999999, 10.553329999999999,           10.55351,\n       10.553689999999998, 10.553869999999998, 10.554049999999998,\n       10.554229999999999],\n      dtype='float64', name='lon', length=1024))timePandasIndexPandasIndex(DatetimeIndex(['2019-07-23', '2019-07-27', '2019-07-31', '2019-08-04',\n               '2019-08-08', '2019-08-12', '2019-08-16', '2019-08-20',\n               '2019-08-24', '2019-08-28', '2019-09-01', '2019-09-05',\n               '2019-09-09', '2019-09-13', '2019-09-17', '2019-09-21'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\nmci_cube.mci.attrs[\"long_name\"] = \"Maximum Chlorophyll Index\"\nmci_cube.mci.attrs[\"units\"] = \"unitless\"\nmci_cube.mci\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'mci' (time: 16, lat: 278, lon: 1024)&gt;\ndask.array&lt;transpose, shape=(16, 278, 1024), dtype=float64, chunksize=(1, 278, 512), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float64 54.33 54.33 54.33 54.33 ... 54.28 54.28 54.28 54.28\n  * lon      (lon) float64 10.37 10.37 10.37 10.37 ... 10.55 10.55 10.55 10.55\n  * time     (time) datetime64[ns] 2019-07-23 2019-07-27 ... 2019-09-21\nAttributes:\n    long_name:  Maximum Chlorophyll Index\n    units:      unitlessxarray.DataArray'mci'time: 16lat: 278lon: 1024dask.array&lt;chunksize=(1, 278, 512), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n34.75 MiB\n1.09 MiB\n\n\nShape\n(16, 278, 1024)\n(1, 278, 512)\n\n\nDask graph\n32 chunks in 19 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float6454.33 54.33 54.33 ... 54.28 54.28array([54.32995, 54.32977, 54.32959, ..., 54.28045, 54.28027, 54.28009])lon(lon)float6410.37 10.37 10.37 ... 10.55 10.55array([10.37009, 10.37027, 10.37045, ..., 10.55387, 10.55405, 10.55423])time(time)datetime64[ns]2019-07-23 ... 2019-09-21array(['2019-07-23T00:00:00.000000000', '2019-07-27T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-04T00:00:00.000000000',\n       '2019-08-08T00:00:00.000000000', '2019-08-12T00:00:00.000000000',\n       '2019-08-16T00:00:00.000000000', '2019-08-20T00:00:00.000000000',\n       '2019-08-24T00:00:00.000000000', '2019-08-28T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-09-05T00:00:00.000000000',\n       '2019-09-09T00:00:00.000000000', '2019-09-13T00:00:00.000000000',\n       '2019-09-17T00:00:00.000000000', '2019-09-21T00:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([54.329950000000004,           54.32977,           54.32959,\n                 54.32941,           54.32923,           54.32905,\n                 54.32887,           54.32869,           54.32851,\n                 54.32833,\n       ...\n       54.281710000000004, 54.281530000000004,           54.28135,\n                 54.28117,           54.28099,           54.28081,\n                 54.28063,           54.28045,           54.28027,\n                 54.28009],\n      dtype='float64', name='lat', length=278))lonPandasIndexPandasIndex(Index([          10.37009,           10.37027,           10.37045,\n                 10.37063, 10.370809999999999, 10.370989999999999,\n                 10.37117,           10.37135,           10.37153,\n                 10.37171,\n       ...\n       10.552609999999998, 10.552789999999998, 10.552969999999998,\n       10.553149999999999, 10.553329999999999,           10.55351,\n       10.553689999999998, 10.553869999999998, 10.554049999999998,\n       10.554229999999999],\n      dtype='float64', name='lon', length=1024))timePandasIndexPandasIndex(DatetimeIndex(['2019-07-23', '2019-07-27', '2019-07-31', '2019-08-04',\n               '2019-08-08', '2019-08-12', '2019-08-16', '2019-08-20',\n               '2019-08-24', '2019-08-28', '2019-09-01', '2019-09-05',\n               '2019-09-09', '2019-09-13', '2019-09-17', '2019-09-21'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (2)long_name :Maximum Chlorophyll Indexunits :unitless\n\n\n\nmci_cube.mci.plot.imshow(\n    col=\"time\", col_wrap=4, vmin=-0.001, vmax=0.005, cmap=\"viridis\"\n)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fb86d897790&gt;\n\n\n\n\n\n\n\n\nTime Series\nThe data cube consists of 16 time steps, each representing a four-day period (see Access Data). When plotting time series, gaps may appear between some points/time intervals. This is due to missing values within those intervals, often caused by factors such as cloud cover, which has been masked above (see Masking). Alternatively, scatter plots could be used to avoid these gaps.\nTime series at a given point:\n\nmci_cube.mci.sel(lat=54.31, lon=10.45, method=\"nearest\").plot.line(marker=\"x\")\n\n\n\n\nTime series of the means of each time step:\n\nmci_cube.mci.mean(dim=(\"lat\", \"lon\"), skipna=True).plot.line(marker=\"x\")\n\n\n\n\nMean of all time steps:\n\nmci_mean = mci_cube.mci.mean(dim=\"time\")\n\n\nmci_mean.plot.imshow(vmin=-0.005, vmax=0.005, cmap=\"plasma\", figsize=(16, 10))\n\n&lt;matplotlib.image.AxesImage at 0x7fb86d266cb0&gt;\n\n\n\n\n\nAnomaly w.r.t. to the mean for each time step:\n\nmci_anomaly = mci_cube.mci - mci_mean\n\n\nmci_anomaly.plot.imshow(col=\"time\", col_wrap=4, vmin=-0.005, vmax=0.005, cmap=\"bwr\")\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7fb86d8957b0&gt;\n\n\n\n\n\n\n\n\nExport result cube\nSave the cube locally:\n\nimport shutil\n\nshutil.rmtree(\"mci_cube.zarr\", ignore_errors=True)  # Delete, if already exists\n\n\nmci_cube.to_zarr(\"mci_cube.zarr\")\n\n&lt;xarray.backends.zarr.ZarrStore at 0x7fb86d8e1e00&gt;"
  },
  {
    "objectID": "notebook-samples/sentinelhub/cloudless_process_api.html",
    "href": "notebook-samples/sentinelhub/cloudless_process_api.html",
    "title": "How to access Sentinel-2 Level 3 Cloudless Quarterly Mosaics using the Process API",
    "section": "",
    "text": "For more details on the individual steps in this notebook, compare with the “Introduction to Sentinel Hub API-s” notebook here: First we import dependencies:\n# Utilities\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport getpass\n\nfrom sentinelhub import (\n    SHConfig,\n    DataCollection,\n    SentinelHubCatalog,\n    SentinelHubRequest,\n    SentinelHubStatistical,\n    BBox,\n    bbox_to_dimensions,\n    CRS,\n    MimeType,\n    Geometry,\n)\n\nfrom utils import plot_image"
  },
  {
    "objectID": "notebook-samples/sentinelhub/cloudless_process_api.html#credentials",
    "href": "notebook-samples/sentinelhub/cloudless_process_api.html#credentials",
    "title": "How to access Sentinel-2 Level 3 Cloudless Quarterly Mosaics using the Process API",
    "section": "Credentials",
    "text": "Credentials\nCredentials for Sentinel Hub services (client_id & client_secret) can be obtained in your Dashboard. In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant documentation page.\nIf you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:\n\n# Only run this cell if you have not created a configuration.\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n# config.save(\"cdse\")\n\nHowever, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing profile_name.\n\n# config = SHConfig(\"profile_name\")"
  },
  {
    "objectID": "notebook-samples/sentinelhub/cloudless_process_api.html#setting-an-area-of-interest",
    "href": "notebook-samples/sentinelhub/cloudless_process_api.html#setting-an-area-of-interest",
    "title": "How to access Sentinel-2 Level 3 Cloudless Quarterly Mosaics using the Process API",
    "section": "Setting an area of interest",
    "text": "Setting an area of interest\nThe bounding box in WGS84 coordinate system is [(longitude and latitude coordinates of lower left and upper right corners)]. You can get the bbox for a different area at the bboxfinder website.\nAll requests require a bounding box to be given as an instance of sentinelhub.geometry.BBox with corresponding Coordinate Reference System (sentinelhub.constants.CRS). In our case it is in WGS84 and we can use the predefined WGS84 coordinate reference system from sentinelhub.constants.CRS.\n\naoi_coords_wgs84 = [12.292349, 47.810849, 12.569037, 47.967123]\n\nWhen the bounding box bounds have been defined, you can initialize the BBox of the area of interest. Using the bbox_to_dimensions utility function, you can provide the desired resolution parameter of the image in meters and obtain the output image shape. For a Process API request, the limit is 2500*2500 pixels, if the output below has larger values, you have to limit the bouding box or reduce the resolution.\n\nresolution = 100\naoi_bbox = BBox(bbox=aoi_coords_wgs84, crs=CRS.WGS84)\naoi_size = bbox_to_dimensions(aoi_bbox, resolution=resolution)\n\nprint(f\"Image shape at {resolution} m resolution: {aoi_size} pixels\")\n\nImage shape at 100 m resolution: (213, 167) pixels"
  },
  {
    "objectID": "notebook-samples/sentinelhub/cloudless_process_api.html#process-api-request-for-cloudless-mosaics-as-a-byoc",
    "href": "notebook-samples/sentinelhub/cloudless_process_api.html#process-api-request-for-cloudless-mosaics-as-a-byoc",
    "title": "How to access Sentinel-2 Level 3 Cloudless Quarterly Mosaics using the Process API",
    "section": "Process API request for Cloudless mosaics as a BYOC",
    "text": "Process API request for Cloudless mosaics as a BYOC\n\nExample 1: True Color Image\nWe build the request according to the API Reference, using the SentinelHubRequest class. Each Process API request also needs an evalscript.\nThe information that we specify in the SentinelHubRequest object is: - an evalscript, - a list of input data collections with time interval, - a format of the response, - a bounding box and its size (size or resolution). .\nThe evalscript in the example is used to select the appropriate bands. We return the RGB (B04, B03, B02) Sentinel-2 L2A bands with some contrast enhancement.\n\nHere the data collection is defined as a BYOC (Bring Your own COG). The collection ID for this collection can be found here:\n\n\nS2l3_cloudless_mosaic = DataCollection.define_byoc(\n    collection_id=\"5460de54-082e-473a-b6ea-d5cbe3c17cca\"\n)\n\nThen, we define the evalscript. The evalscript can be copied directly from the browser, eg. this scene (click on the &lt;/&gt; icon for the selected layer): It is different from the generic true color evalscript for a single Sentinel-2 image, since it has to take care of contrast enhancement to provide a visually pleasing image for a large area.\n\nevalscript_true_color = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B04\",\"B03\",\"B02\", \"dataMask\"],\n    output: { bands: 4 }\n  };\n}\n\n// Contrast enhance / highlight compress\n\nconst maxR = 3.0; // max reflectance\nconst midR = 0.13;\nconst sat = 1.2;\nconst gamma = 1.8;\nconst scalefac = 10000;\n\nfunction evaluatePixel(smp) {\n  const rgbLin = satEnh(sAdj(smp.B04/scalefac), sAdj(smp.B03/scalefac), sAdj(smp.B02/scalefac));\n  return [sRGB(rgbLin[0]), sRGB(rgbLin[1]), sRGB(rgbLin[2]), smp.dataMask];\n}\n\nfunction sAdj(a) {\n  return adjGamma(adj(a, midR, 1, maxR));\n}\n\nconst gOff = 0.01;\nconst gOffPow = Math.pow(gOff, gamma);\nconst gOffRange = Math.pow(1 + gOff, gamma) - gOffPow;\n\nfunction adjGamma(b) {\n  return (Math.pow((b + gOff), gamma) - gOffPow)/gOffRange;\n}\n\n// Saturation enhancement\nfunction satEnh(r, g, b) {\n  const avgS = (r + g + b) / 3.0 * (1 - sat);\n  return [clip(avgS + r * sat), clip(avgS + g * sat), clip(avgS + b * sat)];\n}\n\nfunction clip(s) {\n  return s &lt; 0 ? 0 : s &gt; 1 ? 1 : s;\n}\n\n//contrast enhancement with highlight compression\nfunction adj(a, tx, ty, maxC) {\n  var ar = clip(a / maxC, 0, 1);\n  return ar * (ar * (tx/maxC + ty -1) - ty) / (ar * (2 * tx/maxC - 1) - tx/maxC);\n}\n\nconst sRGB = (c) =&gt; c &lt;= 0.0031308 ? (12.92 * c) : (1.055 * Math.pow(c, 0.41666666666) - 0.055);\"\"\"\n\nNow we run the Process API Request. We set - the evalscript to the script we defined in the cell above, - the input data - collection to the cloudless mosaic BYOC collection we defined in the previous cell, - the time of interest directly in the request parameter - responses of the script - the bounding box and AOI size as defined above - and finally the credentials as defined in the config (the second cell) - optionally, you can also give a location to save data by mentioning the path (data_folder='./data')\n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=S2l3_cloudless_mosaic,\n            time_interval=(\"2023-04-01\", \"2023-04-02\"),\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=aoi_bbox,\n    size=aoi_size,\n    config=config,\n    data_folder=\"./data\",\n)\n\nThe method get_data() will always return a list of length 1 with the available image from the requested time interval in the form of numpy arrays.If you want to save the data, make sure to set save_data=True\n\ntrue_color_imgs = request_true_color.get_data(save_data=True)\n\n\nprint(\n    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n)\nprint(\n    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n)\n\nReturned data is of type = &lt;class 'list'&gt; and length 1.\nSingle element in the list is of type &lt;class 'numpy.ndarray'&gt; and has shape (167, 213, 4)\n\n\nNow we call the plot function to print the image that we defined above.\n\nimage = true_color_imgs[0]\nprint(f\"Image type: {image.dtype}\")\n\n# plot function\n# factor 1/255 to scale between 0-1\n# factor 1 to keep brightness scaling that was already handled in the evalscript\nplot_image(image, factor=1 / 255, clip_range=(0, 1))\n\nImage type: uint8"
  },
  {
    "objectID": "notebook-samples/sentinelhub/cloudless_process_api.html#summary",
    "href": "notebook-samples/sentinelhub/cloudless_process_api.html#summary",
    "title": "How to access Sentinel-2 Level 3 Cloudless Quarterly Mosaics using the Process API",
    "section": "Summary",
    "text": "Summary\nSo what have we learnt in this notebook?\n\nHow to modify a Process API request to access a BYOC dataset such as Sentinel-2 Level3 Cloud Free Mosaics.\nVisualising the derived image in a simple way"
  },
  {
    "objectID": "notebook-samples/sentinelhub/soil_erosion_risk.html",
    "href": "notebook-samples/sentinelhub/soil_erosion_risk.html",
    "title": "Estimation of erosion risk based on bare soil periods and digital elevation model slope",
    "section": "",
    "text": "The universal soil loss equation (Wischmeier and Smith 1978) calculates soil loss per unit area based on precipitation and runoff, soil erodibility, slope length, slope steepness, land cover and management and support practices. This notebook shows how to calculate a simplified estimate based on a combination of terrain steepness and the number of days without vegetation cover. It aims to illustrate the accessibility of such datasets in the Copernicus Data Space Ecosystem, which can be complemented by local information on the other factors such as precipitation or soil properties. The notebook uses the Sentinel Hub APIs to access the data and evaluation scripts to perform the calculations on the server side. No downloading of data is needed. For a more detailed introduction to the Sentinel Hub APIs, please refer to this document.\nIn the first step, the dependencies are imported, including getpass for managing credentials, matplotlib for visualization of images, and a number of functions from the Sentinel Hub package.\nimport getpass\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sentinelhub import (\n    CRS,\n    BBox,\n    DataCollection,\n    MimeType,\n    MosaickingOrder,\n    SentinelHubRequest,\n    SentinelHubStatistical,\n    SHConfig,\n)"
  },
  {
    "objectID": "notebook-samples/sentinelhub/soil_erosion_risk.html#get-the-number-of-bare-soil-days-over-a-certain-time-interval",
    "href": "notebook-samples/sentinelhub/soil_erosion_risk.html#get-the-number-of-bare-soil-days-over-a-certain-time-interval",
    "title": "Estimation of erosion risk based on bare soil periods and digital elevation model slope",
    "section": "Get the number of bare soil days over a certain time interval",
    "text": "Get the number of bare soil days over a certain time interval\nEvalscripts are short sections of code that perform a pixel-by-pixel mathematical operation on the spectral bands of an image or series of images. More information on evalscripts functions and features can be found in the documentation here and here respectively.\nThis evalscript combines cloud masking based on scene classification of Level-2 pixels with bare soil detection based on the Barren Soil Custom Script, and outputs counts of days of bare and vegetated soil within the requested time frame.\n\nevalscript_bare_soil = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B04\", \"B08\", \"B11\", \"B12\", \"SCL\", \"dataMask\"],\n    output: { bands: 3, sampleType: \"UINT16\" },\n    mosaicking: \"ORBIT\",\n  };\n}\n\nfunction isCloud(sample) {\n  // Define codes as invalid:\n  const invalid = [\n    0, // NO_DATA\n    1, // SATURATED_DEFECTIVE\n    3, // CLOUD_SHADOW\n    7, // CLOUD_LOW_PROBA\n    8, // CLOUD_MEDIUM_PROBA\n    9, // CLOUD_HIGH_PROBA\n    10, // THIN_CIRRUS\n  ];\n  return !invalid.includes(sample.SCL);\n}\n\nfunction evaluatePixel(samples) {\n  let [nBare, nNonBare, nCloud] = [0, 0, 0];\n  for (let i = 0; i &lt; samples.length; i++) {\n    let s = samples[i];\n    if (s.dataMask === 0) {\n      continue;\n    }\n    if (isCloud(s)) {\n      nCloud++;\n      continue;\n    }\n    // bareness index\n    let mbi =\n      (2.5 * (s.B11 + s.B04 - (s.B08 + s.B02))) /\n      (s.B11 + s.B04 + (s.B08 + s.B02));\n    if (mbi &gt; 0) {\n      nBare++;\n    } else {\n      nNonBare++;\n    }\n  }\n  return [nBare, nNonBare, nCloud];\n}\n\n\"\"\"\n\nIn the next step, we define a Sentinel Hub Process API request. For this request, we use the evalscript defined above for counting bare soil days, together with the bounding box and time range already defined. The authorization for this request is handled via the config.\nFor additional information on the Process API you can find documentation and additional code examples here.\n\nrequest = SentinelHubRequest(\n    evalscript=evalscript_bare_soil,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                \"s2l2a\", service_url=config.sh_base_url\n            ),\n            time_interval=time_interval,\n            mosaicking_order=MosaickingOrder.MOST_RECENT,\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=bbox,\n    resolution=resolution,\n    config=config,\n    data_folder=\"./data\",\n)\n\n\nbare_soil = request.get_data(save_data=True)[0]\n\nThen we calculate the ratio of bare soil days to total cloud-free days for each pixel: this bare soil ratio is our final output parameter. We use matplotlib pyplot to visualize an image of this index for the study area.\nBlue areas have no bare soil but are covered by vegetation during the whole studied period. Yellow areas have high bare soil ratio, potentially open soil during the whole studied period.\n\ntotal_clear = bare_soil[:, :, 1] + bare_soil[:, :, 0]\nbare_ratio = bare_soil[:, :, 0] / total_clear\n\n# show image of bare soil ratio for the area of interest.\n\nplt.figure(figsize=(12, 12))\nplt.imshow(bare_ratio, cmap=\"cividis\");"
  },
  {
    "objectID": "notebook-samples/sentinelhub/soil_erosion_risk.html#get-terrain-slope-for-the-same-area",
    "href": "notebook-samples/sentinelhub/soil_erosion_risk.html#get-terrain-slope-for-the-same-area",
    "title": "Estimation of erosion risk based on bare soil periods and digital elevation model slope",
    "section": "Get terrain slope for the same area",
    "text": "Get terrain slope for the same area\nCopernicus Data Space Ecosystem allows users to upload their own data in a Cloud Optimized Geotiff (COG) format, ingesting it with the Bring Your Own COG (“BYOC”) API. Once a dataset is ingested, in can be made available privately or publicly, and other Sentinel Hub API requests can access the data collection via its BYOC collection ID (example here)\nTerrain slopes were calculated using the GDALDEM slope function, running on the full global 30 meter Copernicus DEM dataset. The following command was used:\n`gdaldem slope input_dem.tif slope-byoc.tif -of COG -co COMPRESS=DEFLATE -co BLOCKSIZE=1024 -co RESAMPLING=NEAREST -co OVERVIEWS=IGNORE_EXISTING` \nThis dataset of slopes is made available in Copernicus Data Space Ecosystem as a public BYOC collection, making streamlined querying and processing possible.\nHere we also define a simple evalscript that returns slope angle values directly from the dataset\n\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"slope\", \"dataMask\"],\n    output: { bands: 1, sampleType: \"FLOAT32\" },\n  };\n}\n\nfunction evaluatePixel(samples) {\n  return [samples.slope];\n}\n\"\"\"\n\nThen we define a Sentinel Hub Process API request again that calls data from the custom BYOC collection holding the slope data. We use the previously defined bounding box and time interval.\n\nslope_collection = DataCollection.define_byoc(\n    \"f57baa78-b28b-4bf1-b6b1-cc26d292007e\", service_url=config.sh_base_url\n)\nslope_request = SentinelHubRequest(\n    evalscript=evalscript,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=slope_collection,\n            time_interval=time_interval,\n            mosaicking_order=MosaickingOrder.MOST_RECENT,\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=bbox,\n    resolution=resolution,\n    config=config,\n    data_folder=\"./data\",\n)\n\nWe collect raster slope data of the area of interest and timeframe from the Process API request into a the variable slope\n\nslope = slope_request.get_data(save_data=True)[0]\n\nWe visualize an image of the slopevariable using pyplot again.\n\nplt.figure(figsize=(12, 12))\nplt.imshow(slope, cmap=\"cividis\");\n\n\n\n\nThis image shows the terrain patterns and drainage network of the area. Forested areas have noisy topography, and forest edges stand out as narrow lines of steeper slopes, due to constraints of the Copernicus 30 DEM terrain dataset."
  },
  {
    "objectID": "notebook-samples/sentinelhub/soil_erosion_risk.html#calculate-relative-erosion-risk",
    "href": "notebook-samples/sentinelhub/soil_erosion_risk.html#calculate-relative-erosion-risk",
    "title": "Estimation of erosion risk based on bare soil periods and digital elevation model slope",
    "section": "Calculate relative erosion risk",
    "text": "Calculate relative erosion risk\nTo estimate erosion risk at a specific location and time, we simply calculate the product of the terrain slope and the ratio of days with bare soil. The respective weighting of these two parameters can be modified by the user.\n\nWEIGHT_BARE_SOIL = 1\nWEIGHT_SLOPE = 1\nrelative_erosion_risk = (slope / 90) * WEIGHT_SLOPE * bare_ratio * WEIGHT_BARE_SOIL\n\nTo optimize the visualization, the minimum and maximum of the erosion risk within the area and timeframe of interest is calculated below. You can set the visualization parameters vmin and vmax according to the minimum and maximum statistics to scale visualization of the result. Finally, an image is created showing the estimated erosion risk for each pixel.\n\nrelative_erosion_risk.min()\n\n0.0\n\n\n\nrelative_erosion_risk.max()\n\n0.02239825470106942\n\n\n\n# First we define a colormap. White for risk of zero, green for low risk, yellow for moderate, red for high.\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\n    \"green yellow red\",\n    [\n        (0, (1, 1, 1)),\n        (0.001, (0, 0.5, 0)),\n        (relative_erosion_risk.max(), (1, 1, 0)),\n        (1, (1, 0, 0)),\n    ],\n    N=256,\n)\n\n\nplt.figure(figsize=(12, 12))\n\nplt.imshow(relative_erosion_risk, cmap=cmap)\n\n&lt;matplotlib.image.AxesImage at 0x11159653b90&gt;\n\n\n\n\n\nNow we prepare a side-by-side comparison of a true colour image of the same area and the erosion risk estimation map. The true colour image will be based on the quarterly cloudless Sentinel-2 mosaic of the same area and time - another public BYOC collection. The visualization is done via a Process API request, based on the data source collection and the evalscript for the true colour visualisation.\nFirst we define the data collection - the cloudless mosaic.\n\nS2l3_cloudless_mosaic = DataCollection.define_byoc(\n    collection_id=\"5460de54-082e-473a-b6ea-d5cbe3c17cca\"\n)\n\nThen we define an evalscript for the true color image.\n\nevalscript_true_color = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B04\",\"B03\",\"B02\", \"dataMask\"],\n    output: { bands: 4 }\n  };\n}\n\n// Contrast enhance / highlight compress\n\nconst maxR = 3.0; // max reflectance\nconst midR = 0.13;\nconst sat = 1.2;\nconst gamma = 1.8;\nconst scalefac = 10000;\n\nfunction evaluatePixel(smp) {\n  const rgbLin = satEnh(sAdj(smp.B04/scalefac), sAdj(smp.B03/scalefac), sAdj(smp.B02/scalefac));\n  return [sRGB(rgbLin[0]), sRGB(rgbLin[1]), sRGB(rgbLin[2]), smp.dataMask];\n}\n\nfunction sAdj(a) {\n  return adjGamma(adj(a, midR, 1, maxR));\n}\n\nconst gOff = 0.01;\nconst gOffPow = Math.pow(gOff, gamma);\nconst gOffRange = Math.pow(1 + gOff, gamma) - gOffPow;\n\nfunction adjGamma(b) {\n  return (Math.pow((b + gOff), gamma) - gOffPow)/gOffRange;\n}\n\n// Saturation enhancement\nfunction satEnh(r, g, b) {\n  const avgS = (r + g + b) / 3.0 * (1 - sat);\n  return [clip(avgS + r * sat), clip(avgS + g * sat), clip(avgS + b * sat)];\n}\n\nfunction clip(s) {\n  return s &lt; 0 ? 0 : s &gt; 1 ? 1 : s;\n}\n\n//contrast enhancement with highlight compression\nfunction adj(a, tx, ty, maxC) {\n  var ar = clip(a / maxC, 0, 1);\n  return ar * (ar * (tx/maxC + ty -1) - ty) / (ar * (2 * tx/maxC - 1) - tx/maxC);\n}\n\nconst sRGB = (c) =&gt; c &lt;= 0.0031308 ? (12.92 * c) : (1.055 * Math.pow(c, 0.41666666666) - 0.055);\"\"\"\n\nFinally, we create a Process API request to get a true color image for the study area.\n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=S2l3_cloudless_mosaic,\n            time_interval=time_interval,\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=bbox,\n    resolution=(10, 10),\n    config=config,\n    data_folder=\"./data\",\n)\n\nWe load the data from the request into a variable again.\n\ntrue_color_imgs = request_true_color.get_data(save_data=True)\nimage = true_color_imgs[0]\n\nAnd we create a graphic of two subplots, one from the true colour image defined above, and the other from the scaled soil erosion risk map.\n\nplt.figure(figsize=(12, 24))\n\nplt.subplot(121)\nplt.imshow(image)\n\nplt.subplot(122)\nplt.imshow(relative_erosion_risk, cmap=cmap)\n\nplt.show\n\nThe resulting map of estimated erosion risk shows that forested areas have a low erosion risk, although they often occupy the steep slopes unsuitable for cultivation. The agricultural areas with steeper slopes near the valleys sides have the highest erosion risk according to this simple model.\nThis notebook shows how to use the Copernicus Data Space Ecosystem Process API to integrate image and DEM data. The API allows querying a time series of images to quantify the number of days with vegetation and bare soil respectively. Also, DEM slopes from a custom data collection can be queried from the same area. Finally these can be combined in a data product that provides a first estimation of soil erosion risk for the date and location. This workflow can be carried out anywhere in the world as a first check, but for quantitative analysis purposes, must be complemented by local information on precipitation, cultivation and soil type."
  },
  {
    "objectID": "notebook-samples/sentinelhub/soil_erosion_risk.html#adjusting-the-data-fusion-evalscript-for-statistics-api",
    "href": "notebook-samples/sentinelhub/soil_erosion_risk.html#adjusting-the-data-fusion-evalscript-for-statistics-api",
    "title": "Estimation of erosion risk based on bare soil periods and digital elevation model slope",
    "section": "Adjusting the data fusion evalscript for statistics API",
    "text": "Adjusting the data fusion evalscript for statistics API\nStatistics API is an API that allows calculation of statistical parameters of pixels or pixel time series in a streamlined way, directly outputting a table of the requested statistical metrics. These can be calculated both for an area of interest on a single image and a time series of images. For more details on Statistics API, check the documentation and some code examples here.\nIn order to run Statistics API requests with this evalscript, we need to modify it to provide erosion risk as the default output, complemented with a data mask.\n\nstat_evalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s2\",\n        bands: [\"B02\", \"B04\", \"B08\", \"B11\", \"B12\", \"SCL\", \"dataMask\"],\n        mosaicking: \"ORBIT\",\n      },\n      {\n        datasource: \"slope\",\n        bands: [\"slope\", \"dataMask\"],\n        mosaicking: \"SIMPLE\",\n      },\n    ],\n    output: [\n      {\n        id: \"erosion_risk\",\n        bands: [\"ErosionRisk\"]\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }\n    ]\n  };\n}\n\nconst WEIGHT_BARE_SOIL = 1;\nconst WEIGHT_SLOPE = 1;\n\nfunction isCloud(sample) {\n  // Define codes as invalid:\n  const invalid = [\n    0, // NO_DATA\n    1, // SATURATED_DEFECTIVE\n    3, // CLOUD_SHADOW\n    7, // CLOUD_LOW_PROBA\n    8, // CLOUD_MEDIUM_PROBA\n    9, // CLOUD_HIGH_PROBA\n    10, // THIN_CIRRUS\n  ];\n  return !invalid.includes(sample.SCL);\n}\n\nfunction evaluatePixel(samples) {\n  let [nBare, nNonBare, nCloud] = [0, 0, 0];\n  for (let i = 0; i &lt; samples.s2.length; i++) {\n    let s = samples.s2[i];\n    if (s.dataMask === 0) {\n      continue;\n    }\n    if (isCloud(s)) {\n      nCloud++;\n      continue;\n    }\n    // bareness index\n    let mbi =\n      (2.5 * (s.B11 + s.B04 - (s.B08 + s.B02))) /\n      (s.B11 + s.B04 + (s.B08 + s.B02));\n    if (mbi &gt; 0) {\n      nBare++;\n    } else {\n      nNonBare++;\n    }\n  }\n  const bareRatio = nBare / (nBare + nNonBare);\n  const normalizedSlope = samples.slope[0].slope / 90;\n  const erosionRisk =\n    bareRatio * WEIGHT_BARE_SOIL * normalizedSlope * WEIGHT_SLOPE;\n  return {\n      erosion_risk: [erosionRisk],\n      dataMask: [samples.slope[0].dataMask]\n    };\n}\n\n\"\"\"\n\nNow that we have the modified evalscript, we define a process API request. In order to save data processing capacity, first we define a smaller bounding box of a few parcels. For the Process API request, it is important that we set the aggregation_interval to P1M, which means aggregation for monthly intervals. We then define the two sources of input data: Sentinel-2 L2A and the slope dataset defined from the BYOC collection before.\n\nsmaller_bbox = BBox(\n    (7.485831, 48.563885, 7.491689, 48.566001), crs=CRS.WGS84\n).transform(3035)\n\nerosion_stat = SentinelHubStatistical(\n    aggregation=SentinelHubStatistical.aggregation(\n        evalscript=stat_evalscript,\n        time_interval=time_interval,\n        aggregation_interval=\"P1M\",\n        resolution=(20, 20),\n    ),\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                \"s2l2a\", service_url=config.sh_base_url\n            ),\n            time_interval=time_interval,\n            mosaicking_order=MosaickingOrder.MOST_RECENT,\n            identifier=\"s2\",\n        ),\n        SentinelHubRequest.input_data(\n            data_collection=slope_collection,\n            time_interval=time_interval,\n            mosaicking_order=MosaickingOrder.MOST_RECENT,\n            identifier=\"slope\",\n        ),\n    ],\n    bbox=bbox,\n    config=config,\n)\n\n\nerosion_stats = erosion_stat.get_data()\n\nThe raw output of the statistics API is a set of statistics (min, max, mean, stDev, samplecount, and nodatacount), calculated for each month.\n\nerosion_stats\n\n[{'data': [{'interval': {'from': '2023-04-01T00:00:00Z',\n     'to': '2023-05-01T00:00:00Z'},\n    'outputs': {'erosion_risk': {'bands': {'ErosionRisk': {'stats': {'min': 0.0,\n         'max': 0.04839124530553818,\n         'mean': 0.00120173637284527,\n         'stDev': 0.0032264424239037037,\n         'sampleCount': 141512,\n         'noDataCount': 0}}}}}},\n   {'interval': {'from': '2023-05-01T00:00:00Z', 'to': '2023-06-01T00:00:00Z'},\n    'outputs': {'erosion_risk': {'bands': {'ErosionRisk': {'stats': {'min': 0.0,\n         'max': 0.03731662034988403,\n         'mean': 0.0009038782154514808,\n         'stDev': 0.002478568673644408,\n         'sampleCount': 141512,\n         'noDataCount': 0}}}}}},\n   {'interval': {'from': '2023-06-01T00:00:00Z', 'to': '2023-07-01T00:00:00Z'},\n    'outputs': {'erosion_risk': {'bands': {'ErosionRisk': {'stats': {'min': 0.0,\n         'max': 0.08981089293956757,\n         'mean': 0.001446468833208199,\n         'stDev': 0.004060823818060092,\n         'sampleCount': 141512,\n         'noDataCount': 0}}}}}},\n   {'interval': {'from': '2023-07-01T00:00:00Z', 'to': '2023-08-01T00:00:00Z'},\n    'outputs': {'erosion_risk': {'bands': {'ErosionRisk': {'stats': {'min': 0.0,\n         'max': 0.07137357443571091,\n         'mean': 0.0022370956518501695,\n         'stDev': 0.0051342346270820275,\n         'sampleCount': 141512,\n         'noDataCount': 0}}}}}},\n   {'interval': {'from': '2023-08-01T00:00:00Z', 'to': '2023-09-01T00:00:00Z'},\n    'outputs': {'erosion_risk': {'bands': {'ErosionRisk': {'stats': {'min': 0.0,\n         'max': 0.06835917383432388,\n         'mean': 0.0020893739542424536,\n         'stDev': 0.005594925166278125,\n         'sampleCount': 141512,\n         'noDataCount': 0}}}}}}],\n  'status': 'OK'}]\n\n\n\n# define functions to extract statistics for all acquisition dates\n\n\ndef extract_stats(date, stat_data):\n    d = {}\n    for key, value in stat_data[\"outputs\"].items():\n        stats = value[\"bands\"][\"ErosionRisk\"][\"stats\"]\n        if stats[\"sampleCount\"] != stats[\"noDataCount\"]:\n            d[\"date\"] = [date]\n            for stat_name, stat_value in stats.items():\n                if stat_name not in [\"sampleCount\", \"noDataCount\"]:\n                    d[f\"{key}_{stat_name}\"] = [stat_value]\n    return pd.DataFrame(d)\n\n\ndef read_acquisitions_stats(stat_data):\n    df_li = []\n    for aq in stat_data:\n        date = aq[\"interval\"][\"from\"][:10]\n        df_li.append(extract_stats(date, aq))\n    return pd.concat(df_li)\n\n\nresult_df1 = read_acquisitions_stats(erosion_stats[0][\"data\"])\nresult_df1\n\n\n\n\n\n\n\n\ndate\nerosion_risk_min\nerosion_risk_max\nerosion_risk_mean\nerosion_risk_stDev\n\n\n\n\n0\n2023-04-01\n0.0\n0.048391\n0.001202\n0.003226\n\n\n0\n2023-05-01\n0.0\n0.037317\n0.000904\n0.002479\n\n\n0\n2023-06-01\n0.0\n0.089811\n0.001446\n0.004061\n\n\n0\n2023-07-01\n0.0\n0.071374\n0.002237\n0.005134\n\n\n0\n2023-08-01\n0.0\n0.068359\n0.002089\n0.005595\n\n\n\n\n\n\n\n\nfig_stat, ax_stat = plt.subplots(1, 1, figsize=(12, 6))\nt1 = result_df1[\"date\"]\nmean_field1 = result_df1[\"erosion_risk_mean\"]\nstd_field1 = result_df1[\"erosion_risk_stDev\"]\nax_stat.plot(t1, mean_field1, label=\"field 1 mean\")\nax_stat.fill_between(\n    t1,\n    mean_field1 - std_field1,\n    mean_field1 + std_field1,\n    alpha=0.3,\n    label=\"field 1 stDev\",\n)\nax_stat.tick_params(axis=\"x\", labelrotation=30, labelsize=12)\nax_stat.tick_params(axis=\"y\", labelsize=12)\nax_stat.set_xlabel(\"Date\", size=15)\nax_stat.set_ylabel(\"Erosion Risk/unitless\", size=15)\nax_stat.legend(loc=\"lower right\", prop={\"size\": 12})\nax_stat.set_title(\"Erosion Risk time series\", fontsize=20)\nfor label in ax_stat.get_xticklabels()[1::2]:\n    label.set_visible(False)"
  },
  {
    "objectID": "notebook-samples/sentinelhub/from_browser_to_jupyter.html",
    "href": "notebook-samples/sentinelhub/from_browser_to_jupyter.html",
    "title": "Observing increase in damage done by Wildfires in Greece - from browsing to coding",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem supports learning programming for Earth Observation data analysis in series of incremental steps. This notebook takes you from viewing satellite imagery and experimenting with different visualizations through understanding API requests in a graphical interface, to writing and editing your own code in Jupyter Notebooks."
  },
  {
    "objectID": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#step-1-copernicus-browser",
    "href": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#step-1-copernicus-browser",
    "title": "Observing increase in damage done by Wildfires in Greece - from browsing to coding",
    "section": "Step 1: Copernicus Browser",
    "text": "Step 1: Copernicus Browser\nTo get first impression of what wildfires look like on Sentinel-2 imagery, take a look at the extensive forest fires in Canada here - the link will take you directly to the same scene as the image below, in the Copernicus Browser.\n\n\n\nTrue Color Sentinel-2 image of Wildfires near Lac de la Frégate, Quebec, Canada\n\n\nYou can experiment with different visualizations in the layer list in the sidebar. Which one provides the most intuitive view of the fire activity? Probably the SWIR visualisation, which is based on bands B12, B8A and B4. It shows the thermal activity of the fire and partly avoids the obscuring effect of the smoke.\n\n\n\nSWIR composite Sentinel-2 image of wildfires near Lac de la Frégate, Quebec, Canada\n\n\nIt is also possible to create new custom visualisations using the Browser’s composite tool: Here you can drag and drop individual image bands onto the Red, Green and Blue channels or the operands of a spectral index.\n\n\n\nHere you can test various visualizations in a graphical interface\n\n\nFinally, you can edit the actual visualisation code in Javascript by using the Custom Script window. Custom scripts - or evalscripts as we call them when editing API requests - are short pieces of code that combine a mathematical operation performed on each pixel of the image (essentially a spectral index) and a visualisation that we perform on the results (a palette). Each visualisation layer has its own custom script that can be read and written directly in the Browser.\n\n\n\nHere you can edit the visualization code directly\n\n\nIn addition, there is a large repository of Custom Scripts that contains evalscript code for many different use cases across a range of satellite datasets and application areas.\n\nVisualizing wildfire effects with the Burnt Area Visualization Custom Script\nThe Burnt Area Visualization Custom Script combines the Normalized Differential Vegetation Index (NDVI), the Normalized Difference Moisture Index (NDMI) and a custom index using bands B12, B11 and B08 to detect burnt areas.\n//VERSION=3\n// Burned area detection\n// Author: Monja B. Šebela\n\nfunction setup() {\n    return {\n        input: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"dataMask\"],\n        output: { bands: 4 }\n      };\n}\n\nfunction evaluatePixel(samples) {\n    var NDWI=index(samples.B03, samples.B08); \n    var NDVI=index(samples.B08, samples.B04);\n    var INDEX= ((samples.B11 - samples.B12) / (samples.B11 + samples.B12))+(samples.B08);\n\n    if((INDEX&gt;0.1)||(samples.B02&gt;0.1)||(samples.B11&lt;0.1)||(NDVI&gt;0.3)||(NDWI &gt; 0.1)){\n        return[2.5*samples.B04, 2.5*samples.B03, 2.5*samples.B02, samples.dataMask]\n    }\n    else {\n    return [1, 0, 0, samples.dataMask]\n    }\n}\nThe script creates a small decision tree that combines different thresholds applied to these indices and highlights the areas identified as burnt in red. Copy the script into the Custom Script panel of the Copernicus Browser to see how the burnt areas are highlighted. Try adjusting the individual thresholds to see if you can improve the detection of burnt area!\n\n\n\nVisualization of burnt area in Canada using a custom script\n\n\nIn the next step, you will examine this time series of burnt area from the example of the 2023 wildfires near Alexandropouli, Greece:\n\n\n\nVisualization of burnt area in Greece using a custom script\n\n\nNote that the custom script in the Browser has an additional output: in the setup section, besides “default”, “burnMask” is also defined, with the values calculated from the decision tree: 0 if the area is not burned, 1 if it is burnt. This property of the script will be useful when we switch to Requests Builder and Jupyter Notebooks."
  },
  {
    "objectID": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#step-2-request-builder",
    "href": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#step-2-request-builder",
    "title": "Observing increase in damage done by Wildfires in Greece - from browsing to coding",
    "section": "Step 2: Request Builder",
    "text": "Step 2: Request Builder\nRequest Builder is an online graphical interface to the Sentinel Hub API-s. This tool makes it easier to create and debug API requests, and supports the export of the resulting code in various programming languages. In this tutorial, we will create a Process API request for downloading raster images of the burnt area from the location of the wildfire in Greece that we have already examined in the Browser. Just like a Process API request in code, a request created with the Requests Builder consists of 5 main parts: - Data Collection - Time Range - Area of Interest - Output - Evalscript\n\n\n\nScreenshot of Requests Builder for the Alexandropouli wildfire test location\n\n\nThese can be set individually in the interface. Use the following settings for the Alexandropouli Wildfire example here:\n\nData Collection: sentinel-2 l2a\nTime Range: From 12.09.2023 to 13.09.2023\nArea of Interest: [25.558398, 40.806995, 26.298798, 41.270524]\n\nClick Parse to parse the area of interest - it should be displayed in the map window and zoom in to the rectangle of interest.\n\nEvalscript: use the evalscript from the previous Copernicus Browser example:\n\nfunction setup() {\n    return {\n        input: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"dataMask\"],\n        output: [\n            { id: \"default\", bands: 4 },\n            { id: \"burnMask\", bands: 1, sampleType: \"UINT8\" },\n        ]\n      };\n}\nfunction evaluatePixel(samples) {\n    var NDWI=index(samples.B03, samples.B08); \n    var NDVI=index(samples.B08, samples.B04);\n    var INDEX= ((samples.B11 - samples.B12) / (samples.B11 + samples.B12))+(samples.B08);\n\n    if((INDEX&gt;0.1)||(samples.B02&gt;0.1)||(samples.B11&lt;0.1)||(NDVI&gt;0.3)||(NDWI &gt; 0.1)){\n        return{\n            default:[2.5*samples.B04, 2.5*samples.B03, 2.5*samples.B02, samples.dataMask],\n            burnMask:[0]\n        }\n    }\n    else {\n    return {\n        default: [1, 0, 0, samples.dataMask],\n        burnMask: [1]}\n    }\n}\n\nOutput: Remember, the evalscript has two different outputs, default with 4 bands (Red, Green, Blue and Transparency/Data Mask), and burnMask, which returns 1 where the area is burnt and 0 where it is not. We need to set up the output so that a raster is created from these two outputs for each image. As the area of interest is rather large and the limit for a single request is 2500×2500 pixels, we will reduce the resolution to 40 meters. First click on the resolution tab, then select\n\nRes X in meters: 40\nRes Y in meters: 40\nImage format: TIFF\nIdentifier: default\n\n\nNow you need to click on the Add Response button to create a second output dataset. This should have the same format and be named according to the second output in the evalscript: - Image format: TIFF - Identifier: burnMask\nFinally, you must set the language of the Request Preview to python-requests in the dropdown menu. This will create request code you can later use in the Jupyter Notebook as well.\nIf you now click on Send request, you will be prompted to save the request and download the response. The response is a .tar file containing default.tif and burnMask.tif. To download raster datasets of the burnt area for a time series, you only need to change the Time Range parameter and repeat running the request. Possible dates can be the following:\n\n03.08.2023 - 04.08.2023\n23.08.2023 - 24.08.2023\n28.08.2023 - 29.08.2023\n02.09.2023 - 03.08.2023\n12.08.2023 - 13.08.2023\n\nThe result is a series of TIFF files, including burnt/non-burnt masks, which you can process locally in GIS software to calculate quantitative results. But more importantly, you can copy the code from the Request Preview window and use it in a Jupyter Notebook of the same test case."
  },
  {
    "objectID": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#step-3-jupyter-notebooks",
    "href": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#step-3-jupyter-notebooks",
    "title": "Observing increase in damage done by Wildfires in Greece - from browsing to coding",
    "section": "Step 3: Jupyter notebooks",
    "text": "Step 3: Jupyter notebooks\n\nImporting necessary libraries\nTo run this example, you do not need any additional GIS-specific libraries. You can of course improve the workflow with additional libraries, but for someone new to earth observation coding, this basic notebook could be a good place to start.\n\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tarfile\nimport getpass\n\n\n\nCredentials\nYou can obtain credentials for the Sentinel Hub services (client_id & client_secret) by navigating to your Dashboard. In the User Settings, you can create a new OAuth Client to generate these credentials. More detailed instructions can be found on the corresponding documentation page.\nOnce you run the next cell, you will be prompted to enter the client_id and client_secret.\n\n# Your client credentials\n# client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\n\nIf you have the credentials, you will need a session token to make requests. This token is generated in the following function.\n\ndef getauth_token():\n    # Create a session\n    client = BackendApplicationClient(client_id=client_id)\n    oauth = OAuth2Session(client=client)\n    # Get token for the session\n    token = oauth.fetch_token(\n        token_url=\"https://identity.cloudferro.com/auth/realms/CDSE/protocol/openid-connect/token\",\n        client_id=client_id,\n        client_secret=client_secret,\n    )\n    # All requests using this session will have an access token automatically added\n\n    return oauth"
  },
  {
    "objectID": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#defining-time-slots",
    "href": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#defining-time-slots",
    "title": "Observing increase in damage done by Wildfires in Greece - from browsing to coding",
    "section": "Defining time slots",
    "text": "Defining time slots\nUsing the Requests Builder and the Browser, we can see all the images acquisitions that match our criteria. In the following cell, we enter these time slots to create a time series and understand the extent of the damage caused.\n\nslots = [\n    (\"2023-08-03\", \"2023-08-04\"),\n    (\"2023-08-23\", \"2023-08-24\"),\n    (\"2023-08-28\", \"2023-08-29\"),\n    (\"2023-09-02\", \"2023-09-03\"),\n    (\"2023-09-12\", \"2023-09-13\"),\n]\nprint(\"Time Slots:\\n\")\nfor slot in slots:\n    print(slot[0])\n\nNext, we can enter the evalscript for Burnt Area Mapping that we used earlier in the Browser and Requests Builder - feel free to copy it from the Browser window.\n\nevalscript = \"\"\"\n//VERSION=3\n// Burneed area detection\n// Author: Monja B. Šebela\n\nfunction setup() {\n    return {\n        input: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"dataMask\"],\n        output: [\n            { id: \"default\", bands: 3 },\n            { id: \"burnMask\", bands: 1, sampleType: \"UINT8\" },\n        ]\n      };\n}\n\nfunction evaluatePixel(samples) {\n    var NDWI=index(samples.B03, samples.B08); \n    var NDVI=index(samples.B08, samples.B04);\n    var INDEX= ((samples.B11 - samples.B12) / (samples.B11 + samples.B12))+(samples.B08);\n\n    if((INDEX&gt;0.15)||(samples.B02&gt;0.1)||(samples.B11&lt;0.1)||(NDVI&gt;0.3)||(NDWI &gt; 0.1)){\n        return{\n            default:[2.5*samples.B04, 2.5*samples.B03, 2.5*samples.B02, samples.dataMask],\n            burnMask:[0]\n        }\n    }\n    else {\n    return {\n        default: [1, 0, 0],\n        burnMask: [1]}\n    }\n}\n\n\"\"\"\n\nHere, we have defined a function that contains the request code made in the Requests Builder and references the evalscript defined above. Since we are requesting multiple files in the output, we can expect a compressed .tar file containing the files.\n\ndef get_request(slot):\n    request = {\n        \"input\": {\n            \"bounds\": {\n                \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n                \"bbox\": [25.558398, 40.806995, 26.298798, 41.270524],\n            },\n            \"data\": [\n                {\n                    \"dataFilter\": {\n                        \"timeRange\": {\n                            \"from\": slot[0] + \"T00:00:00Z\",\n                            \"to\": slot[1] + \"T00:00:00Z\",\n                        }\n                    },\n                    \"type\": \"sentinel-2-l2a\",\n                }\n            ],\n        },\n        \"output\": {\n            \"width\": 1247.7098306086236,\n            \"height\": 1031.9962449583074,\n            \"responses\": [\n                {\n                    \"identifier\": \"default\",\n                    \"format\": {\"type\": \"image/tiff\"},\n                },\n                {\n                    \"identifier\": \"burnMask\",\n                    \"format\": {\"type\": \"image/tiff\"},\n                },\n            ],\n        },\n        \"evalscript\": evalscript,\n    }\n\n    url = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\n    response = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n    # print(response.status_code)\n\n    if response.status_code in (200,):\n        with open(f\"tarfile_wildfire_{slot[0]}.tar\", \"wb\") as tarfile:\n            tarfile.write(response.content)\n\n    return response\n\nNow, we are ready to make the request for each of the previously defined time slots. We first get the token created with the OAuth client before making the request and loop through the request for all time slots. This cell might take 10-20 seconds as this is where the requests are actually performed. If it runs successfully, we should see that the compressed .tar files are created in the local folder.\n\noauth = getauth_token()\nresponses = [get_request(slot) for slot in slots]\n\nTo further display and analyse the images further, we need to extract the files. This cell does exactly that and we can see the names of the files in the extracted folder as a result.\n\nfor slot in slots:\n    # open file\n    file = tarfile.open(f\"tarfile_wildfire_{slot[0]}.tar\")\n\n    # print file names\n    print(file.getnames())\n\n    # extract files\n    file.extractall(f\"./wildfire_{slot[0]}\")\n\n    # close file\n    file.close()\n\nNow, we create a series of plots to display the images we have requested. If we look at them together, we can see how the damaged area has increased over time.\n\nncols = 3\nnrows = 2\naspect_ratio = 1000 / 1000\nsubplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\n\nfig, axs = plt.subplots(\n    ncols=ncols,\n    nrows=nrows,\n    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n    subplot_kw=subplot_kw,\n)\n\nfor idx, slot in enumerate(slots):\n    img = plt.imread(f\"wildfire_{slot[0]}/default.tif\")\n    ax = axs[idx // ncols][idx % ncols]\n    ax.imshow(img)\n    ax.set_title(f\"{slot[0]}  -  {slot[1]}\", fontsize=10)\n\nplt.tight_layout()\n\nWe make a similar visualization with the binary mask that represents the burnt area.\n\nncols = 3\nnrows = 2\naspect_ratio = 1000 / 1000\nsubplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\n\nfig, axs = plt.subplots(\n    ncols=ncols,\n    nrows=nrows,\n    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n    subplot_kw=subplot_kw,\n)\n\nfor idx, slot in enumerate(slots):\n    img = plt.imread(f\"wildfire_{slot[0]}/burnMask.tif\")\n    ax = axs[idx // ncols][idx % ncols]\n    ax.imshow(img, cmap=\"gray\")\n    ax.set_title(f\"{slot[0]}  -  {slot[1]}\", fontsize=10)\n\nplt.tight_layout()\n\nTo calculate the burnt area in square meters, we perform a simple count of the pixels labelled as burnt area. From the binary burnMask, we can define a function that calculates the number of pixels that have been classified as burnt regions and multiplies this by the resolution to get the area. The next cell runs this function that takes the mask and the resolution (in meters) as the input and outputs the total area for each time slot.\n\ndef burnt_area(burnMask, resolution):\n    burn_pixel_count = np.sum(burnMask)\n    burnt_area = burn_pixel_count * (resolution * resolution) / 1000000\n\n    return burnt_area\n\n\nburnt_area_arr = []\nfor idx, slot in enumerate(slots):\n    burnMask = plt.imread(f\"wildfire_{slot[0]}/burnMask.tif\")\n    burnt_area_arr.append(burnt_area(burnMask, resolution=50))\n    print(\n        f\"The total burnt area is approximately {round(burnt_area(burnMask,resolution = 50),1)} km\\u00b2\"\n    )\n\nWe can visualise the increasing damaged area by plotting these burnt pixels on a simple line chart as shown below.\n\nxlabels = [slot[0] for slot in slots]\nx = range(len(slots))\nplt.plot(range(len(slots)), burnt_area_arr)\nplt.title(\"Time Series of area burnt in the wildfires.\")\nplt.xticks(np.arange(0, 5, step=1), xlabels, rotation=30, ha=\"center\")\nplt.xlabel(\"Time slots\")\nplt.ylabel(\"Area burnt (in $km^2$)\")\nplt.show()"
  },
  {
    "objectID": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#summary",
    "href": "notebook-samples/sentinelhub/from_browser_to_jupyter.html#summary",
    "title": "Observing increase in damage done by Wildfires in Greece - from browsing to coding",
    "section": "Summary",
    "text": "Summary\nThis was a brief guide on how to go from viewing satellite imagery in the Copernicus Browser using Custom Scripts and requests from the Requests Builder to preparing a Jupyter Notebook to begin your analysis. From here, you can analyse the pixels, derive statistics and create a workflow that is suitable for your problem. The next step could be to familiarize yourself with various other Sentinel Hub API-s, using this notebook example. You can find more information about the different APIs and various examples in the documentation. Check out the Custom Scripts Repository for more examples of evalscripts for a wide range of applications."
  },
  {
    "objectID": "notebook-samples/sentinelhub/migration_from_scihub_guide.html",
    "href": "notebook-samples/sentinelhub/migration_from_scihub_guide.html",
    "title": "Migrating your workflows from The Copernicus Open Access Hub to the Copernicus Data Space Ecosystem",
    "section": "",
    "text": "The purpose of this notebook is to demonstrate how easy it is to migrate your workflow from accessing the data through the Copernicus Open Access Hub to using APIs to access the data via the Copernicus Data Space Ecosystem. In this notebook, we will show you how to: - setup your credentials - search, discover and download Sentinel-2 L2A Granules using Open Data Protocol (OData) - search, discover and download Sentinel-2 L2A data using Sentinel Hub APIs.\nFirst we need to import some prerequisite libraries:\n# Utilities\nimport os\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nimport getpass\n\nfrom sentinelhub import (\n    SHConfig,\n    DataCollection,\n    SentinelHubCatalog,\n    SentinelHubRequest,\n    BBox,\n    bbox_to_dimensions,\n    CRS,\n    MimeType,\n    Geometry,\n)\n\nfrom utils import plot_image\n\n/Users/williamray/miniconda3/envs/sentinelhub_base/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "notebook-samples/sentinelhub/migration_from_scihub_guide.html#odata",
    "href": "notebook-samples/sentinelhub/migration_from_scihub_guide.html#odata",
    "title": "Migrating your workflows from The Copernicus Open Access Hub to the Copernicus Data Space Ecosystem",
    "section": "OData",
    "text": "OData\nOData (Open Data Protocol) is a standard that specifies a variety of best practices for creating and using REST APIs. OData makes it possible to build REST-based data services that let Web clients publish and edit resources that are recognized by Uniform Resource Locators (URLs) and described in a data model using straightforward HTTP messages. This is the method you will want to use if your workflow requires the download of full products/granules/tiles from Open Access Hub. In the following cells we will show you how to do the same with Copernicus Data Space Ecosystem. The documentation regarding this can be found here.\nIn this example, we will search the catalogue, generate the required credentials and then download a Sentinel-2 L2A granule using this protocol:\n\nSetting our search parameters\nFirstly, we need to define our start_date and end_date, the data_collection and the area of interest (aoi). We define them in the next cell and will insert them into our request as string variables.\n\nstart_date = \"2022-06-01\"\nend_date = \"2022-06-10\"\ndata_collection = \"SENTINEL-2\"\naoi = \"POLYGON((4.220581 50.958859,4.521264 50.953236,4.545977 50.906064,4.541858 50.802029,4.489685 50.763825,4.23843 50.767734,4.192435 50.806369,4.189689 50.907363,4.220581 50.958859))'\"\n\nTo search the catalogue we use the following code block:\n\njson = requests.get(\n    f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{aoi}) and ContentDate/Start gt {start_date}T00:00:00.000Z and ContentDate/Start lt {end_date}T00:00:00.000Z\"\n).json()\npd.DataFrame.from_dict(json[\"value\"]).head(5)\n\n\n\n\n\n\n\n\n@odata.mediaContentType\nId\nName\nContentType\nContentLength\nOriginDate\nPublicationDate\nModificationDate\nOnline\nEvictionDate\nS3Path\nChecksum\nContentDate\nFootprint\nGeoFootprint\n\n\n\n\n0\napplication/octet-stream\n4b4c6531-ad85-588d-8088-102e7ac30abc\nS2B_MSIL1C_20220604T104619_N0400_R051_T31UES_2...\napplication/octet-stream\n0\n2022-06-04T14:48:01.546Z\n2022-06-04T14:55:02.871Z\n2022-06-04T14:55:02.871Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2022/06/04/S2B_MSIL...\n[]\n{'Start': '2022-06-04T10:46:19.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((2.9997121758513 ...\n{'type': 'Polygon', 'coordinates': [[[2.999712...\n\n\n1\napplication/octet-stream\nacdd7b9a-a5d4-5d10-9ac8-554623b8a0c9\nS2B_MSIL2A_20220604T104619_N0400_R051_T31UES_2...\napplication/octet-stream\n0\n2022-06-04T19:59:16.441Z\n2022-06-04T20:08:35.955Z\n2022-06-04T20:08:35.955Z\nTrue\n\n/eodata/Sentinel-2/MSI/L2A/2022/06/04/S2B_MSIL...\n[]\n{'Start': '2022-06-04T10:46:19.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((2.9997121758513 ...\n{'type': 'Polygon', 'coordinates': [[[2.999712...\n\n\n2\napplication/octet-stream\nf2790789-52f5-571a-9a61-17650852d9bd\nS2A_MSIL1C_20220609T104631_N0400_R051_T31UES_2...\napplication/octet-stream\n0\n2022-06-09T17:48:40.156Z\n2022-06-09T17:56:27.164Z\n2022-06-09T17:56:27.164Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2022/06/09/S2A_MSIL...\n[]\n{'Start': '2022-06-09T10:46:31.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((2.9997121758513 ...\n{'type': 'Polygon', 'coordinates': [[[2.999712...\n\n\n3\napplication/octet-stream\na5f89f85-362d-5bb3-a321-403044ca872e\nS2A_MSIL2A_20220609T104631_N0400_R051_T31UES_2...\napplication/octet-stream\n0\n2022-06-09T19:18:07.672Z\n2022-06-10T06:39:41.624Z\n2022-06-10T06:39:41.624Z\nTrue\n\n/eodata/Sentinel-2/MSI/L2A/2022/06/09/S2A_MSIL...\n[]\n{'Start': '2022-06-09T10:46:31.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((2.9997121758513 ...\n{'type': 'Polygon', 'coordinates': [[[2.999712...\n\n\n4\napplication/octet-stream\nf83b6e00-8669-5f6c-96e1-f8c82dd98909\nS2B_MSIL1C_20220601T103629_N0400_R008_T31UES_2...\napplication/octet-stream\n0\n2022-06-01T15:17:31.386Z\n2022-06-01T15:22:27.713Z\n2022-06-01T15:22:27.713Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2022/06/01/S2B_MSIL...\n[]\n{'Start': '2022-06-01T10:36:29.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((4.0716897587579 ...\n{'type': 'Polygon', 'coordinates': [[[4.071689...\n\n\n\n\n\n\n\n\n\nRetrieving your Access Token\nWhilst no credentials are needed to search the catalogue; in order to download products from Copernicus Data Space Ecosystem catalogue using OData and OpenSearch API users are required to have an Access token. This token can be generated in both Linux and Window OS using either cURL or python script. You can generate this token using the following code block (more information can be found in our documentation).\nTo obtain your token, you are required to provide your Copernicus Data Space Ecosystem username and password. In this example, we import them from a Python file called creds.py and import the credentials as variables into the get_access_token() function (To run the following cell yourself, you will need to create this file yourself).\n\n# Import credentials\n# from creds import *\n\n\ndef get_access_token(username: str, password: str) -&gt; str:\n    data = {\n        \"client_id\": \"cdse-public\",\n        \"username\": username,\n        \"password\": password,\n        \"grant_type\": \"password\",\n    }\n    try:\n        r = requests.post(\n            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n            data=data,\n        )\n        r.raise_for_status()\n    except Exception as e:\n        raise Exception(\n            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n        )\n    return r.json()[\"access_token\"]\n\n\naccess_token = get_access_token(\"USERNAME\", \"PASSWORD\")\n\n\naccess_token = get_access_token(\n    getpass.getpass(\"Enter your username\"),\n    getpass.getpass(\"Enter your password\"),\n)\n\nEnter your username ········\nEnter your password ········\n\n\nOnce you have your token, you require a product Id which can be found in the response of the products search: https://catalogue.dataspace.copernicus.eu/odata/v1/Products. This can either be parsed or copied from the json response we have already generated. We insert the product Id into the url parameter in the below cell.\nRunning the cell will download the zipped Sentinel-2 L2A Granule to the same directory as the Jupyter Notebook:\n\nurl = f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products(acdd7b9a-a5d4-5d10-9ac8-554623b8a0c9)/$value\"\n\nheaders = {\"Authorization\": f\"Bearer {access_token}\"}\n\nsession = requests.Session()\nsession.headers.update(headers)\nresponse = session.get(url, headers=headers, stream=True)\n\nwith open(\"product.zip\", \"wb\") as file:\n    for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n            file.write(chunk)"
  },
  {
    "objectID": "notebook-samples/sentinelhub/migration_from_scihub_guide.html#accessing-data-via-sentinel-hub-apis",
    "href": "notebook-samples/sentinelhub/migration_from_scihub_guide.html#accessing-data-via-sentinel-hub-apis",
    "title": "Migrating your workflows from The Copernicus Open Access Hub to the Copernicus Data Space Ecosystem",
    "section": "Accessing data via Sentinel Hub APIs",
    "text": "Accessing data via Sentinel Hub APIs\nThe Sentinel Hub API is a RESTful API interface that provides access to various satellite imagery archives. It allows you to access raw satellite data, rendered images, statistical analysis, and other features.\nIn these examples, we will be using the sentinelhub python package. The sentinelhub Python package is the official Python interface for Sentinel Hub services. The package provides a collection of basic tools and utilities for working with geospatial and satellite data. It builds on top of well known packages such as numpy, shapely, pyproj.\nTo successfully run this notebook, make sure that you install or upgrade to at least Version 3.9.1.\n\nCredentials\nCredentials for Sentinel Hub services (client_id & client_secret) can be obtained in your Dashboard. In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant documentation page.\nNow that you have your client_id & client_secret, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found here. Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won’t need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook.\nIf you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:\n\n# Only run this cell if you have not created a configuration.\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n# config.save(\"cdse\")\n\nHowever, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing profile_name.\n\n# config = SHConfig(\"profile_name\")\n\n\n\nSetting an area of interest\nThe bounding box in WGS84 coordinate system is [(longitude and latitude coordinates of lower left and upper right corners)]. You can get the bbox for a different area at the bboxfinder website.\nAll requests require a bounding box to be given as an instance of sentinelhub.geometry.BBox with corresponding Coordinate Reference System (sentinelhub.constants.CRS). In our case it is in WGS84 and we can use the predefined WGS84 coordinate reference system from sentinelhub.constants.CRS.\n\naoi_coords_wgs84 = [4.20762, 50.764694, 4.487708, 50.916455]\n\nWhen the bounding box bounds have been defined, you can initialize the BBox of the area of interest. Using the bbox_to_dimensions utility function, you can provide the desired resolution parameter of the image in meters and obtain the output image shape.\n\nresolution = 10\naoi_bbox = BBox(bbox=aoi_coords_wgs84, crs=CRS.WGS84)\naoi_size = bbox_to_dimensions(aoi_bbox, resolution=resolution)\n\nprint(f\"Image shape at {resolution} m resolution: {aoi_size} pixels\")\n\nImage shape at 10 m resolution: (1941, 1723) pixels\n\n\n\n\nCatalog API\nTo search and discover data, you can use the Catalog API. Sentinel Hub Catalog API (or shortly “Catalog”) is an API implementing the STAC Specification, providing geospatial information for data available in Sentinel Hub. Firstly, to initialise the SentinelHubCatalog class we will use:\n\ncatalog = SentinelHubCatalog(config=config)\n\nNow we can build the Catalog API request; to do this we use the aoi_bbox we defined earlier as well as time_interval and insert these into the request:\n\naoi_bbox = BBox(bbox=aoi_coords_wgs84, crs=CRS.WGS84)\ntime_interval = \"2022-07-01\", \"2022-07-20\"\n\nsearch_iterator = catalog.search(\n    DataCollection.SENTINEL2_L2A,\n    bbox=aoi_bbox,\n    time=time_interval,\n    fields={\"include\": [\"id\", \"properties.datetime\"], \"exclude\": []},\n)\n\nresults = list(search_iterator)\nprint(\"Total number of results:\", len(results))\n\nresults\n\nTotal number of results: 16\n\n\n[{'id': 'S2A_MSIL2A_20220719T105041_N0400_R051_T31UES_20220719T170208.SAFE',\n  'properties': {'datetime': '2022-07-19T10:57:07.477Z'}},\n {'id': 'S2A_MSIL2A_20220719T105041_N0400_R051_T31UFS_20220719T170208.SAFE',\n  'properties': {'datetime': '2022-07-19T10:57:03.296Z'}},\n {'id': 'S2A_MSIL2A_20220716T103641_N0400_R008_T31UES_20220716T183414.SAFE',\n  'properties': {'datetime': '2022-07-16T10:47:11.385Z'}},\n {'id': 'S2A_MSIL2A_20220716T103641_N0400_R008_T31UFS_20220716T183414.SAFE',\n  'properties': {'datetime': '2022-07-16T10:47:07.153Z'}},\n {'id': 'S2B_MSIL2A_20220714T104629_N0400_R051_T31UES_20220714T134329.SAFE',\n  'properties': {'datetime': '2022-07-14T10:57:00.047Z'}},\n {'id': 'S2B_MSIL2A_20220714T104629_N0400_R051_T31UFS_20220714T134329.SAFE',\n  'properties': {'datetime': '2022-07-14T10:56:55.872Z'}},\n {'id': 'S2B_MSIL2A_20220711T103629_N0400_R008_T31UES_20220711T121934.SAFE',\n  'properties': {'datetime': '2022-07-11T10:47:03.695Z'}},\n {'id': 'S2B_MSIL2A_20220711T103629_N0400_R008_T31UFS_20220711T121934.SAFE',\n  'properties': {'datetime': '2022-07-11T10:46:59.439Z'}},\n {'id': 'S2A_MSIL2A_20220709T105041_N0509_R051_T31UES_20221214T145059.SAFE',\n  'properties': {'datetime': '2022-07-09T10:57:08.699Z'}},\n {'id': 'S2A_MSIL2A_20220709T105041_N0509_R051_T31UFS_20221214T145059.SAFE',\n  'properties': {'datetime': '2022-07-09T10:57:04.507Z'}},\n {'id': 'S2A_MSIL2A_20220706T103641_N0400_R008_T31UES_20220706T183816.SAFE',\n  'properties': {'datetime': '2022-07-06T10:47:12.125Z'}},\n {'id': 'S2A_MSIL2A_20220706T103641_N0400_R008_T31UFS_20220706T183816.SAFE',\n  'properties': {'datetime': '2022-07-06T10:47:07.92Z'}},\n {'id': 'S2B_MSIL2A_20220704T104629_N0400_R051_T31UES_20220704T123505.SAFE',\n  'properties': {'datetime': '2022-07-04T10:57:00.844Z'}},\n {'id': 'S2B_MSIL2A_20220704T104629_N0400_R051_T31UFS_20220704T123505.SAFE',\n  'properties': {'datetime': '2022-07-04T10:56:56.657Z'}},\n {'id': 'S2B_MSIL2A_20220701T103629_N0400_R008_T31UES_20220701T122344.SAFE',\n  'properties': {'datetime': '2022-07-01T10:47:04.245Z'}},\n {'id': 'S2B_MSIL2A_20220701T103629_N0400_R008_T31UFS_20220701T122344.SAFE',\n  'properties': {'datetime': '2022-07-01T10:47:00.021Z'}}]\n\n\n\n\nProcess API\n\nExample 1: True Color Image\nWe build the request according to the API Reference, using the SentinelHubRequest class. Each Process API request also needs an evalscript.\nThe information that we specify in the SentinelHubRequest object is: - an evalscript, - a list of input data collections with time interval, - a format of the response, - a bounding box and it’s size (size or resolution). - mosaickingOrder (optional): in this example we have used leastCC which will return pixels from the least cloudy acquisition in the specified time period.\nThe evalscript in the example is used to select the appropriate bands. We return the RGB (B04, B03, B02) Sentinel-2 L2A bands.\nThe least cloudy image from the time period is downloaded. Without any additional parameters in the evalscript, the downloaded data will correspond to reflectance values in UINT8 format (values in 0-255 range).\n\nevalscript_true_color = \"\"\"\n    //VERSION=3\n\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B02\", \"B03\", \"B04\"]\n            }],\n            output: {\n                bands: 3\n            }\n        };\n    }\n\n    function evaluatePixel(sample) {\n        return [sample.B04, sample.B03, sample.B02];\n    }\n\"\"\"\n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                name=\"s2\", service_url=\"https://sh.dataspace.copernicus.eu\"\n            ),\n            time_interval=(\"2022-07-01\", \"2022-07-20\"),\n            other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=aoi_bbox,\n    size=aoi_size,\n    config=config,\n)\n\nThe method get_data() will always return a list of length 1 with the available image from the requested time interval in the form of numpy arrays.\n\ntrue_color_imgs = request_true_color.get_data()\n\n\nprint(\n    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n)\nprint(\n    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n)\n\nReturned data is of type = &lt;class 'list'&gt; and length 1.\nSingle element in the list is of type &lt;class 'numpy.ndarray'&gt; and has shape (1723, 1941, 3)\n\n\n\nimage = true_color_imgs[0]\nprint(f\"Image type: {image.dtype}\")\n\n# plot function\n# factor 1/255 to scale between 0-1\n# factor 3.5 to increase brightness\nplot_image(image, factor=3.5 / 255, clip_range=(0, 1))\n\nImage type: uint8\n\n\n\n\n\n\n\nExample 2: NDVI Image\nSecondly, we will also show you an example of how to calculate and visualise NDVI using the same API. NDVI calculation is a very commonly used spectral vegetation index for vegetation monitoring, for example, monitoring crop growth and yields. As you will notice in the codeblock below, the evalscript has changed substantially: - we are only using Band 4 and Band 8 as an input into our script. - The ColorGradientVisualizer function has also been utilised in this script. More about this can be found in the documentation here.\n\nevalscript_ndvi = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\",\n        \"dataMask\"\n      ]\n    }],\n    output: {\n      bands: 4\n    }\n  }\n}\n  \nlet viz = ColorGradientVisualizer.createWhiteGreen(-1.0, 1.0);\n\nfunction evaluatePixel(samples) {\n    let val = (samples.B08 - samples.B04) / (samples.B08 + samples.B04);\n    val = viz.process(val);\n    val.push(samples.dataMask);\n    return val;\n}\n\"\"\"\n\nrequest_ndvi_img = SentinelHubRequest(\n    evalscript=evalscript_ndvi,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                name=\"s2\", service_url=\"https://sh.dataspace.copernicus.eu\"\n            ),\n            time_interval=(\"2022-07-01\", \"2022-07-20\"),\n            other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=aoi_bbox,\n    size=aoi_size,\n    config=config,\n)\n\nThe same method as before is used to request and then visualise the data. In the visualisation, the lighter greens indicate a higher NDVI value (vegetation, forest) and the darker greens (urban areas and water bodies) represent areas with lower NDVI values.\n\nndvi_img = request_ndvi_img.get_data()\n\n\nprint(\n    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n)\nprint(\n    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n)\n\nReturned data is of type = &lt;class 'list'&gt; and length 1.\nSingle element in the list is of type &lt;class 'numpy.ndarray'&gt; and has shape (1723, 1941, 3)\n\n\n\nimage = ndvi_img[0]\nprint(f\"Image type: {image.dtype}\")\n\n# plot function\n# factor 1/255 to scale between 0-1\n# factor 3.5 to increase brightness\nplot_image(image, factor=1 / 255, clip_range=(-1, 1))\n\nImage type: uint8"
  },
  {
    "objectID": "notebook-samples/sentinelhub/migration_from_scihub_guide.html#summary",
    "href": "notebook-samples/sentinelhub/migration_from_scihub_guide.html#summary",
    "title": "Migrating your workflows from The Copernicus Open Access Hub to the Copernicus Data Space Ecosystem",
    "section": "Summary",
    "text": "Summary\nSo what have we learnt in this notebook?\n\nSetting up credentials to access the Copernicus Data Space Ecosystem through OData and Sentinel Hub APIs.\nSearching and discovering Copernicus data through OData protocols and downloading full Sentinel-2 L2A Granules.\nHow to quickly discover and access satellite imagery using Sentinel Hub APIs.\n\nWith this short guide, we hope to have clarified how the Copernicus Data Space Ecosystem can serve you beyond simply downloading raw images. At this point, you may want to consider which steps of your processing pipeline you prefer to do in house after downloading the data, and which operations you would rather have as part of the pre-processing in the cloud."
  },
  {
    "objectID": "Usecase.html",
    "href": "Usecase.html",
    "title": "Jupyter Notebook Samples",
    "section": "",
    "text": "In this page, we present a compilation of Jupyter Notebooks showcasing the utilization of available data analyzed through the openEO API and the Sentinel Hub API across various use cases. These notebooks are categorized and will expand over time. Written in Python, they are designed to be executed in a Jupyter environment.\nThe notebooks serve as a foundational resource for users, offering a starting point to acquaint themselves with the openEO API and the Sentinel Hub API. Users are encouraged to explore these notebooks when developing their workflow."
  },
  {
    "objectID": "Usecase.html#land-monitoring-related-examples",
    "href": "Usecase.html#land-monitoring-related-examples",
    "title": "Jupyter Notebook Samples",
    "section": "Land monitoring related examples",
    "text": "Land monitoring related examples\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nBest available pixel composite\n\n\n\n\n\n\n\n\n\n\n\nClassification of Ice and Open Water in Nizhnesvirsky Lower Bay using Sentinel-1 IW Product\n\n\n\n\n\n\n\n\n\n\n\nDeforestation Monitoring using Sentinel 2 and xarray\n\n\n\n\n\n\n\n\n\n\n\nEstimation of erosion risk based on bare soil periods and digital elevation model slope\n\n\n\n\n\n\n\n\n\n\n\nParcel delineation using Sentinel-2\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Usecase.html#atmospheric-monitoring-related-examples",
    "href": "Usecase.html#atmospheric-monitoring-related-examples",
    "title": "Jupyter Notebook Samples",
    "section": "Atmospheric monitoring related examples",
    "text": "Atmospheric monitoring related examples\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nComparing statistics of NO2 pollution for European cities\n\n\n\n\n\n\n\n\n\n\n\nNO2 emission and COVID Lockdown effects\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Usecase.html#agriculture-related-examples",
    "href": "Usecase.html#agriculture-related-examples",
    "title": "Jupyter Notebook Samples",
    "section": "Agriculture related examples",
    "text": "Agriculture related examples\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nCalculate Radar Vegetation Index(RVI) using Sentinel-1 GRD collection\n\n\n\n\n\n\n\n\n\n\n\nHow to create an NDVI Time series using openEO\n\n\n\n\n\n\n\n\n\n\n\nRank composites\n\n\n\n\n\n\n\n\n\n\n\nSurface Soil Moisture (SSM)\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Usecase.html#disaster-monitoring-examples",
    "href": "Usecase.html#disaster-monitoring-examples",
    "title": "Jupyter Notebook Samples",
    "section": "Disaster monitoring examples",
    "text": "Disaster monitoring examples\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nHeatwave in the Netherlands\n\n\n\n\n\n\n\n\n\n\n\nNDVI-based approach to study Landslide areas\n\n\n\n\n\n\n\n\n\n\n\nOil spill mapping using Sentinel-1\n\n\n\n\n\n\n\n\n\n\n\nWildfire mapping using Sentinel-2\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Usecase.html#additional-examples",
    "href": "Usecase.html#additional-examples",
    "title": "Jupyter Notebook Samples",
    "section": "Additional examples",
    "text": "Additional examples\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\nCreating a smoothed dataset using Whittaker\n\n\n\n\n\n\n\n\n\n\n\nCreating multi-mission, multi-temporal datacube\n\n\n\n\n\n\n\n\n\n\n\nFirst Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs\n\n\n\n\n\n\n\n\n\n\n\nHow to access Sentinel-2 Level 3 Cloudless Quarterly Mosaics using the Process API\n\n\n\n\n\n\n\n\n\n\n\nMigrating your workflows from The Copernicus Open Access Hub to the Copernicus Data Space Ecosystem\n\n\n\n\n\n\n\n\n\n\n\nUser-Defined Functions (UDF) in openEO\n\n\n\n\n\n\n\n\n\n\n\nUser-Defined Processes (UDP) in openEO\n\n\n\n\n\n\n\n\n\n\n\nopenEO Basics: How to load a data cube from a data collection?\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Applications.html",
    "href": "Applications.html",
    "title": "Applications",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem provides a wide range of applications that can be used to access, process, and visualize Copernicus data. These applications are designed for users of different skill level to be used for various applications.\nFor users, new to the Copernicus Data Space Ecosystem, we advise to explore the Browser as starting point. In this platform users can explore various datasets and discover intuitive tools for visualizing, comparing and downloading data.\n\nAdvanced users, interested in designing custom EO processing workflows, can explore the JupyterLab environment for a more interactive analysis. Within the Jupyterlab environment, users can execute Python code and access data interactively.\n\nMoreover, these jupyterlab environments support the use of openEO to access and process the data interactively. More information on the openEO API can be found here. Nevertheless, for users interested in using openEO API in a GUI environment, we recommend exploring the openEO Web Editor documentation. This tool allows users to interact with the openEO API more visually.\n\nThe Copernicus Data Space Ecosystem supports FAIR and Open Science principles through sharing EO algorithms. The openEO Algorithm Plaza documentation introduces the plaza. It explains how one can use the platform to share and use EO algorithms.\n\nFurthermore, the ecosystem encompasses a QGIS Plugin designed to view satellite data from the Copernicus Data Space Ecosystem or from Sentinel Hub directly within a QGIS workspace.\nFrom the Copernicus Data Space Browser it is possible to access the Data workspace, which is a tool to manage and order satellite products. These products can then be further processed and downloaded for various purposes.\nAdditionally, the Catalogue CSV documentation provides access to additional information on Sentinel product lists in CSV format.\nAmong the array of applications, there is also the Copernicus Dashboard. This public platform helps users to monitor activities within the ecosystem and keep track of ongoing updates.\n\nBelow is the comprehensive list of applications available within the Copernicus Data Space Ecosystem:\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\nAbout Data Workspace\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the Browser\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatalogue CSV\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Data Space Ecosystem Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\nJupyterLab\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel Hub QGIS Plugin\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenEO Algorithm Plaza\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenEO Web Editor\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cdse_doc.html",
    "href": "cdse_doc.html",
    "title": "Documentation",
    "section": "",
    "text": "We’re delighted you signed up!\nNow, let’s explore our extensive documentation to gain insights into our comprehensive Earth Observation data collection and the array of data access and data processing capabilities.\nOur documentation is a living resource, continuously updated to provide you with the latest information.\nDiscover within this documentation:\n\nData: Explore large amounts of open and free Earth Observation datasets, including Sentinel Data, Copernicus Contributing Missions, Federated Datasets, and Complementary Data, with detailed information.\nAPIs: Find the perfect interface for your needs in our suite of APIs. Whether you seek catalog access, product downloads, data visualization, or processing capabilities, our offerings encompass a range of options, including S3, STAC, openEO, and Sentinel Hub APIs.\nApplications: Simplify your satellite data journey and engage with data using our user-friendly applications for searching, visualizing, modifying, and downloading data effortlessly.\nQuotas and Limitations: Know the quotas and limitations that come with your user type and plan your data download and processing pipelines accordingly.\n\nFor quick answers to common queries, check out our FAQ section. If you have any questions that remain unanswered on this portal, our Support team is here to assist you. Feel free to reach out at any time.\nWelcome aboard\nReady to get started? Explore our documentation now and unlock the full potential of the Copernicus Data Space Ecosystem.\nLet’s embark on an Earth Observation data-driven journey together!"
  },
  {
    "objectID": "cdse_doc.html#welcome-to-the-copernicus-data-space-ecosystem-documentation-portal",
    "href": "cdse_doc.html#welcome-to-the-copernicus-data-space-ecosystem-documentation-portal",
    "title": "Documentation",
    "section": "",
    "text": "We’re delighted you signed up!\nNow, let’s explore our extensive documentation to gain insights into our comprehensive Earth Observation data collection and the array of data access and data processing capabilities.\nOur documentation is a living resource, continuously updated to provide you with the latest information.\nDiscover within this documentation:\n\nData: Explore large amounts of open and free Earth Observation datasets, including Sentinel Data, Copernicus Contributing Missions, Federated Datasets, and Complementary Data, with detailed information.\nAPIs: Find the perfect interface for your needs in our suite of APIs. Whether you seek catalog access, product downloads, data visualization, or processing capabilities, our offerings encompass a range of options, including S3, STAC, openEO, and Sentinel Hub APIs.\nApplications: Simplify your satellite data journey and engage with data using our user-friendly applications for searching, visualizing, modifying, and downloading data effortlessly.\nQuotas and Limitations: Know the quotas and limitations that come with your user type and plan your data download and processing pipelines accordingly.\n\nFor quick answers to common queries, check out our FAQ section. If you have any questions that remain unanswered on this portal, our Support team is here to assist you. Feel free to reach out at any time.\nWelcome aboard\nReady to get started? Explore our documentation now and unlock the full potential of the Copernicus Data Space Ecosystem.\nLet’s embark on an Earth Observation data-driven journey together!"
  },
  {
    "objectID": "APIs/Sentinel-1 SLC Burst.html",
    "href": "APIs/Sentinel-1 SLC Burst.html",
    "title": "Sentinel-1 SLC Bursts",
    "section": "",
    "text": "Sentinel-1 satellites, operating in a near-polar sun-synchronous orbit with a 6-day repeat cycle, utilize TOPSAR (Terrain Observation with Progressive Scans SAR) modes for SAR imaging. The SLC (Single Look Complex) Burst products capture data in bursts, which are segments of radar echoes acquired by cyclically switching the antenna beam across multiple sub-swaths. A burst cycle comprises three consecutive bursts in IW mode (IW1, IW2, IW3) or five consecutive bursts in EW mode (EW1, EW2, EW3, EW4, EW5).\nSLC Burst products maintain both amplitude and phase information, making them ideal for applications such as interferometry, land deformation studies, and ocean monitoring, providing high-resolution, repeatable observations essential for detailed environmental and land surface analysis.\nThe Copernicus Data Space Ecosystems provides access to Sentinel-1 SLC Bursts products which are extracted from the following Sentinel-1 product types:\nTo access the Sentinel-1 SLC Bursts, users are invited to:"
  },
  {
    "objectID": "APIs/Sentinel-1 SLC Burst.html#sentinel-1-slc-bursts-endpoint",
    "href": "APIs/Sentinel-1 SLC Burst.html#sentinel-1-slc-bursts-endpoint",
    "title": "Sentinel-1 SLC Bursts",
    "section": "Sentinel-1 SLC Bursts endpoint",
    "text": "Sentinel-1 SLC Bursts endpoint\nThe Copernicus Data Space Ecosystem OData API for searching Sentinel-1 SLC Bursts products can be accessed using the following URL:\n\nOData SLC Burst endpoint\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts\n\n\n\nUsers can access the Bursts by their unique Id assigned within Copernicus Data Space Ecosystem Catalogue:\n\nHTTPS RequestOData SLC Bursts API Response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts(46577370-83b3-4410-89de-09b4bb23e5dd)\n\n\n{\n  \"@odata.context\": \"$metadata#Bursts/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"46577370-83b3-4410-89de-09b4bb23e5dd\",\n  \"Name\": \"s1a-20240802t060723-015805-iw1-vh-040352\",\n  \"ContentType\": \"application/octet-stream\",\n  \"S3Path\": \"eodata/Sentinel-1/SAR/IW_SLC__1S/2024/08/02/S1A_IW_SLC__1SDV_20240802T060719_20240802T060746_055030_06B44E_E7CC.SAFE/measurement/s1a-iw1-slc-vh-20240802t060719-20240802t060744-055030-06b44e-001.tiff\",\n  \"ContentDate\": {\n    \"Start\": \"2024-08-02T06:07:23.426535Z\",\n    \"End\": \"2024-08-02T06:07:26.616535Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((3.393735 51.141248, 2.762036 51.221625, 2.174971 51.293199, 2.114652 51.089904, 2.700475 51.022923, 3.330851 50.947163, 3.393735 51.141248))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          3.393735,\n          51.141248\n        ],\n        [\n          2.762036,\n          51.221625\n        ],\n        [\n          2.174971,\n          51.293199\n        ],\n        [\n          2.114652,\n          51.089904\n        ],\n        [\n          2.700475,\n          51.022923\n        ],\n        [\n          3.330851,\n          50.947163\n        ],\n        [\n          3.393735,\n          51.141248\n        ]\n      ]\n    ]\n  },\n  \"ParentProductId\": \"e463365f-728b-4890-b123-97c76941c878\",\n  \"ParentProductName\": \"S1A_IW_SLC__1SDV_20240802T060719_20240802T060746_055030_06B44E_E7CC.SAFE\",\n  \"ByteOffset\": 138832827,\n  \"BurstId\": 15805,\n  \"AbsoluteBurstId\": 118199091,\n  \"AzimuthTime\": \"2024-08-02T06:07:22.288930Z\",\n  \"AzimuthAnxTime\": 2120.883319884,\n  \"ParentProductType\": \"IW_SLC__1S\",\n  \"SwathIdentifier\": \"IW1\",\n  \"RelativeOrbitNumber\": 8,\n  \"OrbitDirection\": \"DESCENDING\",\n  \"PlatformSerialIdentifier\": \"A\",\n  \"PolarisationChannels\": \"VH\",\n  \"OperationalMode\": \"IW\",\n  \"DatatakeID\": 40352,\n  \"Lines\": 1508,\n  \"LinesPerBurst\": 1508,\n  \"SamplesPerBurst\": 22998,\n  \"BeginningDateTime\": \"2024-08-02T06:07:23.426535Z\",\n  \"EndingDateTime\": \"2024-08-02T06:07:26.616535Z\"\n}\n\n\n\n\nSentinel-1 SLC Bursts search\nOData SLC Bursts endpoint supports the following searching options:\n\nfilter\norderby\ntop\nskip\ncount\n\nSearch options should always be preceded with $ and consecutive options should be separated with &.\nConsecutive filters within filter option should be separated with and or or.\nTo search for Bursts:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=BurstId eq 15804 and SwathIdentifier eq 'IW2' and ContentDate/Start gt 2024-08-01 and ContentDate/Start lt 2024-08-15 and ParentProductType eq 'IW_SLC__1S' and PolarisationChannels eq 'VV' and OrbitDirection eq 'DESCENDING'&$orderby=ContentDate/Start&$count=True\n\n\n\n\n\nFilter option\nSentinel-1 SLC Bursts endpoint provide users the option to search for specific SLC Bursts by filtering them by:\n\nContentDate Start/End (date-time)\nGeographic Criteria (string)\nParentProductName (string) - Name of the original product from which the Burst is extracted\nParentProductType (string) - Product type of the original product from which the Burst is extracted\nParentProductId (string) - Id of the original product from which the Burst is extracted\nBurstId (integer)\nAbsoluteBurstID (integer)\nPolarisationChannels (string)\nOperationalMode (string)\nSwathIdentifier (string)\nOrbitDirection (string)\nPlatformSerialIdentifier (string)\nRelativeOrbitNumber (integer)\nDatatakeID (integer)\n\n\nQuery by Sensing Date\nTo search for Bursts products acquired between two dates:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ContentDate/Start ge 2024-08-01T00:00:00.000Z and ContentDate/Start le 2024-08-03T00:00:00.000Z\n\n\n\n\n\nQuery by Geographic Criteria\nTo search for Bursts products intersecting the specified polygon:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))')\n\n\n\n\n\nQuery by Source Product Name\nTo search for Bursts products by source product name:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ParentProductName eq 'S1A_IW_SLC__1SDV_20240802T060719_20240802T060746_055030_06B44E_E7CC.SAFE'\n\n\n\n\n\nQuery by Source Product Type\nTo search for Bursts products by source product type:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ParentProductType eq 'IW_SLC__1S'\n\n\n\n\n\nQuery by Source Product Id\nTo search for Bursts products by source product id:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ParentProductId eq 'e463365f-728b-4890-b123-97c76941c878'\n\n\n\n\n\nQuery by Burst Id\nTo search for Bursts products by Burst Id:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=BurstId eq 15804\n\n\n\n\n\nQuery by Absolute Burst Id\nTo search for Bursts products by Absolute Burst Id:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=AbsoluteBurstId eq 118199090\n\n\n\n\n\nQuery by Polarisation\nTo search for Bursts products by polarisation:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=PolarisationChannels eq 'VV'\n\n\n\n\n\nQuery by Mode\nTo search for Bursts products by operational mode:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=OperationalMode eq 'IW'\n\n\n\n\n\nQuery by Subswath\nTo search for Bursts products by subswath:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=SwathIdentifier eq 'IW1'\n\n\n\n\n\nQuery by Orbit Direction\nTo search for Bursts products by orbit direction:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=OrbitDirection eq 'ASCENDING'\n\n\n\n\n\nQuery by Platform Serial Identifier\nTo search for Bursts products by satellite platfrom serial identifier:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=PlatformSerialIdentifier eq 'A'\n\n\n\n\n\nQuery by Relative Orbit Number\nTo search for Bursts products by relative orbit number:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=RelativeOrbitNumber eq 8\n\n\n\n\n\nQuery by Relative DatatakeID\nTo search for Bursts products by DatatkeID:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=DatatakeID eq 40352\n\n\n\n\n\n\nOrderby option\nOrderby option can be used to order the Bursts products in an ascending (asc) or descending (desc) direction. If asc or desc is not specified, then the resources will be ordered in ascending order.\n\n\n\n\n\n\nTip\n\n\n\nUsing the orderby option will exclude potential duplicates from the search results.\n\n\nTo order Bursts products by ContentDate/Start in a descending direction:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ContentDate/Start gt 2024-08-01T00:00:00.000Z and ContentDate/Start lt 2024-08-03T00:00:00.000Z and OrbitDirection eq 'DESCENDING' and PolarisationChannels eq 'VV'&$orderby=ContentDate/Start desc\n\n\n\nBy default, if the orderby option is not used, the results are not ordered. If orderby option is used, additional orderby by id is also used, so that the results are fully ordered, and no products are lost while paginating through the results.\nThe acceptable arguments for this option: ContentDate/Start, ContentDate/End in directions: asc, desc.\n\n\nTop option\nTop option specifies the maximum number of Bursts items returned from a query.\nTo limit the number of results:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ContentDate/Start gt 2024-08-01T00:00:00.000Z and ContentDate/Start lt 2024-08-03T00:00:00.000Z and OrbitDirection eq 'DESCENDING' and PolarisationChannels eq 'VV'&$orderby=ContentDate/Start desc&$top=5\n\n\n\nThe default value is set to 20.\nThe acceptable arguments for this option: Integer &lt;0,1000&gt;.\n\n\nSkip option\nThe skip option can be used to skip a specific number of results. Exemplary application of this option would be paginating through the results, however, for performance reasons, we recommend limiting queries with small time intervals as a substitute for skipping in a more generic query.\nTo skip a specific number of results:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ContentDate/Start gt 2024-08-01T00:00:00.000Z and ContentDate/Start lt 2024-08-03T00:00:00.000Z and OrbitDirection eq 'DESCENDING' and PolarisationChannels eq 'VV'&$orderby=ContentDate/Start desc&$skip=5\n\n\n\nThe default value is set to 0.\nWhenever a query results in more products than 20 (default top value), the API provides a nextLink at the bottom of the page: “@OData.nextLink”\nThe acceptable arguments for this option: Integer &lt;0,10000&gt;.\n\n\nCount option\nThe count option lets users get the exact number of Bursts products matching the query. This option is disabled by default to accelerate the query performance.\n\n\n\n\n\n\nTip\n\n\n\nDon’t use count option if not necessary, it slows down the execution of the request.\n\n\nTo get the exact number of products for a given query:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Bursts?$filter=ContentDate/Start gt 2024-08-01T00:00:00.000Z and ContentDate/Start lt 2024-08-03T00:00:00.000Z and OrbitDirection eq 'DESCENDING' and PolarisationChannels eq 'VV'&$orderby=ContentDate/Start desc&$count=True"
  },
  {
    "objectID": "APIs/Sentinel-1 SLC Burst.html#downloading-bursts",
    "href": "APIs/Sentinel-1 SLC Burst.html#downloading-bursts",
    "title": "Sentinel-1 SLC Bursts",
    "section": "Downloading Bursts",
    "text": "Downloading Bursts\nThe GitLab repository provides the description how to download Sentinel-1 SLC Bursts on a client side using GDAL version &gt; 3.9 and CDSE S3 keys.\nThe tool enables extraction of:\n\na single Sentinel-1 SLC Burst given the:\n\nCopernicus Data Space Ecosystem S3 credentials\nSentinel-1 SLC TOPSAR product name e.g. S1A_IW_SLC__1SDH_20240201T085352_20240201T085422_052363_0654EE_5132.SAFE\nSAR polarization e.g. vv\nsubswath ID e.g. iw3\nrealtive burst ID e.g. 202680\n\nmultiple Sentinel-1 SLC bursts given the:\n\nCopernicus Data Space Ecosystem S3 credentials\nLatitude of a Point of Interest (POI)\nLongitude of a Point of Interest (POI)\nSAR polarization e.g. vv\nContent Start Date in format YYYY-MM-DD\nContent End Date in format YYYY-MM-DD"
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html",
    "href": "APIs/Others/ReleaseNotes.html",
    "title": "Release notes",
    "section": "",
    "text": "This page includes the list of changes made to Catalog APIs for each release."
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#catalogue-api-change-parameters-limits-2024-11-12",
    "href": "APIs/Others/ReleaseNotes.html#catalogue-api-change-parameters-limits-2024-11-12",
    "title": "Release notes",
    "section": "Catalogue API Change: parameters’ limits [2024-11-12]",
    "text": "Catalogue API Change: parameters’ limits [2024-11-12]\nWe would like to inform you about an upcoming change to our OData, OpenSearch and STAC API interfaces, effective 12 November 2024.\nStarting from 12 November 2024, the number of skipped results will be limited to 10 000 items. Currently, there is no limit on the number of skipped items.\nThis limit will affect the following parameters in each Catalogue API interface:\n\nOData interface: maximum value for the skip parameter will be set to 10 000\nOpenSearch interface: maximum value for ‘(page - 1) * maxRecords + index - 1’ will be set to 10 000, where by deafult maxRecords = 20, page = 1 and index = 1; maximum value for ‘index’ will be set to 10001\nSTAC interface: maximum value for ‘(page - 1) * limit’ will be set to 10 000, where by deafult page = 1 and limit = 20\n\nPlease find below a list of parameters for each Catalogue API that will be affected by the change as well as the response which will be returned when the limit is exceeded.\nThe affected query parameters for OData Catalogue interface:\nOData Catalogue API\nskip\nOData API request\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-3' and ContentDate/Start gt 2024-09-01&$skip=10001&$count=True\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n{\n    \"@odata.context\": \"$metadata#Products\",\n    \"@odata.count\": 457588,\n    \"value\": [...],\n    \"@odata.nextLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?%24filter=Collection%2FName+eq+%27SENTINEL-3%27+and+ContentDate%2FStart+gt+2024-09-01&%24skip=10021&%24count=True\"\n}\n\n\n422 Unprocessable Entity\n \n{\n    \"detail\": [\n        {\n            \"type\": \"less_than_equal\",\n            \"loc\": [\n                \"query\",\n                \"$skip\"\n            ],\n            \"msg\": \"Input should be less than or equal to 10000\",\n            \"input\": \"10001\",\n            \"ctx\": {\n                \"le\": 10000\n            }\n        }\n    ]\n}\n\n\n\nThe @odata.nextLink field, which is normally included in the API response to provide the next link for pagination, will not be shown if the skip parameter in the next link exceeds a limit of 10,000. Here’s a step-by-step breakdown:\n1. OData query Example:\n{url}odata/v1/Products?$skip=10000&$top=1\nIn this query:\n\n$skip=10000 means you’re telling the API to skip the first 10,000 records.\n$top=1 means you want to retrieve only 1 record in the response.\n\n2. Pagination Logic: The OData API usually provides a @odata.nextLink in the response, which gives the URL to retrieve the next set of records in the sequence. This next link works by adjusting the skip value to keep moving forward through the data.\n3. Skip + Top: In this example, skip + top = 10000 + 1 = 10001. This value exceeds the skip limit of 10,000.\n4. Impact: Because this skip value (10001) exceeds the limit of 10,000, the API will not include the @odata.nextLink in the response. Essentially, the API is signaling that it cannot paginate beyond this point.\nImportant Note\nAfter the change, when the skip value exceeds 10,000, the API will no longer return the @odata.nextLink for further pagination, meaning you can’t retrieve records beyond this limit via pagination using @odata.nextLink.\nThe affected query parameters for OpenSearch interface:\nOpenSearch Catalogue API\nmaxRecords\npage\nindex\nOpenSearch API request\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&page=667&productType=IW_SLC__1S\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"a70c9f16-fc45-5ecb-b1e9-4433f14946f4\",\n    \"totalResults\": 2381730,\n    \"exactCount\": 1,\n    \"startIndex\": 10002,\n    \"itemsPerPage\": 15,\n    \"query\": {\n      \"originalFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 10.239139726\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&page=667&productType=IW_SLC__1S\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"previous\",\n        \"type\": \"application/json\",\n        \"title\": \"previous\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=666\"\n      },\n      {\n        \"rel\": \"first\",\n        \"type\": \"application/json\",\n        \"title\": \"first\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=1\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=668\"\n      },\n      {\n        \"rel\": \"last\",\n        \"type\": \"application/json\",\n        \"title\": \"last\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=158782\"\n      }\n    ]\n  },\n    \"features\": [...]\n}\n\n\n400 Bad Request\n \n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"page\",\n                    \"maxRecords\",\n                    \"index\"\n                ],\n                \"msg\": \"The '(page - 1) * maxRecords + index - 1' should be less than 10000. By default, 'maxRecords' is set to 20, while both 'page' and 'index' are set to 1 if not specified.\"\n            }\n        ],\n        \"RequestID\": \"5ec27fc6-2b2d-4513-b61e-ccc09b8f0bba\"\n    }\n}\n\n\n\nIndex limit in OpenSearch Catalogue API\nOpenSearch API request\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?&index=10002&productType=IW_SLC__1S\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"a70c9f16-fc45-5ecb-b1e9-4433f14946f4\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 10002,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.296336496\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?&index=10002&productType=IW_SLC__1S\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?index=10002&productType=IW_SLC__1S&page=2\"\n      }\n    ]\n  },\n  \"features\": [...]\n}\n\n\n400 Bad Request\n \n{\n  \"detail\": {\n    \"ErrorMessage\": \"Validation error.\",\n    \"ErrorCode\": 400,\n    \"ErrorDetail\": [\n      {\n        \"loc\": [\n          \"index\"\n        ],\n        \"msg\": \"Input should be less than or equal to 10001.\"\n      }\n    ],\n    \"RequestID\": \"09ff2890-c925-497a-a11d-95b308437b8e\"\n  }\n}\n\n\n\nImportant Note\nAfter the change, when the ‘(page - 1) * maxRecords + index - 1’ value exceeds 10,000, the API will no longer return the ‘next’ link for further pagination, meaning you can’t retrieve records beyond this limit via pagination using ‘next’ link.\nThe affected query parameters for STAC Catalogue API interface:\nSTAC Catalogue API\nlimit\npage\nSTAC API request\nhttps://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=12&sortby=datetime\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n\"type\": \"FeatureCollection\",\n\"features\": [...],\n\"links\": [\n    {\n        \"rel\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=13&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"prev\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=11&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"first\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=1&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=12&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"root\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac\",\n        \"type\": \"application/json\"\n    }\n ]\n} \n\n\n400 Bad Request\n \n{\n    \"code\": \"400\",\n    \"description\": \"The '(page - 1) * limit' should be less than 10000. By default, 'limit' is set to 20 and 'page' is set to 1 if not specified.\"\n    \"request_id\": \"4e681b43-5b47-4437-a0b1-f11b3c36a24a\"\n}\n\n\n\nImportant Note\nAfter the change, when the ‘(page - 1) * limit’ value exceeds 10,000, the API will no longer return the ‘next’ link for further pagination, meaning you can’t retrieve records beyond this limit via pagination using ‘next’ link.\nWe recommend reviewing the upcoming changes to Catalogue OData, OpenSearch and STAC API interfaces described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#odata-catalogue-api-response-format-change-2024-09-24",
    "href": "APIs/Others/ReleaseNotes.html#odata-catalogue-api-response-format-change-2024-09-24",
    "title": "Release notes",
    "section": "OData Catalogue API: Response Format Change [2024-09-24]",
    "text": "OData Catalogue API: Response Format Change [2024-09-24]\nWe would like to inform you about an upcoming change to our OData API response format, effective 24th September 2024.\nStarting from 24th September 2024, the format of the @odata.context property in the OData API responses will be updated when using the $expand query option. The new @odata.context format will list all expanded entities separated by commas. This change is aimed at aligning Catalogue API with the OData standards and improving the consistency and clarity of Catalogue OData API responses.\n\nCurrent FormatNew Format\n\n\n\"@odata.context\": \"$metadata#Products(Attributes())(Locations())(Assets())\"\n\n\n\"@odata.context\": \"$metadata#Products(Attributes(),Locations(),Assets())\"\n\n\n\nBelow please find the example of ODate Catalogue API response before and after the described change:\n\nHTTPS RequestCurrent ResponseNew Response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Name eq 'S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE'&$expand=Attributes&$expand=Locations&$expand=Assets\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes())(Locations())(Assets())\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"72ac8773-c55d-526a-a9a9-22ddb9b595ef\",\n      \"Name\": \"S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 0,\n      \"OriginDate\": \"2021-02-20T13:38:02.110Z\",\n      \"PublicationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"ModificationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"9999-12-31T23:59:59.999Z\",\n      \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"Checksum\": [\n         \n      ],\n      \"ContentDate\": {\n        \"Start\": \"2016-01-07T11:00:59.171Z\",\n        \"End\": \"2016-01-07T11:01:26.112Z\"\n      },\n      \"Footprint\": \"geography'SRID=4326;MULTIPOLYGON (((-79.007256 -3.315278, -78.661591 -1.686242, -80.881958 -1.204264, -81.230759 -2.827857, -79.007256 -3.315278)))'\",\n      \"GeoFootprint\": {\n        \"type\": \"MultiPolygon\",\n        \"coordinates\": [\n          [\n            [\n              [\n                -79.007256,\n                -3.315278\n              ],\n              [\n                -78.661591,\n                -1.686242\n              ],\n              [\n                -80.881958,\n                -1.204264\n              ],\n              [\n                -81.230759,\n                -2.827857\n              ],\n              [\n                -79.007256,\n                -3.315278\n              ]\n            ]\n          ]\n        ]\n      },\n      \"Attributes\": [\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"authority\",\n          \"Value\": \"ESA\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"coordinates\",\n          \"Value\": \"-3.315278,-79.007256 -2.827857,-81.230759 -1.204264,-80.881958 -1.686242,-78.661591 -3.315278,-79.007256\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"orbitNumber\",\n          \"Value\": 9387,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productType\",\n          \"Value\": \"IW_SLC__1S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"sliceNumber\",\n          \"Value\": 3,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productClass\",\n          \"Value\": \"S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"endingDateTime\",\n          \"Value\": \"2016-01-07T11:01:26.112Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"orbitDirection\",\n          \"Value\": \"DESCENDING\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productGroupId\",\n          \"Value\": \"55660\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"operationalMode\",\n          \"Value\": \"IW\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"processingLevel\",\n          \"Value\": \"LEVEL1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"swathIdentifier\",\n          \"Value\": \"IW1 IW2 IW3\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"beginningDateTime\",\n          \"Value\": \"2016-01-07T11:00:59.171Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformShortName\",\n          \"Value\": \"SENTINEL-1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"spatialResolution\",\n          \"Value\": \"2.3\",\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"instrumentShortName\",\n          \"Value\": \"SAR\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"relativeOrbitNumber\",\n          \"Value\": 40,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"polarisationChannels\",\n          \"Value\": \"VV\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformSerialIdentifier\",\n          \"Value\": \"A\",\n          \"ValueType\": \"String\"\n        }\n      ],\n      \"Assets\": [\n         \n      ],\n      \"Locations\": [\n        {\n          \"FormatType\": \"Extracted\",\n          \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(72ac8773-c55d-526a-a9a9-22ddb9b595ef)/$value\",\n          \"ContentLength\": 0,\n          \"Checksum\": [\n             \n          ],\n          \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\"\n        }\n      ]\n    }\n  ]\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes(),Locations(),Assets())\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"72ac8773-c55d-526a-a9a9-22ddb9b595ef\",\n      \"Name\": \"S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 0,\n      \"OriginDate\": \"2021-02-20T13:38:02.110Z\",\n      \"PublicationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"ModificationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"9999-12-31T23:59:59.999Z\",\n      \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"Checksum\": [\n         \n      ],\n      \"ContentDate\": {\n        \"Start\": \"2016-01-07T11:00:59.171Z\",\n        \"End\": \"2016-01-07T11:01:26.112Z\"\n      },\n      \"Footprint\": \"geography'SRID=4326;MULTIPOLYGON (((-79.007256 -3.315278, -78.661591 -1.686242, -80.881958 -1.204264, -81.230759 -2.827857, -79.007256 -3.315278)))'\",\n      \"GeoFootprint\": {\n        \"type\": \"MultiPolygon\",\n        \"coordinates\": [\n          [\n            [\n              [\n                -79.007256,\n                -3.315278\n              ],\n              [\n                -78.661591,\n                -1.686242\n              ],\n              [\n                -80.881958,\n                -1.204264\n              ],\n              [\n                -81.230759,\n                -2.827857\n              ],\n              [\n                -79.007256,\n                -3.315278\n              ]\n            ]\n          ]\n        ]\n      },\n      \"Attributes\": [\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"authority\",\n          \"Value\": \"ESA\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"coordinates\",\n          \"Value\": \"-3.315278,-79.007256 -2.827857,-81.230759 -1.204264,-80.881958 -1.686242,-78.661591 -3.315278,-79.007256\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"orbitNumber\",\n          \"Value\": 9387,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productType\",\n          \"Value\": \"IW_SLC__1S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"sliceNumber\",\n          \"Value\": 3,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productClass\",\n          \"Value\": \"S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"endingDateTime\",\n          \"Value\": \"2016-01-07T11:01:26.112Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"orbitDirection\",\n          \"Value\": \"DESCENDING\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productGroupId\",\n          \"Value\": \"55660\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"operationalMode\",\n          \"Value\": \"IW\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"processingLevel\",\n          \"Value\": \"LEVEL1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"swathIdentifier\",\n          \"Value\": \"IW1 IW2 IW3\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"beginningDateTime\",\n          \"Value\": \"2016-01-07T11:00:59.171Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformShortName\",\n          \"Value\": \"SENTINEL-1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"spatialResolution\",\n          \"Value\": \"2.3\",\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"instrumentShortName\",\n          \"Value\": \"SAR\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"relativeOrbitNumber\",\n          \"Value\": 40,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"polarisationChannels\",\n          \"Value\": \"VV\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformSerialIdentifier\",\n          \"Value\": \"A\",\n          \"ValueType\": \"String\"\n        }\n      ],\n      \"Assets\": [\n         \n      ],\n      \"Locations\": [\n        {\n          \"FormatType\": \"Extracted\",\n          \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(72ac8773-c55d-526a-a9a9-22ddb9b595ef)/$value\",\n          \"ContentLength\": 0,\n          \"Checksum\": [\n             \n          ],\n          \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\"\n        }\n      ]\n    }\n  ]\n}\n\n\n\nWe recommend reviewing the upcoming changes to Catalogue OData API described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#datetime-precision-change-for-odata-opensearch-and-stac-apis-2024-09-02",
    "href": "APIs/Others/ReleaseNotes.html#datetime-precision-change-for-odata-opensearch-and-stac-apis-2024-09-02",
    "title": "Release notes",
    "section": "Datetime precision change for OData, OpenSearch and STAC APIs [2024-09-02]",
    "text": "Datetime precision change for OData, OpenSearch and STAC APIs [2024-09-02]\nPlease be informed that starting from 2 September 2024, the display precision of all attributes in datetime format within OData, OpenSearch and STAC API interfaces was extended to 6 digits. This change was made to align with the values provided in the product’s metadata and by data sources as well as to standardize the precision across all datetime attributes.\nThe searching queries for all datetime attributes in OData, OpenSearch ans STAC API interfaces remained unchanged.\nPlease find below a list of attributes within our API interfaces that could be affected by this change:\n\nOdata API\n\n\nContentDate/Start\nContentDate/End\nOriginDate\nPublicationDate\nModificationDate\nEvictionDate\nbeginningDateTime\nendingDateTime\nprocessingDate\nproductGeneration\nsegmentStartTime\n\n\n\n\nOpeanSerach API\n\n\nstartDate\ncompletionDate\nupdated\npublished\n\n\n\n\nSTAC API\n\n\ndatetime\nstart_datetime\nend_datetime\nprocessingDate\nsegmentStartTime\nproductGeneration\n\n\n\nBelow please find the examples of the Catalogue API response before and after the described change for the exemplary product with id=521bd8f9-48a5-4e1f-9435-58f97cb64d39.\n\nOData API RequestPrevious responseCurrent response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products(521bd8f9-48a5-4e1f-9435-58f97cb64d39)?$expand=Attributes\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes())/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n  \"Name\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 2121095339,\n  \"OriginDate\": \"2024-06-04T12:12:03.404Z\",\n  \"PublicationDate\": \"2024-06-04T12:18:40.284Z\",\n  \"ModificationDate\": \"2024-06-04T12:19:02.061Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"\",\n  \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"4eec48737ecab7aab2bf590a8dec23b1\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-06-04T12:18:58.614664Z\"\n    },\n    {\n      \"Value\": \"d4e3b3f4fb3e076edc9a841fe74d2a33833fba6685c7854e22026a341038e291\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-06-04T12:19:01.850023Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-06-04T11:01:00.495Z\",\n    \"End\": \"2024-06-04T11:01:31.090Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((113.605408 -29.713636, 116.147186 -29.10652, 115.581909 -27.285719, 113.080147 -27.882109, 113.605408 -29.713636))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"Attributes\": [\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"origin\",\n      \"Value\": \"ESA\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"datatakeID\",\n      \"Value\": 431760,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"timeliness\",\n      \"Value\": \"Fast-24h\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"cycleNumber\",\n      \"Value\": 324,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"orbitNumber\",\n      \"Value\": 54172,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"sliceNumber\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"totalSlices\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productClass\",\n      \"Value\": \"S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorName\",\n      \"Value\": \"Sentinel-1 IPF\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"orbitDirection\",\n      \"Value\": \"ASCENDING\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"processingDate\",\n      \"Value\": \"2024-06-04T12:03:49.113620+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"operationalMode\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingLevel\",\n      \"Value\": \"LEVEL1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"swathIdentifier\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingCenter\",\n      \"Value\": \"Production Service-SERCO\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorVersion\",\n      \"Value\": \"003.71\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"segmentStartTime\",\n      \"Value\": \"2024-06-04T10:59:16.794000+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.BooleanAttribute\",\n      \"Name\": \"sliceProductFlag\",\n      \"Value\": false,\n      \"ValueType\": \"Boolean\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformShortName\",\n      \"Value\": \"SENTINEL-1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productComposition\",\n      \"Value\": \"Slice\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"instrumentShortName\",\n      \"Value\": \"SAR\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"relativeOrbitNumber\",\n      \"Value\": 25,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"polarisationChannels\",\n      \"Value\": \"VV&VH\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformSerialIdentifier\",\n      \"Value\": \"A\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"instrumentConfigurationID\",\n      \"Value\": 7,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"startTimeFromAscendingNode\",\n      \"Value\": 5419578.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"completionTimeFromAscendingNode\",\n      \"Value\": 5450173.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productType\",\n      \"Value\": \"IW_GRDH_1S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"beginningDateTime\",\n      \"Value\": \"2024-06-04T11:01:00.495Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"endingDateTime\",\n      \"Value\": \"2024-06-04T11:01:31.090Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    }\n  ]\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes())/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n  \"Name\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 2121095339,\n  \"OriginDate\": \"2024-06-04T12:12:03.404000Z\",\n  \"PublicationDate\": \"2024-06-04T12:18:40.284131Z\",\n  \"ModificationDate\": \"2024-06-04T12:19:02.060894Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"\",\n  \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"4eec48737ecab7aab2bf590a8dec23b1\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-06-04T12:18:58.614664Z\"\n    },\n    {\n      \"Value\": \"d4e3b3f4fb3e076edc9a841fe74d2a33833fba6685c7854e22026a341038e291\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-06-04T12:19:01.850023Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-06-04T11:01:00.494767Z\",\n    \"End\": \"2024-06-04T11:01:31.089640Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((113.605408 -29.713636, 116.147186 -29.10652, 115.581909 -27.285719, 113.080147 -27.882109, 113.605408 -29.713636))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"Attributes\": [\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"origin\",\n      \"Value\": \"ESA\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"datatakeID\",\n      \"Value\": 431760,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"timeliness\",\n      \"Value\": \"Fast-24h\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"cycleNumber\",\n      \"Value\": 324,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"orbitNumber\",\n      \"Value\": 54172,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"sliceNumber\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"totalSlices\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productClass\",\n      \"Value\": \"S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorName\",\n      \"Value\": \"Sentinel-1 IPF\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"orbitDirection\",\n      \"Value\": \"ASCENDING\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"processingDate\",\n      \"Value\": \"2024-06-04T12:03:49.113620+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"operationalMode\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingLevel\",\n      \"Value\": \"LEVEL1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"swathIdentifier\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingCenter\",\n      \"Value\": \"Production Service-SERCO\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorVersion\",\n      \"Value\": \"003.71\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"segmentStartTime\",\n      \"Value\": \"2024-06-04T10:59:16.794000+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.BooleanAttribute\",\n      \"Name\": \"sliceProductFlag\",\n      \"Value\": false,\n      \"ValueType\": \"Boolean\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformShortName\",\n      \"Value\": \"SENTINEL-1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productComposition\",\n      \"Value\": \"Slice\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"instrumentShortName\",\n      \"Value\": \"SAR\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"relativeOrbitNumber\",\n      \"Value\": 25,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"polarisationChannels\",\n      \"Value\": \"VV&VH\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformSerialIdentifier\",\n      \"Value\": \"A\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"instrumentConfigurationID\",\n      \"Value\": 7,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"startTimeFromAscendingNode\",\n      \"Value\": 5419578.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"completionTimeFromAscendingNode\",\n      \"Value\": 5450173.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productType\",\n      \"Value\": \"IW_GRDH_1S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"beginningDateTime\",\n      \"Value\": \"2024-06-04T11:01:00.494767Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"endingDateTime\",\n      \"Value\": \"2024-06-04T11:01:31.089640Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    }\n  ]\n}\n\n\n\n\nOpenSearch API RequestPrevious responseCurrent response\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=521bd8f9-48a5-4e1f-9435-58f97cb64d39\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"24391a89-5880-56c2-bdf5-f35f3a41178b\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.030846153\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=521bd8f9-48a5-4e1f-9435-58f97cb64d39\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [\n              113.605408,\n              -29.713636\n            ],\n            [\n              116.147186,\n              -29.10652\n            ],\n            [\n              115.581909,\n              -27.285719\n            ],\n            [\n              113.080147,\n              -27.882109\n            ],\n            [\n              113.605408,\n              -29.713636\n            ]\n          ]\n        ]\n      },\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2024-06-04T11:01:00.495Z\",\n        \"completionDate\": \"2024-06-04T11:01:31.090Z\",\n        \"productType\": \"IW_GRDH_1S\",\n        \"processingLevel\": \"LEVEL1\",\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": \"IW\",\n        \"orbitNumber\": 54172,\n        \"quicklook\": null,\n        \"thumbnail\": \"https://catalogue.dataspace.copernicus.eu/get-object?path=/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE/preview/thumbnail.png\",\n        \"updated\": \"2024-06-04T12:19:02.061Z\",\n        \"published\": \"2024-06-04T12:18:40.284Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": \"&lt;gml:Polygon srsName=\\\"EPSG:4326\\\"&gt;&lt;gml:outerBoundaryIs&gt;&lt;gml:LinearRing&gt;&lt;gml:coordinates&gt;113.605408,-29.713636 116.147186,-29.10652 115.581909,-27.285719 113.080147,-27.882109 113.605408,-29.713636&lt;/gml:coordinates&gt;&lt;/gml:LinearRing&gt;&lt;/gml:outerBoundaryIs&gt;&lt;/gml:Polygon&gt;\",\n        \"centroid\": {\n          \"type\": \"Point\",\n          \"coordinates\": [\n            114.604262987064,\n            -28.4994608096117\n          ]\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"orbitDirection\": \"ASCENDING\",\n        \"timeliness\": \"Fast-24h\",\n        \"relativeOrbitNumber\": 25,\n        \"processingBaseline\": 3.71,\n        \"polarisation\": \"VV&VH\",\n        \"swath\": \"IW\",\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 2121095339\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/521bd8f9-48a5-4e1f-9435-58f97cb64d39.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"24391a89-5880-56c2-bdf5-f35f3a41178b\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.030846153\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=521bd8f9-48a5-4e1f-9435-58f97cb64d39\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [\n              113.605408,\n              -29.713636\n            ],\n            [\n              116.147186,\n              -29.10652\n            ],\n            [\n              115.581909,\n              -27.285719\n            ],\n            [\n              113.080147,\n              -27.882109\n            ],\n            [\n              113.605408,\n              -29.713636\n            ]\n          ]\n        ]\n      },\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2024-06-04T11:01:00.494767Z\",\n        \"completionDate\": \"2024-06-04T11:01:31.089640Z\",\n        \"productType\": \"IW_GRDH_1S\",\n        \"processingLevel\": \"LEVEL1\",\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": \"IW\",\n        \"orbitNumber\": 54172,\n        \"quicklook\": null,\n        \"thumbnail\": \"https://catalogue.dataspace.copernicus.eu/get-object?path=/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE/preview/thumbnail.png\",\n        \"updated\": \"2024-06-04T12:19:02.060894Z\",\n        \"published\": \"2024-06-04T12:18:40.284131Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": \"&lt;gml:Polygon srsName=\\\"EPSG:4326\\\"&gt;&lt;gml:outerBoundaryIs&gt;&lt;gml:LinearRing&gt;&lt;gml:coordinates&gt;113.605408,-29.713636 116.147186,-29.10652 115.581909,-27.285719 113.080147,-27.882109 113.605408,-29.713636&lt;/gml:coordinates&gt;&lt;/gml:LinearRing&gt;&lt;/gml:outerBoundaryIs&gt;&lt;/gml:Polygon&gt;\",\n        \"centroid\": {\n          \"type\": \"Point\",\n          \"coordinates\": [\n            114.604262987064,\n            -28.4994608096117\n          ]\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"orbitDirection\": \"ASCENDING\",\n        \"timeliness\": \"Fast-24h\",\n        \"relativeOrbitNumber\": 25,\n        \"processingBaseline\": 3.71,\n        \"polarisation\": \"VV&VH\",\n        \"swath\": \"IW\",\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 2121095339\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/521bd8f9-48a5-4e1f-9435-58f97cb64d39.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n\n\nSTAC API RequestPrevious responseCurrent response\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?ids=S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\n\n\n{\n  \"type\": \"Feature\",\n  \"stac_version\": \"1.0.0\",\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/alternate-assets/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/storage/v1.0.0/schema.json\"\n  ],\n  \"id\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"collection\": \"SENTINEL-1\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"properties\": {\n    \"origin\": \"ESA\",\n    \"datatakeID\": 431760,\n    \"timeliness\": \"Fast-24h\",\n    \"cycleNumber\": 324,\n    \"orbitNumber\": 54172,\n    \"sliceNumber\": 5,\n    \"totalSlices\": 5,\n    \"productClass\": \"S\",\n    \"processorName\": \"Sentinel-1 IPF\",\n    \"orbitDirection\": \"ASCENDING\",\n    \"processingDate\": \"2024-06-04T12:03:49.113620+00:00\",\n    \"operationalMode\": \"IW\",\n    \"processingLevel\": \"LEVEL1\",\n    \"swathIdentifier\": \"IW\",\n    \"processingCenter\": \"Production Service-SERCO\",\n    \"processorVersion\": \"003.71\",\n    \"segmentStartTime\": \"2024-06-04T10:59:16.794000+00:00\",\n    \"sliceProductFlag\": false,\n    \"platformShortName\": \"SENTINEL-1\",\n    \"productComposition\": \"Slice\",\n    \"instrumentShortName\": \"SAR\",\n    \"relativeOrbitNumber\": 25,\n    \"polarisationChannels\": \"VV&VH\",\n    \"platformSerialIdentifier\": \"A\",\n    \"instrumentConfigurationID\": 7,\n    \"startTimeFromAscendingNode\": 5419578.0,\n    \"completionTimeFromAscendingNode\": 5450173.0,\n    \"datetime\": \"2024-06-04T11:01:00.495Z\",\n    \"end_datetime\": \"2024-06-04T11:01:31.090Z\",\n    \"start_datetime\": \"2024-06-04T11:01:00.495Z\",\n    \"productType\": \"IW_GRDH_1S\"\n  },\n  \"bbox\": [\n    113.080147,\n    -29.713636,\n    116.147186,\n    -27.285719\n  ],\n  \"links\": [\n    {\n      \"rel\": \"root\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac\"\n    },\n    {\n      \"rel\": \"self\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\"\n    },\n    {\n      \"rel\": \"collection\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1\"\n    }\n  ],\n  \"assets\": {\n    \"QUICKLOOK\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(28feb491-5c4f-41d4-8444-1e5d8426c478)/$value\",\n      \"title\": \"QUICKLOOK\",\n      \"type\": \"image/png\"\n    },\n    \"PRODUCT\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(521bd8f9-48a5-4e1f-9435-58f97cb64d39)/$value\",\n      \"title\": \"Product\",\n      \"type\": \"application/octet-stream\",\n      \"alternate\": {\n        \"s3\": {\n          \"href\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n          \"storage:platform\": \"CLOUDFERRO\",\n          \"storage:region\": \"waw\",\n          \"storage:requester_pays\": false,\n          \"storage:tier\": \"Online\"\n        }\n      }\n    }\n  }\n}\n\n\n{\n  \"type\": \"Feature\",\n  \"stac_version\": \"1.0.0\",\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/alternate-assets/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/storage/v1.0.0/schema.json\"\n  ],\n  \"id\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"collection\": \"SENTINEL-1\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"properties\": {\n    \"origin\": \"ESA\",\n    \"datatakeID\": 431760,\n    \"timeliness\": \"Fast-24h\",\n    \"cycleNumber\": 324,\n    \"orbitNumber\": 54172,\n    \"sliceNumber\": 5,\n    \"totalSlices\": 5,\n    \"productClass\": \"S\",\n    \"processorName\": \"Sentinel-1 IPF\",\n    \"orbitDirection\": \"ASCENDING\",\n    \"processingDate\": \"2024-06-04T12:03:49.113620+00:00\",\n    \"operationalMode\": \"IW\",\n    \"processingLevel\": \"LEVEL1\",\n    \"swathIdentifier\": \"IW\",\n    \"processingCenter\": \"Production Service-SERCO\",\n    \"processorVersion\": \"003.71\",\n    \"segmentStartTime\": \"2024-06-04T10:59:16.794000+00:00\",\n    \"sliceProductFlag\": false,\n    \"platformShortName\": \"SENTINEL-1\",\n    \"productComposition\": \"Slice\",\n    \"instrumentShortName\": \"SAR\",\n    \"relativeOrbitNumber\": 25,\n    \"polarisationChannels\": \"VV&VH\",\n    \"platformSerialIdentifier\": \"A\",\n    \"instrumentConfigurationID\": 7,\n    \"startTimeFromAscendingNode\": 5419578.0,\n    \"completionTimeFromAscendingNode\": 5450173.0,\n    \"datetime\": \"2024-06-04T11:01:00.494767Z\",\n    \"end_datetime\": \"2024-06-04T11:01:31.089640Z\",\n    \"start_datetime\": \"2024-06-04T11:01:00.494767Z\",\n    \"productType\": \"IW_GRDH_1S\"\n  },\n  \"bbox\": [\n    113.080147,\n    -29.713636,\n    116.147186,\n    -27.285719\n  ],\n  \"links\": [\n    {\n      \"rel\": \"root\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac\"\n    },\n    {\n      \"rel\": \"self\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\"\n    },\n    {\n      \"rel\": \"collection\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1\"\n    }\n  ],\n  \"assets\": {\n    \"QUICKLOOK\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(28feb491-5c4f-41d4-8444-1e5d8426c478)/$value\",\n      \"title\": \"QUICKLOOK\",\n      \"type\": \"image/png\"\n    },\n    \"PRODUCT\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(521bd8f9-48a5-4e1f-9435-58f97cb64d39)/$value\",\n      \"title\": \"Product\",\n      \"type\": \"application/octet-stream\",\n      \"alternate\": {\n        \"s3\": {\n          \"href\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n          \"storage:platform\": \"CLOUDFERRO\",\n          \"storage:region\": \"waw\",\n          \"storage:requester_pays\": false,\n          \"storage:tier\": \"Online\"\n        }\n      }\n    }\n  }\n}"
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#odata-catalogue-api-evictiondate-attribute-update-for-null-values-2024-07-31",
    "href": "APIs/Others/ReleaseNotes.html#odata-catalogue-api-evictiondate-attribute-update-for-null-values-2024-07-31",
    "title": "Release notes",
    "section": "OData Catalogue API: EvictionDate Attribute Update for Null Values [2024-07-31]",
    "text": "OData Catalogue API: EvictionDate Attribute Update for Null Values [2024-07-31]\nPlease be informed that, starting from July 31, 2024, the EvictionDate attribute for products that are not intended to be deleted from the Catalogue has been updated. The new value for such products was set to \"9999-12-31T23:59:59.999Z\". Before, their eviction date was set to \"EvictionDate\": \"\".\nBelow please find the example of ODate API response before and after the described change:\n\nHTTPS RequestPreviuos responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products(02807be7-482e-460e-8a29-7e4ab2320758)\n\n\n\n{\n  \"@odata.context\": \"$metadata#Products/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"02807be7-482e-460e-8a29-7e4ab2320758\",\n  \"Name\": \"S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 28102178,\n  \"OriginDate\": \"2024-07-09T08:35:54.000Z\",\n  \"PublicationDate\": \"2024-07-09T08:44:52.873Z\",\n  \"ModificationDate\": \"2024-07-09T08:45:31.989Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"\",\n  \"S3Path\": \"/eodata/Sentinel-2/MSI/L1C/2024/07/09/S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"3b44d7346a9e8c2e59487f8a9cb86a3f\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.631883Z\"\n    },\n    {\n      \"Value\": \"1c83b6242f72f7daa6a90c350853df95060c755a611542d1b825de781dfe0de2\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.698943Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-07-09T07:46:19.024Z\",\n    \"End\": \"2024-07-09T07:46:19.024Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((62.65582103662547 71.92607834342967, 63.06489612730844 71.91662516221517, 63.18741013834509 72.27252875746295, 63.1849607572785 72.27097856029074, 62.97493608061198 72.13567642459181, 62.76758219658625 72.00017809777046, 62.65582103662547 71.92607834342967))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [...\n    ]\n  }\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"02807be7-482e-460e-8a29-7e4ab2320758\",\n  \"Name\": \"S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 28102178,\n  \"OriginDate\": \"2024-07-09T08:35:54.000Z\",\n  \"PublicationDate\": \"2024-07-09T08:44:52.873Z\",\n  \"ModificationDate\": \"2024-07-09T08:45:31.989Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"9999-12-31T23:59:59.999Z\",\n  \"S3Path\": \"/eodata/Sentinel-2/MSI/L1C/2024/07/09/S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"3b44d7346a9e8c2e59487f8a9cb86a3f\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.631883Z\"\n    },\n    {\n      \"Value\": \"1c83b6242f72f7daa6a90c350853df95060c755a611542d1b825de781dfe0de2\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.698943Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-07-09T07:46:19.024Z\",\n    \"End\": \"2024-07-09T07:46:19.024Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((62.65582103662547 71.92607834342967, 63.06489612730844 71.91662516221517, 63.18741013834509 72.27252875746295, 63.1849607572785 72.27097856029074, 62.97493608061198 72.13567642459181, 62.76758219658625 72.00017809777046, 62.65582103662547 71.92607834342967))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [...\n    ]\n  }\n}"
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#opensearch-catalogue-api-geometry-attribute-handling-updated-for-null-values-2024-07-08",
    "href": "APIs/Others/ReleaseNotes.html#opensearch-catalogue-api-geometry-attribute-handling-updated-for-null-values-2024-07-08",
    "title": "Release notes",
    "section": "OpenSearch Catalogue API: Geometry Attribute Handling Updated for Null Values [2024-07-08]",
    "text": "OpenSearch Catalogue API: Geometry Attribute Handling Updated for Null Values [2024-07-08]\nPlease be informed that our OpenSearch API interface was changed on 8th July, 2024: the geometry attribute handling for Null values was updated.\nAs of 8th July 2024, the geometry attribute in case of empty product geometries returns null (\"geometry\": null,) instead of empty array (\"geometry\": [ ],). This update was designed to improve the clarity and consistency of the data returned by our APIs. For non-empty products geometries, the behaviour remained unchanged.\nBelow please find the example of OpenSearch API response before and after the described change:\n\nHTTPS RequestPrevious responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=cf1a597c-ec22-11ee-8006-fa163e7968e5\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"8cf9566f-ad45-5ed7-b586-7ff0f8d6c677\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.121144462\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=cf1a597c-ec22-11ee-8006-fa163e7968e5\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n      \"geometry\": [\n         \n      ],\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2014-09-08T00:00:00.000Z\",\n        \"completionDate\": \"2014-09-08T00:00:00.000Z\",\n        \"productType\": \"AUX_CAL\",\n        \"processingLevel\": null,\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": null,\n        \"orbitNumber\": 0,\n        \"quicklook\": null,\n        \"thumbnail\": null,\n        \"updated\": \"2024-03-27T10:15:07.353Z\",\n        \"published\": \"2024-03-27T10:15:06.733Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": null,\n        \"centroid\": {\n          \"type\": null,\n          \"coordinates\": null\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/AUX/AUX_CAL/2014/09/08/S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE.zip\",\n        \"orbitDirection\": null,\n        \"timeliness\": null,\n        \"relativeOrbitNumber\": 0,\n        \"processingBaseline\": 0,\n        \"polarisation\": null,\n        \"swath\": null,\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 505960\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/cf1a597c-ec22-11ee-8006-fa163e7968e5.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"8cf9566f-ad45-5ed7-b586-7ff0f8d6c677\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.121144462\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=cf1a597c-ec22-11ee-8006-fa163e7968e5\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n      \"geometry\": null,\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2014-09-08T00:00:00.000Z\",\n        \"completionDate\": \"2014-09-08T00:00:00.000Z\",\n        \"productType\": \"AUX_CAL\",\n        \"processingLevel\": null,\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": null,\n        \"orbitNumber\": 0,\n        \"quicklook\": null,\n        \"thumbnail\": null,\n        \"updated\": \"2024-03-27T10:15:07.353Z\",\n        \"published\": \"2024-03-27T10:15:06.733Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": null,\n        \"centroid\": {\n          \"type\": null,\n          \"coordinates\": null\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/AUX/AUX_CAL/2014/09/08/S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE.zip\",\n        \"orbitDirection\": null,\n        \"timeliness\": null,\n        \"relativeOrbitNumber\": 0,\n        \"processingBaseline\": 0,\n        \"polarisation\": null,\n        \"swath\": null,\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 505960\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/cf1a597c-ec22-11ee-8006-fa163e7968e5.json\"\n          }\n        ]\n      }\n    }\n  ]\n}"
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#default-timezone-change-for-odata-and-opensearch-apis-2024-05-28",
    "href": "APIs/Others/ReleaseNotes.html#default-timezone-change-for-odata-and-opensearch-apis-2024-05-28",
    "title": "Release notes",
    "section": "Default Timezone Change for OData and OpenSearch APIs [2024-05-28]",
    "text": "Default Timezone Change for OData and OpenSearch APIs [2024-05-28]\nPlease be informed that the OData and OpenSearch APIs were updated on 28th May 2024; the default timzone has been changed to UTC.\nAs of 28th May 2024, all API requests without a specified timezone are treated by default as datetime provided in UTC format.\nPreviously, if a client did not specify a timezone in their date request, it defaulted to Warsaw local time. However, starting form 28th May 2024, all API requests without a specified timezone default to datetime provided in UTC format.\nThis change is aimed at standardizing our API responses and ensuring uniformity.\nTo specify a timezone within the request:\nOData API (e.g. UTC-4)\n\nOData Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start gt 2022-05-03T00:00:00.000-04:00 and ContentDate/Start lt 2022-05-04T00:00:00.000-04:00&$top=2\n\n\n\nOpenSearch API (e.g. UTC+1)\n\nOpenSearch Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00+01:00&maxRecords=2\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note while making an API request, any special characters need to be encoded in a specific way so that they can be interpreted correctly by the server. In the case of the plus sign (+), it’s a reserved character in URLs and is interpreted as a space. So, if you want to represent a literal plus sign within your API request, you need to encode it as ‘%2b’.\n\n\nExamples of API requests without the timezone and API responses before and after the change:\nOData API Example\n\nHTTP RequestPrevious responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start gt 2022-05-03T00:00:00.000 and ContentDate/Start lt 2022-05-04T00:00:00.000&$top=2\n\n\n{\n  \"@odata.context\": \"$metadata#Products\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n      \"Name\": \"S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 8231197123,\n      \"OriginDate\": \"2022-05-02T23:30:02.126Z\",\n      \"PublicationDate\": \"2022-05-02T23:40:02.825Z\",\n      \"ModificationDate\": \"2024-03-16T03:19:06.436Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"\",\n      \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2022/05/02/S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n      \"Checksum\": [\n        {\n          \"Value\": \"65940707f71f444b0fa05141657cc387\",\n          \"Algorithm\": \"MD5\",\n          \"ChecksumDate\": \"2024-03-16T03:18:49.857391Z\"\n        },\n        {\n          \"Value\": \"3d2d07a95aad14f1fb77ea5ba49485b6efee667578a74257d07b9edbd9d4912a\",\n          \"Algorithm\": \"BLAKE3\",\n          \"ChecksumDate\": \"2024-03-16T03:19:07.058832Z\"\n        }\n      ],\n      \"ContentDate\": {\n        \"Start\": \"2022-05-02T22:06:17.548Z\",\n        \"End\": \"2022-05-02T22:06:47.359Z\"\n      },\n      \"Footprint\": \"geography'SRID=4326;POLYGON ((-57.750202 -2.026322, -57.366844 -3.828814, -55.15321 -3.341953, -55.540607 -1.545508, -57.750202 -2.026322))'\",\n      \"GeoFootprint\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [\n              -57.750202,\n              -2.026322\n            ],\n            [\n              -57.366844,\n              -3.828814\n            ],\n            [\n              -55.15321,\n              -3.341953\n            ],\n            [\n              -55.540607,\n              -1.545508\n            ],\n            [\n              -57.750202,\n              -2.026322\n            ]\n          ]\n        ]\n      }\n    },\n    {...\n    }\n  ],\n  \"@odata.nextLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?%24filter=Collection%2FName+eq+%27SENTINEL-1%27+and+ContentDate%2FStart+gt+2022-05-03T00%3A00%3A00.000+and+ContentDate%2FStart+lt+2022-05-04T00%3A00%3A00.000&%24top=2&%24skip=2\"\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n      \"Name\": \"S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 2663000,\n      \"OriginDate\": \"2022-05-10T02:30:11.130Z\",\n      \"PublicationDate\": \"2023-10-25T13:45:19.736Z\",\n      \"ModificationDate\": \"2023-11-14T22:50:17.708Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"\",\n      \"S3Path\": \"/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n      \"Checksum\": [\n        {\n          \"Value\": \"6a99572d2baaa3c9a83bd851ba3ba70f\",\n          \"Algorithm\": \"MD5\",\n          \"ChecksumDate\": \"2023-11-14T22:50:17.595702Z\"\n        },\n        {\n          \"Value\": \"d42f79c1ab8840db09d7596dea4ee40b175df7795dd186f812eafe4a5fa21aab\",\n          \"Algorithm\": \"BLAKE3\",\n          \"ChecksumDate\": \"2023-11-14T22:50:17.616477Z\"\n        }\n      ],\n      \"ContentDate\": {\n        \"Start\": \"2022-05-03T00:00:04.000Z\",\n        \"End\": \"2022-05-03T23:59:54.000Z\"\n      },\n      \"Footprint\": null,\n      \"GeoFootprint\": null\n    },\n    {...\n    }\n  ],\n  \"@odata.nextLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?%24filter=Collection%2FName+eq+%27SENTINEL-1%27+and+ContentDate%2FStart+gt+2022-05-03T00%3A00%3A00.000Z+and+ContentDate%2FStart+lt+2022-05-04T00%3A00%3A00.000Z&%24top=2&%24skip=2\"\n}\n\n\n\nOpenSearch API Example\n\nHTTP RequestPrevious responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00&maxRecords=2\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"04139de2-34f6-56d0-b36f-122f1a3c290a\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 2,\n    \"query\": {\n      \"originalFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.163432102\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00&maxRecords=2\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00%3A00%3A00&maxRecords=2&page=2\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [...\n        ]\n      },\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/SAR/SLC/2022/05/02/S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": \"ESA\",\n        \"startDate\": \"2022-05-02T22:06:17.548Z\",\n        \"completionDate\": \"2022-05-02T22:06:47.359Z\",\n        \"productType\": \"IW_SLC__1S\",\n        \"processingLevel\": \"LEVEL1\",\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 2.3,\n        \"sensorMode\": \"IW\",\n        \"orbitNumber\": 43037,\n        \"quicklook\": null,\n        \"thumbnail\": \"https://catalogue.dataspace.copernicus.eu/get-object?path=/Sentinel-1/SAR/SLC/2022/05/02/S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE/preview/thumbnail.png\",\n        \"updated\": \"2024-03-16T03:19:06.436Z\",\n        \"published\": \"2022-05-02T23:40:02.825Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": \"&lt;gml:Polygon srsName=\\\"EPSG:4326\\\"&gt;&lt;gml:outerBoundaryIs&gt;&lt;gml:LinearRing&gt;&lt;gml:coordinates&gt;-57.750202,-2.026322 -57.366844,-3.828814 -55.15321,-3.341953 -55.540607,-1.545508 -57.750202,-2.026322&lt;/gml:coordinates&gt;&lt;/gml:LinearRing&gt;&lt;/gml:outerBoundaryIs&gt;&lt;/gml:Polygon&gt;\",\n        \"centroid\": {\n          \"type\": \"Point\",\n          \"coordinates\": [\n            -56.45314692566288,\n            -2.68610524638\n          ]\n        },\n        \"orbitDirection\": \"ASCENDING\",\n        \"timeliness\": \"Fast-24h\",\n        \"relativeOrbitNumber\": 90,\n        \"processingBaseline\": 0,\n        \"polarisation\": \"VV&VH\",\n        \"swath\": \"IW1 IW2 IW3\",\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 8231197123\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/896aeef0-eee1-5e28-acaa-7f420bb23e8c.json\"\n          }\n        ]\n      }\n    },\n    {...\n    }\n  ]\n}\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"1f7387ef-7456-5a77-ba63-fa036a7659cd\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 2,\n    \"query\": {\n      \"originalFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00Z\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00Z\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.032169373\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00Z&maxRecords=2\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00%3A00%3A00Z&maxRecords=2&page=2\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n      \"geometry\": [\n         \n      ],\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2022-05-03T00:00:04.000Z\",\n        \"completionDate\": \"2022-05-03T23:59:54.000Z\",\n        \"productType\": \"AUX_GNSSRD\",\n        \"processingLevel\": null,\n        \"platform\": \"SENTINEL-1\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": null,\n        \"orbitNumber\": 0,\n        \"quicklook\": null,\n        \"thumbnail\": null,\n        \"updated\": \"2023-11-14T22:50:17.708Z\",\n        \"published\": \"2023-10-25T13:45:19.736Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": null,\n        \"centroid\": {\n          \"type\": null,\n          \"coordinates\": null\n        },\n        \"orbitDirection\": null,\n        \"timeliness\": null,\n        \"relativeOrbitNumber\": 0,\n        \"processingBaseline\": 0,\n        \"polarisation\": null,\n        \"swath\": null,\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 2663000\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/1d42f2d3-2456-485f-a93e-92f08bdd5c51.json\"\n          }\n        ]\n      }\n    },\n    {...\n    }\n  ]\n}"
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#opensearch-catalog-api-updates",
    "href": "APIs/Others/ReleaseNotes.html#opensearch-catalog-api-updates",
    "title": "Release notes",
    "section": "OpenSearch Catalog API updates",
    "text": "OpenSearch Catalog API updates\n\nOpenSearch API error handling update [2023-10-24]\nPlease be informed that the OpenSearch API error handling was updated on 24th October 2023.\nPlease note that new responses with errors provide the RequestId, which is intended to help identify the requests with errors. Including the RequestId in your issues submitted to the support team is strongly recommended in case of Catalog API problems.\nThe new error handling is described below.\n\nIncorrect collection name\n\nPrevious responseNew response\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"loadFromStore - Not Found\",\n        \"ErrorCode\": 404\n    }\n}\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown collection.\",\n        \"ErrorCode\": 404,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"collection\"\n                ],\n                \"msg\": \"Collection '&lt;collection name presented in query&gt;' does not exist.\",\n            },\n        ],\n        \"RequestId\": &lt;request_id&gt;,\n    }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinl2/search.json\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown collection.\",\n        \"ErrorCode\": 404,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"collection\"\n                ],\n                \"msg\": \"Collection 'Sentinl2' does not exist.\"\n            },\n        ],\n        \"RequestID\": \"70970f42-e374-4e26-8778-41a1463e700d\"\n    }\n}\n\n\n\n\n\n\nIncorrect name of the query parameter\n(when the collection is not specified)\n\nPrevious responseNew response\n\n\nNo error is returned. The incorrect query parameter is ignored and not reflected in appliedFilters.\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of unexisting parameters&gt;],\n                \"msg\": \"Query parameters do not exist.\",\n            },\n        ],\n        \"RequestId\": &lt;request_id&gt;,\n    }\n}\n\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?productsType=S2MSI1C\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": {\n            \"loc\": [\n                \"productsType\"\n            ],\n            \"msg\": \"Query parameters do not exist.\"\n        },\n        \"RequestID\": \"d9f22173-4d56-44fd-ab18-35d6018c49d7\"\n    }\n}\n\n\n\n\n\nIncorrect name of the query parameter\n(when the collection is specified)\n\nPrevious responseNew response\n\n\nNo error is returned. The incorrect query parameter is ignored and not reflected in appliedFilters.\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of unexisting parameters&gt;],\n                \"msg\": \"Query parameters do not exist or are not available for specified collection.\",\n            },\n        ],\n        \"RequestId\": &lt;request_id&gt;,\n  }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?productType=S2MSI1C&startDat=2023-06-11&completionDte=2023-06-22\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": {\n            \"loc\": [\n                \"startDat\",\n                \"completionDte\",\n                        ],\n            \"msg\": \"Query parameters do not exist or are not available for specified collection.\"\n        },\n        \"RequestID\": \"25d522af-ba4e-4152-a368-9635d560e649\"\n    }\n}\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note that dataset parameter is not supported anymore. Any query with the dataset parameter results in an error.\n\n\n\n\nIncorrect value of the query parameter\n(maxRecords, index, page, sortParam, sortOrder, exactCount, geometry, box, lon, lat, radius, startDate, completionDate, updated, published, publishedAfter, publishedBefore, status)\n\nPrevious responseNew response\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": &lt;error message&gt;,\n        \"ErrorCode\": 400\n    }\n}\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of parameters that error \"msg\" field relate to&gt;],\n                \"msg”: &lt;error message&gt;}&gt;,\n            },\n        ]\n        \"RequestId\": &lt;request_id&gt;,\n  }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?startDate=2021-07-01T00:00:00Z&completionDate=2021-07-31T23:59:59Z&maxRecords=2001\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"maxRecords\"\n                ],\n                \"msg\": \"Input should be less than or equal to 2000.\"\n            }\n        ],\n        \"RequestID\": \"b3b4c0bb-9697-4ff8-b90c-4eb1b97a9914\"\n    }\n}\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe kindly remind you that for status parameter the only acceptable values are:\n\nONLINE\nOFFLINE\nALL\n\nAny other value results in an error.\n\n\n\n\nIncorrect value type of the query parameter\n\nPrevious responseNew response\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": &lt;error message&gt;,\n        \"ErrorCode\": 400\n    }\n}\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of parameters that error \"msg\" field relate to&gt;],\n                \"msg”: &lt;error message&gt;}&gt;,\n            },\n        ]\n        \"RequestId\": &lt;request_id&gt;,\n  }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?orbitNumber=ascending\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"orbitNumber\"\n                ],\n                \"msg\": \"Proper value types for specified attribute query parameters are: 'orbitNumber'-integer\"\n            }\n        ],\n        \"RequestID\": \"33e3ebb0-7d44-4dcd-8cb2-f60216c11cef\"\n    }\n}\n\n\n\nPlease also note about the following change:\n\nupdate of the last link\n\nThe last link is provided only when exactCount is used in the request.\n\nLink last example\n\n\n{\n    \"rel\": \"last\",\n    \"type\": \"application/json\",\n    \"title\": \"last\",\n    \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?page=19168&processingLevel=S2MSI1C&startDate=2023-07-01&completionDate=2023-07-31&sortParam=startDate&exactCount=1\"\n}\n\n\n\n\n\nupdate of the exactCount parameter\n\nThe exactCount parameter is not set by default to 1 (True) anymore. Now, users need to declare its value as 1 to receive totalResults count."
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#odata-catalog-api-updates",
    "href": "APIs/Others/ReleaseNotes.html#odata-catalog-api-updates",
    "title": "Release notes",
    "section": "OData Catalog API updates",
    "text": "OData Catalog API updates\n\nOData API single product response update [2023-10-02]\nTo comply with the OData PRIP interfaces, the OData API response for a single product query was updated. The response returns the product itself instead of a list containing the product (assigned to the field value). The example reflecting the change is presented below:\n\nOData API request example\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products(f5439b10-deb4-420e-9b84-146035fbb16c)\n\n\n\n\nOData API response change\n\nNew responsePrevious response\n\n\n{\n    \"@odata.context\": \"$metadata#Products\",\n    \"@odata.mediaContentType\": \"application/octet-stream\",\n    \"Id\": \"f5439b10-deb4-420e-9b84-146035fbb16c\",\n    \"Name\": \"S3B_SL_2_LST____20230824T223252_20230824T223552_20230825T011047_0179_083_158_1080_PS2_O_NR_004.SEN3\",\n    \"ContentType\": \"application/octet-stream\",\n    \"ContentLength\": 63028951,\n    \"OriginDate\": \"2023-08-25T01:11:48.997Z\",\n    \"PublicationDate\": \"2023-08-25T01:17:46.464Z\",\n    \"ModificationDate\": \"2023-08-25T01:18:00.817Z\",\n    \"Online\": true,\n    \"EvictionDate\": \"2023-09-09T00:00:00.000Z\",\n    \"S3Path\": \"/eodata/Sentinel-3/SLSTR/SL_2_LST___/2023/08/24/S3B_SL_2_LST____20230824T223252_20230824T223552_20230825T011047_0179_083_158_1080_PS2_O_NR_004.SEN3\",\n    \"Checksum\": [\n        {\n            \"Value\": \"ef069a2764616473ad472a5eb844ec5a\",\n            \"Algorithm\": \"MD5\",\n            \"ChecksumDate\": \"2023-08-25T01:18:00.289795Z\"\n        },\n        {\n            \"Value\": \"8d70cd42aed5626184a3a314a939153c937ea030a7a593b8a1f121b375faa77f\",\n            \"Algorithm\": \"BLAKE3\",\n            \"ChecksumDate\": \"2023-08-25T01:18:00.495218Z\"\n        }\n    ],\n    \"ContentDate\": {\n        \"Start\": \"2023-08-24T22:32:51.550Z\",\n        \"End\": \"2023-08-24T22:35:51.550Z\"\n    },\n    \"Footprint\": \"geography'SRID=4326;POLYGON ((...))'\",\n    \"GeoFootprint\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [...\n        ]\n    }\n}\n\n\n\n{\n    \"@odata.context\": \"$metadata#Products\",\n    \"value\": [\n        {\n            \"@odata.mediaContentType\": \"application/octet-stream\",\n            \"Id\": \"f5439b10-deb4-420e-9b84-146035fbb16c\",\n            \"Name\": \"S3B_SL_2_LST____20230824T223252_20230824T223552_20230825T011047_0179_083_158_1080_PS2_O_NR_004.SEN3\",\n            \"ContentType\": \"application/octet-stream\",\n            \"ContentLength\": 63028951,\n            \"OriginDate\": \"2023-08-25T01:11:48.997Z\",\n            \"PublicationDate\": \"2023-08-25T01:17:46.464Z\",\n            \"ModificationDate\": \"2023-08-25T01:18:00.817Z\",\n            \"Online\": true,\n            \"EvictionDate\": \"2023-09-09T00:00:00.000Z\",\n            \"S3Path\": \"/eodata/Sentinel-3/SLSTR/SL_2_LST___/2023/08/24/S3B_SL_2_LST____20230824T223252_20230824T223552_20230825T011047_0179_083_158_1080_PS2_O_NR_004.SEN3\",\n            \"Checksum\": [\n                {\n                    \"Value\": \"ef069a2764616473ad472a5eb844ec5a\",\n                    \"Algorithm\": \"MD5\",\n                    \"ChecksumDate\": \"2023-08-25T01:18:00.289795Z\"\n                },\n                {\n                    \"Value\": \"8d70cd42aed5626184a3a314a939153c937ea030a7a593b8a1f121b375faa77f\",\n                    \"Algorithm\": \"BLAKE3\",\n                    \"ChecksumDate\": \"2023-08-25T01:18:00.495218Z\"\n                }\n            ],\n            \"ContentDate\": {\n                \"Start\": \"2023-08-24T22:32:51.550Z\",\n                \"End\": \"2023-08-24T22:35:51.550Z\"\n            },\n            \"Footprint\": \"geography'SRID=4326;POLYGON ((...))'\",\n            \"GeoFootprint\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [...]\n            }\n        }\n    ]\n}"
  },
  {
    "objectID": "APIs/Others/ReleaseNotes.html#stac-catalog-api-updates",
    "href": "APIs/Others/ReleaseNotes.html#stac-catalog-api-updates",
    "title": "Release notes",
    "section": "STAC Catalog API updates",
    "text": "STAC Catalog API updates\n\nSTAC API interface update [2023-10-09]\nThe STAC API interface was updated to comply with the STAC item specification. The beginningDateTime and endingDateTime attributes were replaced by start_datetime and end_datetime.\n\nSTAC Request Example\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-3/items?\n\n\n\n\nSTAC API response change\n\nNew responsePrevious response\n\n\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"stac_version\": \"1.0.0\",\n            \"stac_extensions\": [],\n            \"id\": \"S3B_SY_2_VG1____20230901T000000_20230901T235959_20230902T123317_WEST_ASIA_________PS2_O_ST_002.SEN3\",\n            \"collection\": \"SENTINEL-3\",\n            \"geometry\": {...},\n            \"properties\": {\n                \"landCover\": 42.043158,\n                \"cloudCover\": 4.955728,\n                \"timeliness\": \"ST\",\n                \"cycleNumber\": 83,\n                \"orbitNumber\": 27607,\n                \"processorName\": \"PUG\",\n                \"orbitDirection\": \"ascending\",\n                \"processingDate\": \"2023-09-02T12:41:10+00:00\",\n                \"snowOrIceCover\": 0.135336,\n                \"operationalMode\": \"Earth Observation\",\n                \"processingLevel\": \"2\",\n                \"processorVersion\": \"03.49\",\n                \"platformShortName\": \"SENTINEL-3\",\n                \"baselineCollection\": \"002\",\n                \"instrumentShortName\": \"SYNERGY\",\n                \"relativeOrbitNumber\": 0,\n                \"platformSerialIdentifier\": \"B\",\n                \"datetime\": null,\n                \"end_datetime\": \"2023-09-01T23:59:59.000Z\",\n                \"start_datetime\": \"2023-09-01T00:00:00.000Z\",\n                \"productType\": \"SY_2_VG1___\"\n            },\n            \"bbox\": [...],\n            \"links\": [...],\n            \"assets\": {...}\n        }\n    ]\n}\n\n\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"stac_version\": \"1.0.0\",\n            \"stac_extensions\": [],\n            \"id\": \"S3B_SY_2_VG1____20230901T000000_20230901T235959_20230902T123317_WEST_ASIA_________PS2_O_ST_002.SEN3\",\n            \"collection\": \"SENTINEL-3\",\n            \"geometry\": {...},\n            \"properties\": {\n                \"landCover\": 42.043158,\n                \"cloudCover\": 4.955728,\n                \"timeliness\": \"ST\",\n                \"cycleNumber\": 83,\n                \"orbitNumber\": 27607,\n                \"processorName\": \"PUG\",\n                \"orbitDirection\": \"ascending\",\n                \"processingDate\": \"2023-09-02T12:41:10+00:00\",\n                \"snowOrIceCover\": 0.135336,\n                \"operationalMode\": \"Earth Observation\",\n                \"processingLevel\": \"2\",\n                \"processorVersion\": \"03.49\",\n                \"platformShortName\": \"SENTINEL-3\",\n                \"baselineCollection\": \"002\",\n                \"instrumentShortName\": \"SYNERGY\",\n                \"relativeOrbitNumber\": 0,\n                \"platformSerialIdentifier\": \"B\",\n                \"datetime\": null,\n                \"end_datetime\": \"2023-09-01T23:59:59.000Z\",\n                \"start_datetime\": \"2023-09-01T00:00:00.000Z\",\n                \"endingDateTime\": \"2023-09-01T23:59:59.000Z\",\n                \"beginningDateTime\": \"2023-09-01T00:00:00.000Z\",\n                \"productType\": \"SY_2_VG1___\"\n            },\n            \"bbox\": [...],\n            \"links\": [...],\n            \"assets\": {...}\n        }\n    ]\n}"
  },
  {
    "objectID": "APIs/Traceability.html",
    "href": "APIs/Traceability.html",
    "title": "Traceability Service",
    "section": "",
    "text": "Traceability Service allows the user to track a data product’s lifecycle. It acts as a historian of the product’s lifecycle, collecting the traces of all related events. These traces then can be used to check the integrity of the product, its current whereabouts, its impact on other products or, ultimately, its inadequacy for continued use in case of obsolescence. Digital signatures on the traces provide users with the ability to verify the authenticity and integrity of the traces themselves – this also enables users to detect any alterations of the product during its lifecycle.\nUsers may interact directly with Traceability API, e.g., curl, or through the open-source Traceability command line utility.\nTraceability Service API OpenAPI endpoint documentation: https://trace.dataspace.copernicus.eu/api/docs\nTraceability Service command line utility: https://github.com/eu-cdse/trace-cli"
  },
  {
    "objectID": "APIs/Traceability.html#example",
    "href": "APIs/Traceability.html#example",
    "title": "Traceability Service",
    "section": "Example",
    "text": "Example\nInteraction with Traceability Service by using the curl command on Linux:\ncurl -X 'GET' 'https://trace.dataspace.copernicus.eu/api/v1/traces/name/S2A_MSIL1C_20230420T100021_N0509_R122_T33UVP_20230420T120027.SAFE.zip' -H 'accept: application/json'\nPlease be aware that curl command might have a different syntax on Windows. Please refer to the curl official documentation if you have any questions (https://curl.se/docs/manual.html)."
  },
  {
    "objectID": "APIs/Traceability.html#direct-access",
    "href": "APIs/Traceability.html#direct-access",
    "title": "Traceability Service",
    "section": "Direct access",
    "text": "Direct access\nInteraction with Traceability Service directly via the Traceability Service API: https://trace.dataspace.copernicus.eu/api/v1/traces/name/S2A_MSIL1C_20230420T100021_N0509_R122_T33UVP_20230420T120027.SAFE.zip"
  },
  {
    "objectID": "APIs/Traceability.html#digital-certificates",
    "href": "APIs/Traceability.html#digital-certificates",
    "title": "Traceability Service",
    "section": "Digital Certificates",
    "text": "Digital Certificates\nThe Traceability service uses X.509 Certificates to sign information sent to the API. This allows verification of the identity of the user who created the trace.\nUse of certificates with the trace-cli tool:\nhttps://github.com/eu-cdse/trace-cli#digital-signatures\nUsers may use certificates from a trusted public authority or the Copernicus Data Space Ecosystem Certificate Authority.\n\nAbout Our Certificate Authority (CA)\nCopernicus Data Space Ecosystem operates its own Certificate Authority (CA). A CA is a trusted entity that issues digital certificates, which are data files used to link an entity with a public key cryptographically. The role of our CA is to guarantee that the individual granted the unique certificate is, who they claim to be. This is crucial for secure digital identification and trust.\n\n\nWhy Do You Need Our Root Certificates?\nCertificates are used to sign information sent to our Traceability service. By installing our root certificates, your system will trust the information signed with our certificates generated by the Copernicus Data Space Ecosystem CA. This is crucial for ensuring the integrity and the authenticity of the messages you receive.\n\n\nHow to Install Our Root Certificates?\nUsers can download the root certificates by using the links below. Once downloaded, you can install them into your operating system certificate store. The process varies depending on your operating system and browser, so please refer to their respective documentation for detailed instructions.\n\nLink to certificate: https://ca.cloudferro.com/certs/ca.crt\n\nLink to certificate revocation list: https://ca.cloudferro.com/certs/cdse-ca.crl"
  },
  {
    "objectID": "APIs/openEO/credit_usage.html",
    "href": "APIs/openEO/credit_usage.html",
    "title": "Credit Usage",
    "section": "",
    "text": "To access and use openEO functionalities, users must log in with a CDSE account. The API is a publicly accessible service that is available free of charge, but it is backed by a limited amount of cloud resources. To ensure fair distribution of these free cloud resources, each user is allocated a set of ‘credits’ on a monthly basis.\n**Every registered CDSE user is by default has the Copernicus General user type. Users can check the typology of their account under the Account info in Copernicus Data Space Ecosystem Dashboard .\nA Copernicus General user receives 10000 free openEO credits automatically on the first of every month to explore and start using the openEO API.** Users can check their current credit balance in the openEO Algorithm Plaza under the ‘Billing’ tab. If the provided credits are insufficient, you can Submit a request for further support and guidance.\nHigher quota (credits) is granted for Collaborative Ground Segment users or Copernicus Service users based on their specific usage needs. They receive a total of 20000 credits per month that is replenished at the beginning of each month.\nCopernicus General users can request an upgrade to a higher-tier user types by taking into account the user definitions and eligibility criteria explained in Which users are qualified for higher tier accounts?."
  },
  {
    "objectID": "APIs/openEO/credit_usage.html#factors-affecting-credit-usage",
    "href": "APIs/openEO/credit_usage.html#factors-affecting-credit-usage",
    "title": "Credit Usage",
    "section": "Factors affecting credit usage",
    "text": "Factors affecting credit usage\nCredits will deducted based on the chosen openEO services or processes. The amount varies depending on the processing complexity of the used services. Important parameters to take into consideration are:\n\nCPU usage (cores/second)\nMemory usage (GB/second)\nStorage (GB/day)\nData access to specific layers (e.g. Sentinel Hub or commercial)\nUsage of services contributed by third parties through an ‘added value’ cost (e.g. per hectare)\nUsage of synchronous requests, which currently have a fixed cost of six credits per request.\nStartup & storage cost of batch jobs is currently two credits.\n\nFor example, consider a scenario where we compute a Sentinel-2 based NDWI for 10 hectares, resulting in a series of GeoTiffs (one for each observation). When running this example, the batch job reports its usage, which can be monitored in the OpenEO Web Editor by clicking the  button for an individual batch job:\n\nUsage metrics are shown in the OpenEO Web Editor for an example batch job:\n\n13,400,773 mb-seconds corresponds to 3.64 GB hours or 3.64 credits\n5099 CPU seconds corresponds to 1.4 CPU hour, which translates to 2.12 credits\nA fixed cost of 2 credits is charged per batch job for management overhead and storage.\n\nSumming this up, the total comes to 7.76 credits. With the current free tier offering 10,000 credits(¹), quite a few jobs like this can be run! However, it is important to note that resource consumption (CPU and memory in this case) is not fixed over time. Performance characteristics of a particular cloud can fluctuate depending on the overall load.\nCloud providers try to avoid this, but generally, they only manage to do so within the limits of a given SLA.\n(¹)Temporary Boost of monthly openEO Credits to 10 000. Read our news item published on September 3, 2024."
  },
  {
    "objectID": "APIs/openEO/credit_usage.html#estimating-resource-usage",
    "href": "APIs/openEO/credit_usage.html#estimating-resource-usage",
    "title": "Credit Usage",
    "section": "Estimating resource usage",
    "text": "Estimating resource usage\nOften, users want to know in advance the costs incurred by using openEO, especially when generating larger results or running the same job at scheduled times. This is not trivial to determine without running the job, as resource consumption heavily depends on the specific combination of processes used.\nTherefore, to estimate job usage, it is advisable to start small. For instance, for a query at a 10m resolution, begin with a 10ha area and simply run that job. As shown above, this will usually incur a cost of only a few cents. In the worst case, it might cost a few euros, but this would also indicate that the job takes multiple hours to run.\nOnce an initial cost for a small area is established, it is possible to extrapolate to a larger area. If simple linear extrapolation shows that a larger job is affordable, proceed with a job on larger areas, like 50ha or up to 100x100km. This will show how the job scales and the associated cost. If at any point the cost appears unreasonable, do not hesitate to reach out on our forum or help desk!\nFor more comprehensive guidelines on large-scale processing, please refer to our documentation here."
  },
  {
    "objectID": "APIs/openEO/openEO.html",
    "href": "APIs/openEO/openEO.html",
    "title": "openEO",
    "section": "",
    "text": "openEO represents an innovative community standard that revolutionizes geospatial data processing and analysis. This groundbreaking framework provides a novel approach to accessing, processing, and analyzing diverse Earth observation data. By adopting openEO, developers, researchers, and data scientists gain access to a unified and interoperable platform, empowering them to harness distributed computing environments and leverage cloud-based resources for addressing complex geospatial challenges.\n\n\n\nWith openEO’s collaborative nature, users can seamlessly share code, workflows, and data processing methods across platforms and tools, fostering collaboration and advancing the accessibility, scalability, and reproducibility of Earth observation data. Additionally, openEO provides intuitive programming libraries that enable easy analysis of diverse Earth observation datasets. These libraries facilitate efficient access and processing of large-scale data across multiple infrastructures, supporting various applications, including exploratory research, detailed mapping, and information extraction from Earth observation. Moreover, this streamlined approach enhances the development process, enabling the utilization of Earth observation data for a wide range of applications and services.\nThe endpoint for the public service is 100% open source and compatible with Pangeo technology. Read more about it here.\nEndpoint: https://openeo.dataspace.copernicus.eu/\nConcrete examples of what you can do with openEO can be found in the notebooks section of this documentation."
  },
  {
    "objectID": "APIs/openEO/openEO.html#overview",
    "href": "APIs/openEO/openEO.html#overview",
    "title": "openEO",
    "section": "",
    "text": "openEO represents an innovative community standard that revolutionizes geospatial data processing and analysis. This groundbreaking framework provides a novel approach to accessing, processing, and analyzing diverse Earth observation data. By adopting openEO, developers, researchers, and data scientists gain access to a unified and interoperable platform, empowering them to harness distributed computing environments and leverage cloud-based resources for addressing complex geospatial challenges.\n\n\n\nWith openEO’s collaborative nature, users can seamlessly share code, workflows, and data processing methods across platforms and tools, fostering collaboration and advancing the accessibility, scalability, and reproducibility of Earth observation data. Additionally, openEO provides intuitive programming libraries that enable easy analysis of diverse Earth observation datasets. These libraries facilitate efficient access and processing of large-scale data across multiple infrastructures, supporting various applications, including exploratory research, detailed mapping, and information extraction from Earth observation. Moreover, this streamlined approach enhances the development process, enabling the utilization of Earth observation data for a wide range of applications and services.\nThe endpoint for the public service is 100% open source and compatible with Pangeo technology. Read more about it here.\nEndpoint: https://openeo.dataspace.copernicus.eu/\nConcrete examples of what you can do with openEO can be found in the notebooks section of this documentation."
  },
  {
    "objectID": "APIs/openEO/openEO.html#added-value-of-openeo-api",
    "href": "APIs/openEO/openEO.html#added-value-of-openeo-api",
    "title": "openEO",
    "section": "Added value of openEO API",
    "text": "Added value of openEO API\nThe key benefits of using openEO API can be summarized as follows:\n\nUnified and straightforward access to multiple Earth observation datasets.\nScalable and efficient processing capabilities.\nA standardized system that works across different platforms.\nIndependence from underlying technologies and software libraries.\nReproducibility through transparent workflows, supporting principles of FAIR (Findable, Accessible, Interoperable, and Reusable) and Open Science.\n\nWhen using the openEO API, users can choose JavaScript, Python, or R as their client library. This allows them to work with any backend and compare them based on capacity, cost, and result quality.\nNevertheless, if you are unfamiliar with programming, you could start using the web-based editor for openEO. It supports visual modelling of your algorithms and simplified JavaScript-based access to the openEO workflows and providers. An overview of the openEO web-editor is available in the Applications section of this documentation."
  },
  {
    "objectID": "APIs/openEO/openEO.html#datacubes",
    "href": "APIs/openEO/openEO.html#datacubes",
    "title": "openEO",
    "section": "Datacubes",
    "text": "Datacubes\nIn openEO, a datacube is a fundamental concept and a key component of the platform. Data is represented as datacubes in openEO, which are multi-dimensional arrays with additional information about their dimensionality. Datacubes can provide a nice and tidy interface for spatiotemporal data as well as for the operations you may want to execute on them. An in-depth introduction to datacubes and processing them with openEO can be found here."
  },
  {
    "objectID": "APIs/openEO/openEO.html#collections",
    "href": "APIs/openEO/openEO.html#collections",
    "title": "openEO",
    "section": "Collections",
    "text": "Collections\nIn openEO, a backend offers set of collections to be processed. A user can access and process from this comprehensive list of data collections available in the Copernicus Data Space Ecosystem backend through openEO. They can load (a subset of) a collection using the load_collection process, which returns a raster datacube."
  },
  {
    "objectID": "APIs/openEO/openEO.html#file-formats",
    "href": "APIs/openEO/openEO.html#file-formats",
    "title": "openEO",
    "section": "File formats",
    "text": "File formats\nDepending on the data cube that your process graph creates and on your later use case, some file formats are more suitable to export your data (save_result) than others. A user can choose among possible output formats within openEO."
  },
  {
    "objectID": "APIs/openEO/openEO.html#processes",
    "href": "APIs/openEO/openEO.html#processes",
    "title": "openEO",
    "section": "Processes",
    "text": "Processes\nIn openEO, a user can find several processes to perform operations on Earth Observation data. These can range from simple add() to complex predict_probabilities(). Furthermore, a user can create complex workflows by chaining multiple processes together."
  },
  {
    "objectID": "APIs/openEO/openEO.html#support",
    "href": "APIs/openEO/openEO.html#support",
    "title": "openEO",
    "section": "Support",
    "text": "Support\nNeed help locating your preferred programming language? Or you need help finding functionalities that you want to use. Then you have the option to report issues via Submit a request or actively proposing API changes through Pull Request to our GitHub repo."
  },
  {
    "objectID": "APIs/openEO/openEO.html#general-limitations",
    "href": "APIs/openEO/openEO.html#general-limitations",
    "title": "openEO",
    "section": "General limitations",
    "text": "General limitations\n\nSize of retrieved data\nAs a rule of thumb, openEO is tested up to areas of 100x100km at 10m resolution. Larger extents may work, but we recommend always starting first with smaller areas. Also, consider that large area jobs may result in outputs of multiple gigabytes that become inconveniently large to download or handle on your local machine.\nAlso, the temporal extent can have a significant impact, so here, we recommend starting with a small extent. Note that it is possible to increase job resources.\n\n\nFree tier limitations\nThe following limitations need to be taken into account:\n\nSynchronous requests are limited to 2 concurrent requests\nBatch jobs are limited to 2 concurrent jobs\n\nThese limits are in place to prevent individual users from overloading the service. However, if these limits cause issues with your use case, then Submit a request and our support team will do their best to help you."
  },
  {
    "objectID": "APIs/openEO/Collections.html",
    "href": "APIs/openEO/Collections.html",
    "title": "Data collections",
    "section": "",
    "text": "In the Earth Observation domain, different terms are used to describe EO data(sets). In openEO, it is referred to as a “Collection”. Thus, a collection is a sequence of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board of a satellite and having the same mode of operation.\nIn openEO, a back-end offers a set of collections to be processed. A user can load (a subset of) a collection using the load_collection process, which returns a raster data cube.\nThe following are the list of data collections available currently in the Copernicus Data Space Ecosystem backend through openEO."
  },
  {
    "objectID": "APIs/openEO/federation/backends/collections.html",
    "href": "APIs/openEO/federation/backends/collections.html",
    "title": "Collections",
    "section": "",
    "text": "This page describes the requirements on the STAC collection metadata for backend providers in the openEO federation. These requirements are considered an addition to what is already required by the openEO API and the API profiles.\n\n\nThe Copernicus Dataspace ecosystem federation is looking for collections that match certain criteria. In the initial phase of the federation, the focus is on having a stable service, avoiding complex situations that may potentially cause issues. These criteria will likely relax over time, allowing to gradually integrate and enable more complex scenarios.\nThe current criteria are:\n\nCollections must be complementary to the existing offer, avoiding collections with the same name or similar content.\nOnly non-experimental collections will be considered.\nFull-archive collections, or collections that fully cover a specific region are encouraged. The intention is to avoid ‘demo’ collections that only offer a marginal part of the actual dataset.\n\n\n\n\nCollections are key assets of the federation. It is important for users to know what they can expect from any given collection. The federation relies on openEO collection metadata following the STAC specification to communicate this to the user. Backend providers need to comply with these requirements for each of their collections.\nThe ‘experimental’ field available in the STAC version extension is required for experimental collections. A collection can be experimental either due to issues with the actual data or catalog or backend specific issues that make the use of the collection unstable. Experimental collections do not need to comply with further requirements.\n\n\nNon-experimental features should aim to maximize user satisfaction by ensuring stability and usability and complying with federation agreements fostering a unified service. The list below outlines this more explicitly, but does not cover all aspects. Therefor providers are expected to properly assess whether a specific collection can be considered non-experimental.\n\nCollections need to indicate the key ‘providers’ responsible for ensuring data access and continuity for active missions. The user may depend on the guarantees offered by these providers with respect to the properties (timeliness, completeness,…) of a specific collection. The providers with role ‘host’ and ‘producer’ are mandatory.\nThe collection description and extents needs to specify known limitations with respect to the original collection. For instance, if only a subset of the full archive is available, this should be indicated. Extents can be rough approximations to avoid requiring very detailed geometry in the metadata.\nCollections without an end time are assumed to be active missions. By default, 99% of the items in these collections should be available within 48 hours after publication by the producer. This gives users a basic guarantee with respect to timeliness of products.\nCollection metadata should be valid STAC metadata and must include all extensions in stac_extensions. Tools like STAC-validator can identify obvious issues.\nFAIR principle R1: (Meta)data are richly described with a plurality of accurate and relevant attributes\nCollections must follow harmonization guidelines specified below, if applicable.\nCollections naming (id, dimensions, bands) should remain constant.\nBackwards incompatible changes or removal need to be announced with a lead time of 3 months, together with a migration path, as described here.\nMinimum availability for non-experimental collections must be 98% on a monthly basis. Users must be notified of backwards incompatible changes or removals six months in advance, alongside a migration path. In the case it is not possible to measure the availability per collection, the overall backend availability is used.\nCollections may require special conditions to work, for instance in the case of commercial data. Note that a separate extension is available for commercial data that requires ordering.\nCollection metadata uses the datacube extension to advertise available dimensions and dimension labels.\n\n\n\n\n\nWhen back-ends offer/mirror the same datasets, alignment of the names and metadata is required. For the following collections and metadata this alignment has already been achieved. All part of the Copernicus Missions, the standard names refer to the archives prepared and distributed by ESA. If it is not possible/desirable to use this name as the collection ID, a common_name can be added alongside the id property to identify the collection as a standard archive.\n\nSENTINEL1_GRD\nSENTINEL2_L1C\nSENTINEL2_L2A\nSENTINEL3_OLCI_L1B\n\n\n\nA uniform structure for all collections on the federation makes it easier for users to navigate between collections. It is therefor recommended to adhere to the following common naming convention*:\n\nNames should be written in capital letters (“all caps”)\nNames should consist of a combination of different optional attributes (see table)\nThe different attributes should be separated by an underscore\n\nBroadly speaking, collections can be divided into two categories, but, in reality, form a spectrum with intermediate gradations:\n\nCollections containing raw data or processing levels of that data. This data, measured directly by a satellite or other measurement platform, is often distributed by the platform operator (e.g., ESA).\nDerived collections, which are based on (pre-processed) raw data that has been processed for a specific purpose (e.g., a land cover map). These are often distributed by the institution, or a service of an institution, that created the collection.\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\nExamples\n\n\n\n\nProvider\nstring\nOften used for derived collections produced or ordered by the listed provider.\nESA, CNSE, EMODNET, TERRASCOPE. CAMS, CGLS\n\n\nSatellite/Platform\nstring\nName of the satellite/platform that acquired the data in the collection.\nSENTINEL2, LANDSAT8, PALSAR2\n\n\nProcessing level\nstring\nName of the level to which the data was processed (often processed raw data).\nL2A, L3, L2_1\n\n\nVersion\nstring\nOften used for derived collections that are produced in several versions.\nV1, V2\n\n\nResolution\nstring (number + unit or string)\nUsually added if the resolution is of particular importance for the collection (e.g., novel product with this resolution).\n10M, 120M, EUROPE, GLOBAL\n\n\nProduct Description\nstring\nHuman readable label of the data within the collection. Can also be an abbreviation or acronym.\nLAND_COVER_MAP, WORLDCOVER, NDVI, LAI\n\n\nYear\nnumber\nOften used for derived products that were updated in the specified year or created based on data of the specified year.\n2022\n\n\n\n\nCollections containing raw data or processing levels of that data often use a combination of satellite/platform and processing level (e.g., SENTINEL2_L1C ). Derived collections often use a combination of provider and product description (e.g., CNES_LAND_COVER_MAP).\n\n\n\n\nThe common name for this collection is SENTINEL2_L2A. It refers to the L2A products generated by the Sen2Cor software, which can be made compatible with the ESA generated products. It’s worth noting that the products in the ESA archive were also processed using different versions of Sen2Cor. Hence, it is not possible to specify a specific version or configuration of the processing chain.\n\n\nFor spectral bands, band names follow the Bxx naming convention used by ESA. For example: B01, B02, B03, B08, B8A, B12.\nThe collection also provides access to other bands:\n\n\n\n\n\n\n\nBand\nDescription\n\n\n\n\nSCL\nThe Sen2Cor scene classification band\n\n\napproximateViewAzimuth\nThe collective term for the mean and accurate viewing azimuth angle. Depending on which backend is processing the data, the mean angle (for Sentinel Hub) or the accurate angle (for Terrascope) is used. If the accurate angle (viewAzimuthAngles) or the mean angle (viewAzimuthMean) is explicitly specified, the data is processed on the backend that holds the specified band.\n\n\nviewZenithMean\nThe collective term for the mean and accurate viewing zenith angle. Depending on which backend is processing the data, the mean angle (for Sentinel Hub) or the accurate angle (for Terrascope) is used. If the accurate angle (viewZenithMean) or the mean angle (viewZenithAngles) is explicitly specified, the data is processed on the backend that holds those bands.\n\n\nsunAzimuthAngles/sunZenithAngles\nThe collective term for the exact sun azimuth and sun zenith angle.\n\n\n\n\n\n\n\nFollowing is a set of common properties, that can be relevant for multiple collections. Collections are strongly encouraged to use these properties instead of using a different name for the same property.\n\n\n\nsat:orbit_state\nsat:relative_orbit\n\n\n\n\n\neo:cloud_cover\n\n\n\n\n\nsar:instrument_mode"
  },
  {
    "objectID": "APIs/openEO/federation/backends/collections.html#criteria-for-eligible-collections",
    "href": "APIs/openEO/federation/backends/collections.html#criteria-for-eligible-collections",
    "title": "Collections",
    "section": "",
    "text": "The Copernicus Dataspace ecosystem federation is looking for collections that match certain criteria. In the initial phase of the federation, the focus is on having a stable service, avoiding complex situations that may potentially cause issues. These criteria will likely relax over time, allowing to gradually integrate and enable more complex scenarios.\nThe current criteria are:\n\nCollections must be complementary to the existing offer, avoiding collections with the same name or similar content.\nOnly non-experimental collections will be considered.\nFull-archive collections, or collections that fully cover a specific region are encouraged. The intention is to avoid ‘demo’ collections that only offer a marginal part of the actual dataset."
  },
  {
    "objectID": "APIs/openEO/federation/backends/collections.html#collection-availability",
    "href": "APIs/openEO/federation/backends/collections.html#collection-availability",
    "title": "Collections",
    "section": "",
    "text": "Collections are key assets of the federation. It is important for users to know what they can expect from any given collection. The federation relies on openEO collection metadata following the STAC specification to communicate this to the user. Backend providers need to comply with these requirements for each of their collections.\nThe ‘experimental’ field available in the STAC version extension is required for experimental collections. A collection can be experimental either due to issues with the actual data or catalog or backend specific issues that make the use of the collection unstable. Experimental collections do not need to comply with further requirements.\n\n\nNon-experimental features should aim to maximize user satisfaction by ensuring stability and usability and complying with federation agreements fostering a unified service. The list below outlines this more explicitly, but does not cover all aspects. Therefor providers are expected to properly assess whether a specific collection can be considered non-experimental.\n\nCollections need to indicate the key ‘providers’ responsible for ensuring data access and continuity for active missions. The user may depend on the guarantees offered by these providers with respect to the properties (timeliness, completeness,…) of a specific collection. The providers with role ‘host’ and ‘producer’ are mandatory.\nThe collection description and extents needs to specify known limitations with respect to the original collection. For instance, if only a subset of the full archive is available, this should be indicated. Extents can be rough approximations to avoid requiring very detailed geometry in the metadata.\nCollections without an end time are assumed to be active missions. By default, 99% of the items in these collections should be available within 48 hours after publication by the producer. This gives users a basic guarantee with respect to timeliness of products.\nCollection metadata should be valid STAC metadata and must include all extensions in stac_extensions. Tools like STAC-validator can identify obvious issues.\nFAIR principle R1: (Meta)data are richly described with a plurality of accurate and relevant attributes\nCollections must follow harmonization guidelines specified below, if applicable.\nCollections naming (id, dimensions, bands) should remain constant.\nBackwards incompatible changes or removal need to be announced with a lead time of 3 months, together with a migration path, as described here.\nMinimum availability for non-experimental collections must be 98% on a monthly basis. Users must be notified of backwards incompatible changes or removals six months in advance, alongside a migration path. In the case it is not possible to measure the availability per collection, the overall backend availability is used.\nCollections may require special conditions to work, for instance in the case of commercial data. Note that a separate extension is available for commercial data that requires ordering.\nCollection metadata uses the datacube extension to advertise available dimensions and dimension labels."
  },
  {
    "objectID": "APIs/openEO/federation/backends/collections.html#harmonization",
    "href": "APIs/openEO/federation/backends/collections.html#harmonization",
    "title": "Collections",
    "section": "",
    "text": "When back-ends offer/mirror the same datasets, alignment of the names and metadata is required. For the following collections and metadata this alignment has already been achieved. All part of the Copernicus Missions, the standard names refer to the archives prepared and distributed by ESA. If it is not possible/desirable to use this name as the collection ID, a common_name can be added alongside the id property to identify the collection as a standard archive.\n\nSENTINEL1_GRD\nSENTINEL2_L1C\nSENTINEL2_L2A\nSENTINEL3_OLCI_L1B\n\n\n\nA uniform structure for all collections on the federation makes it easier for users to navigate between collections. It is therefor recommended to adhere to the following common naming convention*:\n\nNames should be written in capital letters (“all caps”)\nNames should consist of a combination of different optional attributes (see table)\nThe different attributes should be separated by an underscore\n\nBroadly speaking, collections can be divided into two categories, but, in reality, form a spectrum with intermediate gradations:\n\nCollections containing raw data or processing levels of that data. This data, measured directly by a satellite or other measurement platform, is often distributed by the platform operator (e.g., ESA).\nDerived collections, which are based on (pre-processed) raw data that has been processed for a specific purpose (e.g., a land cover map). These are often distributed by the institution, or a service of an institution, that created the collection.\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\nExamples\n\n\n\n\nProvider\nstring\nOften used for derived collections produced or ordered by the listed provider.\nESA, CNSE, EMODNET, TERRASCOPE. CAMS, CGLS\n\n\nSatellite/Platform\nstring\nName of the satellite/platform that acquired the data in the collection.\nSENTINEL2, LANDSAT8, PALSAR2\n\n\nProcessing level\nstring\nName of the level to which the data was processed (often processed raw data).\nL2A, L3, L2_1\n\n\nVersion\nstring\nOften used for derived collections that are produced in several versions.\nV1, V2\n\n\nResolution\nstring (number + unit or string)\nUsually added if the resolution is of particular importance for the collection (e.g., novel product with this resolution).\n10M, 120M, EUROPE, GLOBAL\n\n\nProduct Description\nstring\nHuman readable label of the data within the collection. Can also be an abbreviation or acronym.\nLAND_COVER_MAP, WORLDCOVER, NDVI, LAI\n\n\nYear\nnumber\nOften used for derived products that were updated in the specified year or created based on data of the specified year.\n2022\n\n\n\n\nCollections containing raw data or processing levels of that data often use a combination of satellite/platform and processing level (e.g., SENTINEL2_L1C ). Derived collections often use a combination of provider and product description (e.g., CNES_LAND_COVER_MAP).\n\n\n\n\nThe common name for this collection is SENTINEL2_L2A. It refers to the L2A products generated by the Sen2Cor software, which can be made compatible with the ESA generated products. It’s worth noting that the products in the ESA archive were also processed using different versions of Sen2Cor. Hence, it is not possible to specify a specific version or configuration of the processing chain.\n\n\nFor spectral bands, band names follow the Bxx naming convention used by ESA. For example: B01, B02, B03, B08, B8A, B12.\nThe collection also provides access to other bands:\n\n\n\n\n\n\n\nBand\nDescription\n\n\n\n\nSCL\nThe Sen2Cor scene classification band\n\n\napproximateViewAzimuth\nThe collective term for the mean and accurate viewing azimuth angle. Depending on which backend is processing the data, the mean angle (for Sentinel Hub) or the accurate angle (for Terrascope) is used. If the accurate angle (viewAzimuthAngles) or the mean angle (viewAzimuthMean) is explicitly specified, the data is processed on the backend that holds the specified band.\n\n\nviewZenithMean\nThe collective term for the mean and accurate viewing zenith angle. Depending on which backend is processing the data, the mean angle (for Sentinel Hub) or the accurate angle (for Terrascope) is used. If the accurate angle (viewZenithMean) or the mean angle (viewZenithAngles) is explicitly specified, the data is processed on the backend that holds those bands.\n\n\nsunAzimuthAngles/sunZenithAngles\nThe collective term for the exact sun azimuth and sun zenith angle.\n\n\n\n\n\n\n\nFollowing is a set of common properties, that can be relevant for multiple collections. Collections are strongly encouraged to use these properties instead of using a different name for the same property.\n\n\n\nsat:orbit_state\nsat:relative_orbit\n\n\n\n\n\neo:cloud_cover\n\n\n\n\n\nsar:instrument_mode"
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html",
    "href": "APIs/openEO/federation/backends/contract.html",
    "title": "Federation Contract",
    "section": "",
    "text": "For a federation to work, all providers need to agree with a common federation contract. For the Copernicus Data Space Ecosystem, the proposed contract is based upon the groundwork established through ESA’s openEO platform.\nThis contract has 2 main goals:\n\nBoost user satisfaction, which can be measured in terms of user growth and number of complaints versus the usage of a specific feature.\nAgree on interfaces and harmonization rules to align the different services within the federation.\n\nTo join the federation, providers are expected to fulfill the requirements outlined on this page. However, to accommodate and encourage new backend entries into the federation, these requirements are open to negotiation if there are good arguments for changing the current contract agreements.\nFor more detailed information, please refer to the different parts of the federation contract:\n\nAPI\nCollections\nFile Formats\nProcesses\n\n\n\nThe federation provides a unique opportunity for smaller organizations to join strengths, allowing them to build an offering that matches or even exceeds capabilities of well-known large scale cloud providers. This collaboration is a key element for the long term sustainability of participants. The short-term benefits of joining the federation include:\n\nIncreased visibility: the federation serves a single endpoint for users to access multiple services, increasing the visibility of your service.\nIncreased user base: thanks to the common authentication within the federation, users can access your service without any additional registration. This opens your service up to the larger openEO user community.\nJoint outreach: outreach and promotion activities are performed together with other members of the federation, significantly reducing the effort to attract new users.\nShared accounting: credit usage is centrally tracked, allowing for efficient and transparent use of your service in commercial settings.\n\n\n\n\n\nAssess if your system is compliant with the above requirements and identify the eligible collections.\nContact the federation through our support team, showing your interest and detailing your offering.\nIntegrate the CDSE identity provider as your openEO service will need to accept CDSE users.\nIntegrate with the accounting component by reporting the usage of CDSE users through a JSON HTTP API. The API details will be provided upon request.\n\nFinally, an official agreement will need to be established confirming that the service provider agrees with the federation contract and SLA.\n\nImportant When joining the openEO federation, the provider agrees to the statements outlined in the following policies:\n\nPrivacy Policy\nTerms & Conditions\n\n\n\n\n\nThe federation aims to be inclusive towards onboarding new features and components to stimulate growth and innovation. In an environment supporting user-critical workflows, such new features need to be distinctly identified. For example, designating a process, collection, or backend as ‘experimental’ and indicating this in descriptions or official documentation.\nGenerally, we assume that an implementor can determine when a feature is mature enough. However, in case of any uncertainty, we provide the following guidelines:\n\nBy default, or if unsure, the feature is considered experimental.\nIf a feature is new or unused, designate it as experimental.\nIf still in doubt, consult with partner providers.\n\nSuppose a non-experimental component exceeds the error budget, such as when downtime exceeds the objectives. In this case, the provider is expected to stop working on new features and improve reliability or to mark the component as experimental. Reverting a ‘stable’ feature to ‘experimental’ should be considered as a backwards incompatible change, requiring communication towards the user and proper consideration of the impact.\n\n\n\nAs specified, service providers need to log the resource usage in the federation’s central accounting service. This will allow them to be compensated for incurred usage. The value of a credit is fixed, and service providers are free to choose the resources they log. Typical examples include memory and cpu hours, but it can also be based on a more complex logic, such as amount of input data or specific processes that are used. To ensure full transparency, service providers are requested to document this logic.\n\n\n\nProviders are encouraged to continuously enhance their backend systems to optimize processing efficiency and improve the overall user experience. Each provider is responsible for ensuring maximum service availability and strives to minimize downtime.\nUpon joining the federation, the provider’s openEO endpoint will be added to a monitoring system that checks the endpoint’s availability every 5 minutes. These metrics are collected in dashboards and evaluated monthly. The following diagram provides an overview of the evaluation flow.\n\nThe main points from this diagram are:\n\nIf the provider achieves a monthly uptime of 99.5% or higher, the provider will maintain a stable status.\nIf the total monthly uptime for a backend is lower than 99.5% for two consecutive months, the provider will be marked as unstable.\nIf the total monthly uptime for a backend is lower than 95%, the provider will receive a warning.\nIf the provider receives two warnings within a six-month period, ESA has the right to temporarily exclude the backend from the federation until the provider demonstrates improvements.\n\n\nGrace period for new providers\nNew providers will have a 3-month grace period during which the minimum availability threshold to receive a warning is set at 90% to accommodate initial setup and unforeseen issues. After the initial 3 months, standard thresholds apply.\n\n\nScheduled maintenances\nScheduled maintenances will not be considered in the uptime evaluation, provided the federation is informed in advance as outlined here.\n\n\n\n\nThe Copernicus Data Space Ecosystem forum is considered the main support channel for users and service providers to post updates or service events. Service providers are also requested to provide an email address, to which support questions from users can be forwarded.\nFor communicating important changes towards users, service providers are required to comply with the announcement procedure described here. The overall lead time for the announcement of planned breaking changes is 3 months.\nDowntime (planned or unplanned) is also announced in the same manner.\n\n\n\nIn the event that any breaches with the statements in this page or its related documents are detected, ESA reserves the right to take appropriate actions to maintain the stability and reliability of the CDSE federation These actions can include a temporary or permanent exclusion from the federation."
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#benefits-of-joining-the-federation",
    "href": "APIs/openEO/federation/backends/contract.html#benefits-of-joining-the-federation",
    "title": "Federation Contract",
    "section": "",
    "text": "The federation provides a unique opportunity for smaller organizations to join strengths, allowing them to build an offering that matches or even exceeds capabilities of well-known large scale cloud providers. This collaboration is a key element for the long term sustainability of participants. The short-term benefits of joining the federation include:\n\nIncreased visibility: the federation serves a single endpoint for users to access multiple services, increasing the visibility of your service.\nIncreased user base: thanks to the common authentication within the federation, users can access your service without any additional registration. This opens your service up to the larger openEO user community.\nJoint outreach: outreach and promotion activities are performed together with other members of the federation, significantly reducing the effort to attract new users.\nShared accounting: credit usage is centrally tracked, allowing for efficient and transparent use of your service in commercial settings."
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#how-to-join-the-federation",
    "href": "APIs/openEO/federation/backends/contract.html#how-to-join-the-federation",
    "title": "Federation Contract",
    "section": "",
    "text": "Assess if your system is compliant with the above requirements and identify the eligible collections.\nContact the federation through our support team, showing your interest and detailing your offering.\nIntegrate the CDSE identity provider as your openEO service will need to accept CDSE users.\nIntegrate with the accounting component by reporting the usage of CDSE users through a JSON HTTP API. The API details will be provided upon request.\n\nFinally, an official agreement will need to be established confirming that the service provider agrees with the federation contract and SLA.\n\nImportant When joining the openEO federation, the provider agrees to the statements outlined in the following policies:\n\nPrivacy Policy\nTerms & Conditions"
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#implementation-of-experimental-features",
    "href": "APIs/openEO/federation/backends/contract.html#implementation-of-experimental-features",
    "title": "Federation Contract",
    "section": "",
    "text": "The federation aims to be inclusive towards onboarding new features and components to stimulate growth and innovation. In an environment supporting user-critical workflows, such new features need to be distinctly identified. For example, designating a process, collection, or backend as ‘experimental’ and indicating this in descriptions or official documentation.\nGenerally, we assume that an implementor can determine when a feature is mature enough. However, in case of any uncertainty, we provide the following guidelines:\n\nBy default, or if unsure, the feature is considered experimental.\nIf a feature is new or unused, designate it as experimental.\nIf still in doubt, consult with partner providers.\n\nSuppose a non-experimental component exceeds the error budget, such as when downtime exceeds the objectives. In this case, the provider is expected to stop working on new features and improve reliability or to mark the component as experimental. Reverting a ‘stable’ feature to ‘experimental’ should be considered as a backwards incompatible change, requiring communication towards the user and proper consideration of the impact."
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#credit-reporting",
    "href": "APIs/openEO/federation/backends/contract.html#credit-reporting",
    "title": "Federation Contract",
    "section": "",
    "text": "As specified, service providers need to log the resource usage in the federation’s central accounting service. This will allow them to be compensated for incurred usage. The value of a credit is fixed, and service providers are free to choose the resources they log. Typical examples include memory and cpu hours, but it can also be based on a more complex logic, such as amount of input data or specific processes that are used. To ensure full transparency, service providers are requested to document this logic."
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#availability-of-the-service",
    "href": "APIs/openEO/federation/backends/contract.html#availability-of-the-service",
    "title": "Federation Contract",
    "section": "",
    "text": "Providers are encouraged to continuously enhance their backend systems to optimize processing efficiency and improve the overall user experience. Each provider is responsible for ensuring maximum service availability and strives to minimize downtime.\nUpon joining the federation, the provider’s openEO endpoint will be added to a monitoring system that checks the endpoint’s availability every 5 minutes. These metrics are collected in dashboards and evaluated monthly. The following diagram provides an overview of the evaluation flow.\n\nThe main points from this diagram are:\n\nIf the provider achieves a monthly uptime of 99.5% or higher, the provider will maintain a stable status.\nIf the total monthly uptime for a backend is lower than 99.5% for two consecutive months, the provider will be marked as unstable.\nIf the total monthly uptime for a backend is lower than 95%, the provider will receive a warning.\nIf the provider receives two warnings within a six-month period, ESA has the right to temporarily exclude the backend from the federation until the provider demonstrates improvements.\n\n\nGrace period for new providers\nNew providers will have a 3-month grace period during which the minimum availability threshold to receive a warning is set at 90% to accommodate initial setup and unforeseen issues. After the initial 3 months, standard thresholds apply.\n\n\nScheduled maintenances\nScheduled maintenances will not be considered in the uptime evaluation, provided the federation is informed in advance as outlined here."
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#providing-user-support",
    "href": "APIs/openEO/federation/backends/contract.html#providing-user-support",
    "title": "Federation Contract",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem forum is considered the main support channel for users and service providers to post updates or service events. Service providers are also requested to provide an email address, to which support questions from users can be forwarded.\nFor communicating important changes towards users, service providers are required to comply with the announcement procedure described here. The overall lead time for the announcement of planned breaking changes is 3 months.\nDowntime (planned or unplanned) is also announced in the same manner."
  },
  {
    "objectID": "APIs/openEO/federation/backends/contract.html#breaches",
    "href": "APIs/openEO/federation/backends/contract.html#breaches",
    "title": "Federation Contract",
    "section": "",
    "text": "In the event that any breaches with the statements in this page or its related documents are detected, ESA reserves the right to take appropriate actions to maintain the stability and reliability of the CDSE federation These actions can include a temporary or permanent exclusion from the federation."
  },
  {
    "objectID": "APIs/openEO/federation/backends/api.html",
    "href": "APIs/openEO/federation/backends/api.html",
    "title": "Federation API",
    "section": "",
    "text": "The general contract is the openEO API in the latest stable version of the 1.x branch.\nThe aggregator, which serves as a proxy for the back-ends in the federation, not only implements the same API, but also the “Federation Extension” (currently in draft state).\n\n\nIn addition to the general openEO API specification and their API Profiles, openEO federation also requires the implementation of an additional API profile, as shown in the image below.\nLP: Required for openEO Federation, which requires the openEO profile L2: Recommended. The requirement to implement two of L1A, L1B, and L1C has been restricted for the openEO federation to always require L1A: Synchronous Processing and L1B: Batch Jobs (see req. no. 703). This means that L1C: Secondary Web Services is optional.\n\n\n\nAn overview of the openEO and openEO Platform API profiles.\n\n\n\n\nThe profile only lists requirements that are not covered by the openEO profile L2: Recommended yet.\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n14\nAll &gt; Billing\nSupports the openEO Platform credit system\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n45\nGET /file_formats\nFile format names and parameters aligned with openEO Platform as defined for the pre-defined file formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n90\nGET /health\nReturns 2XX or 5XX http status code (without authentication)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n113\nGET /credentials/oidc\nSupports EGI as identity provider (including tokens)\n\n\n118\nGET /credentials/oidc\nSupports the required entitlements of the vo.openeo.cloud virtual organization, especially the claim eduperson_entitlement\n\n\n\nFor more details about Authentication and Authorization, please see the corresponding chapters below.\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n205\nGET /processes &gt; processes\nAll processes are valid according to the specification (id, description, parameters, returns are required)\n\n\n208\nGET /processes &gt; processes\nProcesses are marked as experimental or deprecated if applicable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n311\nGET /collections &gt; collections\nCollections are marked as experimental or deprecated if applicable\n\n\n324\nGET /collections/{id} &gt; id\nIDs follow the openEO federation naming convention\n\n\n327\nGET /collections/{id} &gt; providers\nEach collection needs to expose the backend offering the data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n703\n\nBatch jobs and synchronous processing are implemented (secondary web services are optional)\n\n\n704\n\nTime after which batch job results get automatically deleted: 90 days or later\n\n\n705\n\nTime after which batch job metadata gets automatically deleted: 1 year or later\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n873\nGET /jobs/{id}/results &gt; public access link\nDefault expiry time of the signed URLs for results: 7 days\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n920\nPOST /result &gt; timeout\nThe timeout for synchronous calls is: 5 minutes\n\n\n\n\n\n\n\n\nAuthentication is performed via the CDSE OIDC provider.\n\n\nThe authorization also depends on the number of credits available to a user. These credits allow the federation to limit the volume of data access and processing operations that a user can perform during a given time frame. The number of available credits depends on the user’s active subscription. When the user’s credit balance goes below zero, processing operations can be blocked.\n\n\n\nBased on the user’s subscription and available credits, the aggregator will apply the following rules:\n\nA general subscription check to validate the user account.\nCredit checks to block starting of batch jobs, synchronous requests to /result and viewing services\n\n\n\n\nCertain authorization rules will need to be enforced by the backend systems:\n\nBasic access and access to user specific resources based on subscription role\nNumber of concurrent batch jobs\nAvailable processing resources, batch job priorities\nBatch job result data volume\nAccess to restricted collections"
  },
  {
    "objectID": "APIs/openEO/federation/backends/api.html#profiles",
    "href": "APIs/openEO/federation/backends/api.html#profiles",
    "title": "Federation API",
    "section": "",
    "text": "In addition to the general openEO API specification and their API Profiles, openEO federation also requires the implementation of an additional API profile, as shown in the image below.\nLP: Required for openEO Federation, which requires the openEO profile L2: Recommended. The requirement to implement two of L1A, L1B, and L1C has been restricted for the openEO federation to always require L1A: Synchronous Processing and L1B: Batch Jobs (see req. no. 703). This means that L1C: Secondary Web Services is optional.\n\n\n\nAn overview of the openEO and openEO Platform API profiles.\n\n\n\n\nThe profile only lists requirements that are not covered by the openEO profile L2: Recommended yet.\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n14\nAll &gt; Billing\nSupports the openEO Platform credit system\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n45\nGET /file_formats\nFile format names and parameters aligned with openEO Platform as defined for the pre-defined file formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n90\nGET /health\nReturns 2XX or 5XX http status code (without authentication)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n113\nGET /credentials/oidc\nSupports EGI as identity provider (including tokens)\n\n\n118\nGET /credentials/oidc\nSupports the required entitlements of the vo.openeo.cloud virtual organization, especially the claim eduperson_entitlement\n\n\n\nFor more details about Authentication and Authorization, please see the corresponding chapters below.\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n205\nGET /processes &gt; processes\nAll processes are valid according to the specification (id, description, parameters, returns are required)\n\n\n208\nGET /processes &gt; processes\nProcesses are marked as experimental or deprecated if applicable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n311\nGET /collections &gt; collections\nCollections are marked as experimental or deprecated if applicable\n\n\n324\nGET /collections/{id} &gt; id\nIDs follow the openEO federation naming convention\n\n\n327\nGET /collections/{id} &gt; providers\nEach collection needs to expose the backend offering the data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n703\n\nBatch jobs and synchronous processing are implemented (secondary web services are optional)\n\n\n704\n\nTime after which batch job results get automatically deleted: 90 days or later\n\n\n705\n\nTime after which batch job metadata gets automatically deleted: 1 year or later\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n873\nGET /jobs/{id}/results &gt; public access link\nDefault expiry time of the signed URLs for results: 7 days\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nFunctionality\nDescription\n\n\n\n\n920\nPOST /result &gt; timeout\nThe timeout for synchronous calls is: 5 minutes"
  },
  {
    "objectID": "APIs/openEO/federation/backends/api.html#authentication-and-authorization",
    "href": "APIs/openEO/federation/backends/api.html#authentication-and-authorization",
    "title": "Federation API",
    "section": "",
    "text": "Authentication is performed via the CDSE OIDC provider.\n\n\nThe authorization also depends on the number of credits available to a user. These credits allow the federation to limit the volume of data access and processing operations that a user can perform during a given time frame. The number of available credits depends on the user’s active subscription. When the user’s credit balance goes below zero, processing operations can be blocked.\n\n\n\nBased on the user’s subscription and available credits, the aggregator will apply the following rules:\n\nA general subscription check to validate the user account.\nCredit checks to block starting of batch jobs, synchronous requests to /result and viewing services\n\n\n\n\nCertain authorization rules will need to be enforced by the backend systems:\n\nBasic access and access to user specific resources based on subscription role\nNumber of concurrent batch jobs\nAvailable processing resources, batch job priorities\nBatch job result data volume\nAccess to restricted collections"
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html",
    "href": "APIs/openEO/Python_Client/Python.html",
    "title": "Getting started with the openEO Python client",
    "section": "",
    "text": "This Getting Started guide will just give a small taste of using the openEO Python client library in the context of the Copernicus Data Space Ecosystem. Consult the official openEO Python client library documentation for more in-depth information and a broader coverage of its functionality."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#installation",
    "href": "APIs/openEO/Python_Client/Python.html#installation",
    "title": "Getting started with the openEO Python client",
    "section": "Installation",
    "text": "Installation\n\n\n\n\n\n\nTip\n\n\n\n\n\nAs with any Python project, it is recommended to work in some kind of virtual environment (venv, virtualenv, conda, docker, …) to avoid interference with other projects or applications.\n\n\n\nThe openEO Python client library is available on PyPI and can easily be installed with a tool like pip, for example:\npip install openeo\nThe client library is also available on Conda Forge and can be easily installed in a conda environment, for example:\nconda install -c conda-forge openeo\n\n\n\n\n\n\nTip\n\n\n\n\n\nSee the official openeo installation docs for more details, alternative installation procedures or troubleshooting tips."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#exploring-a-back-end",
    "href": "APIs/openEO/Python_Client/Python.html#exploring-a-back-end",
    "title": "Getting started with the openEO Python client",
    "section": "Exploring a back-end",
    "text": "Exploring a back-end\nFor this tutorial we will use the openEO back-end of Copernicus Data Space Ecosystem, which is available at https://openeo.dataspace.copernicus.eu. We establish a connection to this back-end as follows:\nimport openeo\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\nThe Connection object we created here is the central gateway to interact with the back-end:\n\nlist data collections, available processes, file formats and other capabilities of the back-end\nstart building your openEO algorithm from the desired data on the back-end\nexecute and monitor (batch) jobs on the back-end\netc.\n\n\nEO Collections\nEO data in openEO is organized in so-called collections, which are used as the input data for your openEO jobs (see the glossary for more info). Collections can be listed and inspected programmatically:\n# List collections available on the openEO back-end\nconnection.list_collection_ids()\n\n# Get detailed metadata of a certain collection\nconnection.describe_collection(\"SENTINEL2_L2A\")\nHowever, it is often easier to browse collections through the openEO collection listing page or the collection listing sidebar of the openEO Web Editor.\n\n\nopenEO Processes\nProcesses in openEO are operations that can be applied on (EO) data (see the the openEO glossary for more info). For example: calculate the mean of an array, mask out pixels outside a given polygon or calculate spatial aggregations. The output of one process can be used as the input of another process, and by doing so, multiple processes can be connected that way in a larger “process graph”, as illustrated in this conceptual diagram:\n\nWhile it is possible to programmatically list and inspect the available processes (e.g. connection.list_processes() with the openEO python client), it is recommended to just consult the process listing page, the process listing sidebar of the openEO Web Editor, or the official openeo.org processes listing."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#authentication",
    "href": "APIs/openEO/Python_Client/Python.html#authentication",
    "title": "Getting started with the openEO Python client",
    "section": "Authentication",
    "text": "Authentication\nBasic metadata about collection and processes, as discussed above is publicly available and does not require being logged in. However, for downloading EO data or running processing workflows, it is necessary to authenticate so that permissions, resource usage, etc. can be managed properly.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to complete your Copernicus Data Space Ecosystem registration before attempting to do the authentication explained below.\n\n\nOnce properly registered, you will be able to authenticate your connection handle in your Python code with Connection.authenticate_oidc(), just like this:\nconnection.authenticate_oidc()\n\nBy default, the first time authenticate_oidc() is called, instructions to visit a certain URL will be printed, e.g.:\nVisit https://auth.example/?user_code=EAXD-RQXV to authenticate.\nVisit this URL and follow the login flow using your Copernicus Data Space Ecosystem credentials.\nOther times, when you still have valid (refresh) tokens on your system, the manual login process is skipped and you will immediately see\nAuthenticated using refresh token.\n\nIn any case, your connection is now authenticated and capable to make download/processing requests.\nFind more in-depth information on authentication here or in the openEO Python client documentation."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#working-with-datacube",
    "href": "APIs/openEO/Python_Client/Python.html#working-with-datacube",
    "title": "Getting started with the openEO Python client",
    "section": "Working with Datacube",
    "text": "Working with Datacube\nNow that we know how to discover the capabilities of the back-end and how to authenticate, let’s do some real work and process some EO data in a batch job. We’ll build the desired algorithm by working on so-called “Datacubes”, which is the central concept in openEO to represent EO data.\n\nCreating a Datacube\nThe first step is loading the desired slice of a data collection with Connection.load_collection:\ndatacube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent={\"west\": 5.14, \"south\": 51.17, \"east\": 5.17, \"north\": 51.19},\n    temporal_extent = [\"2021-02-01\", \"2021-04-30\"],\n    bands=[\"B02\", \"B04\", \"B08\"],\n    max_cloud_cover=85,\n)\nThis results in a Datacube object containing the “SENTINEL2_L2A” data restricted to the given spatial extent, the given temporal extend and the given bands .\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can also filter the datacube step by step or at a later stage by using the following filter methods:\ndatacube = datacube.filter_bbox(west=5.14, south=51.17, east=5.17, north=51.19)\ndatacube = datacube.filter_temporal(start_date=\"2021-02-01\", end_date=\"2021-04-30\")\ndatacube = datacube.filter_bands([\"B02\", \"B04\", \"B08\"])\nStill, it is recommended to always use the filters directly in load_collection to avoid loading too much data upfront.\n\n\n\n\n\nApplying processes\nBy applying an openEO process on a datacube, we create a new datacube object that represents the manipulated data. The standard way to do this with the Python client is to call the appropriate Datacube object method. The most common or popular openEO processes have a dedicated Datacube method (e.g. mask, aggregate_spatial, filter_bbox, …). Other processes without a dedicated method can still be applied in a generic way. An on top of that, there are also some convenience methods that implement openEO processes is a compact, Pythonic interface.\nFor example, the min_time method implements a reduce_dimension process along the temporal dimension, using the max process as reducer function:\ndatacube = datacube.max_time()\nThis creates a new datacube (we overwrite the existing variable), where the time dimension is eliminated and for each pixel we just have the minimum value of the corresponding timeseries in the original datacube.\nSee the Python client Datacube API for a more complete listing of methods that implement openEO processes.\n\n\n\n\n\n\nNote\n\n\n\nStill unsure on how to make use of processes with the Python client? Visit the official documentation on working with processes."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#execution",
    "href": "APIs/openEO/Python_Client/Python.html#execution",
    "title": "Getting started with the openEO Python client",
    "section": "Execution",
    "text": "Execution\nIt’s important to note that all the datacube processes we applied up to this point are not actually executed yet, neither locally nor remotely on the back-end. We just built an abstract representation of the algorithm (input data and processing chain), encapsulated in a local Datacube object (e.g. the result variable above). To trigger an actual execution (on the back-end) we have to explicitly send this representation to the back-end.\n\nBatch job execution\nMost of the simple, basic openEO usage examples show synchronous downloading of results. This only works properly if the processing doesn’t take too long and is focused on a smaller area of interest. However, you have to use batch jobs for the heavier work (larger regions of interest, larger time series, more intensive processing).\n# While not necessary, it is also recommended to give your batch job a descriptive title so it’s easier to identify in your job listing.\njob = cube.execute_batch()\n\nThis documentation mainly discusses how to programmatically create and interact with batch job using the openEO Python client library. The openEO API however does not enforce usage of the same tool for each step in the batch job life cycle.\nFor example: if you prefer a graphical, web-based interactive environment to manage and monitor your batch jobs, feel free to switch to an openEO web editor like openeo.dataspace.copernicus.eu/ at any time. After logging in with the same account you use in your Python scripts, you should see your batch jobs listed under the “Data Processing” tab. More information on using openEO web editor is discussed here.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe official openEO Python Client documentation has more information on batch job basics {target=“_blank”} or more detailed batch job (result) management"
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#full-example",
    "href": "APIs/openEO/Python_Client/Python.html#full-example",
    "title": "Getting started with the openEO Python client",
    "section": "Full Example",
    "text": "Full Example\nIn this chapter we will show a full example of an earth observation use case using the Python client.\nA common task in earth observation is to apply a formula to a number of spectral bands in order to compute an ‘index’, such as NDVI, NDWI, EVI, … In this tutorial we’ll go through a couple of steps to extract EVI (enhanced vegetation index) values and timeseries\nimport openeo\n\n# First, we connect to the back-end and authenticate. \ncon = openeo.connect(\"openeo.dataspace.copernicus.eu\")\ncon.authenticate_oidc()\n\n# Now that we are connected, we can initialize our datacube object with the area of interest \n# and the time range of interest using Sentinel 1 data.\ndatacube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent={\"west\": 5.14, \"south\": 51.17, \"east\": 5.17, \"north\": 51.19},\n    temporal_extent = [\"2021-02-01\", \"2021-04-30\"],\n    bands=[\"B02\", \"B04\", \"B08\"],\n    max_cloud_cover=85,\n)\n\n# By filtering as early as possible (directly in load_collection() in this case), \n# we make sure the back-end only loads the data we are interested in and avoid incurring unneeded costs.\n\n\n#From this data cube, we can now select the individual bands with the DataCube.band() method and rescale the digital number values to physical reflectances:\nblue = sentinel2_cube.band(\"B02\") * 0.0001\nred = sentinel2_cube.band(\"B04\") * 0.0001\nnir = sentinel2_cube.band(\"B08\") * 0.0001\n\n\n# We now want to compute the enhanced vegetation index and can do that directly with these band variables:\nevi_cube = 2.5 * (nir - red) / (nir + 6.0 * red - 7.5 * blue + 1.0)\n\n# Now we can use the compact “band math” feature again to build a binary mask with a simple comparison operation:\n# Select the \"SCL\" band from the data cube\nscl_band = s2_scl.band(\"SCL\")\n# Build mask to mask out everything but class 4 (vegetation)\nmask = (scl_band != 4)\n\n# Before we can apply this mask to the EVI cube we have to resample it, as the “SCL” layer has a “ground sample distance” of 20 meter, while it is 10 meter for the “B02”, “B04” and “B08” bands. We can easily do the resampling by referring directly to the EVI cube.\nmask_resampled = mask.resample_cube_spatial(evi_cube)\n\n# Apply the mask to the `evi_cube`\nevi_cube_masked = evi_cube.mask(mask_resampled)\n\n# Because GeoTIFF does not support a temporal dimension, we first eliminate it by taking the temporal maximum value for each pixel:\nevi_composite = evi_cube.max_time()\n\n# Now we can download this to a local file:\nevi_composite.download(\"evi-composite.tiff\")\nNow, you can inspect the result for the EVI map."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#user-defined-functions",
    "href": "APIs/openEO/Python_Client/Python.html#user-defined-functions",
    "title": "Getting started with the openEO Python client",
    "section": "User Defined Functions",
    "text": "User Defined Functions\nIf your use case can not be accomplished with the default processes of openEO, you can define a user defined function. Therefore, you can create a Python function that will be executed at the back-end and functions as a process in your process graph.\nDetailed information about Python UDFs can be found in the official documentation as well as examples in the Python client repository."
  },
  {
    "objectID": "APIs/openEO/Python_Client/Python.html#useful-links",
    "href": "APIs/openEO/Python_Client/Python.html#useful-links",
    "title": "Getting started with the openEO Python client",
    "section": "Useful links",
    "text": "Useful links\nAdditional information and resources about the openEO Python Client Library:\n\nExample scripts\nExample Jupyter Notebooks\nOfficial openEO Python Client Library Documentation\nRepository on GitHub"
  },
  {
    "objectID": "APIs/openEO/Glossary.html",
    "href": "APIs/openEO/Glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "The following glossary provides an introduction to the key technical terms commonly used when working with the openEO API."
  },
  {
    "objectID": "APIs/openEO/Glossary.html#general-terms",
    "href": "APIs/openEO/Glossary.html#general-terms",
    "title": "Glossary",
    "section": "General terms",
    "text": "General terms\n\nEO: Earth Observation\nAPI: Application Programming Interface (wikipedia), a (standardized) communication protocol, for example between a client application and back-end service\nclient: Software tool, framework or environment that an end-user directly interacts with. For example: a Jupyter notebook, a Python script/application, an RStudio session, a JavaScript based web app, etc.\nback-end: server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it. Additionally, here, industry and researchers can analyse large amounts of EO data."
  },
  {
    "objectID": "APIs/openEO/Glossary.html#processes",
    "href": "APIs/openEO/Glossary.html#processes",
    "title": "Glossary",
    "section": "Processes",
    "text": "Processes\nA process is an operation that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages.\nA pre-defined process is a process provided by the back-end, typically one of the processes centrally defined by openeo.org.\nA user-defined process is a process defined by the user. It can directly be part of another process graph or be stored as custom process on a back-end. Internally it is a process graph with optional additional metadata.\nA process graph chains specific process calls from the set of pre-defined and user-defined processes together. A process graph itself is a (user-defined) process again. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually.\nProcess graphs can be parameterized by using placeholders (parameters) instead of actual values in various places of the process graph. This turns them into so-called user-defined processes that can reused in a flexible way in other process graphs. It also simplifies sharing and reusing openEO workflows across users.\nUser-defined processes can for example be shared in the openEO algorithm plaza. You can find more information in this page"
  },
  {
    "objectID": "APIs/openEO/Glossary.html#eo-data-collections",
    "href": "APIs/openEO/Glossary.html#eo-data-collections",
    "title": "Glossary",
    "section": "EO data (Collections)",
    "text": "EO data (Collections)\nIn the Earth Observation domain, different terms are used to describe EO data(sets). Within openEO, a granule (sometimes also called item or asset in the specification) typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites). A collection is a sequence of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board of a satellite and having the same mode of operation.\nThe CEOS OpenSearch Best Practice Document v1.2 lists the following synonyms used by other organizations:\n\ngranule: dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA)\ncollection: dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA)\n\nIn openEO, a back-end offers a set of collections to be processed. All collections can be requested using a client and are described using the STAC (SpatioTemporal Asset Catalog) metadata specification as STAC collections. A user can load (a subset of) a collection using a special process, which returns a (spatial) datacube. All further processing is then applied to the datacube on the back-end."
  },
  {
    "objectID": "APIs/openEO/Glossary.html#spatial-datacubes",
    "href": "APIs/openEO/Glossary.html#spatial-datacubes",
    "title": "Glossary",
    "section": "Spatial datacubes",
    "text": "Spatial datacubes\nA spatiotemporal datacube is a multidimensional array with one or more spatial or temporal dimensions. In the EO domain, it is common to be implicit about the temporal dimension and just refer to them as spatial datacubes in short. Special cases are raster and vector datacubes. Learn more about datacubes in the datacube documentation."
  },
  {
    "objectID": "APIs/openEO/Glossary.html#vector-data",
    "href": "APIs/openEO/Glossary.html#vector-data",
    "title": "Glossary",
    "section": "Vector data",
    "text": "Vector data\nIn general, vector data represent specific things (also called “features”) in a space, e.g. on the surface of the Earth. It comprises of individual points stored as coordinate pairs that indicate a physical location in the world. These points can be joined, in a particular order, to form lines or joined into closed areas to form polygons.\nA coordinate represents a specific point in space.\nA feature is a thing that usually has a geometry (e.g. the outline of an agricultural field, a forest or an urban area) and it may have additional properties assigned (e.g. a name, a color or a population). In rare cases features may not have a geometry, which is often referred to as an “empty geometry”.\nGeometries consist of one or more coordinates that may be connected and then form a specific type of geometry, e.g. two points can be connected to a straight line and four straight lines can be connected to rectangle.\nCommonly used types of geometries are: - Point - LineString (connected straight line pieces) - Polygon (connected straight line pieces forming a closed ring, possibly with holes - for example a triangle or rectangle)\nFurther more, these base geometries can be grouped to form, for example, so-called “multi-point” or “multi-polygon” geometries. It’s important to note that these multi-geometries act as single entities with regard to their associated properties.\nFeatures and geometries are specified by the OGC in the Simple Feature Access specification (and ISO 19125). See the specification for more details."
  },
  {
    "objectID": "APIs/openEO/Glossary.html#user-defined-function-udf",
    "href": "APIs/openEO/Glossary.html#user-defined-function-udf",
    "title": "Glossary",
    "section": "User-defined function (UDF)",
    "text": "User-defined function (UDF)\nThe abbreviation UDF stands for user-defined function. With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, or applied to a particular dimension or set of dimensions, allowing custom server-side calculations. See the section on UDFs for more information."
  },
  {
    "objectID": "APIs/openEO/Glossary.html#data-processing-modes",
    "href": "APIs/openEO/Glossary.html#data-processing-modes",
    "title": "Glossary",
    "section": "Data Processing modes",
    "text": "Data Processing modes\nProcesses can run in three different ways:\n\nResults can be pre-computed by creating a batch job. They are submitted to the back-end’s processing system, but will remain inactive until explicitly put into the processing queue. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming and user interaction is not possible although log files are generated for them. This is the only mode that allows to get an estimate about time, volume and costs beforehand.\nProcesses can also be executed on-demand (i.e. synchronously). Results are delivered with the request itself and no job is created. Only lightweight computations, for example previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests.\nThe third way of data processing in openEO is client-side processing. The client-side processing functionality allows to test and use openEO with its processes locally, i.e. without any connection to an openEO back-end. It relies on the projects openeo-pg-parser-networkx, which provides an openEO process graph parsing tool, and openeo-processes-dask, which provides an Xarray and Dask implementation of most openEO processes."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/LandslideNDVI/LandslidesNDVI.html",
    "href": "APIs/openEO/openeo-community-examples/python/LandslideNDVI/LandslidesNDVI.html",
    "title": "NDVI-based approach to study Landslide areas",
    "section": "",
    "text": "Research indicates that NDVI can play a crucial role in identifying landslide zones. While an advanced workflow is possible, this notebook opts for a simple approach using the difference of NDVI and applying a threshold to the result to detect landslide occurred areas. Our analysis relies on the Sentinel-2 Level 2A collection fetched from the Copernicus Data Space Ecosystem using openEO.\n\nimport openeo\nimport rasterio\nfrom rasterio.plot import show\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport matplotlib.patches as mpatches\n\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nHere, we do the test for Ischia in Italy for the fall of 2022, where several landsides were registered.\n\nspatial_extent = {\n    \"west\": 13.882567409197492,\n    \"south\": 40.7150627793427,\n    \"east\": 13.928593792166282,\n    \"north\": 40.747050251559216,\n}\n\nLet us load pre Sentinel2 collection\n\ns2pre = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2022-08-25\", \"2022-11-25\"],\n    spatial_extent=spatial_extent,\n    bands=[\"B04\", \"B08\"],\n)\n\nNow, let’s calculate pre-NDVI and take a mean over the temporal extent.\n\nprendvi = s2pre.ndvi().mean_time()\n\nFor the post datacube, let’s load the Sentinel2 collection with the temporal extent starting from the end of the pre-event temporal extent.\n\ns2post = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2022-11-26\", \"2022-12-25\"],\n    spatial_extent=spatial_extent,\n    bands=[\"B04\", \"B08\"],\n)\n\n\n# calculate post NDVI and take a mean over temporal extent\npostndvi = s2post.ndvi().mean_time()\n\n\n# calculate difference in NDVI\ndiff = postndvi - prendvi\n\n\n# lets execute the process\ndiff.download(\"NDVIDiff.tiff\")\n\n\nPlot the result\n\n# load the calculated data\nimg = rasterio.open(\"NDVIDiff.tiff\")\n\nTo better visualise the output, we apply a threshold to define landslide areas.\n\nvalue = img.read(1)\ncmap = matplotlib.colors.ListedColormap([\"black\", \"red\"])\nfig, ax = plt.subplots(figsize=(8, 6), dpi=80)\nim = show(\n    ((value &lt; -0.48) & (value &gt; -1)),\n    vmin=0,\n    vmax=1,\n    cmap=cmap,\n    transform=img.transform,\n    ax=ax,\n)\nvalues = [\"Absence\", \"Presence\"]\ncolors = [\"black\", \"red\"]\nax.set_title(\"Detected Landslide Area\")\nax.set_xlabel(\"X Coordinates\")\nax.set_ylabel(\"Y Coordinates\")\npatches = [\n    mpatches.Patch(color=colors[i], label=\"Landslide {l}\".format(l=values[i]))\n    for i in range(len(values))\n]\nfig.legend(handles=patches, bbox_to_anchor=(0.83, 1.03), loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f72025a6390&gt;\n\n\n\n\n\nA general observation drawn from the aforementioned result is that the red regions indicate potential landslide activity or vegetation loss.\nThe red region corresponds to an area where notably large landslides were recorded. However, some false positives can also be noticed in the northwest—an urban area, though not many landslides were reported in those regions. Hence, it is conceivable that there is a change in NDVI, but it might be due to many factors, such as cloud cover, harvesting, vegetation loss, etc. This shows that the difference in the NDVI approach is suitable for rapid mapping but can also contain a lot of misclassified pixels and needs further validation.\n\nWhen comparing the result with the ground truth, it resembles a few landslides that were reported and as shown in this Wikipedia resource.\n\n\n\nimage.png"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html",
    "title": "Best available pixel composite",
    "section": "",
    "text": "In this notebook a monthly composite image is created based on the Best Available Pixel (BAP) method in OpenEO. This is a scientifically sound method of generating composites, which minimizes the mixing of pixels from different observations. Also for each pixel, all bands will be taken from a single observation. As a result, the spectral signature in the composite is observed rather than being a combination of multiple observations.\nWe refer to the scientific references for a full discussion of this method, but can generally recommend it both for its correctness and performance. We do still observe some residual cloud (shadow) in certain cases, which can be attributed to the quality of the SCL band.\nThe BAP score is a weighted average of three (four) scores:\nThe final BAP score is a weighted average of the three aforementioned scores. The weights are 1, 0.5 and 0.8 for the Distance-to-Cloud, Coverage and Date Score respectively.\nThe current implementation and the various parameters are still subject to change, so please do evaluate the correctness when using this method in your project."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#loading-sentinel-2-scene-classification",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#loading-sentinel-2-scene-classification",
    "title": "Best available pixel composite",
    "section": "Loading Sentinel-2 scene classification",
    "text": "Loading Sentinel-2 scene classification\nThe score is calculated based on the ‘SCL’ band only. This is very efficient because it’s a lower resolution (20m) band, which is relatively quick to read. All other bands are read based on the score, which allows openEO to avoid loading data that is not used in the composite.\nFirst a sample period and region are defined. It is necessary to specify your region of interest as a Polygon.\n\nimport geopandas as gpd\n\ngdf = gpd.read_file('test_area.geojson')\ngdf = gdf.to_crs(epsg=4326)\narea = eval(gdf.to_json())\n\n\ntemporal_extent = [\"2022-07-01\", \"2022-07-31\"]\nmax_cloud_cover = 70\nspatial_resolution = 20\n\nNote that the Python code behind the calculation of the BAP scores can be found in utils_BAP.py. All functions are defined there, and are loaded in here.\n\nimport openeo\nimport numpy as np\n\nfrom openeo.processes import if_, is_nan\n\nfrom utils_BAP import (calculate_cloud_mask, calculate_cloud_coverage_score,\n                           calculate_date_score, calculate_distance_to_cloud_score,\n                           calculate_distance_to_cloud_score, aggregate_BAP_scores,\n                           create_rank_mask)\n\n\nc=openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nLoad in the SCL band from Sentinel-2 which will be used to calculate all cloud-related scores.\n\nscl = c.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=temporal_extent,\n    bands=[\"SCL\"],\n    max_cloud_cover=max_cloud_cover\n).resample_spatial(spatial_resolution).filter_spatial(area)\n\nscl = scl.apply(lambda x: if_(is_nan(x), 0, x))\n\nCreate a binary cloud mask, which gives 1 if a pixel is classified as cloud by SCL, an 0 otherwise.\n\ncloud_mask =  calculate_cloud_mask(scl)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#coverage-score",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#coverage-score",
    "title": "Best available pixel composite",
    "section": "Coverage score",
    "text": "Coverage score\nFirst of all, the coverage score is calculated, which is equal to 1 - the percentage of pixels in the spatial_extent that are classified as cloud in SCL.\n\nA polygon is created that is slighty larger than the original spatial_extent\nThe cloud coverage percentage is calculated by taking the average of the cloud binary (defined above), by using the aggregate_spatial process\nSince the aggregate_spatial process results in vectordata, the data is rasterized again to the original dimensions, using the (experimental) vector_to_raster process\nThe coverage score is then equal to 1 - the cloud coverage percentage. I.e. a smaller cloud coverage percentage indicates a larger score\n\n\ncoverage_score = calculate_cloud_coverage_score(cloud_mask, area, scl)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#date-score",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#date-score",
    "title": "Best available pixel composite",
    "section": "Date Score",
    "text": "Date Score\nBoth the date and distance-to-cloud score are calculated via a UDF.\n\ndate_score = calculate_date_score(scl)\n\n/home/vverelst/anaconda3/envs/openeo-app/lib/python3.9/site-packages/openeo/api/process.py:22: UserWarning: Parameter without description: using name as description.\n  warnings.warn(\"Parameter without description: using name as description.\")"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#distance-to-cloud-score",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#distance-to-cloud-score",
    "title": "Best available pixel composite",
    "section": "Distance to Cloud Score",
    "text": "Distance to Cloud Score\nFinally, the Distance to Cloud Score is calculated by applying a Gaussian kernel to the binary. The kernel size, defined below, indicates the maximum distance from a cloud, above which a pixel is automatically assigned score 1. In case of a kernel size of 151, the maximum distance is 150 pixels. The kernel size is 151 for a resolution of 20m and will be rescaled accordingly.\n\ndtc_score = calculate_distance_to_cloud_score(cloud_mask, spatial_resolution)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#aggregating-scores",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#aggregating-scores",
    "title": "Best available pixel composite",
    "section": "Aggregating Scores",
    "text": "Aggregating Scores\nThen, the scores are aggregated together, by using a weighted average. As for now the coverage score is excluded. To make sure that pixels which contain no data (SCL score 0) are not selected, they are explicitly masked out of the final score. B01 score is excluded.\n\nweights = [1, 0.8, 0.5]\nscore = aggregate_BAP_scores(dtc_score, date_score, coverage_score, weights)\nscore = score.mask(scl.band(\"SCL\") == 0)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#masking-and-compositing",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/bap_composite.html#masking-and-compositing",
    "title": "Best available pixel composite",
    "section": "Masking and Compositing",
    "text": "Masking and Compositing\nNext, a mask is created. This serves to mask every pixel, except the one with the highest score, for each month.\n\nrank_mask = create_rank_mask(score)\n\nNext, some bands of interest from Sentinel-2 are loaded. They are then masked by the BAP mask constructed above. Then they are aggregated per month, to obtain a composite image per month. By using the “first” process as an aggregator, the situation where there are potentially more than one days in a month selected by the algorithm is immediately handled.\n\nrgb_bands = c.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent = temporal_extent,\n    bands = [\"B02\", \"B03\",\"B04\"],\n    max_cloud_cover=max_cloud_cover\n).filter_spatial(area)\n\ncomposite = rgb_bands.mask(rank_mask).mask(cloud_mask).aggregate_temporal_period(\"month\",\"first\")\n\nNext, the final results are downloaded and a composite image for the month of June is shown as an example.\n\njob = composite.execute_batch(\n    title=\"BAP Composite\",\n    out_format=\"netcdf\"\n)\nresults = job.get_results()\nresults.download_files('./results/')\n\n0:00:00 Job 'j-24041209f0e64b119b20e266cb049c51': send 'start'\n0:00:13 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:00:18 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:00:25 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:00:34 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:00:44 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:00:57 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:01:13 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:01:33 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:02:00 Job 'j-24041209f0e64b119b20e266cb049c51': running (progress N/A)\n0:02:31 Job 'j-24041209f0e64b119b20e266cb049c51': finished (progress N/A)\n\n\n[PosixPath('results/openEO.nc'), PosixPath('results/job-results.json')]\n\n\n\nimport xarray as xr\ncomposite_ds = xr.open_dataset('./results/openEO.nc')\n\n\nimport numpy as np\nrgb_array=composite_ds.to_array(dim=\"bands\").sel(bands=[\"B04\",\"B03\",\"B02\"]).astype(np.float32)/10000\n\n\nxr.plot.imshow(rgb_array,vmin=0,vmax=0.18,rgb=\"bands\",col='t',col_wrap=2)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7faea8e03be0&gt;\n\n\n/home/vverelst/anaconda3/envs/openeo-app/lib/python3.9/site-packages/matplotlib/cm.py:494: RuntimeWarning: invalid value encountered in cast\n  xx = (xx * 255).astype(np.uint8)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/SurfaceSoilMoisture/SoilMoisture.html",
    "href": "APIs/openEO/openeo-community-examples/python/SurfaceSoilMoisture/SoilMoisture.html",
    "title": "Surface Soil Moisture (SSM)",
    "section": "",
    "text": "In this notebook, we follow the concept presented in https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-1/soil_moisture_estimation/ to estimate the soil moisture content using the concept of change detection over three year of time interval as a openEO workflow.\nReferences: * https://eo4society.esa.int/projects/s1-for-surface-soil-moisture/ * https://www.sciencedirect.com/science/article/pii/S2352340921006296?via%3Dihub\nTherefore the surface soil moisture is calculated using the following formula:\n\\[SSM=\\frac{\\sigma_0-dry_{ref }}{wet_{ref }-dry_{ref }}\\]\nwhere, \\(\\sigma_{0}\\) = Current backscatter intensity\n\\(dry_{ref}\\) = historic minimum backscatter in past 3 years\n\\(wet_{ref}\\) = historic maximum backscatter in past 3 years\nimport openeo\nimport openeo.processes \n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\nLet us define the area of interest.\n# defining area of interest\nspatial_extent = {\"west\": 139.194, \"south\": -35.516, \"east\": 139.535, \"north\":  -35.284}\nLet us start by creating a reference cube with a time interval of 3 years, followed by a datacube representing the current situation for approximately a month.\n#Reference Cube\ns1_ref = connection.load_collection(\n    \"SENTINEL1_GRD\",\n    temporal_extent=[\"2017-02-02\",\"2020-02-02\"],\n    spatial_extent=spatial_extent,\n    bands=[\"VV\"],\n)\ns1_ref = s1_ref.sar_backscatter(coefficient=\"sigma0-ellipsoid\")\n#Current DateCube\ns1_cur = connection.load_collection(\n    \"SENTINEL1_GRD\",\n    temporal_extent=[\"2020-02-02\", \"2020-02-28\"],\n    spatial_extent=spatial_extent,\n    bands=[\"VV\"],\n)\ns1_cur = s1_cur.sar_backscatter(coefficient=\"sigma0-ellipsoid\")\nAs we need maximum and minimum values for the reference period, we reduce the temporal dimension to derive spatially variable minimum and maximum values. For the current observation datacube, we select the most recent image, representing the latest available image available within the selected time period.\ns1_cur = s1_cur.reduce_dimension(dimension='t',reducer='last')\ndry_ref = s1_ref.reduce_dimension(dimension='t',reducer='min')\nwet_ref = s1_ref.reduce_dimension(dimension='t',reducer='max')\n#calculate Surface Soil Moisture\nSSM = (s1_cur-dry_ref)/(wet_ref-dry_ref)\nFurthermore, to filter wet and urban areas from our soil moisture dataset, we reduce the temporal dimension of our datacube by using a mean reducer to get an average reference datacube. This is followed by normalising the backscatter values by Log with base 10.\naverage_ref = s1_ref.reduce_dimension(dimension='t',reducer='mean')\naverage_ref = average_ref.apply(process=lambda data: 10 * openeo.processes.log(data, base=10))\nWe create a mask to filter out values above -6dB (probably urban area) and below -17dB (probably water bodies), following the thresholds suggested in https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-1/soil_moisture_estimation/ .\nVV = average_ref.band(\"VV\")\nmask = ((VV &gt; -6) | (VV &lt; -17))\n\n#now let us apply the mask\nSSM = SSM.mask(mask)\nThis generates a final SSM datacube after applying urban and permanent water body mask.\n# Let's download the data\nSSM.execute_batch(title=\"Surface Soil Moisture Australia\", outputfile=\"SoilMoisture.nc\")\n\n0:00:00 Job 'j-24022621028b46459728273e37b8d71a': send 'start'\n0:00:12 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:00:18 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:00:24 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:00:33 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:00:43 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:00:55 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:01:11 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:01:30 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:01:54 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:02:24 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:03:03 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:03:50 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:04:48 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:05:49 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:06:49 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:07:49 Job 'j-24022621028b46459728273e37b8d71a': created (progress N/A)\n0:08:50 Job 'j-24022621028b46459728273e37b8d71a': finished (progress N/A)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/SurfaceSoilMoisture/SoilMoisture.html#let-us-plot-the-result",
    "href": "APIs/openEO/openeo-community-examples/python/SurfaceSoilMoisture/SoilMoisture.html#let-us-plot-the-result",
    "title": "Surface Soil Moisture (SSM)",
    "section": "Let us plot the result",
    "text": "Let us plot the result\n\nimport matplotlib.pyplot as plt\nimport xarray as xr\n\n\nds = xr.load_dataset(\"SoilMoisture.nc\")\n\n\ndata = ds[[\"VV\"]].to_array(dim=\"bands\")\n\n\nfig, axes = plt.subplots(ncols=1, figsize=(8, 8), dpi=100, sharey=True)\ndata[0].plot.imshow(ax=axes, vmax=0.6, vmin=0)\naxes.set_title(\"Surface Soil Moisture\")\n\nText(0.5, 1.0, 'Surface Soil Moisture')\n\n\n\n\n\nAs suggested here, to avoid the effect of outliers, the soil moisture ranges from 0 to 0.6 is plotted. Here the white colour represents the masked-out area, including permanent water bodies and urban areas."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/OilSpill/OilSpillMapping.html",
    "href": "APIs/openEO/openeo-community-examples/python/OilSpill/OilSpillMapping.html",
    "title": "Oil spill mapping using Sentinel-1",
    "section": "",
    "text": "Several advanced techniques have been discussed and presented by researchers; however, in this notebook, we want to showcase a basic methodology for mapping oil spills using openEO.\nThough openEO is fully capable of executing advanced workflows, our focus here is to show a simple yet effective method. This notebook was inspired by the algorithm shown here.\nAdditionally, the following references were equally helpful when preparing this usecase:\n# Load the essentials\nimport openeo\nimport openeo.processes\nimport numpy as np\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\nFor this workflow, our area of interest is the southern coast of Kuwait near the resort community of Al Khiran, where an oil spill was reported in 2017.\nhttps://skytruth.org/2017/08/satellite-imagery-reveals-scope-of-last-weeks-massive-oil-spill-in-kuwait/\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [48.325487506118264, 28.742803969343313],\n            [48.325487506118264, 28.414218984218607],\n            [48.75387693420447, 28.414218984218607],\n            [48.75387693420447, 28.742803969343313],\n            [48.325487506118264, 28.742803969343313],\n        ]\n    ],\n}\ns1_image = connection.load_collection(\n    \"SENTINEL1_GRD\",\n    temporal_extent=[\"2017-08-09\", \"2017-08-12\"],\n    spatial_extent=aoi,\n    bands=[\"VV\"],\n)\n\ns1_image = s1_image.sar_backscatter(coefficient=\"sigma0-ellipsoid\")\nUsually the distribution of SAR backscatter values is too heavy tailed and difficult to analyse (visually and numerically). Therefore let’s normalize the backscatter values by Log with base 10. For further information See https://forum.step.esa.int/t/sigma0-vh-vs-sigma0-vhdb/2661?u=abraun\ns1_image = s1_image.apply(process=lambda data: 10 * openeo.processes.log(data, base=10))\nNow let us define the adaptive thresholding. It is a two-step process where we first identify the mean value within a window and then select anything below the mean of that threshold.\nfilter_window = np.ones([601, 601])\n# calculate factor\nfactor = 1 / np.prod(filter_window.shape)\nthresholds = s1_image.apply_kernel(kernel=filter_window, factor=factor)\nthreshold_shift = 3.5\nthresholds = thresholds - threshold_shift\nthresholds = thresholds.rename_labels(dimension=\"bands\", target=[\"threshold\"])\nIn the above cell, we selected the threshold based on the https://eo4society.esa.int/wp-content/uploads/2022/01/OCEA03_OilSpill_Kuwait.pdf and it can be changed to any value based on the requirements.\nNow we have a separate datacube with the threshold for the entire study area. Let us compare it with our Sentinel-1 datacube. But before that, to have similar properties, we are renaming the amplitude bands and applying the merge_cube process.\ns1_image = s1_image.rename_labels(dimension=\"bands\", target=[\"amplitude\"])\ns1_image = s1_image.merge_cubes(thresholds)\noil_spill = s1_image.band(\"amplitude\") &lt; s1_image.band(\"threshold\")\n# execute the workflow\noil_spill.execute_batch(title=\"Oil Spill Data\", outputfile=\"OilSpill.nc\")\n\n0:00:00 Job 'j-2402068af9e9445cba04c4dd08152f5b': send 'start'\n0:00:22 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:00:28 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:00:36 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:00:46 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:00:58 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:01:12 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:01:32 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:01:52 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:02:17 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:02:49 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:03:29 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:04:19 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:05:20 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:06:21 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:07:24 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:08:26 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:09:33 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:10:35 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:11:36 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:12:38 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:13:40 Job 'j-2402068af9e9445cba04c4dd08152f5b': running (progress N/A)\n0:14:43 Job 'j-2402068af9e9445cba04c4dd08152f5b': finished (progress N/A)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/OilSpill/OilSpillMapping.html#let-us-plot-the-result",
    "href": "APIs/openEO/openeo-community-examples/python/OilSpill/OilSpillMapping.html#let-us-plot-the-result",
    "title": "Oil spill mapping using Sentinel-1",
    "section": "Let us plot the result",
    "text": "Let us plot the result\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport xarray as xr\nimport matplotlib.patches as mpatches\nfrom rasterio.plot import show\n\n\noilspill = xr.load_dataset(\"OilSpill.nc\")\n\n\ndata = oilspill[[\"var\"]].to_array(dim=\"bands\")\n\n\ncmap = matplotlib.colors.ListedColormap([\"black\", \"#FFFFED\"])\nvalues = [\"Absence\", \"Presence\"]\ncolors = [\"black\", \"#FFFFED\"]\n\noilspill_array = data.squeeze().values[600:-600, 600:-600]\nfig, axes = plt.subplots(ncols=1, figsize=(5, 5), dpi=100)\naxes.imshow(oilspill_array, vmin=0, vmax=1, cmap=cmap)\naxes.set_title(\"Oil Spill Image\")\n\npatches = [\n    mpatches.Patch(color=colors[i], label=\"Oil {l}\".format(l=values[i]))\n    for i in range(len(values))\n]\nfig.legend(handles=patches, bbox_to_anchor=(0.9, 0.3), loc=1)\naxes.axes.get_xaxis().set_visible(False)\naxes.axes.get_yaxis().set_visible(False)\n\n\n\n\nIf we compare with the available manually mapped ground truth shown in the image below, we can conclude that our workflow effectively showed the oil spill in the selected area of interest.\n)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RescaleChunks/rescale_chunks.html",
    "href": "APIs/openEO/openeo-community-examples/python/RescaleChunks/rescale_chunks.html",
    "title": "Rescale RGB image for spatial chunks",
    "section": "",
    "text": "This notebook shows a simple process for rescaling Sentinel 2 RGB images within polygon chunks that also showcases how to use chunk_polygon() with a (User Defined Function) UDF. (To be noted: chunk_polygon are experimental at the moment)\n\n# import necessary packages\nimport openeo\nfrom openeo.api.process import Parameter\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport rasterio\n\n# connect with the backend\neoconn = openeo.connect(\"openeo.vito.be\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nUser can choose among different backend available here to connect to the backend of their choice. Regarding the authentication process OpenID connect (oidc) is recommended, but not always straightforward to use. In cases where you are unable to connect with the backend use basic authentication method explained here.\n\n# function to load geojson file\ndef read_json(path: Path) -&gt; dict:\n    with open(path) as input:\n        field = json.load(input)\n        input.close()\n    return field\n\nTo use the data collection, a user must use the correct backend with the data collection. Then using load_collection, they can specify bands, temporal extent (i.e. interested time interval) and even spatial extent. In this example, we have loaded the entire collection so that process (including UDF) can later be applied to spatial chunks.\n\n# Load your data cube based on your prefernce\n\nS2_cube = eoconn.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent = [\"2022-06-04\", \"2022-08-04\"],\n    bands = [\"B02\", \"B03\", \"B04\"]\n)\n\nHere we tried in presenting a method to create and use UDF as an openEO feature. In a similar manner user can create their own UDF as needed to apply to their data cube. More information on UDF.\n\n# Create a UDF object from inline source code.\nmy_udf = openeo.UDF(\"\"\"\nfrom openeo.udf import XarrayDataCube\n\ndef apply_datacube(cube: XarrayDataCube, context: dict) -&gt; XarrayDataCube:\n    array = cube.get_array()\n    array.values = 0.0001 * array.values\n    return cube\n\"\"\")\n\nWe used the chunk_polygon method to apply our UDF over a spatial chunk of the datacube. In the case of a simple process that does not require UDF, you can directly load your spatial extent in the dataset.\nFurthermore, since we loaded our collection for specific time intervals, it can include multiple time dimensions. Thus reduce_dimension applies a reducer to a data cube dimension by collapsing all the pixel values along the time dimension into an output value computed by the reducer.\n\n# apply rescale to chunks of polygon\naoi = read_json(\"cologne_aoi.geojson\")\nrescaled_chunks = S2_cube.chunk_polygon(chunks=aoi,process=my_udf)\n\n# perform time dimension reduction\nRrescaled_chunks = rescaled_chunks.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n\nOnce the process is completed, you can also save it as your process using save_user_defined_process that can later be used for a similar task. Otherwise, you can download the result either by direct download (in case of the small spatial extent with few processing) or perform create a batch job in case it is a heavy task over a large extent.\n\n## download your result either using synchronous method or batch\n# synchronous download\n# rescaled_chunks.download(\"rescaled_test_v1.tiff\")\n# \n# Or perform batch processing if area is comparatively large\nbatch_job = Rrescaled_chunks.create_job(out_format = \"GTiff\", title=\"rescaled_chunks2\")\nbatch_job.start_and_wait()\nresults = batch_job.get_results()\nresults.download_files()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/BasicSentinelMerge/sentinel_merge.html",
    "href": "APIs/openEO/openeo-community-examples/python/BasicSentinelMerge/sentinel_merge.html",
    "title": "Creating multi-mission, multi-temporal datacube",
    "section": "",
    "text": "import openeo\n\nThis notebooks shows how to combine timeseries data from two popular missions, Sentinel-1 and Sentinel-2 in a single datacube for further processing. It can be considered a basic template for many use cases.\nThe uses precomputed backscatter if available, and falls back to compute backscatter on the fly, which works globally, but also consumes more credits.\nWe also create 10-daily composites, and apply linear interpolation to avoid gaps. Specific methods may of course require different cloud masking and preprocessing options.\n\nc=openeo.connect(\"openeo.dataspace.copernicus.eu\")\nc.authenticate_oidc()\n\nsentinel2 = c.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent = [\"2022-06-04\", \"2022-08-04\"],\n    bands = [\"B02\", \"B03\", \"B04\",\"SCL\"],\n    max_cloud_cover=95\n)\n\nsentinel2 = sentinel2.process(\n            \"mask_scl_dilation\",\n            data=sentinel2,\n            scl_band_name=\"SCL\",\n            kernel1_size=17, kernel2_size=77,\n            mask1_values=[2, 4, 5, 6, 7],\n            mask2_values=[3, 8, 9, 10, 11],\n            erosion_kernel_size=3)\n\nsentinel2 = sentinel2.aggregate_temporal_period(\"dekad\",reducer=\"median\")\\\n    .apply_dimension(dimension=\"t\", process=\"array_interpolate_linear\")\n\n\nAuthenticated using refresh token.\n\n\nSome openEO backends offer precomputed Sentinel-1 backscatter. We inspect the backend metadata to check if such a collection is available, otherwise we start from raw GRD and compute it on the fly.\n\nS1_collection = \"SENTINEL1_GRD\"\nif \"SENTINEL1_GRD_SIGMA0\" in c.list_collection_ids():\n    S1_collection = \"SENTINEL1_GRD_SIGMA0\"\n\nS1_collection\n\n'SENTINEL1_GRD'\n\n\n\n\nsentinel1 = c.load_collection(\n    S1_collection,\n    temporal_extent = [\"2022-06-04\", \"2022-08-04\"],\n    bands = [\"VV\",\"VH\"]\n)\n\nif S1_collection == \"SENTINEL1_GRD\":\n    sentinel1 = sentinel1.sar_backscatter(\n        coefficient='sigma0-ellipsoid',\n        local_incidence_angle=False,\n        elevation_model='COPERNICUS_30')\n\nsentinel1 = sentinel1.aggregate_temporal_period(\"dekad\",reducer=\"median\")\\\n    .apply_dimension(dimension=\"t\", process=\"array_interpolate_linear\")\n\nNow we can simply combine both cubes. Resampling is performed implicitly if needed, but explicit resampling can also be specified.\n\nmerged = sentinel2.merge_cubes(sentinel1)\n\nThe next block receives the combined Sentinel-1 and Sentinel-2 input, and transforms it using whatever method. This can be for instance a neural network based on PyTorch.\nThis example uses blocks of 128x128 pixels, with an 8 pixel overlap. Sizes for the time and bands dimensions are not specified, which means they will be fully included.\nThe UDF in this example also shows how to print statements to the logging, this is an easy way to get a better sense of the XArray data that is passed in. More information on UDF’s can be found in the documentation.\n\nmy_udf = openeo.UDF(\"\"\"\nfrom openeo.udf import XarrayDataCube\nfrom openeo.udf.debug import inspect\n\ndef apply_datacube(cube: XarrayDataCube, context: dict) -&gt; XarrayDataCube:\n    array = cube.get_array()\n    inspect(array,level=\"ERROR\",message=\"inspecting input cube\")\n    array.values = 0.0001 * array.values\n    return cube\n\"\"\")\n\nfused = merged.apply_neighborhood(my_udf, size=[\n        {'dimension': 'x', 'value': 112, 'unit': 'px'},\n        {'dimension': 'y', 'value': 112, 'unit': 'px'}\n    ], overlap=[\n        {'dimension': 'x', 'value': 8, 'unit': 'px'},\n        {'dimension': 'y', 'value': 8, 'unit': 'px'}\n    ])\n\n\nspatial_extent = {'west': 4.45, 'east': 4.70, 'south': 51.16, 'north': 51.22, 'crs': 'epsg:4326'}\njob=fused.filter_bbox(spatial_extent).execute_batch(\"result.nc\", title=\"Sentinel composite\", filename_prefix=\"merged_cube\")\n\n0:00:00 Job 'j-beec61e8511149d19cc3b6627a19888a': send 'start'\n0:00:11 Job 'j-beec61e8511149d19cc3b6627a19888a': created (progress N/A)\n0:00:16 Job 'j-beec61e8511149d19cc3b6627a19888a': created (progress N/A)\n0:00:23 Job 'j-beec61e8511149d19cc3b6627a19888a': created (progress N/A)\n0:00:31 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:00:42 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:00:55 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:01:10 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:01:30 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:01:54 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:02:24 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:03:01 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:03:48 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:04:46 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:05:47 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:06:47 Job 'j-beec61e8511149d19cc3b6627a19888a': running (progress N/A)\n0:07:47 Job 'j-beec61e8511149d19cc3b6627a19888a': finished (progress N/A)\n\n\nWhen the job is finished, are downloaded as netCDF and can be inspected using XArray or a desktop viewer like QGis.\n\nimport xarray as xr\nxr.open_dataset(\"result.nc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (t: 7, x: 1762, y: 706)\nCoordinates:\n  * t        (t) datetime64[ns] 2022-06-01 2022-06-11 ... 2022-07-21 2022-08-01\n  * x        (x) float64 6.013e+05 6.013e+05 6.013e+05 ... 6.189e+05 6.189e+05\n  * y        (y) float64 5.676e+06 5.676e+06 5.676e+06 ... 5.669e+06 5.669e+06\nData variables:\n    crs      |S1 ...\n    B02      (t, y, x) float64 ...\n    B03      (t, y, x) float64 ...\n    B04      (t, y, x) float64 ...\n    SCL      (t, y, x) float64 ...\n    VV       (t, y, x) float64 ...\n    VH       (t, y, x) float64 ...\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platform - Geotrellis backend: 0.14.1a1\n    description:  \n    title:        xarray.DatasetDimensions:t: 7x: 1762y: 706Coordinates: (3)t(t)datetime64[ns]2022-06-01 ... 2022-08-01standard_name :tlong_name :taxis :Tarray(['2022-06-01T00:00:00.000000000', '2022-06-11T00:00:00.000000000',\n       '2022-06-21T00:00:00.000000000', '2022-07-01T00:00:00.000000000',\n       '2022-07-11T00:00:00.000000000', '2022-07-21T00:00:00.000000000',\n       '2022-08-01T00:00:00.000000000'], dtype='datetime64[ns]')x(x)float646.013e+05 6.013e+05 ... 6.189e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([601265., 601275., 601285., ..., 618855., 618865., 618875.])y(y)float645.676e+06 5.676e+06 ... 5.669e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5675665., 5675655., 5675645., ..., 5668635., 5668625., 5668615.])Data variables: (7)crs()|S1...crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32631\"]]spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32631\"]][1 values with dtype=|S1]B02(t, y, x)float64...long_name :B02units :grid_mapping :crs[8707804 values with dtype=float64]B03(t, y, x)float64...long_name :B03units :grid_mapping :crs[8707804 values with dtype=float64]B04(t, y, x)float64...long_name :B04units :grid_mapping :crs[8707804 values with dtype=float64]SCL(t, y, x)float64...long_name :SCLunits :grid_mapping :crs[8707804 values with dtype=float64]VV(t, y, x)float64...long_name :VVunits :grid_mapping :crs[8707804 values with dtype=float64]VH(t, y, x)float64...long_name :VHunits :grid_mapping :crs[8707804 values with dtype=float64]Indexes: (3)tPandasIndexPandasIndex(DatetimeIndex(['2022-06-01', '2022-06-11', '2022-06-21', '2022-07-01',\n               '2022-07-11', '2022-07-21', '2022-08-01'],\n              dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Float64Index([601265.0, 601275.0, 601285.0, 601295.0, 601305.0, 601315.0,\n              601325.0, 601335.0, 601345.0, 601355.0,\n              ...\n              618785.0, 618795.0, 618805.0, 618815.0, 618825.0, 618835.0,\n              618845.0, 618855.0, 618865.0, 618875.0],\n             dtype='float64', name='x', length=1762))yPandasIndexPandasIndex(Float64Index([5675665.0, 5675655.0, 5675645.0, 5675635.0, 5675625.0, 5675615.0,\n              5675605.0, 5675595.0, 5675585.0, 5675575.0,\n              ...\n              5668705.0, 5668695.0, 5668685.0, 5668675.0, 5668665.0, 5668655.0,\n              5668645.0, 5668635.0, 5668625.0, 5668615.0],\n             dtype='float64', name='y', length=706))Attributes: (4)Conventions :CF-1.9institution :openEO platform - Geotrellis backend: 0.14.1a1description :title :\n\n\nYou can also inspect the result in the openEO editor:"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/LoadStac/load-stac-item-example.html",
    "href": "APIs/openEO/openeo-community-examples/python/LoadStac/load-stac-item-example.html",
    "title": "1. Create example GTiff",
    "section": "",
    "text": "This notebook showcases how to use your own files on openEO by creating a stac item for it.\nIt’s content is as follows: 1. Getting an example GTiff file from openEO 2. Creating a stac item for the file 3. Using load_stac to load the file into openEO\nNote: This notebook creates a stac item for a result file from openEO for the sake of demonstration. In practice, openEO creates stac items for all files it produces. You can use these results directly in load_stac\nFirst we download a simple GTiff file containing ndvi data from an openEO provider.\nWe calculate the NDVI from the S2 bands for the month of June 2021 and download the result as a GTiff file.\nNote: the temporal_extend can be set as “2021-06” without specifying the days. More info here.\nimport openeo\nimport pystac\nfrom datetime import datetime\nimport rasterio\nimport rasterio.warp\nfrom rasterio.plot import show\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\ncollection = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent={\"west\": 5.0, \"south\": 51.2, \"east\": 5.1, \"north\": 51.3},\n    temporal_extent=\"2021-06\",\n    bands=[\"B04\", \"B08\", \"SCL\"],\n    max_cloud_cover=95\n)\ncollection = collection.process(\"mask_scl_dilation\", scl_band_name=\"SCL\", data=collection)\ncollection = collection.ndvi(red=\"B04\",nir=\"B08\", target_band=\"NDVI-band\").filter_bands(\"NDVI-band\")\ncollection = collection.reduce_temporal(\"mean\")\nndvi_job = collection.create_job(title=\"NDVI\", out_format=\"GTiff\")\nndvi_job.start_and_wait().get_results().download_file(\"ndvi-file.tif\")\n\n0:00:00 Job 'j-240430fa4ef14b43838af02be5686907': send 'start'\n0:00:15 Job 'j-240430fa4ef14b43838af02be5686907': created (progress N/A)\n0:00:20 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:00:27 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:00:35 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:00:45 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:01:00 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:01:16 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:01:35 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:01:59 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:02:29 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:03:07 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:03:54 Job 'j-240430fa4ef14b43838af02be5686907': running (progress N/A)\n0:04:52 Job 'j-240430fa4ef14b43838af02be5686907': finished (progress N/A)\n\n\nWindowsPath('ndvi-file.tif')\nwith rasterio.open(\"ndvi-file.tiff\") as src:\n    show(src)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/LoadStac/load-stac-item-example.html#create-a-stac-item",
    "href": "APIs/openEO/openeo-community-examples/python/LoadStac/load-stac-item-example.html#create-a-stac-item",
    "title": "1. Create example GTiff",
    "section": "2. Create a STAC item",
    "text": "2. Create a STAC item\nWe create a STAC item for the GTiff file using pystac.\nA STAC item is a JSON file that describes a geospatial dataset. It contains metadata about the dataset, such as the spatial extent, the time range, the bands, and the projection. The STAC item also contains links to the data files in assets.\nIt is important that the GTiff file is accessible for the openEO backend. This can be done by uploading the file to a cloud storage service like AWS S3 or Google Cloud Storage, or by storing the file in your public directory on a terrascope uservm (when using the openeo.vito.be backend).\n\n# url to the tiff file\ntiff_url = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/openeo/load-stac-example/ndvi-file.tiff\"\n\n# datetime of the tiff file\n# In case your data spans multiple days, you can use the datetime of the first day of the data\n# When specifying a temporal range in load_stac later on, the stac item will be filteren only on this datetime\ndt = datetime.fromisoformat(\"2021-06-01\")\n\n#desired output path for the stac item\noutput_path = \"ndvi-example-stac-item.json\"\n\nMake sure the band names are set correctly in the STAC item. In this case the band name is “NDVI-band”. These band names are required by openEO to identify the bands in the data and name them.\nThe other metadata in the stac item are extracted from the GTiff file using rasterio.\n\nwith rasterio.open(tiff_url) as src:\n    proj_bounds = list(src.bounds)\n    left, bottom, right, top = rasterio.warp.transform_bounds(src.crs, \"EPSG:4326\", *src.bounds)\n    item = pystac.Item(\n        id=\"ndvi-example-stac-item\",\n        geometry={\n            \"type\": \"Polygon\",\n            \"coordinates\": [[\n                [left, bottom],\n                [right, bottom],\n                [right, top],\n                [left, top],\n                [left, bottom]\n            ]]\n        },\n        bbox=[left, bottom, right, top],\n        datetime= dt,\n        properties={ # These properties are optional, but can speed up the loading of the data.\n            \"proj:epsg\": src.crs.to_epsg(),\n            \"proj:shape\": src.shape, # Caveat: this is [height, width] and not [width, height] if you want to set them yourself\n            \"proj:bbox\": proj_bounds,\n        },\n        stac_extensions=[\n            \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\",\n            \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\",\n        ],\n        assets={\n            \"ndvi\": pystac.Asset(\n                href=tiff_url,\n                title=\"Normalized Difference Vegetation Index\",\n                extra_fields={\n                    \"eo:bands\": [ # REQUIRED: define the bands in the eo extension for openEO to be able to load it\n                        {\n                            \"name\": \"NDVI-band\",\n                        }\n                    ],\n                }\n            )\n        }\n    )\n    item.validate()\n    item.save_object(dest_href=output_path, include_self_link=False)\n\nUpload the resulting STAC item to a place where the openEO backend can access it. This can be a cloud storage service or a public directory on a terrascope uservm (when using the openeo.vito.be backend)."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/LoadStac/load-stac-item-example.html#load-the-stac-item-in-openeo-as-a-collection",
    "href": "APIs/openEO/openeo-community-examples/python/LoadStac/load-stac-item-example.html#load-the-stac-item-in-openeo-as-a-collection",
    "title": "1. Create example GTiff",
    "section": "3. Load the STAC item in openEO as a collection",
    "text": "3. Load the STAC item in openEO as a collection\nWe load the STAC item in openEO as a collection using load_stac and download the result as a GTiff file. Instead of directly downloading, this datacube can also be further processed or merged with other datacubes.\nIMPORTANT: Make sure the STAC item and the GTiff are accessible for the openEO backend. This can be done by uploading the STAC item to a cloud storage service like AWS S3 or Google Cloud Storage, or by storing the STAC item in your public directory on a terrascope uservm (when using the openEO.vito.be backend).\n\ndatacube = connection.load_stac(\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/openeo/load-stac-example/ndvi-example-stac-item.json\")\ndatacube.download(\"ndvi-from-stac.tiff\")\n\n\nwith rasterio.open(\"ndvi-from-stac.tiff\") as src:\n    show(src)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/StatisticalDataFill/StatisticalDataFill.html",
    "href": "APIs/openEO/openeo-community-examples/python/StatisticalDataFill/StatisticalDataFill.html",
    "title": "Using Lowess Regression in openEO for Filling Missing Time Series Data",
    "section": "",
    "text": "In numerous scenarios, time series data may exhibit gaps or missing values. Such gaps can arise due to various reasons, including cloud cover. These missing values can significantly impact the quality of the data and the subsequent analysis.\nIn statistical analysis, various tools are available for filling in such missing values, and herein, we employ one such tool. In this notebook we will show how the Lowess model can be used to fill such gaps in time series data, using a regression technique [ref.].\nIt provides a flexible method to identify patterns and relationships within the data. The main idea of Lowess is to fit simple models to small subsets of the data, thus creating a function that reflects the underlying structure.\nHere, we implement Lowess smoothing directly within the openEO User-Defined Function (UDF).\nReferences: * https://en.wikipedia.org/wiki/Local_regression * https://sites.stat.washington.edu/courses/stat527/s14/readings/Cleveland_JASA_1979.pdf\n\nimport openeo\nimport matplotlib.pyplot as plt\nimport xarray as xr\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nFor this use case, we implement the Lowess regression method specifically on the NO2 band of the SENTINEL_5P_L2 collection for a particular spatio-temporal extent.\n\n# Load Data for three months\nNo2 = connection.load_collection(\n    \"SENTINEL_5P_L2\",\n    temporal_extent=[\"2021-04-01\", \"2021-06-01\"],\n    spatial_extent={\n        \"west\": -118.36136,\n        \"south\": 33.850356,\n        \"east\": -117.90809,\n        \"north\": 34.15711,\n    },\n    bands=[\"NO2\"],\n)\n\n# Now aggregate by day\nNo2 = No2.aggregate_temporal_period(reducer=\"mean\", period=\"day\")\n\nIn this UDF, we import an external library to apply lowess on the input DataArray. This array is checked for NaN values over time, and then LOWESS interpolation is used to fill in the NaNs. If all values are NaN, that part of the result array is filled with zeros. The result is the processed data with either interpolated or zero-filled values in those gaps.\nFor more information, please visit this page.\n\n# define UDF\n\nudf = openeo.UDF(\n    \"\"\"\nimport xarray as xr\nimport numpy as np\nfrom openeo.udf import inspect\nfrom openeo.metadata import CollectionMetadata\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\ndef lowess_along_axis(arr: np.ndarray) -&gt; np.ndarray:\n    if not np.all(np.isnan(arr)):\n        # Get non-NaN indices\n        not_nan_indices = np.where(~np.isnan(arr))[0]\n        \n        # Interpolate NaN values using lowess\n        smoothed = lowess(arr[not_nan_indices], not_nan_indices, frac=0.2)\n        \n        # Create a copy of the original series\n        filled_series = arr.copy()\n        \n        # Replace NaN values with interpolated values\n        filled_series[np.isnan(filled_series)] = np.interp(np.arange(len(filled_series)),\n                                                smoothed[:, 0],\n                                                smoothed[:, 1])[np.isnan(filled_series)]\n    else:\n        filled_series = arr.copy()\n    return filled_series\n    \ndef apply_datacube(cube: xarray.DataArray, context: dict) -&gt; xarray.DataArray:\n    \n    res_arr = xr.apply_ufunc(lowess_along_axis, \n    cube,\n    input_core_dims=[['t']], \n    output_core_dims=[['t']], \n    vectorize=True)    \n    return res_arr\n    \n\"\"\"\n)\n\n\nNo2_filled = No2.apply_dimension(process=udf, dimension=\"t\")\n\nLet us download the actual observed data with filled result.\n\nNo2_filled.execute_batch(\n    title=\"Filled NO2 data with Lowess\", outputfile=\"NO2_Interpolated.nc\"\n)\n\n0:00:00 Job 'j-240614e312ca43df8dbabe9850558f94': send 'start'\n0:00:20 Job 'j-240614e312ca43df8dbabe9850558f94': queued (progress 0%)\n0:00:25 Job 'j-240614e312ca43df8dbabe9850558f94': queued (progress 0%)\n0:00:32 Job 'j-240614e312ca43df8dbabe9850558f94': queued (progress 0%)\n0:00:40 Job 'j-240614e312ca43df8dbabe9850558f94': queued (progress 0%)\n0:00:50 Job 'j-240614e312ca43df8dbabe9850558f94': queued (progress 0%)\n0:01:02 Job 'j-240614e312ca43df8dbabe9850558f94': queued (progress 0%)\n0:01:17 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:01:37 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:02:01 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:02:31 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:03:08 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:03:55 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:04:56 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:05:56 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:06:57 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:07:57 Job 'j-240614e312ca43df8dbabe9850558f94': running (progress N/A)\n0:08:58 Job 'j-240614e312ca43df8dbabe9850558f94': finished (progress 100%)\n\n\n\n    \n    \n        \n    \n    \n\n\nFor a comparative visualisation, let’s also download the raw observed data without any filled information.\n\njob = No2.execute_batch(\n    title=\"Observed NO2 data before Lowess\", outputfile=\"NO2_withNAN.nc\"\n)\njob\n\n0:00:00 Job 'j-2406142db7f849658a682e11225d66e0': send 'start'\n0:00:20 Job 'j-2406142db7f849658a682e11225d66e0': created (progress 0%)\n0:00:25 Job 'j-2406142db7f849658a682e11225d66e0': created (progress 0%)\n0:00:32 Job 'j-2406142db7f849658a682e11225d66e0': created (progress 0%)\n0:00:40 Job 'j-2406142db7f849658a682e11225d66e0': created (progress 0%)\n0:00:57 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:01:10 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:01:35 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:01:54 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:02:18 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:02:48 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:03:26 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:04:13 Job 'j-2406142db7f849658a682e11225d66e0': running (progress N/A)\n0:05:11 Job 'j-2406142db7f849658a682e11225d66e0': finished (progress 100%)\n\n\n\n    \n    \n        \n    \n    \n\n\nMoreover, upon completing the job, check the amount of credit consumed.\n\nprint(f\"credit consumed is: {job.describe()['costs']} openEO credits\")\n\ncredit consumed is: 2 openEO credits\n\n\nThis information is also accessible in the details section of your job within the web editor or under the reporting section in the marketplace.\n\nLet’s plot the result\nFurthermore, let’s try to plot the mean of the obtained result.\n\nfilled = xr.load_dataset(\"NO2_Interpolated.nc\")\nobserved = xr.load_dataset(\"NO2_withNAN.nc\")\n\n\n# Calculate the mean over the y and x dimensions for both observed and filled data\nmean_obs_NO2 = observed.NO2.mean(dim=[\"y\", \"x\"])\nmean_filled_NO2 = filled.NO2.mean(dim=[\"y\", \"x\"])\n\n\nfig, ax = plt.subplots()\nmean_obs_NO2.plot.line(ax=ax, label=\"Observed NO2\", color=\"red\")\nmean_filled_NO2.plot.line(ax=ax, label=\"Filled NO2\", color=\"green\")\n\nax.legend()\nax.set_title(\"Mean NO2 Over Time\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Mean NO2\")\n\nplt.show()\n\n\n\n\nTherefore, it is noticeable that the gaps observed in the original observations have been successfully filled. Furthermore, the model effectively preserves the overall trend of the observations with smoothening due to the nature of lowess function."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html",
    "href": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html",
    "title": "Calculate Radar Vegetation Index(RVI) using Sentinel-1 GRD collection",
    "section": "",
    "text": "In this notebook, we want to study the health condition of vegetation using radar. Thus, we will use the Sentinel-1 GRD collection available within the Copernicus Data Space Ecosystem. The Radar Vegetation Index (RVI) is a measure used in remote sensing and agriculture to assess the health and condition of vegetation using radar data.\nRVI has been used in several research studies, especially for predicting the growth level of crop vegetation over time and many more. However, here, we will stick to a simple example of calculating RVI.\nThe formula adopted in this notebook is as follows:\n\\[\\mathrm{RVI}=\\frac{4 \\sigma^0_{VH}}{\\sigma^0_{VV}+\\sigma^0_{VH}}\\],\nwhere \\(\\sigma^0_{VH}\\), \\(\\sigma^0_{VV}\\) and \\(\\sigma^0_{VH}\\) are the polarised backscattering coefficients,\nReference: * https://www.mdpi.com/2076-3417/9/4/655 * https://forum.step.esa.int/t/creating-radar-vegetation-index/12444/18\nimport openeo\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport xarray as xr\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html#load-the-collection",
    "href": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html#load-the-collection",
    "title": "Calculate Radar Vegetation Index(RVI) using Sentinel-1 GRD collection",
    "section": "Load the collection",
    "text": "Load the collection\n\ns1 = connection.load_collection(\n    \"SENTINEL1_GRD\",\n    temporal_extent=[\"2017-05-03\", \"2017-08-03\"],\n    spatial_extent={\"west\": 5.15, \"south\": 51.20, \"east\": 5.25, \"north\": 51.35},\n    bands=[\"VV\", \"VH\"],\n)\n\nGiven that the “gamma0-terrain” coefficient is not currently supported in the openEO backend implementation of the Copernicus Data Space Ecosystem at the time of preparing this notebook, we use the “sigma0-ellipsoid” coefficient for SAR backscattering computation.\n\ns1 = s1.sar_backscatter(coefficient=\"sigma0-ellipsoid\")\n\nLet’s apply the formula mentioned above:\n\nrvi = (4 * s1.band(\"VH\")) / (s1.band(\"VV\") + s1.band(\"VH\"))\n\n\nrvi.download(\"RVI.nc\")"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html#lets-plot-the-rvi-result",
    "href": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html#lets-plot-the-rvi-result",
    "title": "Calculate Radar Vegetation Index(RVI) using Sentinel-1 GRD collection",
    "section": "Let’s plot the RVI result",
    "text": "Let’s plot the RVI result\nIn the above process, we download RVI data over a specified time period. Now, let us plot the summary of the RVI for a pixel over the temporal period for the selected region. For example, it showcases the mean value calculated over the given temporal period. Furthermore, let us also, visualise the mean NDVI timeseries for this area plotted with time across RVI value.\n\nds = xr.load_dataset(\"RVI.nc\")\ndata = ds[[\"var\"]].to_array(dim=\"bands\")\n\n\nfig = plt.figure(figsize=(10, 5), dpi=90)\ngs = gridspec.GridSpec(1, 2, width_ratios=[2, 3])\n\n# Plot the image\nax0 = plt.subplot(gs[0])\ndata.mean(dim=\"t\")[0].plot.imshow(vmin=0, vmax=1, ax=ax0)\nax0.set_title(\"Mean RVI\")\n\n# Plot the timeseries\nax1 = plt.subplot(gs[1])\nax1.plot(data.t.to_numpy(), data.mean(dim=(\"x\", \"y\"))[0])\nax1.set_title(\"RVI timeseries\")\n\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n\n\n\nSo, looking at the plot, it looks like the vegetation has increased over time."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html#using-awesome-spectral-indices",
    "href": "APIs/openEO/openeo-community-examples/python/RVI/RVI.html#using-awesome-spectral-indices",
    "title": "Calculate Radar Vegetation Index(RVI) using Sentinel-1 GRD collection",
    "section": "Using Awesome Spectral Indices",
    "text": "Using Awesome Spectral Indices\nThe openEO Python client has a spectral indices feature to simplify building process graphs with indices like RVI. It builds on the Awesome Spectral Indices project and allows specifying indices through their name instead of having to deal with formulas.\nHowever, please note that this API is experimental and may undergo changes. Furthermore, there might be cases where it is not supported for certain collections when executed directly in the openEO web editor.\nFor example, with the compute_indices help,,r you just have to specify “DpRVIVV” (Dual-Polarized Radar Vegetation Index VV) as follows:\n\nfrom openeo.extra.spectral_indices import compute_indices\n\n\nindices = compute_indices(\n    s1,\n    indices=[\"DpRVIVV\"],\n)\n\n\nindices.download(\"RVI_direct.nc\")\n\n\nds_indices = xr.load_dataset(\"RVI_direct.nc\")\ndata = ds_indices[[\"DpRVIVV\"]].to_array(dim=\"bands\")\n\n\nfig = plt.figure(figsize=(10, 5), dpi=90)\ngs = gridspec.GridSpec(1, 2, width_ratios=[2, 3])\n\n# Plot the image\nax0 = plt.subplot(gs[0])\ndata.mean(dim=\"t\")[0].plot.imshow(vmin=0, vmax=1, ax=ax0)\nax0.set_title(\"Mean RVI\")\n\n# Plot the timeseries\nax1 = plt.subplot(gs[1])\nax1.plot(ds_indices.t.to_numpy(), data.mean(dim=(\"x\", \"y\"))[0])\nax1.set_title(\"RVI timeseries\")\n\nplt.xticks(rotation=45)\nplt.tight_layout()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "",
    "text": "In this notebook, we want to show how to create an openEO User Defined Process(UDP). Here, we make use of an apply_dimension process that applies a process to all values along a dimension of a data cube.\nThe notebook involves a section on creating a concrete datacube, inspecting netCDF downloads, and developing a parameterized version stored as a UDP.\nimport json\nimport openeo\nimport xarray\nimport matplotlib.pyplot as plt\nfrom utils import *\n\nfrom openeo.processes import array_create, array_concat, ProcessBuilder\nfrom openeo.api.process import Parameter\nIf you have a local JupyterLab instance running, you need to install the following libraries first: openeo, xarray, ipyleaflet, shapely and matplotlib:\npip install openeo xarray shapely ipyleaflet matplotlib\nMake sure to restart the kernel and refresh the webpage.\n# Set some defaults for plots\nplt.rcParams[\"figure.figsize\"] = [5.0, 3.0]\nplt.rcParams[\"figure.dpi\"] = 75\nConnect to the openEO Platform backend (at openeo.cloud) and authenticate with OIDC.\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#inspect-raw-data",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#inspect-raw-data",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "Inspect raw data",
    "text": "Inspect raw data\nLoad initial data cube with raw S1_GRD_SIGMA0_ASCENDING data for a certain spatio-temporal extent.\n\ncenter = [46.49, 11.35]\nzoom = 15\n\neoMap = openeoMap(center, zoom)\neoMap.map\n\n\n\n\n\nbbox = eoMap.getBbox()\nprint(\"west\", bbox[0], \"\\neast\", bbox[2], \"\\nsouth\", bbox[1], \"\\nnorth\", bbox[3])\n\nwest 11.3409 \neast 11.353779 \nsouth 46.48772 \nnorth 46.493924\n\n\n\nspatial_extent = {\n    \"west\": bbox[0],\n    \"east\": bbox[2],\n    \"south\": bbox[1],\n    \"north\": bbox[3],\n    \"crs\": 4326,\n}\ntemporal_extent = [\"2023-05-01\", \"2023-07-01\"]\n\n\ns1_raw = connection.load_collection(\n    collection_id=\"SENTINEL1_GRD_SIGMA0\",\n    temporal_extent=temporal_extent,\n    spatial_extent=spatial_extent,\n    bands=[\"VH\", \"VV\"],\n)\n\nLet’s download this data cube synchronously as a netCDF file.\nThis download command triggers the actual processing on the back-end: it sends the process graph to the back-end and waits for the result. It is a synchronous operation (the download() call blocks until the result is fully downloaded) and because we work on a small spatio-temporal extent, this should only take a couple of seconds.\n\n%%time\ns1_raw.download(\"s1sar-raw.nc\")\n\nCPU times: user 36 ms, sys: 12 ms, total: 48 ms\nWall time: 1min 3s\n\n\nHowever, batch job-based execution is preferred when it is relatively larger spatial/temporal extent and the process may take some time to process.\n\nds = xarray.load_dataset(\"s1sar-raw.nc\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (t: 10, x: 102, y: 72)\nCoordinates:\n  * t        (t) datetime64[ns] 2023-05-03 2023-05-10 ... 2023-06-20 2023-06-27\n  * x        (x) float64 6.796e+05 6.796e+05 6.797e+05 ... 6.806e+05 6.806e+05\n  * y        (y) float64 5.152e+06 5.152e+06 5.152e+06 ... 5.151e+06 5.151e+06\nData variables:\n    crs      |S1 b''\n    VH       (t, y, x) float32 0.3498 0.2405 0.2339 ... 0.003244 0.003791\n    VV       (t, y, x) float32 0.356 0.356 0.809 ... 0.08211 0.01796 0.02538\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platformxarray.DatasetDimensions:t: 10x: 102y: 72Coordinates: (3)t(t)datetime64[ns]2023-05-03 ... 2023-06-27standard_name :tlong_name :taxis :Tarray(['2023-05-03T00:00:00.000000000', '2023-05-10T00:00:00.000000000',\n       '2023-05-15T00:00:00.000000000', '2023-05-22T00:00:00.000000000',\n       '2023-05-27T00:00:00.000000000', '2023-06-03T00:00:00.000000000',\n       '2023-06-08T00:00:00.000000000', '2023-06-15T00:00:00.000000000',\n       '2023-06-20T00:00:00.000000000', '2023-06-27T00:00:00.000000000'],\n      dtype='datetime64[ns]')x(x)float646.796e+05 6.796e+05 ... 6.806e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([679635., 679645., 679655., 679665., 679675., 679685., 679695., 679705.,\n       679715., 679725., 679735., 679745., 679755., 679765., 679775., 679785.,\n       679795., 679805., 679815., 679825., 679835., 679845., 679855., 679865.,\n       679875., 679885., 679895., 679905., 679915., 679925., 679935., 679945.,\n       679955., 679965., 679975., 679985., 679995., 680005., 680015., 680025.,\n       680035., 680045., 680055., 680065., 680075., 680085., 680095., 680105.,\n       680115., 680125., 680135., 680145., 680155., 680165., 680175., 680185.,\n       680195., 680205., 680215., 680225., 680235., 680245., 680255., 680265.,\n       680275., 680285., 680295., 680305., 680315., 680325., 680335., 680345.,\n       680355., 680365., 680375., 680385., 680395., 680405., 680415., 680425.,\n       680435., 680445., 680455., 680465., 680475., 680485., 680495., 680505.,\n       680515., 680525., 680535., 680545., 680555., 680565., 680575., 680585.,\n       680595., 680605., 680615., 680625., 680635., 680645.])y(y)float645.152e+06 5.152e+06 ... 5.151e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5151615., 5151605., 5151595., 5151585., 5151575., 5151565., 5151555.,\n       5151545., 5151535., 5151525., 5151515., 5151505., 5151495., 5151485.,\n       5151475., 5151465., 5151455., 5151445., 5151435., 5151425., 5151415.,\n       5151405., 5151395., 5151385., 5151375., 5151365., 5151355., 5151345.,\n       5151335., 5151325., 5151315., 5151305., 5151295., 5151285., 5151275.,\n       5151265., 5151255., 5151245., 5151235., 5151225., 5151215., 5151205.,\n       5151195., 5151185., 5151175., 5151165., 5151155., 5151145., 5151135.,\n       5151125., 5151115., 5151105., 5151095., 5151085., 5151075., 5151065.,\n       5151055., 5151045., 5151035., 5151025., 5151015., 5151005., 5150995.,\n       5150985., 5150975., 5150965., 5150955., 5150945., 5150935., 5150925.,\n       5150915., 5150905.])Data variables: (3)crs()|S1b''crs_wkt :PROJCS[\"WGS 84 / UTM zone 32N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 9.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32632\"]]spatial_ref :PROJCS[\"WGS 84 / UTM zone 32N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 9.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32632\"]]array(b'', dtype='|S1')VH(t, y, x)float320.3498 0.2405 ... 0.003244 0.003791long_name :VHunits :grid_mapping :crsarray([[[0.3497991 , 0.240497  , 0.23386185, ..., 0.00583607,\n         0.01114625, 0.01180739],\n        [0.21625957, 0.22582962, 0.27404746, ..., 0.01413637,\n         0.01605362, 0.02861624],\n        [0.21343715, 0.15391256, 0.1240928 , ..., 0.01913997,\n         0.02732791, 0.06749884],\n        ...,\n        [0.00975169, 0.00683461, 0.00614753, ..., 0.00683165,\n         0.00199292, 0.01335612],\n        [0.00809784, 0.00810335, 0.0087291 , ..., 0.00491569,\n         0.00386301, 0.01294566],\n        [0.00524675, 0.01154147, 0.0111153 , ..., 0.0089683 ,\n         0.00496414, 0.00955531]],\n\n       [[0.05656133, 0.07117188, 0.12068269, ..., 0.03960535,\n         0.02495288, 0.01579268],\n        [0.09063352, 0.06542234, 0.06321717, ..., 0.074268  ,\n         0.04379695, 0.02947565],\n        [0.08322202, 0.05983272, 0.06086417, ..., 0.06582635,\n         0.04827164, 0.05478774],\n...\n        [0.02081406, 0.01660735, 0.00927989, ..., 0.00631161,\n         0.00305758, 0.01555643],\n        [0.01103931, 0.01278648, 0.0059051 , ..., 0.00733566,\n         0.00785622, 0.01034456],\n        [0.01053969, 0.01614342, 0.01426238, ..., 0.00684131,\n         0.00470488, 0.00610361]],\n\n       [[0.11626446, 0.10056856, 0.11683535, ..., 0.02819041,\n         0.01466341, 0.0131486 ],\n        [0.0842052 , 0.09308649, 0.08042921, ..., 0.0202127 ,\n         0.01330222, 0.02162029],\n        [0.11007892, 0.10168418, 0.07600641, ..., 0.01328301,\n         0.06286687, 0.09713611],\n        ...,\n        [0.02943419, 0.03480869, 0.03886167, ..., 0.00179048,\n         0.01113027, 0.00225721],\n        [0.01721563, 0.02312371, 0.05103515, ..., 0.00179926,\n         0.00703626, 0.00816311],\n        [0.01056776, 0.01323224, 0.01524322, ..., 0.01825097,\n         0.00324418, 0.00379148]]], dtype=float32)VV(t, y, x)float320.356 0.356 ... 0.01796 0.02538long_name :VVunits :grid_mapping :crsarray([[[0.35600373, 0.3559536 , 0.80896807, ..., 0.3317161 ,\n         0.16445988, 0.04160649],\n        [0.61769664, 0.5578946 , 0.5050262 , ..., 0.22054403,\n         0.10073389, 0.06029705],\n        [0.84790504, 0.6570914 , 0.58305556, ..., 0.16478802,\n         0.05791532, 0.0579203 ],\n        ...,\n        [0.1073544 , 0.10243879, 0.09812227, ..., 0.02534649,\n         0.00922628, 0.01754627],\n        [0.05122279, 0.06052506, 0.06034074, ..., 0.02414759,\n         0.01439755, 0.01660094],\n        [0.04778335, 0.05747797, 0.06184885, ..., 0.02491218,\n         0.02271986, 0.01123959]],\n\n       [[0.4726906 , 0.47493017, 0.82746756, ..., 0.22725   ,\n         0.10644101, 0.04749625],\n        [0.39489403, 0.413578  , 0.57773596, ..., 0.22964555,\n         0.12250378, 0.06308025],\n        [0.4349608 , 0.4983854 , 0.8121271 , ..., 0.14484774,\n         0.11491019, 0.10635231],\n...\n        [0.10983685, 0.09603946, 0.08231169, ..., 0.07407534,\n         0.02241934, 0.17368278],\n        [0.05511827, 0.04872738, 0.0588784 , ..., 0.10989355,\n         0.01494858, 0.09516722],\n        [0.03997884, 0.03643978, 0.03698706, ..., 0.09584986,\n         0.01911887, 0.03895752]],\n\n       [[0.54582274, 0.8157599 , 1.0333498 , ..., 0.10145897,\n         0.07532571, 0.0363849 ],\n        [0.5309309 , 0.97571516, 1.3032748 , ..., 0.1301856 ,\n         0.08703941, 0.06910698],\n        [0.2805328 , 0.6426719 , 0.9920144 , ..., 0.13717869,\n         0.18444583, 0.2799541 ],\n        ...,\n        [0.07433109, 0.08866205, 0.08638017, ..., 0.01586673,\n         0.04167084, 0.0467084 ],\n        [0.06121723, 0.12546135, 0.15904632, ..., 0.01122087,\n         0.02190844, 0.06850047],\n        [0.08498283, 0.09475873, 0.06337849, ..., 0.08210503,\n         0.01796073, 0.02538092]]], dtype=float32)Indexes: (3)tPandasIndexPandasIndex(DatetimeIndex(['2023-05-03', '2023-05-10', '2023-05-15', '2023-05-22',\n               '2023-05-27', '2023-06-03', '2023-06-08', '2023-06-15',\n               '2023-06-20', '2023-06-27'],\n              dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Index([679635.0, 679645.0, 679655.0, 679665.0, 679675.0, 679685.0, 679695.0,\n       679705.0, 679715.0, 679725.0,\n       ...\n       680555.0, 680565.0, 680575.0, 680585.0, 680595.0, 680605.0, 680615.0,\n       680625.0, 680635.0, 680645.0],\n      dtype='float64', name='x', length=102))yPandasIndexPandasIndex(Index([5151615.0, 5151605.0, 5151595.0, 5151585.0, 5151575.0, 5151565.0,\n       5151555.0, 5151545.0, 5151535.0, 5151525.0, 5151515.0, 5151505.0,\n       5151495.0, 5151485.0, 5151475.0, 5151465.0, 5151455.0, 5151445.0,\n       5151435.0, 5151425.0, 5151415.0, 5151405.0, 5151395.0, 5151385.0,\n       5151375.0, 5151365.0, 5151355.0, 5151345.0, 5151335.0, 5151325.0,\n       5151315.0, 5151305.0, 5151295.0, 5151285.0, 5151275.0, 5151265.0,\n       5151255.0, 5151245.0, 5151235.0, 5151225.0, 5151215.0, 5151205.0,\n       5151195.0, 5151185.0, 5151175.0, 5151165.0, 5151155.0, 5151145.0,\n       5151135.0, 5151125.0, 5151115.0, 5151105.0, 5151095.0, 5151085.0,\n       5151075.0, 5151065.0, 5151055.0, 5151045.0, 5151035.0, 5151025.0,\n       5151015.0, 5151005.0, 5150995.0, 5150985.0, 5150975.0, 5150965.0,\n       5150955.0, 5150945.0, 5150935.0, 5150925.0, 5150915.0, 5150905.0],\n      dtype='float64', name='y'))Attributes: (2)Conventions :CF-1.9institution :openEO platform\n\n\nWe got these observations dates:\n\nds.coords[\"t\"].values\n\narray(['2023-05-03T00:00:00.000000000', '2023-05-10T00:00:00.000000000',\n       '2023-05-15T00:00:00.000000000', '2023-05-22T00:00:00.000000000',\n       '2023-05-27T00:00:00.000000000', '2023-06-03T00:00:00.000000000',\n       '2023-06-08T00:00:00.000000000', '2023-06-15T00:00:00.000000000',\n       '2023-06-20T00:00:00.000000000', '2023-06-27T00:00:00.000000000'],\n      dtype='datetime64[ns]')\n\n\nA quick plot for visual inspection.\n\nds[\"VH\"].isel(t=0).plot(vmin=0, vmax=0.5)\n\n&lt;matplotlib.collections.QuadMesh at 0x7ff7a5f59850&gt;\n\n\n\n\n\nThis section presented a straightforward example of retrieving and analyzing a S1_GRD_SIGMA0_ASCENDING data cube from the backend within a defined area of interest during a specified time frame."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#collect-statistics",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#collect-statistics",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "Collect statistics",
    "text": "Collect statistics\nAs part of more detailed processing within the openEO platform, we’ll gather temporal statistics using the apply_dimension process and a collection of statistical measures (minimum, maximum, quantiles, …).\n\ndef get_stats(data: ProcessBuilder) -&gt; ProcessBuilder:\n    \"\"\"\n    Collect stats for `data` (to be interpreted as an array of values along the \"t\" dimension).\n    We should return a new array with the stats.\n    \"\"\"\n    # Put some scalar stats (`min`, `max`, ... return a scalar value) in a new array\n    scalar_stats = array_create(\n        [\n            data.min(),\n            data.max(),\n            data.mean(),\n            data.sd(),\n        ]\n    )\n    # The `quantiles` process returns an array on its own\n    quantile_stats = data.quantiles([0.1, 0.5, 0.9])\n\n    # Combine everything in a single array\n    return array_concat(array1=scalar_stats, array2=quantile_stats)\n\n\ns1_stats = s1_raw.apply_dimension(\n    process=get_stats,\n    dimension=\"t\",\n    target_dimension=\"bands\",\n)\n# Rename band labels, pairing original band names with stat names\ns1_stats = s1_stats.rename_labels(\n    \"bands\",\n    [\n        f\"{b}_{s}\"\n        for b in s1_raw.metadata.band_names\n        for s in [\"min\", \"max\", \"mean\", \"sd\", \"q10\", \"q50\", \"q90\"]\n    ],\n)\n\n\n# %%time\n# s1_stats.download(\"s1grd-stats.nc\")\n\n# let's try batch job based execution in this process\n\njob = s1_stats.execute_batch(\n    title=\"Sentinel1_GRD_Statistics\", outputfile=\"S1grd-stats.nc\"\n)\n\n\n# # Alternatively if you want to seperately save process metadata\n# s1_stats = s1_stats.save_result(format=\"netcdf\")\n# job = s1_stats.execute_batch(title=\"Sentinel 1 Statistics\")\n\n# # fetch your results\n\n# results = job.get_results()\n# results.download_files(\"output/batch_job\")\n\n0:00:00 Job 'vito-j-231106d381904baaae522c536de54d94': send 'start'\n0:00:20 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:00:26 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:00:33 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:00:42 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:00:52 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:01:05 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:01:21 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:01:41 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:02:05 Job 'vito-j-231106d381904baaae522c536de54d94': queued (progress N/A)\n0:02:36 Job 'vito-j-231106d381904baaae522c536de54d94': finished (progress N/A)\n\n\n\nassets = job.get_results().get_assets()\nprint(assets[0].href)\n\nhttps://openeo.vito.be/openeo/1.1/jobs/j-231106d381904baaae522c536de54d94/results/assets/MjUyNTRjNGRiMTkzMGNhNzQwNjg0OTJmM2NhOWIyZjM0N2JhMWU3ZTI0ZTAzY2U0OTMzOTlmZWE1NmVhOTQzN0BlZ2kuZXU%3D/2b1da7b8285e21cfcb5367297761fd95/openEO.nc?expires=1699889359\n\n\n\nds = xarray.load_dataset(\"S1grd-stats.nc\").drop_vars(\"crs\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (x: 102, y: 72)\nCoordinates:\n  * x        (x) float64 6.796e+05 6.796e+05 6.797e+05 ... 6.806e+05 6.806e+05\n  * y        (y) float64 5.152e+06 5.152e+06 5.152e+06 ... 5.151e+06 5.151e+06\nData variables: (12/14)\n    VH_min   (y, x) float32 0.05656 0.07117 0.1168 ... 0.003244 0.003747\n    VH_max   (y, x) float32 0.3573 0.2405 0.2681 ... 0.01825 0.009648 0.01772\n    VH_mean  (y, x) float32 0.1938 0.1531 0.1831 ... 0.01131 0.006023 0.008333\n    VH_sd    (y, x) float32 0.1234 0.05839 0.05198 ... 0.002023 0.004678\n    VH_q10   (y, x) float32 0.05941 0.07411 0.1172 ... 0.003359 0.003751\n    VH_q50   (y, x) float32 0.133 0.1501 0.1768 ... 0.00909 0.005713 0.006541\n    ...       ...\n    VV_max   (y, x) float32 1.072 0.8158 1.447 3.338 ... 0.1091 0.03152 0.07446\n    VV_mean  (y, x) float32 0.5529 0.5677 0.9432 ... 0.05799 0.02198 0.0379\n    VV_sd    (y, x) float32 0.2939 0.1981 0.2196 ... 0.03274 0.007118 0.0214\n    VV_q10   (y, x) float32 0.2752 0.3114 0.7015 ... 0.02147 0.009028 0.01147\n    VV_q50   (y, x) float32 0.4215 0.5806 0.868 1.512 ... 0.06002 0.02159 0.0371\n    VV_q90   (y, x) float32 1.067 0.8109 1.411 3.236 ... 0.1078 0.03147 0.0742\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platform - Geotrellis backend: 0.18.0a1\n    description:  \n    title:        xarray.DatasetDimensions:x: 102y: 72Coordinates: (2)x(x)float646.796e+05 6.796e+05 ... 6.806e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([679635., 679645., 679655., 679665., 679675., 679685., 679695., 679705.,\n       679715., 679725., 679735., 679745., 679755., 679765., 679775., 679785.,\n       679795., 679805., 679815., 679825., 679835., 679845., 679855., 679865.,\n       679875., 679885., 679895., 679905., 679915., 679925., 679935., 679945.,\n       679955., 679965., 679975., 679985., 679995., 680005., 680015., 680025.,\n       680035., 680045., 680055., 680065., 680075., 680085., 680095., 680105.,\n       680115., 680125., 680135., 680145., 680155., 680165., 680175., 680185.,\n       680195., 680205., 680215., 680225., 680235., 680245., 680255., 680265.,\n       680275., 680285., 680295., 680305., 680315., 680325., 680335., 680345.,\n       680355., 680365., 680375., 680385., 680395., 680405., 680415., 680425.,\n       680435., 680445., 680455., 680465., 680475., 680485., 680495., 680505.,\n       680515., 680525., 680535., 680545., 680555., 680565., 680575., 680585.,\n       680595., 680605., 680615., 680625., 680635., 680645.])y(y)float645.152e+06 5.152e+06 ... 5.151e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5151615., 5151605., 5151595., 5151585., 5151575., 5151565., 5151555.,\n       5151545., 5151535., 5151525., 5151515., 5151505., 5151495., 5151485.,\n       5151475., 5151465., 5151455., 5151445., 5151435., 5151425., 5151415.,\n       5151405., 5151395., 5151385., 5151375., 5151365., 5151355., 5151345.,\n       5151335., 5151325., 5151315., 5151305., 5151295., 5151285., 5151275.,\n       5151265., 5151255., 5151245., 5151235., 5151225., 5151215., 5151205.,\n       5151195., 5151185., 5151175., 5151165., 5151155., 5151145., 5151135.,\n       5151125., 5151115., 5151105., 5151095., 5151085., 5151075., 5151065.,\n       5151055., 5151045., 5151035., 5151025., 5151015., 5151005., 5150995.,\n       5150985., 5150975., 5150965., 5150955., 5150945., 5150935., 5150925.,\n       5150915., 5150905.])Data variables: (14)VH_min(y, x)float320.05656 0.07117 ... 0.003747long_name :VH_minunits :grid_mapping :crsarray([[0.05656133, 0.07117188, 0.11683535, ..., 0.00583607, 0.01081081,\n        0.00508684],\n       [0.07500888, 0.06542234, 0.06321717, ..., 0.01094758, 0.01196851,\n        0.01167572],\n       [0.07982897, 0.05711685, 0.05155696, ..., 0.01328301, 0.027206  ,\n        0.02724834],\n       ...,\n       [0.00846185, 0.00683461, 0.00614753, ..., 0.00179048, 0.00129333,\n        0.00225721],\n       [0.00809784, 0.00810335, 0.0053619 , ..., 0.00179926, 0.00333473,\n        0.00225653],\n       [0.00524675, 0.01053381, 0.01038756, ..., 0.00545653, 0.00324418,\n        0.00374676]], dtype=float32)VH_max(y, x)float320.3573 0.2405 ... 0.009648 0.01772long_name :VH_maxunits :grid_mapping :crsarray([[0.35734206, 0.240497  , 0.26810205, ..., 0.03960535, 0.04652854,\n        0.03217617],\n       [0.27547497, 0.22582962, 0.27438593, ..., 0.074268  , 0.0501226 ,\n        0.05067734],\n       [0.21343715, 0.15391256, 0.20813324, ..., 0.06582635, 0.06727563,\n        0.09713611],\n       ...,\n       [0.03269713, 0.03480869, 0.03886167, ..., 0.01295399, 0.01236935,\n        0.02895651],\n       [0.03411233, 0.02388769, 0.05103515, ..., 0.01585265, 0.01147545,\n        0.01942277],\n       [0.02799423, 0.03830879, 0.03787919, ..., 0.01825097, 0.00964783,\n        0.01771976]], dtype=float32)VH_mean(y, x)float320.1938 0.1531 ... 0.006023 0.008333long_name :VH_meanunits :grid_mapping :crsarray([[0.19379085, 0.15306295, 0.18312563, ..., 0.02910196, 0.0209428 ,\n        0.01723359],\n       [0.14099422, 0.14379239, 0.18332611, ..., 0.0316726 , 0.02414871,\n        0.02805693],\n       [0.12428942, 0.1015469 , 0.10913865, ..., 0.02867122, 0.04174799,\n        0.06203262],\n       ...,\n       [0.02315117, 0.02049922, 0.01756792, ..., 0.00845076, 0.00658812,\n        0.01444702],\n       [0.01737622, 0.01484519, 0.01687199, ..., 0.00770858, 0.00725461,\n        0.01071056],\n       [0.01501316, 0.01779779, 0.02084686, ..., 0.01131313, 0.00602277,\n        0.00833274]], dtype=float32)VH_sd(y, x)float320.1234 0.05839 ... 0.004678long_name :VH_sdunits :grid_mapping :crsarray([[0.1234301 , 0.05839253, 0.05198416, ..., 0.01116493, 0.01174154,\n        0.00802417],\n       [0.07352218, 0.0530638 , 0.0823708 , ..., 0.01750424, 0.01386139,\n        0.01197202],\n       [0.05126064, 0.02968998, 0.04590503, ..., 0.01585866, 0.01459462,\n        0.018599  ],\n       ...,\n       [0.00902716, 0.010062  , 0.01015026, ..., 0.00390621, 0.00422678,\n        0.00737783],\n       [0.00868381, 0.00717187, 0.01325622, ..., 0.00384475, 0.00265791,\n        0.00501948],\n       [0.00654324, 0.00838748, 0.0088279 , ..., 0.00490586, 0.0020231 ,\n        0.00467761]], dtype=float32)VH_q10(y, x)float320.05941 0.07411 ... 0.003751long_name :VH_q10units :grid_mapping :crsarray([[0.05940933, 0.07411155, 0.11722008, ..., 0.00716483, 0.01084435,\n        0.0057589 ],\n       [0.07568569, 0.06818876, 0.06493837, ..., 0.01126646, 0.01207127,\n        0.01214764],\n       [0.08016828, 0.05738844, 0.05248768, ..., 0.01339516, 0.02721819,\n        0.02920248],\n       ...,\n       [0.00859083, 0.00720239, 0.006311  , ..., 0.00196476, 0.00136329,\n        0.00259918],\n       [0.00819345, 0.00810346, 0.00541622, ..., 0.0021109 , 0.00338756,\n        0.00268728],\n       [0.00577605, 0.01063458, 0.01046034, ..., 0.00556804, 0.00335882,\n        0.00375124]], dtype=float32)VH_q50(y, x)float320.133 0.1501 ... 0.005713 0.006541long_name :VH_q50units :grid_mapping :crsarray([[0.13300729, 0.15007433, 0.17675401, ..., 0.03204274, 0.01529548,\n        0.01519107],\n       [0.09407672, 0.13539955, 0.17731631, ..., 0.03274555, 0.0182677 ,\n        0.02632112],\n       [0.10193632, 0.10380882, 0.10318546, ..., 0.02546695, 0.03829441,\n        0.06536277],\n       ...,\n       [0.02484563, 0.01899279, 0.01615147, ..., 0.00888117, 0.00703254,\n        0.01407467],\n       [0.01674017, 0.01109307, 0.01389441, ..., 0.00752113, 0.00739714,\n        0.01085477],\n       [0.01335989, 0.01628395, 0.02012532, ..., 0.00908964, 0.00571314,\n        0.00654103]], dtype=float32)VH_q90(y, x)float320.3566 0.2373 ... 0.009549 0.01733long_name :VH_q90units :grid_mapping :crsarray([[0.35658777, 0.23734617, 0.26467803, ..., 0.03950901, 0.0454438 ,\n        0.03181273],\n       [0.26955342, 0.22517808, 0.27435207, ..., 0.0703794 , 0.04949003,\n        0.05011839],\n       [0.21303499, 0.15208827, 0.20175149, ..., 0.06331098, 0.06683476,\n        0.09504365],\n       ...,\n       [0.03268131, 0.0345371 , 0.03776738, ..., 0.01293681, 0.01228781,\n        0.02810093],\n       [0.03357601, 0.02388243, 0.04815632, ..., 0.01540591, 0.01140328,\n        0.01917945],\n       [0.02728007, 0.0370444 , 0.03717419, ..., 0.01816397, 0.00954943,\n        0.01732791]], dtype=float32)VV_min(y, x)float320.2706 0.3071 ... 0.008148 0.01124long_name :VV_minunits :grid_mapping :crsarray([[0.27061334, 0.3071222 , 0.695984  , ..., 0.10145897, 0.06688334,\n        0.02374299],\n       [0.22295584, 0.20867158, 0.395954  , ..., 0.09766635, 0.06968673,\n        0.05215759],\n       [0.2805328 , 0.3170197 , 0.28509635, ..., 0.10177591, 0.05791532,\n        0.0579203 ],\n       ...,\n       [0.05919972, 0.04508727, 0.04939938, ..., 0.01586673, 0.00922628,\n        0.01754627],\n       [0.01951935, 0.04062539, 0.0460576 , ..., 0.01122087, 0.01439755,\n        0.01660094],\n       [0.03997884, 0.03643978, 0.03698706, ..., 0.02123374, 0.00814751,\n        0.01123959]], dtype=float32)VV_max(y, x)float321.072 0.8158 ... 0.03152 0.07446long_name :VV_maxunits :grid_mapping :crsarray([[1.0719821 , 0.8157599 , 1.4465231 , ..., 0.33690315, 0.19723721,\n        0.07042488],\n       [0.8385794 , 0.97571516, 1.3032748 , ..., 0.2828829 , 0.12840499,\n        0.11779951],\n       [0.84790504, 0.66477776, 0.9920144 , ..., 0.2696955 , 0.18787171,\n        0.2799541 ],\n       ...,\n       [0.15173328, 0.21506543, 0.2475289 , ..., 0.07407534, 0.10700826,\n        0.17368278],\n       [0.12819055, 0.31527394, 0.30131364, ..., 0.10989355, 0.06801304,\n        0.09516722],\n       [0.22751552, 0.40916196, 0.44688013, ..., 0.10911988, 0.03151594,\n        0.07445894]], dtype=float32)VV_mean(y, x)float320.5529 0.5677 ... 0.02198 0.0379long_name :VV_meanunits :grid_mapping :crsarray([[0.5528725 , 0.5677464 , 0.943177  , ..., 0.18943602, 0.10289548,\n        0.04630637],\n       [0.5078519 , 0.53124654, 0.6385149 , ..., 0.16875158, 0.10148996,\n        0.07470229],\n       [0.55347466, 0.5266556 , 0.5807746 , ..., 0.16240971, 0.12200638,\n        0.15296336],\n       ...,\n       [0.09600382, 0.10248156, 0.10659529, ..., 0.0411635 , 0.0423783 ,\n        0.06334069],\n       [0.06717557, 0.10029311, 0.11722443, ..., 0.04212674, 0.03074012,\n        0.05524332],\n       [0.09754252, 0.12261887, 0.12331397, ..., 0.05799466, 0.02197856,\n        0.03790025]], dtype=float32)VV_sd(y, x)float320.2939 0.1981 ... 0.007118 0.0214long_name :VV_sdunits :grid_mapping :crsarray([[0.29392862, 0.19808702, 0.21960482, ..., 0.08753916, 0.04366262,\n        0.0147506 ],\n       [0.20480074, 0.21877526, 0.280819  , ..., 0.06148675, 0.01930615,\n        0.02179322],\n       [0.20626569, 0.12942038, 0.19957876, ..., 0.05477447, 0.04675601,\n        0.07778829],\n       ...,\n       [0.02826963, 0.04480741, 0.05676578, ..., 0.02299318, 0.03381887,\n        0.04750483],\n       [0.03085829, 0.07947312, 0.07652301, ..., 0.03100377, 0.01888482,\n        0.02674902],\n       [0.05362873, 0.10700163, 0.1192916 , ..., 0.0327389 , 0.00711785,\n        0.021403  ]], dtype=float32)VV_q10(y, x)float320.2752 0.3114 ... 0.009028 0.01147long_name :VV_q10units :grid_mapping :crsarray([[0.2751771 , 0.31138933, 0.7014966 , ..., 0.10362278, 0.06716368,\n        0.02494995],\n       [0.22528493, 0.22181997, 0.39913344, ..., 0.09886947, 0.07085913,\n        0.05229818],\n       [0.2871844 , 0.32297683, 0.2972333 , ..., 0.10357276, 0.0590695 ,\n        0.06047119],\n       ...,\n       [0.05939193, 0.04718   , 0.05108054, ..., 0.01605947, 0.0092452 ,\n        0.0176414 ],\n       [0.02235149, 0.04143558, 0.04733969, ..., 0.01158348, 0.01445265,\n        0.01682445],\n       [0.0407593 , 0.0385436 , 0.03947324, ..., 0.02146513, 0.00902846,\n        0.01147327]], dtype=float32)VV_q50(y, x)float320.4215 0.5806 ... 0.02159 0.0371long_name :VV_q50units :grid_mapping :crsarray([[0.42152   , 0.5805598 , 0.86803246, ..., 0.14398697, 0.09079412,\n        0.04239713],\n       [0.556627  , 0.50202835, 0.5410211 , ..., 0.15162778, 0.09911549,\n        0.06609362],\n       [0.48160863, 0.5312781 , 0.56976897, ..., 0.14168528, 0.1205292 ,\n        0.12686422],\n       ...,\n       [0.09715581, 0.0989629 , 0.08927221, ..., 0.03616992, 0.03204508,\n        0.04744019],\n       [0.05867583, 0.08330473, 0.10578761, ..., 0.03562438, 0.02128811,\n        0.06257379],\n       [0.08331519, 0.09137604, 0.07936722, ..., 0.06001542, 0.02159299,\n        0.03710367]], dtype=float32)VV_q90(y, x)float321.067 0.8109 ... 0.03147 0.0742long_name :VV_q90units :grid_mapping :crsarray([[1.0667812 , 0.81092757, 1.4112619 , ..., 0.33638445, 0.19395947,\n        0.07008593],\n       [0.82598644, 0.95338386, 1.2640164 , ..., 0.27755916, 0.12781487,\n        0.11575763],\n       [0.8462007 , 0.6643335 , 0.97402567, ..., 0.26723295, 0.18752912,\n        0.2781414 ],\n       ...,\n       [0.14797401, 0.20554605, 0.23791873, ..., 0.07333827, 0.10493214,\n        0.16572711],\n       [0.12596807, 0.2962927 , 0.2870869 , ..., 0.10658363, 0.06703736,\n        0.09342997],\n       [0.21806926, 0.3837865 , 0.41725355, ..., 0.10779288, 0.03146961,\n        0.07420164]], dtype=float32)Indexes: (2)xPandasIndexPandasIndex(Index([679635.0, 679645.0, 679655.0, 679665.0, 679675.0, 679685.0, 679695.0,\n       679705.0, 679715.0, 679725.0,\n       ...\n       680555.0, 680565.0, 680575.0, 680585.0, 680595.0, 680605.0, 680615.0,\n       680625.0, 680635.0, 680645.0],\n      dtype='float64', name='x', length=102))yPandasIndexPandasIndex(Index([5151615.0, 5151605.0, 5151595.0, 5151585.0, 5151575.0, 5151565.0,\n       5151555.0, 5151545.0, 5151535.0, 5151525.0, 5151515.0, 5151505.0,\n       5151495.0, 5151485.0, 5151475.0, 5151465.0, 5151455.0, 5151445.0,\n       5151435.0, 5151425.0, 5151415.0, 5151405.0, 5151395.0, 5151385.0,\n       5151375.0, 5151365.0, 5151355.0, 5151345.0, 5151335.0, 5151325.0,\n       5151315.0, 5151305.0, 5151295.0, 5151285.0, 5151275.0, 5151265.0,\n       5151255.0, 5151245.0, 5151235.0, 5151225.0, 5151215.0, 5151205.0,\n       5151195.0, 5151185.0, 5151175.0, 5151165.0, 5151155.0, 5151145.0,\n       5151135.0, 5151125.0, 5151115.0, 5151105.0, 5151095.0, 5151085.0,\n       5151075.0, 5151065.0, 5151055.0, 5151045.0, 5151035.0, 5151025.0,\n       5151015.0, 5151005.0, 5150995.0, 5150985.0, 5150975.0, 5150965.0,\n       5150955.0, 5150945.0, 5150935.0, 5150925.0, 5150915.0, 5150905.0],\n      dtype='float64', name='y'))Attributes: (4)Conventions :CF-1.9institution :openEO platform - Geotrellis backend: 0.18.0a1description :title :\n\n\n\nds[[\"VH_mean\", \"VV_mean\"]].to_array().plot.imshow(col=\"variable\", vmin=0, vmax=1)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7ff79de13e50&gt;"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#build-s1-sar-stats-udp",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#build-s1-sar-stats-udp",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "Build S1 SAR stats UDP",
    "text": "Build S1 SAR stats UDP\nSuppose we want to save the above-described algorithm as a User-Defined-Process(UDP). Therefore, in this section, we define the input parameters, define the earlier workflow and then save it as a process.\nThe only limitation of this approach, is that your workflow needs to be defined as a single process graph. So workflows that require multiple openEO invocations or complex parameter preprocessing won’t work yet. However, thanks to the flexibility of openEO and the ability to include custom code as a UDF, a lot of algorithms can already be defined in a single openEO graph.\n\nimport openeo\nfrom openeo.api.process import Parameter\nfrom openeo.processes import array_create, array_concat\n\nLet us define the UDP parameters to allow specifying the spatio-temporal extent.\nTo make a service available to users, we might want to replace certain fixed values in your process graph with parameters that can be set by the user of your process. This provides you with a parameterised UDP.\n\ntemporal_extent = Parameter(\n    name=\"temporal_extent\",\n    description=\"The time window to calculate the stats for.\",\n    schema={\"type\": \"array\", \"subtype\": \"temporal-interval\"},\n    default=[\"2023-05-01\", \"2023-07-30\"],\n)\nspatial_extent = Parameter(\n    name=\"spatial_extent\",\n    description=\"The spatial extent to calculate the stats for.\",\n    schema={\"type\": \"object\", \"subtype\": \"bounding-box\"},\n    default={\"west\": 8.82, \"south\": 44.40, \"east\": 8.92, \"north\": 44.45},\n)\n\n\ns1_raw = connection.load_collection(\n    collection_id=\"S1_GRD_SIGMA0_ASCENDING\",\n    temporal_extent=temporal_extent,\n    spatial_extent=spatial_extent,\n    bands=[\"VH\", \"VV\"],\n)\n\n# Unlike above, where we defined the `apply_dimension` process\n# through a regular python function, we do it here compactily with a single \"lambda\".\ns1_stats = s1_raw.apply_dimension(\n    process=lambda data: array_concat(\n        array1=array_create([data.min(), data.max(), data.mean(), data.sd()]),\n        array2=data.quantiles([0.1, 0.5, 0.9]),\n    ),\n    dimension=\"t\",\n    target_dimension=\"bands\",\n)\n# Rename band labels, pairing original band names with stat names\ns1_stats = s1_stats.rename_labels(\n    \"bands\",\n    [\n        f\"{b}_{s}\"\n        for b in s1_stats.metadata.band_names\n        for s in [\"min\", \"max\", \"mean\", \"sd\", \"q10\", \"q50\", \"q90\"]\n    ],\n)\n\nStore this parameterized data cube as a UDP\n\nudp_sar = connection.save_user_defined_process(\n    user_defined_process_id=\"s1_stats\",\n    process_graph=s1_stats,\n    parameters=[temporal_extent, spatial_extent],\n    summary=\"S1 SAR stats\",\n    description=\"Calculate S1 SAR stats (min, max, mean, sd, q10, q50, q90). This service can cost an approximate of 3-5 credits per sq km. This cost is based on resource consumpltion only and added-value cost has not been included.\",\n    public=True,\n)\n\nWhen saving a process, please note that saved processes are private by default, nonetheless can be used multiple times by an individual. Therefore, to share with a large audience, you will need a public URL that can be achieved once the process is saved as public.\n\npublic_url, _ = [\n    l[\"href\"] for l in udp_sar.describe()[\"links\"] if l[\"rel\"] == \"canonical\"\n]"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#use-the-saved-udp-in-the-python-client",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#use-the-saved-udp-in-the-python-client",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "Use the saved UDP in the Python Client",
    "text": "Use the saved UDP in the Python Client\nNow, let’s evaluate our freshly created user-defined processes “s1_stats”. We can use datacube_from_process() to create a DataCube from this process and only have to provide concrete temporal and spatial extents\nNote: Since the spatial_extent and temporal_extent variable were re-assigned as a paramter definition, you might have lost their value, so please don’t forget to re-define your interested extent in the cell below.\n\nsar = connection.datacube_from_process(\n    \"s1_stats\",\n    namespace=public_url,\n    temporal_extent=[\"2023-05-01\", \"2023-07-30\"],\n    spatial_extent={\"west\": 8.82, \"south\": 44.40, \"east\": 8.92, \"north\": 44.45},\n)\n\n\nsar.download(\"sar_udp.nc\")\n\n\nds = xarray.load_dataset(\"sar_udp.nc\").drop_vars(\"crs\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (x: 798, y: 558)\nCoordinates:\n  * x        (x) float64 4.857e+05 4.857e+05 4.857e+05 ... 4.936e+05 4.936e+05\n  * y        (y) float64 4.922e+06 4.922e+06 4.922e+06 ... 4.916e+06 4.916e+06\nData variables: (12/14)\n    VH_min   (y, x) float32 0.007442 0.01634 0.02477 ... 0.0002788 0.0003386\n    VH_max   (y, x) float32 0.03604 0.0584 0.08822 ... 0.003115 0.006632\n    VH_mean  (y, x) float32 0.01978 0.03318 0.05122 ... 0.001801 0.003237\n    VH_sd    (y, x) float32 0.009419 0.01686 0.0247 ... 0.0009503 0.00193\n    VH_q10   (y, x) float32 0.007442 0.01634 0.02477 ... 0.0002788 0.0003386\n    VH_q50   (y, x) float32 0.01642 0.02946 0.04615 ... 0.001851 0.003168\n    ...       ...\n    VV_max   (y, x) float32 0.2145 0.352 0.4421 ... 0.02347 0.02271 0.04126\n    VV_mean  (y, x) float32 0.09626 0.1365 0.1872 ... 0.01012 0.01103 0.01548\n    VV_sd    (y, x) float32 0.05409 0.09378 0.1112 ... 0.006371 0.006519 0.01286\n    VV_q10   (y, x) float32 0.03823 0.0581 0.09845 ... 0.003309 0.002925\n    VV_q50   (y, x) float32 0.08684 0.1051 0.1579 ... 0.008462 0.009489 0.01156\n    VV_q90   (y, x) float32 0.2145 0.352 0.4421 ... 0.02347 0.02271 0.04126\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platformxarray.DatasetDimensions:x: 798y: 558Coordinates: (2)x(x)float644.857e+05 4.857e+05 ... 4.936e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([485665., 485675., 485685., ..., 493615., 493625., 493635.])y(y)float644.922e+06 4.922e+06 ... 4.916e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([4921875., 4921865., 4921855., ..., 4916325., 4916315., 4916305.])Data variables: (14)VH_min(y, x)float320.007442 0.01634 ... 0.0003386long_name :VH_minunits :grid_mapping :crsarray([[7.4415309e-03, 1.6343007e-02, 2.4772413e-02, ..., 5.5835072e-02,\n        4.0328607e-02, 3.1856596e-02],\n       [9.0845078e-03, 1.8613091e-02, 3.0874813e-02, ..., 5.9993275e-02,\n        5.3687811e-02, 4.7377892e-02],\n       [1.0651190e-02, 1.9190351e-02, 3.1387392e-02, ..., 8.2894199e-02,\n        7.3106125e-02, 4.1296154e-02],\n       ...,\n       [3.2187731e-05, 4.6482327e-04, 4.1350679e-04, ..., 3.3551207e-04,\n        1.4962933e-04, 7.3057665e-05],\n       [1.5885655e-05, 3.9615774e-05, 1.2235668e-04, ..., 3.0250914e-04,\n        7.5182226e-04, 1.5585287e-04],\n       [1.5885615e-05, 4.6827008e-05, 2.3712554e-04, ..., 1.6382546e-04,\n        2.7883082e-04, 3.3861815e-04]], dtype=float32)VH_max(y, x)float320.03604 0.0584 ... 0.006632long_name :VH_maxunits :grid_mapping :crsarray([[0.03604072, 0.05839535, 0.08822469, ..., 0.32693267, 0.2599725 ,\n        0.16062291],\n       [0.06696388, 0.10971747, 0.14447221, ..., 0.31547707, 0.19647637,\n        0.10249937],\n       [0.10121775, 0.13220015, 0.14859372, ..., 0.24435118, 0.16842037,\n        0.11494572],\n       ...,\n       [0.00431482, 0.00388348, 0.00630981, ..., 0.00496825, 0.01302709,\n        0.0066534 ],\n       [0.00336156, 0.00744628, 0.00588296, ..., 0.00518299, 0.00960913,\n        0.00696922],\n       [0.00348909, 0.00444956, 0.00253593, ..., 0.00662928, 0.00311544,\n        0.0066321 ]], dtype=float32)VH_mean(y, x)float320.01978 0.03318 ... 0.003237long_name :VH_meanunits :grid_mapping :crsarray([[0.01978468, 0.03317526, 0.05122132, ..., 0.1278438 , 0.11206219,\n        0.07862695],\n       [0.02983127, 0.05295548, 0.07390182, ..., 0.1544519 , 0.11919038,\n        0.08003806],\n       [0.04661358, 0.07003686, 0.08838344, ..., 0.1496976 , 0.11252023,\n        0.07457688],\n       ...,\n       [0.00155804, 0.00161588, 0.00264122, ..., 0.00329824, 0.00361542,\n        0.00206712],\n       [0.00155148, 0.00217963, 0.00238249, ..., 0.00290533, 0.00317651,\n        0.00251921],\n       [0.00116949, 0.00165877, 0.00139102, ..., 0.00218505, 0.00180075,\n        0.00323711]], dtype=float32)VH_sd(y, x)float320.009419 0.01686 ... 0.00193long_name :VH_sdunits :grid_mapping :crsarray([[0.00941931, 0.01685729, 0.02469561, ..., 0.08591115, 0.06487104,\n        0.03994398],\n       [0.01842893, 0.03180607, 0.03907879, ..., 0.07538059, 0.04117164,\n        0.01954442],\n       [0.03133715, 0.03918692, 0.04057009, ..., 0.05113091, 0.03679396,\n        0.02576428],\n       ...,\n       [0.00141927, 0.00125762, 0.0018592 , ..., 0.00169285, 0.00433611,\n        0.00202067],\n       [0.00116786, 0.00235026, 0.0020393 , ..., 0.00185387, 0.00289492,\n        0.0024544 ],\n       [0.00133604, 0.0017832 , 0.00104416, ..., 0.00226273, 0.00095033,\n        0.00192955]], dtype=float32)VH_q10(y, x)float320.007442 0.01634 ... 0.0003386long_name :VH_q10units :grid_mapping :crsarray([[7.4415309e-03, 1.6343007e-02, 2.4772413e-02, ..., 5.5835072e-02,\n        4.0328607e-02, 3.1856596e-02],\n       [9.0845078e-03, 1.8613091e-02, 3.0874813e-02, ..., 5.9993275e-02,\n        5.3687811e-02, 4.7377892e-02],\n       [1.0651190e-02, 1.9190351e-02, 3.1387392e-02, ..., 8.2894199e-02,\n        7.3106125e-02, 4.1296154e-02],\n       ...,\n       [3.2187731e-05, 4.6482327e-04, 4.1350679e-04, ..., 3.3551207e-04,\n        1.4962933e-04, 7.3057665e-05],\n       [1.5885655e-05, 3.9615774e-05, 1.2235668e-04, ..., 3.0250914e-04,\n        7.5182226e-04, 1.5585287e-04],\n       [1.5885615e-05, 4.6827008e-05, 2.3712554e-04, ..., 1.6382546e-04,\n        2.7883082e-04, 3.3861815e-04]], dtype=float32)VH_q50(y, x)float320.01642 0.02946 ... 0.003168long_name :VH_q50units :grid_mapping :crsarray([[0.01641532, 0.02945656, 0.04615069, ..., 0.09937736, 0.09957799,\n        0.06773166],\n       [0.02633959, 0.05312615, 0.07025553, ..., 0.1448748 , 0.11684266,\n        0.08433955],\n       [0.04026382, 0.06566416, 0.09043089, ..., 0.12782589, 0.10272062,\n        0.07347739],\n       ...,\n       [0.0014457 , 0.0012235 , 0.00271203, ..., 0.00357926, 0.00242533,\n        0.00159997],\n       [0.00157032, 0.00178378, 0.00173404, ..., 0.003209  , 0.00216497,\n        0.00176098],\n       [0.00079205, 0.00095888, 0.00126258, ..., 0.00131919, 0.00185058,\n        0.0031679 ]], dtype=float32)VH_q90(y, x)float320.03604 0.0584 ... 0.006632long_name :VH_q90units :grid_mapping :crsarray([[0.03604072, 0.05839535, 0.08822469, ..., 0.32693267, 0.2599725 ,\n        0.16062291],\n       [0.06696388, 0.10971747, 0.14447221, ..., 0.31547707, 0.19647637,\n        0.10249937],\n       [0.10121775, 0.13220015, 0.14859372, ..., 0.24435118, 0.16842037,\n        0.11494572],\n       ...,\n       [0.00431482, 0.00388348, 0.00630981, ..., 0.00496825, 0.01302709,\n        0.0066534 ],\n       [0.00336156, 0.00744628, 0.00588296, ..., 0.00518299, 0.00960913,\n        0.00696922],\n       [0.00348909, 0.00444956, 0.00253593, ..., 0.00662928, 0.00311544,\n        0.0066321 ]], dtype=float32)VV_min(y, x)float320.03823 0.0581 ... 0.002925long_name :VV_minunits :grid_mapping :crsarray([[3.82329002e-02, 5.81044294e-02, 9.84530300e-02, ...,\n        3.23727041e-01, 3.41812849e-01, 2.77307868e-01],\n       [5.36972843e-02, 1.16618790e-01, 1.23972796e-01, ...,\n        3.44944984e-01, 3.67321134e-01, 2.42612064e-01],\n       [8.68130922e-02, 1.28296837e-01, 2.05426112e-01, ...,\n        2.49633417e-01, 1.32964298e-01, 9.90427136e-02],\n       ...,\n       [4.80704126e-04, 8.40302615e-04, 8.09344347e-04, ...,\n        3.48369405e-03, 5.71828044e-04, 3.22331092e-03],\n       [2.80205859e-03, 2.71117990e-03, 1.38480763e-03, ...,\n        4.25597979e-03, 1.65972699e-04, 4.20906581e-03],\n       [2.71791941e-03, 4.16423287e-03, 2.36033788e-03, ...,\n        3.97732435e-03, 3.30928364e-03, 2.92523764e-03]], dtype=float32)VV_max(y, x)float320.2145 0.352 ... 0.02271 0.04126long_name :VV_maxunits :grid_mapping :crsarray([[0.21453455, 0.35201532, 0.44206226, ..., 0.7745028 , 0.7224579 ,\n        0.58535093],\n       [0.19568719, 0.3464168 , 0.5093019 , ..., 0.9756379 , 0.802884  ,\n        0.6578531 ],\n       [0.25700998, 0.40784222, 0.5935021 , ..., 1.0759932 , 0.7719225 ,\n        0.54108393],\n       ...,\n       [0.01855026, 0.0191953 , 0.02520283, ..., 0.04316558, 0.04142256,\n        0.03178509],\n       [0.01793688, 0.01629219, 0.02929817, ..., 0.02198837, 0.02662462,\n        0.03337206],\n       [0.02471384, 0.02900133, 0.03170837, ..., 0.02347184, 0.02271392,\n        0.04125684]], dtype=float32)VV_mean(y, x)float320.09626 0.1365 ... 0.01103 0.01548long_name :VV_meanunits :grid_mapping :crsarray([[0.09625705, 0.13647287, 0.18718082, ..., 0.5496613 , 0.51692694,\n        0.4102068 ],\n       [0.09961063, 0.1633609 , 0.2353696 , ..., 0.6605072 , 0.56492525,\n        0.41471007],\n       [0.14946699, 0.25105995, 0.3585353 , ..., 0.6142653 , 0.45950067,\n        0.3498352 ],\n       ...,\n       [0.00574651, 0.00542928, 0.00741474, ..., 0.01460892, 0.01497063,\n        0.0144419 ],\n       [0.0078292 , 0.00740209, 0.00846404, ..., 0.01040112, 0.01174255,\n        0.01500158],\n       [0.00813532, 0.00973692, 0.00984371, ..., 0.01012099, 0.01102888,\n        0.01548478]], dtype=float32)VV_sd(y, x)float320.05409 0.09378 ... 0.01286long_name :VV_sdunits :grid_mapping :crsarray([[0.05409231, 0.09378087, 0.11117973, ..., 0.17954879, 0.14689186,\n        0.10922512],\n       [0.0447595 , 0.07611188, 0.11615117, ..., 0.22754364, 0.17713465,\n        0.14189446],\n       [0.06593374, 0.08556262, 0.11674429, ..., 0.25365794, 0.22763157,\n        0.141367  ],\n       ...,\n       [0.00583977, 0.00600125, 0.0081784 , ..., 0.01263162, 0.01270936,\n        0.01037051],\n       [0.00507251, 0.00514195, 0.00953319, ..., 0.00733791, 0.0082898 ,\n        0.01043584],\n       [0.0070393 , 0.00821903, 0.0096691 , ..., 0.00637131, 0.00651885,\n        0.01285903]], dtype=float32)VV_q10(y, x)float320.03823 0.0581 ... 0.002925long_name :VV_q10units :grid_mapping :crsarray([[3.82329002e-02, 5.81044294e-02, 9.84530300e-02, ...,\n        3.23727041e-01, 3.41812849e-01, 2.77307868e-01],\n       [5.36972843e-02, 1.16618790e-01, 1.23972796e-01, ...,\n        3.44944984e-01, 3.67321134e-01, 2.42612064e-01],\n       [8.68130922e-02, 1.28296837e-01, 2.05426112e-01, ...,\n        2.49633417e-01, 1.32964298e-01, 9.90427136e-02],\n       ...,\n       [4.80704126e-04, 8.40302615e-04, 8.09344347e-04, ...,\n        3.48369405e-03, 5.71828044e-04, 3.22331092e-03],\n       [2.80205859e-03, 2.71117990e-03, 1.38480763e-03, ...,\n        4.25597979e-03, 1.65972699e-04, 4.20906581e-03],\n       [2.71791941e-03, 4.16423287e-03, 2.36033788e-03, ...,\n        3.97732435e-03, 3.30928364e-03, 2.92523764e-03]], dtype=float32)VV_q50(y, x)float320.08684 0.1051 ... 0.009489 0.01156long_name :VV_q50units :grid_mapping :crsarray([[0.08684144, 0.10507031, 0.15785766, ..., 0.5336267 , 0.4663303 ,\n        0.41561162],\n       [0.08269721, 0.1327595 , 0.201277  , ..., 0.68295944, 0.514695  ,\n        0.4009375 ],\n       [0.12596725, 0.24027355, 0.34175518, ..., 0.5718068 , 0.4059891 ,\n        0.34461385],\n       ...,\n       [0.00349785, 0.00344953, 0.0037301 , ..., 0.01074719, 0.01242693,\n        0.01454905],\n       [0.00563643, 0.00465483, 0.00359291, ..., 0.00712057, 0.01076274,\n        0.01551143],\n       [0.0061425 , 0.00736981, 0.00503082, ..., 0.00846156, 0.00948913,\n        0.01156005]], dtype=float32)VV_q90(y, x)float320.2145 0.352 ... 0.02271 0.04126long_name :VV_q90units :grid_mapping :crsarray([[0.21453455, 0.35201532, 0.44206226, ..., 0.7745028 , 0.7224579 ,\n        0.58535093],\n       [0.19568719, 0.3464168 , 0.5093019 , ..., 0.9756379 , 0.802884  ,\n        0.6578531 ],\n       [0.25700998, 0.40784222, 0.5935021 , ..., 1.0759932 , 0.7719225 ,\n        0.54108393],\n       ...,\n       [0.01855026, 0.0191953 , 0.02520283, ..., 0.04316558, 0.04142256,\n        0.03178509],\n       [0.01793688, 0.01629219, 0.02929817, ..., 0.02198837, 0.02662462,\n        0.03337206],\n       [0.02471384, 0.02900133, 0.03170837, ..., 0.02347184, 0.02271392,\n        0.04125684]], dtype=float32)Indexes: (2)xPandasIndexPandasIndex(Index([485665.0, 485675.0, 485685.0, 485695.0, 485705.0, 485715.0, 485725.0,\n       485735.0, 485745.0, 485755.0,\n       ...\n       493545.0, 493555.0, 493565.0, 493575.0, 493585.0, 493595.0, 493605.0,\n       493615.0, 493625.0, 493635.0],\n      dtype='float64', name='x', length=798))yPandasIndexPandasIndex(Index([4921875.0, 4921865.0, 4921855.0, 4921845.0, 4921835.0, 4921825.0,\n       4921815.0, 4921805.0, 4921795.0, 4921785.0,\n       ...\n       4916395.0, 4916385.0, 4916375.0, 4916365.0, 4916355.0, 4916345.0,\n       4916335.0, 4916325.0, 4916315.0, 4916305.0],\n      dtype='float64', name='y', length=558))Attributes: (2)Conventions :CF-1.9institution :openEO platform\n\n\n\nds[[\"VH_mean\", \"VV_mean\"]].to_array().plot.imshow(col=\"variable\", vmin=0, vmax=1)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7ff79ddb2550&gt;\n\n\n\n\n\nFurthermore, you can directly can open the saved process directly by visiting the link:\nhttps://editor.openeo.cloud/?wizard=UDP&wizard~process=s1_stats&discover=0"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#use-the-saved-udp-in-the-openeo-platform-editor",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#use-the-saved-udp-in-the-openeo-platform-editor",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "Use the saved UDP in the openEO Platform Editor",
    "text": "Use the saved UDP in the openEO Platform Editor\nAlternatively, we can also switch into the openEO Platform Editor to run the newly created UDP in a graphical web interface. Open https://editor.openeo.cloud?discover=0 in your web browser. It opens the editor, connects you to openEO Platform and asks you to login. Once you’ve logged in, you can explore the offerings of openEO Platform and the data associcated with your user account, including batch jobs and UDPs (“Custom Processes” in the Editor).\nThe easiest way to run your UDP is to use the Wizard: 1. In the menu bar at the top you’ll find the “Wizard”. Click it to open. 2. You’ll see a list of wizards, choose the “Run UDP” wizard. 3. It will show all your UDPs, choose the one you just created. 4. You’ll now be asked to fill the parameters that you defined for your UDP. 5. After providing the parameters, you can click “Next” at the right bottom. 6. It will now open a list that allows to select the processing mode of your UDP: 1. Batch Jobs 2. Synchronous Processing 3. Web Services 4. Don’t execute\nSelect “Synchronous Processing” (for small tasks, recommended for this tutorial) or “Batch Jobs” (for larger tasks). 7. Click “Create” and the Editor will send your processing task to the backend. Once completed the result will be shown or downloaded.\n\n\n\nimage.png\n\n\nThere are two other ways to interact with your UDP: 1. On the left side the UDP is listed in the “Processes” list. You can type your UDP name into the search area to find it. You could then drag and drop it in the Visual Model Builder and use it as part of other workflows. 2. In the lower part of the Editor, there’s a tab with the title “Custom Processes”. Here you can view, update and delete your UDPs."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#publishing-your-service-online",
    "href": "APIs/openEO/openeo-community-examples/python/Sentinel1_Stats/Sentinel1_Stats.html#publishing-your-service-online",
    "title": "Publishing an openEO workflow as a User-Defined-Process (UDP)",
    "section": "Publishing your service online",
    "text": "Publishing your service online\nOnce the UDP defined above is saved within the openEO platform, a user also has the option to add this service to the openEO Marketplace. To register a User Defined Process (UDP), you must have a public URL for your service. You’ll also need to provide the saved process ID, which can be located within the public URL.\nA detailed documentation on the process can be followed here: https://documentation.dataspace.copernicus.eu/Applications/PlazaDetails/ManageService.html#register-and-publish-your-service"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html",
    "title": "Getting started with openEO",
    "section": "",
    "text": "openEO is an open-source initiative that simplifies accessing and processing Earth Observation (EO) data.\nTraditional methods involve complex steps like data discovery, download, and pre-processing, which can be time-consuming and challenging, especially when dealing with multiple datasets. openEO standardises this process, providing a unified interface for accessing and processing diverse EO datasets using familiar programming languages like Python etc. It leverages the concept of datacubes, which streamline the representation and manipulation of EO data, making spatiotemporal analysis more intuitive and efficient.\nopenEO is used in several applications across a range of EO scenarios, ranging from simple to complex workflows. However, this notebook aims to guide beginners in starting with openEO using the Python client. We’ll cover the basics, like installing it, authenticating, finding available EO data, accessing it, and performing basic analysis. We’ll also show you how to include some advanced functions that aren’t yet part of openEO.\nThis notebook is compiled using existing examples of openEO; therefore, we recommend exploring the available sample notebooks for more comprehensive explanations. Additionally, for a thorough understanding of various features, we suggest delving into the Eo-college course titled Cubes&Cloud, which provides step-by-step guidance and theoretical explanations. Our focus here is to help users become acquainted with the general openEO workflow.\nAdditionally, we recommend visiting the official openEO Python client documentation for more detailed information on the available functions and their usage."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#installation",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#installation",
    "title": "Getting started with openEO",
    "section": "Installation",
    "text": "Installation\nThe openEO Python client library is available from PyPI and can be easily installed with a tool like pip. However, it is preinstalled if you are using Jupyter Workspace, provided by the openEO platform, Copernicus Dataspace Ecosystem, Terrascope, or EOX.\n\n!pip install openeo\n\nYou can find additional information on openEO installation in this page.\n\nimport openeo"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#connect-and-authenticate",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#connect-and-authenticate",
    "title": "Getting started with openEO",
    "section": "Connect and Authenticate",
    "text": "Connect and Authenticate\nNext, let’s set up a connection to an openEO back-end using its connection URL. You can find these URLs for different backends on the openEO hub. For this notebook, we’ll use the Copernicus Data Space Ecosystem, a cloud platform supported by the European Commission, ESA, and Copernicus. Make sure you have an account to access and process data using openEO.\nWhen using other backends, you can register using your EduGAIN and social logins as suggested here.\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nYou can find additional information on Authentication in this page."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#data-discovery-and-access",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#data-discovery-and-access",
    "title": "Getting started with openEO",
    "section": "Data discovery and access",
    "text": "Data discovery and access\nThe Earth observation data is organised in so-called collections. You can programmatically list the collections that are available on a back-end and their metadata using methods on the connection object. Furthermore, to visualise available collections and metadata in a user-friendly manner, you can also visit the openEO hub or explore backend specific openEO web editor.\n\nData discovery\n\n# Get all collection ids\nconnection.list_collection_ids()\n\n['SENTINEL3_OLCI_L1B',\n 'SENTINEL3_SLSTR',\n 'SENTINEL_5P_L2',\n 'SENTINEL2_L1C',\n 'SENTINEL2_L2A',\n 'SENTINEL1_GRD',\n 'COPERNICUS_30',\n 'LANDSAT8_L2',\n 'SENTINEL3_SLSTR_L2_LST',\n 'COPERNICUS_PLANT_PHENOLOGY_INDEX',\n 'ESA_WORLDCOVER_10M_2021_V2',\n 'COPERNICUS_VEGETATION_INDICES']\n\n\n\n# Get metadata of a single collection\nconnection.describe_collection(\"SENTINEL2_L2A\")\n\n\n    \n    \n        \n    \n    \n\n\nCongrats!!!, you now just did your first real openEO queries to the openEO back-end using the openEO Python client library.\n\n\nProcess discovery\nTo proceed, it’s important to grasp the available built-in processes of openEO. We’ve already utilized a few of these processes in our earlier queries, like list_collection_ids and describe_collection.\n\n# List all processes\nconnection.list_processes()\n\n\n\n    \n    \n        \n    \n    \n\n\n\nconnection.describe_process(\"aggregate_temporal\")\n\n\n    \n    \n        \n    \n    \n\n\nFind more information on these processes in this page.\n\n\nData access\nA common task in earth observation is to apply a formula to a number of spectral bands in order to compute an ‘index’, such as NDVI, NDWI, EVI, … In this tutorial, we’ll go through a couple of steps to extract EVI (enhanced vegetation index) values and timeseries and discuss some openEO concepts along the way.\nFor calculating the EVI, we need the reflectance of the red, blue and (near) infrared spectral components. These spectral bands are part of the well-known Sentinel-2 data set and are available on the current backend under collection ID SENTINEL2_L2A. so, let’s load this collection.\n\nsentinel2_cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent={\"west\": 5.14, \"south\": 51.17, \"east\": 5.17, \"north\": 51.19},\n    temporal_extent=[\"2021-02-01\", \"2021-04-30\"],\n    bands=[\"B02\", \"B04\", \"B08\"],\n)\n\nHere, we use the load_collection process that loads a collection from the current back-end by its id. The collection is loaded as a data cube that is restricted by the parameters spatial_extent, temporal_extent, bands, and properties.\nAdditionally, by filtering as early as possible (directly in load_collection() in this case), we ensure the back-end only loads the data we are interested in for better performance and to keep the processing costs low. In this example we are filtering the data based on the spatial extent, temporal extent, and bands.\nFurthermore, in this example we implemented bbox for spatial_extent, however, user can import their spatial files and feed into the process as a feature collection.\nFind out more about data discovery, loading and filtering at Finding and loading data."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#data-processing-calculate-evi",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#data-processing-calculate-evi",
    "title": "Getting started with openEO",
    "section": "Data processing: Calculate EVI",
    "text": "Data processing: Calculate EVI\nWhile openEO offers a built-in process for calculating NDVI(ndvi()), this capability hasn’t been implemented yet for EVI or other indices. Instead, openEO provides support for most other indices through an auxiliary subpackage called Awesome Spectral Indices. However, users also have the option to perform band math independently, as demonstrated in this notebook. The choice between the two methods depends on user preference.\nFrom this data cube, we can now select the individual bands using the DataCube.band() method and rescale the digital number values to physical reflectances.\n\nblue = sentinel2_cube.band(\"B02\") * 0.0001\nred = sentinel2_cube.band(\"B04\") * 0.0001\nnir = sentinel2_cube.band(\"B08\") * 0.0001\n\nWe now want to compute the enhanced vegetation index and can do that directly with these band variables:\n\nevi_cube = 2.5 * (nir - red) / (nir + 6.0 * red - 7.5 * blue + 1.0)\n\nPlease note that while this looks like an actual calculation, there is no real data processing going on here. The evi_cube object, at this point, is just an abstract representation of our algorithm under construction. The mathematical operators we used here are syntactic sugar for compactly expressing this part of the algorithm.\nAs an illustration of this, you can also have look at the JSON representation of the algorithm so far, by simply printing them as json: print(evi_cube.to_json())"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#execute-the-process",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#execute-the-process",
    "title": "Getting started with openEO",
    "section": "Execute the process",
    "text": "Execute the process\nDepending on the datacube that is created by our process graph and on the later use case, we can export the results to more suitable formats supported by openEO. You can explore the supported file formats in this page.\nHere, let’s download this as a GeoTIFF file. However, a GeoTIFF does not support a temporal dimension, thus, we first should eliminate it by taking the temporal maximum value for each pixel.\n\nevi_composite = evi_cube.max_time()\n\nFinally, to trigger an actual execution (on the backend), we have to explicitly send the above representation to the backend. You can do this either synchronously(simple download) or using the batch-job-based method. Most of the simple, basic openEO usage examples show synchronous downloading of results. Synchronous downloads are handy for quick experimentation on small data cubes.\nThis only works properly if the processing doesn’t take too long and is focused on a smaller area of interest. However, you have to use batch jobs for the heavier work (larger regions of interest, larger time series, more intensive processing). For more information on using batch-job in openEO, visit here.\n\nevi_composite.download(\"evi_composite.tiff\")\n\nThis download command triggers the actual processing on the back-end: it sends the process graph to the back-end and waits for the result."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#visualise-the-results",
    "href": "APIs/openEO/openeo-community-examples/python/1. GettingStarted/GettingStarted.html#visualise-the-results",
    "title": "Getting started with openEO",
    "section": "Visualise the results",
    "text": "Visualise the results\n\nimport rasterio\nimport matplotlib.pyplot as plt\nimg2 = rasterio.open(\"evi_composite.tiff\").read()\nplt.imshow(img2[0], vmax=1,vmin=-0.1)\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7f981e7388d0&gt;\n\n\n\n\n\nWhen we inspect the downloaded image, we observed a significant impact from cloud-related artefacts on the maximum EVI value. While incorporating a cloud mask could mitigate this issue, our primary objective in this notebook was to introduce the basic task in openEO to the openEO beginners. Thus, we recommend exploring the cloud mask in openEO sample notebooks for more advanced users.\nAs we conclude, we encourage further exploration into additional materials for those interested in:\n\nApplying a cloud mask to enhance the workflow. For guidance, refer to link.\nExploring temporal aggregation of the calculated EVI Link.\nUtilizing batch-job-based execution Link.\nExamples of including self-defined functions as user-defined-functions (UDF) in openEO workflow.\nLearn more on user-defined-process to build your own library of reusable algorithms.\nExamples on more comprehensive EO applications, including techniques such as resampling,reduce_dimension, apply_neighborhood and many more."
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html",
    "href": "APIs/openEO/large_scale_processing.html",
    "title": "Large scale processing",
    "section": "",
    "text": "Processing larger areas, especially globally, presents significant challenges in earth observation. Nonetheless, this platform aims to address these challenges. In this context, we highlight one of the best practices by showcasing the example of processing a croptype map for all 27 countries in the European Union. We encourage users to contact the forum with their issues connected to a specific case.\nThe approach described here relies on local files to track production, offering a low-cost solution. However, this method comes with risks, such as the potential loss of local files. A more robust approach for production-grade projects would typically use a database or STAC catalogue service to monitor processing. However, the setup is quite similar in many aspects.\nThe basic strategy for processing large areas involves splitting them into smaller sections, usually according to a regular tile grid. This division reduces the area size that needs to be processed in one batch job and avoids various limitations. For example, when processing within a specific projection, it is necessary to stay within the bounds of that projection. Additionally, the output file size of a job can become impractical when dealing with large areas. Bottlenecks that do not occur with smaller jobs may also arise in the backend implementation. Also, the cost will be smaller when a smaller job fails or requires reprocessing."
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#relevant-openeo-features",
    "href": "APIs/openEO/large_scale_processing.html#relevant-openeo-features",
    "title": "Large scale processing",
    "section": "Relevant openEO features",
    "text": "Relevant openEO features\nWe want to highlight a few key elements that made us choose openEO for large-scale processing:\n\nPerformance & scalability\nSTAC metadata is automatically generated for you, ensuring that your output is ready for dissemination without requiring you to become a metadata expert.\nWhere relevant FAIR principles are taken into account automatically, such as providing provenance information.\nCloud-optimized file formats are generated by default.\nProcessing can be distributed over multiple backends."
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#preparation",
    "href": "APIs/openEO/large_scale_processing.html#preparation",
    "title": "Large scale processing",
    "section": "Preparation",
    "text": "Preparation\nThe concept involves generating a list of tiles to be produced and persistently storing it, along with all the necessary attributes for each tile. This approach offers a comprehensive visual overview of the processing that will take place.\nHaving job parameters in a file is also beneficial for debugging afterwards. If parameters are determined at runtime, there is no absolute certainty over the value of a specific argument due to potential bugs in the code."
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#prepare-tiling-grid",
    "href": "APIs/openEO/large_scale_processing.html#prepare-tiling-grid",
    "title": "Large scale processing",
    "section": "Prepare tiling grid",
    "text": "Prepare tiling grid\nThe choice of the tiling grid depends on the user’s preferred projection system, which is determined by the area of interest. For Europe, users can opt for the EPSG:3035 projection. In contrast, using different projections per UTM zone may be preferable for global processing.\nThe size of tiles in the grid is also crucial and typically ranges from 20km to 100km. A 100km grid can suffice for relatively light workflows, whereas a 20km grid is more suitable for demanding cases. We opted for 20km tiles in our example because the workflow was particularly demanding. A smaller tile size can also minimize unnecessary processing, especially for irregular target areas like most countries and continents.\nA couple of basic grids can be found here: https://artifactory.vgt.vito.be/ui/repos/tree/General/auxdata-public/grids\nThe images below illustrate the overlap in the UTM grids versus a regular LAEA grid.\n\n\n\nUTM 100km\nLAEA 100km\n\n\n\n\n\n\n\n\n\nA grid can be masked based on the countries the user want to load; the following script shows an example:\nimport geopandas as gpd\neurope = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\neurope = europe[europe.continent==\"Europe\"]\ndf = gpd.read_file(\"https://artifactory.vgt.vito.be/auxdata-public/grids/LAEA-20km.gpkg\",mask=europe)"
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#prepare-job-attributes",
    "href": "APIs/openEO/large_scale_processing.html#prepare-job-attributes",
    "title": "Large scale processing",
    "section": "Prepare job attributes",
    "text": "Prepare job attributes\nIn addition to defining the tiling grid, it is advisable to pre-determine other essential properties required for processing jobs. This approach allows for a comprehensive review of these properties before initiating processing. Examples include fundamental elements like job titles or tile-specific processing parameters, as well as attributes necessary to establish the processing sequence.\nDuring this phase, users may also need to establish the precise tile extents within the coordinate system of their chosen tile grid. Providing accurate coordinates in the correct projection ensures precise alignment of tiles at the pixel level."
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#tuning-your-processing-job",
    "href": "APIs/openEO/large_scale_processing.html#tuning-your-processing-job",
    "title": "Large scale processing",
    "section": "Tuning your processing job",
    "text": "Tuning your processing job\nBefore starting large-scale processing, users should verify that the correct output is generated and that they have sufficient credits and resources to complete their jobs. This can be achieved by running test jobs and analyzing the statistics reported in the metadata to determine average parameters. (Refer to the map production section below for guidance on collecting these parameters in a CSV format.)\nFor example, when processing the EU27 croptype map, consisting of ~11000 20km tiles, we conducted the following upfront calculations:\n\nThe average runtime per job was 30 minutes, indicating it would require approximately 15 days of continuous processing with 15 parallel jobs.\nThe average cost per job was below 100 credits, ensuring we could feasibly process within a budget of 1,100,000 credits.\n\nWe had to optimize batch job settings and streamline the overall workflow to achieve these performance metrics.\nA common challenge in parallelization is managing memory consumption, especially understanding the maximum memory allocation per machine in the chosen backend environment. For example, in a cloud setup where each machine has 16GB RAM and 4 CPUs, efficient usage would involve allocating less than 4GB per worker. This configuration allows four parallel workers to be fitted on a single VM. Conversely, requiring 6GB per worker would accommodate only two workers per VM, leaving approximately 4GB unused.\nIn our case, we utilized the openEO backend hosted on the Copernicus Data Space Ecosystem, built on Geotrellis. Detailed execution options and configurations are documented here."
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#starting-map-production",
    "href": "APIs/openEO/large_scale_processing.html#starting-map-production",
    "title": "Large scale processing",
    "section": "Starting map production",
    "text": "Starting map production\nThe openEO Python client offers a powerful tool for executing multiple processing jobs across various backends.\nIt operates by taking a GeoJSON file corresponding to the tile grid and includes job properties for each tile. Whenever a new job needs to be created, it triggers a function provided by the user. Configuration options include setting up multiple backends and specifying the number of parallel jobs per backend.\nMoreover, the tool handles error management seamlessly, making it more resilient than manually implementing a loop.\nThis script utilizes a CSV file to track jobs, enabling it to resume operations from the exact point of interruption. This capability enhances its tolerance to failures during processing.\n\n\n\nTracking jobs by CSV"
  },
  {
    "objectID": "APIs/openEO/large_scale_processing.html#errors-during-production",
    "href": "APIs/openEO/large_scale_processing.html#errors-during-production",
    "title": "Large scale processing",
    "section": "Errors during production",
    "text": "Errors during production\nIt’s common for some tasks to encounter issues during production, which is generally acceptable if it happens occasionally. If a task fails, it is recommended to check the error logs. A simple retry might solve the problem if there’s no obvious reason for the failure. However, if the issue persists, it’s advisable to contact the forum."
  },
  {
    "objectID": "APIs/openEO/fair.html",
    "href": "APIs/openEO/fair.html",
    "title": "FAIR data & open science",
    "section": "",
    "text": "One of the goals of openEO is to support FAIR principles and open science. These principles are seamlessly integrated into the Copernicus Data Space Ecosystem, making it intuitive to adhere to them. Consequently, using openEO allows users to develop FAIR-compliant open solutions automatically.\nThese are a few examples:\n\nF2 Rich metadata openEO generates rich STAC metadata that includes processing info, complete raster metadata, band information, etc.\nR1.2 Detailed provenance In result metadata derived-from links trace back to all input products.\nR1.3 use of domain-relevant (meta)data standard By default, openEO generates STAC metadata. For the data formats, it supports well-known options such as Cloud-optimized Geotiff, netCDF with CF conventions, GeoParquet, and many more.\n\nFind a concrete example of STAC metadata generated using openEO shown below:\n\nSTAC metadata"
  },
  {
    "objectID": "APIs/openEO/fair.html#fair",
    "href": "APIs/openEO/fair.html#fair",
    "title": "FAIR data & open science",
    "section": "",
    "text": "One of the goals of openEO is to support FAIR principles and open science. These principles are seamlessly integrated into the Copernicus Data Space Ecosystem, making it intuitive to adhere to them. Consequently, using openEO allows users to develop FAIR-compliant open solutions automatically.\nThese are a few examples:\n\nF2 Rich metadata openEO generates rich STAC metadata that includes processing info, complete raster metadata, band information, etc.\nR1.2 Detailed provenance In result metadata derived-from links trace back to all input products.\nR1.3 use of domain-relevant (meta)data standard By default, openEO generates STAC metadata. For the data formats, it supports well-known options such as Cloud-optimized Geotiff, netCDF with CF conventions, GeoParquet, and many more.\n\nFind a concrete example of STAC metadata generated using openEO shown below:\n\nSTAC metadata"
  },
  {
    "objectID": "APIs/openEO/fair.html#open-science",
    "href": "APIs/openEO/fair.html#open-science",
    "title": "FAIR data & open science",
    "section": "Open Science",
    "text": "Open Science\nIn the context of open science, a significant benefit of using openEO is that it allows workflows to be saved in a standardized format called openEO “process graphs”. This enables scientists to share algorithms easily without exchanging complex code bases. OpenEO code or process graphs are also easier to understand since the backend manages much of the boilerplate logic.\nThis also impacts the replication of work: the same process graph can be executed on different areas or time periods. This capability allows researchers to determine whether an algorithm is broadly applicable or only effective in a specific environment.\nBelow is a straightforward process graph illustrating the extraction of Sentinel-2 data. While this example is simple, the underlying steps required to generate an analysis-ready datacube from raw Sentinel-2 L2A products are considerably complex. As a result, the process graph will be much easier to understand than the equivalent openEO code."
  },
  {
    "objectID": "APIs/openEO/openeo_processing.html",
    "href": "APIs/openEO/openeo_processing.html",
    "title": "openEO process implementation details",
    "section": "",
    "text": "This page details several relevant aspects of the implementation of specific openEO processes that are relevant for reproducibility.\nThe openEO deployment of CDSE public service is 100% open source and has a specific goal of supporting open science.\n\n\nThe sar_backscatter process allows on-the-fly computation of Sentinel-1 sigma0 backscatter. It is based on the open-source Orfeo Toolbox and sufficiently fast to provide a cost-effective option for large-scale processing.\n\n\nWhen thermal noise is removed, values are set to 0, but Orfeo also uses 0 for nodata.\n\n\n\n\nThe load_collection implementation is quite advanced and tries to optimise data loading from the data space archive.\nWhen no resampling options are specified in the process graph, we try to respect the original pixel grid of the data. This is done to allow processing that is equally exact as file-based processing.\n\n\nProperty filtering depends on the data space catalogue, which currently has limited STAC capabilities. This sometimes prevents the usage of standardised STAC property names, which affects the portability of other process graphs."
  },
  {
    "objectID": "APIs/openEO/openeo_processing.html#sar-backscatter",
    "href": "APIs/openEO/openeo_processing.html#sar-backscatter",
    "title": "openEO process implementation details",
    "section": "",
    "text": "The sar_backscatter process allows on-the-fly computation of Sentinel-1 sigma0 backscatter. It is based on the open-source Orfeo Toolbox and sufficiently fast to provide a cost-effective option for large-scale processing.\n\n\nWhen thermal noise is removed, values are set to 0, but Orfeo also uses 0 for nodata."
  },
  {
    "objectID": "APIs/openEO/openeo_processing.html#load_collection",
    "href": "APIs/openEO/openeo_processing.html#load_collection",
    "title": "openEO process implementation details",
    "section": "",
    "text": "The load_collection implementation is quite advanced and tries to optimise data loading from the data space archive.\nWhen no resampling options are specified in the process graph, we try to respect the original pixel grid of the data. This is done to allow processing that is equally exact as file-based processing.\n\n\nProperty filtering depends on the data space catalogue, which currently has limited STAC capabilities. This sometimes prevents the usage of standardised STAC property names, which affects the portability of other process graphs."
  },
  {
    "objectID": "APIs/openEO/authentication/client_credentials.html",
    "href": "APIs/openEO/authentication/client_credentials.html",
    "title": "Authentication With Client Credentials in openEO",
    "section": "",
    "text": "OpenID Connect provides the so-called “client credentials flow”, which is a non-interactive flow, based on a static client ID and a client secret, making it suitable for machine-to-machine authentication. Often, this is also referred to as “service account” authentication.\nThe openEO Python client library has built-in support for service accounts and the client credentials flow, with the authenticate_oidc_client_credentials() method as follows:"
  },
  {
    "objectID": "APIs/openEO/authentication/client_credentials.html#caveats-and-considerations",
    "href": "APIs/openEO/authentication/client_credentials.html#caveats-and-considerations",
    "title": "Authentication With Client Credentials in openEO",
    "section": "Caveats And Considerations",
    "text": "Caveats And Considerations\n\nTreat the client secret securely, like a password. Take extra care to not leak it accidentally. For example, given the simplicity of the authenticate_oidc_client_credentials() example snippet above, it might be tempting to hard-code the client secret in scripts or notebooks, potentially leading to its permanent storage in version control repositories.\nInstead, read the client secret from a secure location (e.g. a private file outside the reach of the version control repositories), or leverage environment variables (e.g as directly supported by the openEO Python client library).\nThe client credentials only identify an OAuth client or service account, not a personal user account.\n\nThis means that openEO resources such as openEO batch jobs, their results, UDP’s, etc from one identity are not available to the other. For example, batch jobs originally created with a service account cannot be listed when using a personal account.\nLikewise, the balances of processing credits are separate. However, it is possible to link the balance of the service account to a personal account. To enable this, contact support and provide the client ID and user ID.\n\nThe client credentials flow is not supported on the Copernicus Data Space Ecosystem openEO web editor. As mentioned above, this practically means that it can not be used to track the progress and status of batch jobs created with a service account. However, it is still possible to approximate the batch job overview of the web editor with a Jupyter Notebook using the openEO Python client library."
  },
  {
    "objectID": "APIs/openEO/authentication/client_credentials.html#how-to-create-service-accounts-and-obtain-client-credentials",
    "href": "APIs/openEO/authentication/client_credentials.html#how-to-create-service-accounts-and-obtain-client-credentials",
    "title": "Authentication With Client Credentials in openEO",
    "section": "How To Create Service Accounts And Obtain Client Credentials",
    "text": "How To Create Service Accounts And Obtain Client Credentials\nThere are several options to create or request a service account with client credentials. Which one to choose depends on the particular use case. For simple, personal use cases and initial testing, the self-service feature of the Sentinel Hub Dashboard is the easiest option. For larger use cases or projects that should not be tied to a single developer, it is recommended to request a client through the CDSE Account management service.\n\nObtain Client Credentials With The Sentinel Hub Dashboard\nThe Sentinel Hub service in the Copernicus Data Space Ecosystem has a dashboard web app, which includes a self-service feature to register personal OAuth clients. Find more detailed instructions in the documentation on Sentinel Hub Authentication.\nThe client ID and client secret obtained with this dashboard can also be used for the client credentials flow with the openEO service of Copernicus Data Space Ecosystem.\n\n\nRequest A Service Account From The CDSE Account Management Service\nThe account management service in the Copernicus Data Space Ecosystem supports the creation of OIDC/OAuth clients (e.g. service accounts based on the client credentials grant) which can be requested through the help center.\nMake sure to include the following information in the description of the request:\n\nA short, but descriptive project name. It can be a couple of words or a project acronym, but it should be descriptive enough to be unique within the Copernicus Data Space Ecosystem.\nA short description of the project, preferably with some pointers to the project website, contact info, involved parties, …"
  },
  {
    "objectID": "APIs/openEO/authentication/client_credentials.html#obtain-the-oidc-identifier-of-a-service-account",
    "href": "APIs/openEO/authentication/client_credentials.html#obtain-the-oidc-identifier-of-a-service-account",
    "title": "Authentication With Client Credentials in openEO",
    "section": "Obtain The OIDC Identifier Of A Service Account",
    "text": "Obtain The OIDC Identifier Of A Service Account\nThe setup of the service account with client credentials, as discussed above, is a matter between the user creating (or requesting) the service account and the CDSE Account Management Service. Other services like an openEO backend have no direct involvement in this process nor insights into how service accounts are linked to personal accounts or higher level projects.\nWhen you are in the process of linking a service account to another entity, for example for openEO credit accounting purposes, you will be required to provide the “OIDC subject identifier” (the OIDC “sub” claim) associated with the service account. Note that this is different from the client ID of the service account.\nThis “sub” claim can be obtained for example with the following Python code snippet, using the openEO Python client library:\nimport getpass\nimport openeo\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\n\nclient_id = getpass.getpass(\"Client ID: \")\nclient_secret = getpass.getpass(\"Client secret: \")\nconnection.authenticate_oidc_client_credentials(\n    client_id=client_id,\n    client_secret=client_secret,\n)\n\nclient_sub_id = con.describe_account()[\"user_id\"]\nprint(client_sub_id)\n\n\n\n\n\n\nImportant\n\n\n\nTo avoid the bad practice of hardcoding the client secret in scripts or notebooks, this snippet uses the getpass module from Python’s standard library to interactively and securely prompt the user for the client ID and secret, in a way that is compatible with both standard CLI usage and Jupyter notebook contexts."
  },
  {
    "objectID": "APIs/Subscriptions.html",
    "href": "APIs/Subscriptions.html",
    "title": "Subscriptions",
    "section": "",
    "text": "This section provides the detailed description of PUSH and PULL subscriptions within the Copernicus Data Space Ecosystem Catalogue, its workflows and exemplary requests.\nSubscription services allow users to receive notifications in an automated way about the Event that has taken place in the Copernicus Data Space Ecosystem Catalogue. Currently, they inform users about newly added products to the Data Space Catalogue according to the set of filter parameters supplied within the subscription request.\nThere two types of Subscriptions available for users:\nUsers are encouraged to explore examples of processing scripts for Subscriptions which can be found at the following links:"
  },
  {
    "objectID": "APIs/Subscriptions.html#pull-subscriptions",
    "href": "APIs/Subscriptions.html#pull-subscriptions",
    "title": "Subscriptions",
    "section": "PULL Subscriptions",
    "text": "PULL Subscriptions\nFor PULL notifications, the nominal scenario can be described as follows:\n\nCreation of the subscription\n\nThe client submits a subscription create request to Subscription Service. With the creation request, users can specify the filtering parameters for the products within the subscription, status of the subscription, stage order, priority and subscription event to be monitored. All of the parameters are optional. If not provided by the client, the default values will be assigned. The request is then processed by the Subscription Service, and a response is returned which includes a unique identifier assigned to the subscription (subscription’s Id), subscription status (initially set to running), subscription request submission date, filtering parameters, stage order, priority of the subscription and subscription event to be monitored.\n\nProduct Notification\n\nAfter a new product is added to the Data Space Catalogue, the Subscription Service sends the notification to the client’s queue. Therefore users should read and acknowledge the notifications on the queue regularly. In case the maximum size of the queue is exceeded, the oldest products notifications will be automatically removed with no possibility to be retrieved. If the subscription is paused, the new products’ availability notifications will not be sent to the queue.\n\nPULL Subscription Entity Description\nBelow please find the description of the PULL Subscription Entity.\n\n\n\n    \n\n\n\n\n\n\n\n\n\nSubscription Properties\nType\nDescription\nExample\n\n\n\n\nId\nGuid\nIt is a universally unique identifier (UUID). The Id is a local identifier for the Subscription instance within the Catalogue, assigned upon Subscription creation.\n991c4730-cf6f-432a-9f6c-47be0230ff45\n\n\nStatus\nSubscription Status enumeration\nThe allowed values of the subscription status are:\nrunning\npaused\ncancelled\n\nThe default value set to 'Status', if not provided by user, is 'running'.\nrunning\n\n\nSubscriptionEvent\nSubscription Event enumeration\nThe subscription event to be monitored and for which notification is provided:\ncreated\nThe default value set to \"SubscriptionEvent\", if not provided by user, is \"created\".\n\nFor \"SubscriptionEvent\" = \"created\" the notifications about newly added products to the Data Space Catalogue will be sent to the user's endpoint.\ncreated\n\n\nFilterParam\nString\nThe filter parameters of the Subscription (refers to the $filter= parameter of any Products? query). The same filtering parameters as described for [OData Data Sapce Catalogue](https://documentation.dataspace.copernicus.eu/APIs/OData.html#filter-option) are available.\nCollection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any (att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_SLC__1S')\n\n\nSubmissionDate\nDateTimeOffset\nDate and time at which the Subscription was received by the Catalogue. Time is in UTC in the format YYYY-MMDDThh:mm:ss.sssZ\n2024-01-17T09:13:04.654Z\n\n\nLastNotificationDate\nDateTimeOffset\nDate and time corresponding to the last time the Subscription Ack endpoint was queried by the client. Time is in UTC in the format YYYY-MMDDThh:mm:ss.sssZ\n2024-01-17T09:50:10.654Z\n\n\nStageOrder\nBoolean\nAutomatically orders the staging of products fulfilling the subscription.\n\nOnly used if SubscriptionEvent = created \n\nCurrently, the order of staging products is not feasible as all new products in Data Space Catalogue have status set to 'Online' and can be accessed immediately without setting an order.\ntrue\n\n\nPriority\nInt64\nPriority of the created orders resulting from the subscription. Currently automatically fixed to '1' without the possibility to change the value. Within further development of the Subscription Service it is possible to enable Priority function which will determine the priority of the user's orders.\n1\n\n\nAckId\nString\nAcknowledge Id assigned to each product notification. Required for acknowledging the notification messages from the client's queue.\nMTcxMDc1NjcwNjUzMi0wOjk5MWM0NzMwLWNmNmYtNDMyYS05ZjZjLTQ3YmUwMjMwZmY0NQ==\n\n\nAckMessagesNum\nInt64\nThe number of the notifications that were acknowledged.\n20\n\n\nCurrentQueueLength\nInt64\nCurrent length of the individual user's queue.\n2115\n\n\nMaxQueueLength\nInt64\nThe maximum number of notifications on the individual user's queue.\n100000\n\n\n\n\n\n\n\n\nCreate Subscription\nTo create PULL Subscription users need to submit a subscription create request to the Subscription endpoint.\nWhile creating a subscription, users can specify the Event they would like to be notified. For now, the following Event can be chosen:\n\nCreated\n\nWithin the Subscription create request, users can also provide the filtering parameters (e.g. productType, collection, geofootprint). The Subscription Service will then provide the notifications about newly added products to the Copernicus Data Space Ecosystem Catalogue according to the set of filter parameters supplied within the subscription request. If FilterParam is not provided within the subscription request body, it will be automatically set to empty and then the notifications about all newly added products will be generated.\nSubscriptions enables the same filtering parameters as described for OData Data Sapce Catalogue. All filters should be provided in FilterParam field.\nTo create PULL Subscription a request to the Data Space service should be submitted:\n\nHTTP requestResponse example\n\n\nPOST \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions\n \n{\n    \"StageOrder\": true,\n    \"FilterParam\": \"Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_SLC__1S')\",\n    \"Priority\": 1,\n    \"Status\": \"running\",\n    \"SubscriptionEvent\": [\n        \"created\"\n    ]\n}\n\n\n201 Created\n \n{\n    \"Id\": \"991c4730-cf6f-432a-9f6c-47be0230ff45\",\n    \"FilterParam\": \"Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_SLC__1S')\",\n    \"StageOrder\": true,\n    \"Priority\": 1,\n    \"Status\": \"running\",\n    \"SubscriptionEvent\": [\n        \"created\"\n    ],\n    \"SubmissionDate\": \"2024-03-13T09:39:49.404Z\",\n    \"@odata.context\": \"$metadata#OData.CSC.Subscription\"\n}\n\n\n\n\n\nRead Subscription\nTo read the subscription notifications from the client’s queue, the request to subscription Read endpoint should be submitted:\n\nHTTP request\n\n\nGET \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(subscription_id)/Read\n\n\n\nWhile requesting subscription Read endpoint, by default top one notifications will be read. Users can specify to read maxiumum 20 top notifications by adding $top parameter to their request.\n\nExample request\n\n\nGET \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(991c4730-cf6f-432a-9f6c-47be0230ff45)/Read?$top=2\n\n\n\nResponse example\n\nIf Subscription is Active or Paused\n\nThe whole response from the Subscripton Read endpoint is kept on the indivdual user’s queue for three days. After three days, only the endpoint response without value and ProductName is available (as provided below).\n\nResponse exampleResponse example after 3 days\n\n\n200 OK\n \n[\n    {\n        \"@odata.context\": \"$metadata#Notification/$entity\",\n        \"SubscriptionEvent\": \"created\",\n        \"ProductId\": \"d2ff986b-9454-43d6-95e0-1a0ae27019d7\",\n        \"ProductName\": \"S1A_IW_SLC__1SDV_20240313T054626_20240313T054653_052959_066921_3759.SAFE\",\n        \"SubscriptionId\": \"991c4730-cf6f-432a-9f6c-47be0230ff45\",\n        \"NotificationDate\": \"2024-03-13T13:39:59.000Z\",\n        \"AckId\": \"MTcxMDc1NjcwNjUzMi0wOjk5MWM0NzMwLWNmNmYtNDMyYS05ZjZjLTQ3YmUwMjMwZmY0NQ==\",\n        \"value\": {\n            \"@odata.context\": \"$metadata#Products(Assets())(Attributes())(Locations())/$entity\",\n            \"@odata.mediaContentType\": \"application/octet-stream\",\n            \"Id\": \"d2ff986b-9454-43d6-95e0-1a0ae27019d7\",\n            \"Name\": \"S1A_IW_SLC__1SDV_20240313T054626_20240313T054653_052959_066921_3759.SAFE\",\n            \"ContentType\": \"application/octet-stream\",\n            \"ContentLength\": 8217756705,\n            \"OriginDate\": \"2024-03-13T06:47:17.308Z\",\n            \"PublicationDate\": \"2024-03-13T06:56:59.293Z\",\n            \"ModificationDate\": \"2024-03-13T10:47:57.429Z\",\n            \"Online\": true,\n            \"EvictionDate\": \"\",\n            \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_SLC__1S/2024/03/13/S1A_IW_SLC__1SDV_20240313T054626_20240313T054653_052959_066921_3759.SAFE\",\n            \"Checksum\": [\n                {\n                    \"Value\": \"a7443f2e7186a5c114c951c9a24979ea\",\n                    \"Algorithm\": \"MD5\",\n                    \"ChecksumDate\": \"2024-03-13T10:47:45.298465Z\"\n                },\n                {\n                    \"Value\": \"62768a5a51dd61ad4f3f92d3c9926b66fafb1ee945f0e30b0e6ecd0c1590822d\",\n                    \"Algorithm\": \"BLAKE3\",\n                    \"ChecksumDate\": \"2024-03-13T10:47:57.259964Z\"\n                }\n            ],\n            \"ContentDate\": {\n                \"Start\": \"2024-03-13T05:46:26.715Z\",\n                \"End\": \"2024-03-13T05:46:53.707Z\"\n            },\n            \"Footprint\": \"geography'SRID=4326;POLYGON ((13.873887 65.505287, 14.91091 67.090202, 8.940076 67.587387, 8.268893 65.983887, 13.873887 65.505287))'\",\n            \"GeoFootprint\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [...\n                ]\n            },\n            \"Attributes\": [...\n            ],\n            \"Assets\": [\n                {\n                    \"Type\": \"QUICKLOOK\",\n                    \"Id\": \"b43f60cb-3869-4c1d-b1e5-336e0d057f43\",\n                    \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(b43f60cb-3869-4c1d-b1e5-336e0d057f43)/$value\",\n                    \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_SLC__1S/2024/03/13/S1A_IW_SLC__1SDV_20240313T054626_20240313T054653_052959_066921_3759.SAFE\"\n                }\n            ],\n            \"Locations\": [...\n            ]\n        }\n    },\n    {\n        \"@odata.context\": \"$metadata#Notification/$entity\",\n        \"SubscriptionEvent\": \"created\",\n        \"ProductId\": \"33ad5694-9208-4001-ae4f-6d7cd0579ce0\",\n        \"ProductName\": \"S1A_IW_SLC__1SDV_20240313T092716_20240313T092732_052961_066935_89F2.SAFEE\",\n        \"SubscriptionId\": \"991c4730-cf6f-432a-9f6c-47be0230ff45\",\n        \"NotificationDate\": \"2024-03-13T13:39:59.000Z\",\n        \"AckId\": \"MTcxMDc1NjcwNjY2NC0wOjk5MWM0NzMwLWNmNmYtNDMyYS05ZjZjLTQ3YmUwMjMwZmY0NQ==\",\n        \"value\": {\n            \"@odata.context\": \"$metadata#Products(Assets())(Attributes())(Locations())/$entity\"\",\n            \"@odata.mediaContentType\": \"application/octet-stream\",\n            \"Id\": \"33ad5694-9208-4001-ae4f-6d7cd0579ce0\",\n            \"Name\": \"S1A_IW_SLC__1SDV_20240313T092716_20240313T092732_052961_066935_89F2.SAFE\",\n            \"ContentType\": \"application/octet-stream\",\n            \"ContentLength\": 4355824564,\n            \"OriginDate\": \"2024-03-13T11:15:44.930Z\",\n            \"PublicationDate\": \"2024-03-13T11:23:27.577Z\",\n            \"ModificationDate\": \"2024-03-13T11:24:14.138Z\",\n            \"Online\": true,\n            \"EvictionDate\": \"\",\n            \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_SLC__1S/2024/03/13/S1A_IW_SLC__1SDV_20240313T092716_20240313T092732_052961_066935_89F2.SAFE\",\n            \"Checksum\": [\n                {\n                    \"Value\": \"2aa3dd4d604166d9937d1e2ea33d1a84\",\n                    \"Algorithm\": \"MD5\",\n                    \"ChecksumDate\": \"2024-03-13T11:24:04.717777Z\"\n                },\n                {\n                    \"Value\": \"b60f11d44ee700736ceb16bdacf25af89c878b6d2b370ca6e801743fb127e1dc\",\n                    \"Algorithm\": \"BLAKE3\",\n                    \"ChecksumDate\": \"2024-03-13T11:24:13.985182Z\"\n                }\n            ],\n            \"ContentDate\": {\n                \"Start\": \"2024-03-13T09:27:16.441Z\",\n                \"End\": \"2024-03-13T09:27:32.471Z\"\n            },\n            \"Footprint\": \"geography'SRID=4326;POLYGON ((-57.645779 -18.073198, -57.413177 -17.107573, -59.767452 -16.559637, -60.01339 -17.520658, -57.645779 -18.073198))'\",\n            \"GeoFootprint\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [...\n                ]\n            },\n            \"Attributes\": [...\n            ],\n            \"Assets\": [\n                {\n                    \"Type\": \"QUICKLOOK\",\n                    \"Id\": \"f4cfe4c9-5b56-47c2-87ce-05b090c7a4ba\",\n                    \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(f4cfe4c9-5b56-47c2-87ce-05b090c7a4ba)/$value\",\n                    \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_SLC__1S/2024/03/13/S1A_IW_SLC__1SDV_20240313T092716_20240313T092732_052961_066935_89F2.SAFE\"\n                }\n            ],\n            \"Locations\": [...\n            ]\n        }\n    }\n]\n\n\n200 OK\n \n[\n    {\n        \"@odata.context\": \"$metadata#Notification/$entity\",\n        \"SubscriptionEvent\": \"created\",\n        \"ProductId\": \"d2ff986b-9454-43d6-95e0-1a0ae27019d7\",\n        \"SubscriptionId\": \"991c4730-cf6f-432a-9f6c-47be0230ff45\",\n        \"NotificationDate\": \"2024-03-13T13:39:59.000Z\",\n        \"AckId\": \"MTcxMDc1NjcwNjUzMi0wOjk5MWM0NzMwLWNmNmYtNDMyYS05ZjZjLTQ3YmUwMjMwZmY0NQ==\"\n    },\n    {\n        \"@odata.context\": \"$metadata#Notification/$entity\",\n        \"SubscriptionEvent\": \"created\",\n        \"ProductId\": \"33ad5694-9208-4001-ae4f-6d7cd0579ce0\",\n        \"SubscriptionId\": \"991c4730-cf6f-432a-9f6c-47be0230ff45\",\n        \"NotificationDate\": \"2024-03-13T13:39:59.000Z\",\n        \"AckId\": \"MTcxMDc1NjcwNjY2NC0wOjk5MWM0NzMwLWNmNmYtNDMyYS05ZjZjLTQ3YmUwMjMwZmY0NQ==\"\n    }\n]\n\n\n\n\nIf Subscription is Cancelled\n\n\nResponse example\n\n\n404 Not Found\n \n{\n    \"detail\": {\n        \"message\": \"Subscription with id 991c4730-cf6f-432a-9f6c-47be0230ff45 not found.\",\n        \"request_id\": \"aed89ead-b495-4f76-b6e9-51870973a44e\"\n    }\n}\n\n\n\n\n\nAck Subscription\nAll notifications, after reading by the client, should be acknowledged to not exceed the limit of the queue. For acknowledging the notifications, the assigned parametr is added to the notification - ‘AckId’. Each product notification has its own AckId token, which should be used to acknowledge the notifications messages on the client’s queue. Using the ‘AckId’ token for a specific notification means acknowledging receipt of the notification for which the ‘AckId’ was assigned, along with all preceding messages. For example if user use READ endpoint with top parameter set to 10 and then use the AckID of 10th message in Ack endpoint then all 10 messages will be acknowledged and removed from subscription queue.\nTo ack the notifications on the queue:\n\nHTTP Request\n\n\nPOST \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(991c4730-cf6f-432a-9f6c-47be0230ff45)/Ack?$ackid=token\n\n\n\n\nRequest example\n\n\nPOST \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(991c4730-cf6f-432a-9f6c-47be0230ff45)/Ack?$ackid=MTcxMDc1NjcwNjY2NC0wOjk5MWM0NzMwLWNmNmYtNDMyYS05ZjZjLTQ3YmUwMjMwZmY0NQ==\n\n\n\nResponse example\n\nIf Subscription is Active or Paused\n\n\nResponse example\n\n\n200 OK\n \n{\n    \"@odata.context\": \"$metadata#Notification/$entity\",\n    \"AckMessagesNum\": 2,\n    \"CurrentQueueLength\": 2115,\n    \"MaxQueueLength\": 100000\n}\n\n\n\n\nIf Subscription is Cancelled\n\n\nResponse example\n\n\n404 Not Found\n \n{\n    \"detail\": {\n        \"message\": \"Subscription with id 991c4730-cf6f-432a-9f6c-47be0230ff4 not found.\",\n        \"request_id\": \"ae19864a-5bda-4893-9f02-a62eba95fd7a\"\n    }\n}"
  },
  {
    "objectID": "APIs/Subscriptions.html#push-subscriptions",
    "href": "APIs/Subscriptions.html#push-subscriptions",
    "title": "Subscriptions",
    "section": "PUSH Subscriptions",
    "text": "PUSH Subscriptions\nFor PUSH notifications, the nominal scenario can be described as follows:\n\nCreation of the subscription\n\nThe client submits a subscription create request to Subscription Service. The user’s request needs to contain an endpoint URL to which the notifications will be sent and endpoint’s credentials (if client’s endpoint requires authentication). The user’s endpoint credentials are stored safely in the dedicated vault. The request is processed by the Subscription Service, and a response is returned which includes a unique identifier assigned to the subscription (subscription’s Id), subscription status (initially set to running), subscription request submission date, filtering parameters, stage order, priority of the subscription, subscription event to be monitored and client’s endpoint URL. If client’ notification endpoint is not submitted, then PULL subscription will be created.\n\nProduct Notification\n\nAfter a new product is added to the Copernicus Data Space Ecosystem Catalogue, the Subscription Service sends the notification to the client’s endpoint. In case of user’s endpoint unavailability, the PUSH notification is retried three times and then it fails. The notifications are sent to user’s endpoint until the subscription is paused or deleted.\n\nPUSH Subscription Entity Description\nBelow please find the description of the Subscription Entity.\n\n\n\n    \n\n\n\n\n\n\n\n\n\nSubscription Properties\nType\nDescription\nExample\n\n\n\n\nId\nGuid\nIt is a universally unique identifier (UUID). The Id is a local identifier for the Subscription instance within the Catalogue, assigned upon Subscription creation.\n9249dde5-4838-4a34-8925-bac8c1d53f09\n\n\nStatus\nSubscription Status enumeration\nThe allowed values of the subscription status are:\nrunning\npaused\ncancelled\n\nThe default value set to 'Status', if not provided by user, is 'running'.\nrunning\n\n\nSubscriptionEvent\nSubscription Event enumeration\nThe subscription event to be monitored and for which notification is provided:\ncreated\nThe default value set to \"SubscriptionEvent\", if not provided by user, is \"created\".\n\nFor \"SubscriptionEvent\" = \"created\" the notifications about newly added products to the Data Space Catalogue will be sent to the user's endpoint.\ncreated\n\n\nFilterParam\nString\nThe filter parameters of the Subscription (refers to the $filter= parameter of any Products? query). The same filtering parameters as described for [OData Data Sapce Catalogue](https://documentation.dataspace.copernicus.eu/APIs/OData.html#filter-option) are available.\nCollection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A')\n\n\nSubmissionDate\nDateTimeOffset\nDate and time at which the Subscription was received by the Catalogue. Time is in UTC in the format YYYY-MMDDThh:mm:ss.sssZ\n2024-01-17T09:13:04.654Z\n\n\nLastNotificationDate\nDateTimeOffset\nDate and time corresponding to the last time the Subscription Service sent a Notification to the user's endpoint. Time is in UTC in the format YYYY-MMDDThh:mm:ss.sssZ\n2024-01-17T09:50:10.654Z\n\n\nStageOrder\nBoolean\nAutomatically orders the staging of products fulfilling the subscription.\n\nOnly used if SubscriptionEvent = created \n\nCurrently, the order of staging products is not feasible as all new products in Data Space Catalogue have status set to 'Online' and can be accessed immediately without setting an order.\ntrue\n\n\nPriority\nInt64\nPriority of the created orders resulting from the subscription. Currently automatically fixed to '1' without the possibility to change the value. Within further development of the Subscription Service it is possible to enable Priority function which will determine the priority of the user's orders.\n1\n\n\nNotificationEndpoint\nString\nUser's Endpoint URI used by the Data Space Catalogue for subscription notifications.\nAny URI, e.g. https://userservice/notification\n\n\nNotificationEpUsername\nString\nThe username associated with the Endpoint URI. It is mandatory if NotificationEndpoint requires authentication.\nserviceusername\n\n\nNotificationEpPassword\nString\nThe password associated with the Endpoint URI. It is mandatory if NotificationEndpoint requires authentication.\n***********\n\n\n\n\n\n\n\n\nCreate Subscription\nTo create PUSH Subscription users should specify the notification endpoint to which PUSH notifications from the Subscription Service will be sent.\nWhile creating a subscription, users can specify the Event they would like to be notified. For now, the following Event can be chosen:\n\nCreated\n\nWithin the Subscription create request, users can also provide the filtering parameters (e.g. productType, collection, geofootprint). The Subscription Service will then send the notification to the user about newly added products to the Copernicus Data Space Ecosystem Catalogue according to the set of filter parameters supplied within the subscription request. If FilterParam is not provided within the subscription request body, it will be automatically set to empty and then the notifications about all newly added products will be sent to user’s endpoint.\nSubscriptions enables the same filtering parameters as described for OData Data Sapce Catalogue. All filters should be provided in FilterParam field.\nTo create a PUSH Subscription a request to the Subscription Service service should be submitted:\n\nHTTP requestResponse example\n\n\nPOST \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions\n \n{\n  \"StageOrder\": true,\n  \"FilterParam\": \"Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A')\",\n  \"Priority\": 1,\n  \"NotificationEndpoint\": \"https://userservice/notification\",\n  \"NotificationEpUsername\": \"serviceusername\",\n  \"NotificationEpPassword\": \"********\",\n  \"Status\": \"running\",\n  \"SubscriptionEvent\": [\n    \"created\"\n  ]\n}\n\n\n201 Created\n \n{\n    \"Id\": \"7ca682c3-7b21-4e9b-952e-874798101340\",\n    \"FilterParam\": \"Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A')\",\n    \"StageOrder\": true,\n    \"Priority\": 1,\n    \"Status\": \"running\",\n    \"NotificationEndpoint\": \"https://userservice/notification\",\n    \"SubscriptionEvent\": [\n        \"created\"\n    ],\n    \"SubmissionDate\": \"2024-01-18T09:47:34.2672\",\n    \"@odata.context\": \"$metadata#OData.CSC.Subscription\"\n}\n\n\n\n\n\nNotification sender\nAfter creating the subscription, the notifications about new products entering the Data Space Catalogue according to the set of subscription’s filter parameters should be automatically sent to user’s endpoint:\n\nNotification example\n\n\n{\n    \"@odata.context\": \"$metadata#Notification/$entity\",\n    \"SubscriptionEvent\": \"created\",\n    \"ProductId\": \"8d654e59-d7b6-4a53-b086-c9de764e506b\",\n    \"ProductName\": \"S2A_MSIL2A_20240129T062121_N0510_R034_T44VMN_20240129T093752.SAFE\",\n    \"SubscriptionId\": \"c06f2e68-fb9e-4e22-ac57-49566e8621fb\",\n    \"NotificationDate\": \"2024-01-29T10:59:08.698Z\",\n    \"value\": {\n        \"@odata.context\": \"$metadata#Products(Assets())(Attributes())(Locations())/$entity\",\n        \"@odata.mediaContentType\": \"application/octet-stream\",\n        \"Id\": \"8d654e59-d7b6-4a53-b086-c9de764e506b\",\n        \"Name\": \"S2A_MSIL2A_20240129T062121_N0510_R034_T44VMN_20240129T093752.SAFE\",\n        \"ContentType\": \"application/octet-stream\",\n        \"ContentLength\": 769373106,\n        \"OriginDate\": \"2024-01-29T10:42:05.000Z\",\n        \"Checksum\": [\n            {\n                \"Value\": \"902d7e7e85b7392618a957f3e9f94168\",\n                \"Algorithm\": \"MD5\",\n                \"ChecksumDate\": \"2024-01-29T10:59:06.907635Z\"\n            },\n            {\n                \"Value\": \"949f14ebf523a07e722cc50047df89d2f60c2a5bc581d7dbc0c0458d4ae32133\",\n                \"Algorithm\": \"BLAKE3\",\n                \"ChecksumDate\": \"2024-01-29T10:59:08.095992Z\"\n            }\n        ],\n        \"ContentDate\": {\n            \"Start\": \"2024-01-29T06:21:21.024Z\",\n            \"End\": \"2024-01-29T06:21:21.024Z\"\n        },\n        \"Footprint\": \"geography'SRID=4326;POLYGON ((80.89747740096813 61.3325521177604, 79.13124823455819 61.3215912623266, 79.1878753803679 60.33630031682041, 80.0736808744782 60.34174571197013, 80.08271516161926 60.35300228895745, 80.19618187051994 60.4932042236609, 80.31061087184456 60.63330859780532, 80.4259343524467 60.7733392028608, 80.54233622006822 60.91328820862503, 80.65958672113561 61.05317537011384, 80.77810533703949 61.19290341271132, 80.89747740096813 61.3325521177604))'\",\n        \"GeoFootprint\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [...\n                ]\n            ]\n        },\n        \"Attributes\": [...\n        ],\n        \"ModificationDate\": \"2024-01-29T10:59:08.201Z\",\n        \"PublicationDate\": \"2024-01-29T10:58:20.247Z\",\n        \"Online\": true,\n        \"EvictionDate\": \"\",\n        \"S3Path\": \"/eodata/Sentinel-2/MSI/L2A/2024/01/29/S2A_MSIL2A_20240129T062121_N0510_R034_T44VMN_20240129T093752.SAFE\",\n        \"Assets\": [\n            {\n                \"Type\": \"QUICKLOOK\",\n                \"Id\": \"31cef3a2-65e2-4f94-bd0f-138a2897e5cd\",\n                \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(31cef3a2-65e2-4f94-bd0f-138a2897e5cd)/$value\",\n                \"S3Path\": \"/eodata/Sentinel-2/MSI/L2A/2024/01/29/S2A_MSIL2A_20240129T062121_N0510_R034_T44VMN_20240129T093752.SAFE\"\n            }\n        ],\n        \"Locations\": [...\n        ]\n    }\n}"
  },
  {
    "objectID": "APIs/Subscriptions.html#pull-and-push-subscriptions-operations",
    "href": "APIs/Subscriptions.html#pull-and-push-subscriptions-operations",
    "title": "Subscriptions",
    "section": "PULL and PUSH Subscriptions Operations",
    "text": "PULL and PUSH Subscriptions Operations\n\nGet Subscriptions\n\nUser’s subscriptions Info\nUsers are able to check their existing subscriptions.\nTo get information about the user’s Subscriptions (PUSH and PULL) the following endpoint should be used:\n\nHTTP requestResponse example\n\n\nGET \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions/Info\n\n\n200 OK\n\n[\n    {\n        \"Id\": \"7ca682c3-7b21-4e9b-952e-874798101340\",\n        \"FilterParam\": \"Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A')\",\n        \"StageOrder\": true,\n        \"Priority\": 1,\n        \"Status\": \"running\",\n        \"NotificationEndpoint\": \"https://userservice/notification\",\n        \"LastNotificationDate\": \"2024-01-18T09:47:45.540Z\",\n        \"SubscriptionEvent\": [\n            \"created\"\n        ],\n        \"SubmissionDate\": \"2024-01-18T09:47:34.267Z\",\n        \"@odata.context\": \"$metadata#OData.CSC.Subscription\"\n    },\n    {\n        \"Id\": \"991c4730-cf6f-432a-9f6c-47be0230ff45\",\n        \"FilterParam\": \"Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_SLC__1S')\",\n        \"StageOrder\": true,\n        \"Priority\": 1,\n        \"Status\": \"paused\",\n        \"LastNotificationDate\": \"2024-03-20T09:49:29.918Z\",\n        \"SubscriptionEvent\": [\n            \"created\"\n        ],\n        \"SubmissionDate\": \"2024-03-13T09:33:51.371Z\",\n        \"@odata.context\": \"$metadata#OData.CSC.Subscription\"\n    }\n]\n\n\n\n\n\nSubscription Info\nUsers are also able to get information about the specified subscription by its Id.\nTo get information about the specified subscription:\n\nHTTP request\n\n\nGET \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(subscription_id)\n\n\n\n\nRequest exampleResponse example\n\n\nGET \\  \nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(7ca682c3-7b21-4e9b-95 2e-874798101340)\n\n\n200 OK\n\n{\n    \"Id\": \"7ca682c3-7b21-4e9b-952e-874798101340\",\n    \"FilterParam\": \"Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A')\",\n    \"StageOrder\": true,\n    \"Priority\": 1,\n    \"Status\": \"running\",\n    \"NotificationEndpoint\": \"https://userservice/notification\",\n    \"LastNotificationDate\": \"2024-01-18T09:47:45.540Z\",\n    \"SubscriptionEvent\": [\n        \"created\"\n    ],\n    \"SubmissionDate\": \"2024-01-18T09:47:34.267Z\",\n    \"@odata.context\": \"$metadata#OData.CSC.Subscription\"\n}\n\n\n\n\n\nUpdate Subscription\nWithin the PUSH Subscription update, the following parameters can be modified:\n\nStatus\nNotificationEndpoint\nNotificationEpUsername\nNotificationEpPassword\n\nWithin the PULL Subscription update, the following parameter can be modified:\n\nStatus\n\nThe Subscription status can be changed to:\n\nrunning (subscription will be working, for PUSH subscriptions notifications will be sent to user’s endpoint and for PULL subscriptions notifications will be saved to subscription queue),\npaused (subscription will be stopped and push notifications will not be sent, notifications sent before the change of the status to paused will still be available),\ncancelled (subscription will be deleted).\n\nTo update an existing subscription:\n\nHTTP request\n\n\nPATCH \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(subscription_id)\n\n\n\nTo change the status of the subscription (running→paused or paused→running):\n\nRequest exampleResponse example\n\n\nPATCH \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(7ca682c3-7b21-4e9b-952e-874798181340)\n{\n  \"Status\": \"paused\"\n}\n\n\n200 OK\n \n{\n    \"Id\": \"7ca682c3-7b21-4e9b-952e-874798181340\",\n    \"FilterParam\": \"Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A')\",\n    \"StageOrder\": true,\n    \"Priority\": 1,\n    \"Status\": \"paused\",\n    \"NotificationEndpoint\": \"https://userservice/notification\",\n    \"LastNotificationDate\": \"2024-01-18T09:49:29.918Z\",\n    \"SubscriptionEvent\": [\n        \"created\"\n    ],\n    \"SubmissionDate\": \"2024-01-18T09:47:34.267Z\",\n    \"@odata.context\": \"$metadata#OData.CSC.Subscription\"\n}\n\n\n\nTo change the subscription’s status from running/paused to cancelled (Deletion of the Subscription):\n\nRequest example\n\n\nPATCH \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(7ca682c3-7b21-4e9b-952e-874798181340)\n{\n  \"Status\": \"cancelled\"\n}\n\n\n\n\n\nDelete Subscription\nIn case of the Subscription deletion (PUSH or PULL), it is requested to provide subscription Id.\nTo delete an existing subscription:\n\nHTTP request\n\n\nDELETE \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(subscription_id)\n\n\n\n\nRequest exampleResponse example\n\n\nDELETE \\\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Subscriptions(7ca682c3-7b21-4e9b-952e-874798181340)\n\n\n204 No Content"
  },
  {
    "objectID": "APIs/Token.html",
    "href": "APIs/Token.html",
    "title": "Copernicus Data Space Ecosystem Token Generation",
    "section": "",
    "text": "Users must have an access token to download products from the CDSE catalogue using OData and OpenSearch API. This token can be generated in Linux and Windows OS using cURL or Python script."
  },
  {
    "objectID": "APIs/Token.html#by-query-with-curl",
    "href": "APIs/Token.html#by-query-with-curl",
    "title": "Copernicus Data Space Ecosystem Token Generation",
    "section": "By query with cURL",
    "text": "By query with cURL\nCURL is a tool to send data to the server using several protocols, such as HTTPS.\nOn Linux:\nIn this example, the output is filtered by grep and awk commands to obtain a token. In the Linux operating system, it’s seen as an environmental variable ACCESS_TOKEN.\n\ncURLcURL with 2FA\n\n\nexport ACCESS_TOKEN=$(curl -d 'client_id=cdse-public' \\\n                    -d 'username=&lt;username&gt;' \\\n                    -d 'password=&lt;password&gt;' \\\n                    -d 'grant_type=password' \\\n                    'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' | \\\n                    python3 -m json.tool | grep \"access_token\" | awk -F\\\" '{print $4}')\n\n\nexport ACCESS_TOKEN=$(curl -d 'client_id=cdse-public' \\\n                    -d 'username=&lt;username&gt;' \\\n                    -d 'password=&lt;password&gt;' \\\n                    -d 'grant_type=password' \\\n                    -d 'totp=&lt;2FA_token&gt;' \\\n                    'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' | \\\n                    python3 -m json.tool | grep \"access_token\" | awk -F\\\" '{print $4}')\n\n\n\nYou can use following command to print the token:\nprintenv ACCESS_TOKEN\nOn Windows:\n\ncURLcURL with 2FA\n\n\ncurl -s -X POST https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=&lt;username&gt;\" -d \"password=&lt;password&gt;\" -d \"grant_type=password\" -d \"client_id=cdse-public\"\n\n\n\ncurl -s -X POST https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=&lt;username&gt;\" -d \"password=&lt;password&gt;\" -d \"totp=&lt;2FA_token&gt;\" -d \"grant_type=password\" -d \"client_id=cdse-public\"\n\n\n\nFor commands to work you need to replace “&lt;username&gt;” and “&lt;password&gt;” with your Copernicus Data Space Ecosystem login credentials \nIf you have any questions, please contact our support."
  },
  {
    "objectID": "APIs/OpenSearch.html",
    "href": "APIs/OpenSearch.html",
    "title": "OpenSearch Catalog web service",
    "section": "",
    "text": "The OpenSearch catalogue allows you to search through Copernicus data using a standardized web service. The OpenSearch specification can be consulted for technical details of the standard. This web service returns results as GeoJSON feature collections. Each feature in the collection represents an earth observation ‘product’, referencing where the actual data can be found.\nWe remark that this version does not implement the OGC OpenSearch standards, and a migration from other APIs named OpenSearch may require significant modifications. It mainly offers compatibility for existing users of a similar API on the CreoDIAS and Wekeo platforms and with client-side tools and workflows that have implemented support for this API."
  },
  {
    "objectID": "APIs/OpenSearch.html#using-opensearch-interface-to-query-data-catalogue",
    "href": "APIs/OpenSearch.html#using-opensearch-interface-to-query-data-catalogue",
    "title": "OpenSearch Catalog web service",
    "section": "Using OpenSearch interface to query Data Catalogue",
    "text": "Using OpenSearch interface to query Data Catalogue\nSince offset is not a recommended form of searching repository pages, we had to implement a limit to a maximum of 200k. The requests over the limit will be rejected with the code 400. Therefore, we encourage you to limit your inquiries by geographic or temporal area.\nAll queries may be executed as simple HTTPS-Get calls by typing the query in the web browser address line, using any HTTPS client, e.g., curl or wget, or from inside the users’ program. The database is accessible free and anonymously (open for anonymous access for everyone; no authorization is used). It may be accessed both from the internal network (virtual machines in Creodias) and from outside, e.g. your home computer. Note that the actual EO data are restricted to authorized users; only the Data Catalogue is open.\n\nGeneral Rules\nThe queries produce results in JSON format. Base URL:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?\").json()\npd.DataFrame.from_dict(json['features']).head(3)\n\n\n\n\n\n\n\n\n\ntype\nid\ngeometry\nproperties\n\n\n\n\n0\nFeature\n3149838d-0bdb-4ca8-980b-02258a1c08ac\nNone\n{'collection': 'SENTINEL-3', 'status': 'ONLINE...\n\n\n1\nFeature\nb584d0b4-d422-48a6-b8db-e9371c985cbe\nNone\n{'collection': 'SENTINEL-3', 'status': 'ONLINE...\n\n\n2\nFeature\nd636272d-9835-4f8e-9cd4-617ffc404024\nNone\n{'collection': 'SENTINEL-3', 'status': 'ONLINE...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost queries are case-sensitive.\n\n\n\n\nCollections\nThe data are organized in so-called collections corresponding to various satellites. A query may search for data in all collections or one particular collection only. If only one satellite is in the field of interest, the second approach is faster and more efficient than filtering the general query. For example, to find the ten most recent Sentinel-2 products with cloud cover below 10%, the query should look like:\n\nCLI\n\n\n$ wget -O - \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?cloudCover=[0,10]&startDate=2022-06-11T00:00:00Z&completionDate=2022-06-22T23:59:59Z&maxRecords=10\"\n\n\n\nwhile if the collection field is missing in the URL, the products from all the satellites are returned:\n\nCLI\n\n\n$ wget -O - \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?cloudCover=[0,10]&startDate=2022-06-11T00:00:00Z&completionDate=2022-06-22T23:59:59Z&maxRecords=10\"\n\n\n\nAs for today, the following collections are defined and may be used:\n\nCopernicus Sentinel Mission\n\nSentinel1 or SENTINEL-1\nSentinel2 or SENTINEL-2\nSentinel3 or SENTINEL-3\nSentinel5P or SENTINEL-5P\nSentinel6 or SENTINEL-6\nSentinel1RTC or SENTINEL-1-RTC (Sentinel-1 Radiometric Terrain Corrected)\n\nComplementary data\n\nGLOBAL-MOSAICS (Sentinel-1 and Sentinel-2 Global Mosaics)\nSMOS (Soil Moisture and Ocean Salinity)\nENVISAT or Envisat (ENVISAT- Medium Resolution Imaging Spectrometer - MERIS)\nLandsat5 or LANDSAT-5\nLandsat7 or LANDSAT-7\nLandsat8 or LANDSAT-8\nCOP-DEM (Copernicus DEM)\nTERRAAQUA (Terra MODIS and Aqua MODIS)\nS2GLC (S2GLC 2017)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that collection names vary slightly from satellite names, as they are used in the EO Data repository. For example, the collection is named Sentinel2, while in the repository, its data are located within /eodata/Sentinel-2/…. branch of the repository tree.\n\n\n\n\nOutput sorting and limiting\nBy default, a maximum of 20 products are returned. You may change the limit (beware of long execution time for queries about thousands of products) using the phrase:\n\nmaxRecords=nnn\n\nIf the query is very general and the number of matching products is large, the following pages of products can be retrieved using:\n\npage=nnn\n\nIt is also possible to alter the sequence in which the products are displayed by using a phrase similar to:\n\nsortParam=startDate\n\nThis will sort the output by observation date. The following orderings can be implemented:\n\nstartDate - the date when the observation was made (start)\ncompletionDate - the date when the observation was made (end)\npublished - the date when the product got published in our repository\nupdated - the date when the product got updated in our repository\n\n\n\n\n\n\n\nTip\n\n\n\nWhen searching for products and adding a wide range of dates to the query, e.g. from 2017 to 2023, we recommend splitting the query into individual years, e.g. from January 1, 2023 to December 31, 2023.\n\n\nEach of these orders can be accompanied by:\n\nsortOrder=ascending or sortOrder=descending\n\nFor example, the query\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?startDate=2021-07-01T00:00:00Z&completionDate=2021-07-31T23:59:59Z&sortParam=startDate&maxRecords=20\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?startDate=2021-07-01T00:00:00Z&completionDate=2021-07-31T23:59:59Z&sortParam=startDate&maxRecords=20\").json()\npd.DataFrame.from_dict(json['features']).head(3)\n\n\n\n\n\n\n\n\n\ntype\nid\ngeometry\nproperties\n\n\n\n\n0\nFeature\n6d96149e-6b5d-4ae4-8135-cb30b7501b39\nNone\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n1\nFeature\ne6ed6849-1517-4a1f-b51f-cbbd25f4cd5d\nNone\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n2\nFeature\n010f2ad1-0199-4bd8-850a-215b5c63b0b9\n{'type': 'Polygon', 'coordinates': [[[158.5965...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n\n\n\n\n\n\n\n\nThe above request will return 20 products from July 2021, whereas the next query will return the next 20:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?startDate=2021-07-01T00:00:00Z&completionDate=2021-07-31T23:59:59Z&sortParam=startDate&maxRecords=20&page=2\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?startDate=2021-07-01T00:00:00Z&completionDate=2021-07-31T23:59:59Z&sortParam=startDate&maxRecords=20&page=2\").json()\npd.DataFrame.from_dict(json['features']).head(3)\n\n\n\n\n\n\n\n\n\ntype\nid\ngeometry\nproperties\n\n\n\n\n0\nFeature\nc42fcc74-5c84-5be2-8252-bf0dadd1f29d\n{'type': 'Polygon', 'coordinates': [[[156.7053...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n1\nFeature\nc737c56b-182f-5862-977f-022a83b285d7\n{'type': 'Polygon', 'coordinates': [[[156.0567...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n2\nFeature\ncdaed6e4-99c3-44db-89e6-efbd21276bef\n{'type': 'Polygon', 'coordinates': [[[156.7053...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n\n\n\n\n\n\n\n\n\n\nFormal queries\nThe formal query is invoked as a sub-phrase sequence, separated by &. The result is a conjunction of all sub-phrases. It is impossible to use an alternative in the question. The query must be specified as a formal query.\nAn example of a formal query - about cloudless (cloud cover lower or equal to 10%) products for a specific location:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?cloudCover=[0,10]&startDate=2021-06-21T00:00:00Z&completionDate=2021-09-22T23:59:59Z&lon=21.01&lat=52.22\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?cloudCover=[0,10]&startDate=2021-06-21T00:00:00Z&completionDate=2021-09-22T23:59:59Z&lon=21.01&lat=52.22\").json()\npd.DataFrame.from_dict(json['features']).head(3)\n\n\n\n\n\n\n\n\n\ntype\nid\ngeometry\nproperties\n\n\n\n\n0\nFeature\na65aa1eb-da2e-5a21-a4e0-0a5184a9cce4\n{'type': 'Polygon', 'coordinates': [[[19.91605...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n1\nFeature\nb8ec026c-9ef0-4615-a984-9d368151c521\n{'type': 'Polygon', 'coordinates': [[[19.53152...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n2\nFeature\na2b06ea8-fff1-59e6-a8ed-62a6f50f102e\n{'type': 'Polygon', 'coordinates': [[[20.99970...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n\n\n\n\n\n\n\n\nThe queries are in the form param=value or param=[minvalue,maxvalue]. Most of the parameters are common for all collections. Still, some are specific for some of them (e.g. cloudCover applies to optical satellites, but polarisation applies to radar ones), or just a single one.\n\n\nGeography and time-frame\nThe common set of parameters are:\n\nstartDate, completionDate - the date limits of the observation. The time may also be specified, e.g. 2021-10-01T21:37:00Z\npublishedAfter, publishedBefore - the date limits when the product was published in our repository\nlon, lat - geographical position, expressed in military style (EPSG:4326, as a decimal fraction of degrees, positive for eastern latitude and northern longitude)\nradius - a region of interest, defined as a circle with centre in point determined by the longitude and latitude with radius expressed in meters (it won’t work with point manually selected in EOFinder/Data Explorer)\ngeometry - region of interest, defined as WKT string (POINT, POLYGON, etc.)\nbox - a region of interest, defined as the rectangle with given (west, south, east, north) values. It should be defined this way: &box=west,south,east,north\n\nFor example, the query can be:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?productType=S2MSI1C&cloudCover=[0,10]&startDate=2022-06-11T00:00:00Z&completionDate=2022-06-22T23:59:59Z&maxRecords=10&box=4,51,4.5,52\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?productType=S2MSI1C&cloudCover=[0,10]&startDate=2022-06-11T00:00:00Z&completionDate=2022-06-22T23:59:59Z&maxRecords=10&box=4,51,4.5,52\").json()\npd.DataFrame.from_dict(json['features']).head(3)\n\n\n\n\n\n\n\n\n\ntype\nid\ngeometry\nproperties\n\n\n\n\n0\nFeature\nf97bbb02-080f-5507-b923-a7fa53a6bcd6\n{'type': 'Polygon', 'coordinates': [[[4.436849...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n1\nFeature\ne6c08c26-60a9-59aa-bc7e-222875703aef\n{'type': 'Polygon', 'coordinates': [[[4.064090...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n2\nFeature\n5fff7c82-7c4f-50d9-ac21-c1f40855ad74\n{'type': 'Polygon', 'coordinates': [[[4.436811...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n\n\n\n\n\n\n\n\nor\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?cloudCover=[0,10]&startDate=2022-06-11T00:00:00Z&completionDate=2022-06-22T23:59:59Z&maxRecords=10&box=-21,23,-24,15\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?cloudCover=[0,10]&startDate=2022-06-11T00:00:00Z&completionDate=2022-06-22T23:59:59Z&maxRecords=10&box=-21,23,-24,15\").json()\npd.DataFrame.from_dict(json['features']).head(3)\n\n\n\n\n\n\n\n\n\ntype\nid\ngeometry\nproperties\n\n\n\n\n0\nFeature\n8e81c47e-58a1-53b1-812c-f5920d70ca48\n{'type': 'Polygon', 'coordinates': [[[-48.1934...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n1\nFeature\n16e54669-fc91-5667-9125-b0db7247cf34\n{'type': 'Polygon', 'coordinates': [[[-7.34594...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n2\nFeature\n64eeed3a-4d69-5861-81c8-49102a93f169\n{'type': 'Polygon', 'coordinates': [[[58.59901...\n{'collection': 'SENTINEL-2', 'status': 'ONLINE...\n\n\n\n\n\n\n\n\n\n\n\n\nVolatile features\nSome terrain-like feature masks are not permanent but describe a single scene only. The most commonly used feature is cloudiness, or cloudCover, which is defined for most of the products coming from optical sensors. For example:\n\ncloudCover=[0,10]\n\nThis parameter selects only those scenes, which are covered by clouds by no more than 10%.\n\n\n\n\n\n\nCaution\n\n\n\nTo be meaningful, the cloudiness must be provided with each product, while in many products is missing. If the cloudiness is unknown for the scene, it is marked by a value of 0 or -1. cloudCover=0 is therefore ambiguous: it may either mean a totally cloudless sky or a cloudy scene for which cloud cover had not been estimated during original data processing.\n\n\n\n\nSatellite features\n\ninstrument - meaningful only for satellites equipped with multiple instruments. The possible values are satellite specific.\nproductType - the actual types possible are specific for every satellite.\nsensorMode - also satellite and sensor specific. E.g. (for Sentinel-1): sensorMode=EW\norbitDirection - ASCENDING or DESCENDING. For most heliosynchronous satellites descending orbits means the day scenes, while ascending means night ones. For many optical satellites (e.g. Sentinel-2) only day scenes are published.\nresolution - expected spatial resolution of the product defined in meters.\nstatus:\n\n\n\nONLINE\nOFFLINE\nALL\n\n\nSome additional parameters are strictly satellite-specific, e.g. polarisation, which is defined only for Sentinel-1.\nFor every satellite (collection) its set of query-able parameters may be obtained by a query such as:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\n\n\n\n\nCode\nurl = 'https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml'\nresponse = requests.get(url)\n\nroot = ET.fromstring(response.content)\n\nfor child in root:\n    if child.tag.endswith('ShortName') or child.tag.endswith('Description'):\n        print(f\"{child.tag}: {child.text}\")\n\n\n{http://a9.com/-/spec/opensearch/1.1/}ShortName: Sentinel-1\n{http://a9.com/-/spec/opensearch/1.1/}Description: Sentinel-1 Collection\n\n\n\n\n\nThe resulting XML file provides a complete list of the collection parameters, with their brief descriptions."
  },
  {
    "objectID": "APIs/OData.html",
    "href": "APIs/OData.html",
    "title": "OData",
    "section": "",
    "text": "OData is an SO/IEC approved, OASIS standard , which is based on https RESTful Application Programming Interfaces. It enables resources, which are identified by URLs and defined in a data model, to be created and edited using simple HTTPS messages. OData makes it possible to build REST-based data services that let Web clients publish and edit resources that are recognized by Uniform Resource Locators (URLs) and described in a data model using straightforward HTTPS messages."
  },
  {
    "objectID": "APIs/OData.html#odata-products-endpoint",
    "href": "APIs/OData.html#odata-products-endpoint",
    "title": "OData",
    "section": "OData Products endpoint",
    "text": "OData Products endpoint\n\n\n\n\n\n\nTip\n\n\n\nCrucial for the search performance is specifying the collection name. Example: Collection/Name eq ‘SENTINEL-3’\nThe additional efficient way to accelerate the query performance is limiting the query by acquisition dates, e.g.: ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-21T00:00:00.000Z\nWhen searching for products and adding a wide range of dates to the query, e.g. from 2017 to 2023, we recommend splitting the query into individual years, e.g. from January 1, 2023 to December 31, 2023.\n\n\n\nQuery structure\nAs a general note, the OData query consists of elements which in this documentation are called “options”. The interface supports the following search options:\n\nfilter\norderby\ntop\nskip\ncount\nexpand\n\nSearch options should always be preceded with $ and consecutive options should be separated with &.\nConsecutive filters within filter option should be separated with and or or. Not operator can also be used e.g.:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=not (Collection/Name eq 'SENTINEL-2') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T00:10:00.000Z&$orderby=ContentDate/Start&$top=100\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=not (Collection/Name eq 'SENTINEL-2') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T00:10:00.000Z&$orderby=ContentDate/Start&$top=100\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n1d42f2d3-2456-485f-a93e-92f08bdd5c51\nS1A_OPER_AUX_GNSSRD_POD__20220510T020122_V2022...\n/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S...\nNone\n\n\n1\n5c744d5c-c082-4a34-a181-81cde73cd25d\nS1B_OPER_AUX_GNSSRD_POD__20220510T023113_V2022...\n/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S...\nNone\n\n\n2\n4a4ef482-84a2-551d-8086-e3de6d39c488\nS3B_SL_1_RBT____20220503T000015_20220503T00031...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2022/05/03/S...\n{'type': 'Polygon', 'coordinates': [[[-29.448,..."
  },
  {
    "objectID": "APIs/OData.html#filter-option",
    "href": "APIs/OData.html#filter-option",
    "title": "OData",
    "section": "Filter option",
    "text": "Filter option\n\nQuery by name\nTo search for a specific product by its exact name:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Name eq 'S1A_IW_GRDH_1SDV_20141031T161924_20141031T161949_003076_003856_634E.SAFE'\n\n\n\nTo search for Copernicus Contributing Mission (CCM) data:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Name eq 'SP07_NAO_MS4_2A_20210729T064948_20210729T064958_TOU_1234_90f0.DIMA'&$expand=Attributes\n\n\n\nAlternatively contains, endswith and startswith can be used to search for products ending or starting with provided string. You should use Collection/Name filter even if it overlaps with startswith or contains clause.\n\n\nQuery by list\nIn case a user desires to search for multiple products by name in one query, the POST method can be used:\nPOST\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products/OData.CSC.FilterList\nRequest body:\n{\n  \"FilterProducts\":\n    [\n     {\"Name\": \"S1A_IW_GRDH_1SDV_20141031T161924_20141031T161949_003076_003856_634E.SAFE\"},\n     {\"Name\": \"S3B_SL_1_RBT____20190116T050535_20190116T050835_20190117T125958_0179_021_048_0000_LN2_O_NT_003.SEN3\"},\n     {\"Name\": \"xxxxxxxx.06.tar\"}\n    ]\n }\nTwo results are returned, as there is no product named xxxxxxxx.06.tar.\n\n\nQuery Collection of Products\nTo search for products within a specific collection:\nFor Sentinel-2:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2'\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2'\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n6a8d07a6-ea80-4bbf-a3b4-f2d632c8bc2e\nS2A_OPER_AUX_PROQUA_POD__20200601T113857_V2015...\n/eodata/Sentinel-2/AUX/AUX_PROQUA/2015/06/26/S...\nNone\n\n\n1\n2d8eb355-3930-4a6f-b02c-f793773cb656\nS2A_OPER_AUX_GNSSRD_POD__20171211T085826_V2015...\n/eodata/Sentinel-2/AUX/AUX_GNSSRD/2015/06/27/S...\nNone\n\n\n2\n5303fa53-2dd4-4ee2-b012-d123a2ccd0b4\nS2A_OPER_AUX_GNSSRD_POD__20171211T085921_V2015...\n/eodata/Sentinel-2/AUX/AUX_GNSSRD/2015/06/28/S...\nNone\n\n\n\n\n\n\n\n\n\n\nFor Copernicus Contributing Missions (CCM):\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM'\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM'\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n3344f33a-b662-420f-9c79-43492b62af78\nDEM1_SAR_DGE_30_20110510T014052_20140221T01325...\n/eodata/CCM/COP-DEM_GLO-30-DGED/SAR_DGE_30_A4A...\n{'type': 'Polygon', 'coordinates': [[[-115.0, ...\n\n\n1\nf8df5315-42c2-49b2-890a-727ecc2bf4cb\nRS02_SAR_SW_SCW_20221115T081207_20221115T08132...\n/eodata/CCM/SAR_SEA_ICE/SAR_SW_SCW_6F15/2022/1...\n{'type': 'Polygon', 'coordinates': [[[3.78, 79...\n\n\n2\nce6dcd43-e2be-4f29-b6da-bd6278102f10\nCS00_SAR_SCH_1B_20221117T005548_20221117T00561...\n/eodata/CCM/SAR_SEA_ICE/SAR_SCH_1B_6F15/2022/1...\n{'type': 'Polygon', 'coordinates': [[[-65.8001...\n\n\n\n\n\n\n\n\n\n\nThe following collections are currently available:\n\nCopernicus Sentinel Mission\n\nSENTINEL-1\nSENTINEL-2\nSENTINEL-3\nSENTINEL-5P\nSENTINEL-6\nSENTINEL-1-RTC (Sentinel-1 Radiometric Terrain Corrected)\n\nComplementary data\n\nGLOBAL-MOSAICS (Sentinel-1 and Sentinel-2 Global Mosaics)\nSMOS (Soil Moisture and Ocean Salinity)\nENVISAT (ENVISAT- Medium Resolution Imaging Spectrometer - MERIS)\nLANDSAT-5\nLANDSAT-7\nLANDSAT-8\nCOP-DEM (Copernicus DEM)\nTERRAAQUA (Terra MODIS and Aqua MODIS)\nS2GLC (S2GLC 2017)\n\nCopernicus Contributing Missions (CCM)\n\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and ContentDate/Start gt 2005-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T00:11:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and ContentDate/Start gt 2005-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T00:11:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n00000298-7cca-4002-bd6c-cdd7d40cf94e\nDEM1_SAR_DGE_10_20101224T155413_20130403T15484...\n/eodata/CCM/COP-DEM_EEA-10-DGED/SAR_DGE_10_931...\n{'type': 'Polygon', 'coordinates': [[[24.0, 64...\n\n\n1\n00000594-e5c6-40e6-8c8a-d1dfabf2c98e\nEW03_WV3_PM4_OR_20150810T092701_20150810T09271...\n/eodata/CCM/VHR_IMAGE_2015/WV3_PM4_OR_71F4/201...\n{'type': 'Polygon', 'coordinates': [[[29.02003...\n\n\n2\n00000e4d-be6d-41e0-b5fa-bbacedfa4646\nDEM1_SAR_DTE_30_20130608T084201_20140731T00573...\n/eodata/CCM/COP-DEM_GLO-30-DTED/SAR_DTE_30_615...\n{'type': 'Polygon', 'coordinates': [[[-71.0, -...\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Publication Date\nTo search for products published between two dates:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=PublicationDate gt 2019-05-15T00:00:00.000Z and PublicationDate lt 2019-05-16T00:00:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=PublicationDate gt 2019-05-15T00:00:00.000Z and PublicationDate lt 2019-05-16T00:00:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n2997cd87-a273-5bbc-998a-1c72fe152b06\nS3A_SL_1_RBT____20160904T192151_20160904T19245...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2016/09/04/S...\n{'type': 'Polygon', 'coordinates': [[[42.9227,...\n\n\n1\n05d3b080-14b1-5e93-b72b-3743f8d8a37c\nS3A_SL_1_RBT____20160904T191051_20160904T19125...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2016/09/04/S...\n{'type': 'Polygon', 'coordinates': [[[50.5057,...\n\n\n2\nd204583c-2328-57c0-9534-f52121048cf1\nS3A_SL_1_RBT____20160904T192451_20160904T19275...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2016/09/04/S...\n{'type': 'Polygon', 'coordinates': [[[41.386, ...\n\n\n\n\n\n\n\n\n\n\nTo define inclusive interval ge and le parameters can be used:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=PublicationDate ge 2019-05-15T00:00:00.000Z and PublicationDate le 2019-05-16T00:00:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=PublicationDate ge 2019-05-15T00:00:00.000Z and PublicationDate le 2019-05-16T00:00:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n2997cd87-a273-5bbc-998a-1c72fe152b06\nS3A_SL_1_RBT____20160904T192151_20160904T19245...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2016/09/04/S...\n{'type': 'Polygon', 'coordinates': [[[42.9227,...\n\n\n1\n05d3b080-14b1-5e93-b72b-3743f8d8a37c\nS3A_SL_1_RBT____20160904T191051_20160904T19125...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2016/09/04/S...\n{'type': 'Polygon', 'coordinates': [[[50.5057,...\n\n\n2\nd204583c-2328-57c0-9534-f52121048cf1\nS3A_SL_1_RBT____20160904T192451_20160904T19275...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2016/09/04/S...\n{'type': 'Polygon', 'coordinates': [[[41.386, ...\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Sensing Date\nTo search for products acquired between two dates:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=ContentDate/Start gt 2019-05-15T00:00:00.000Z and ContentDate/Start lt 2019-05-16T00:00:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=ContentDate/Start gt 2019-05-15T00:00:00.000Z and ContentDate/Start lt 2019-05-16T00:00:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n4725d436-3e90-5480-bee1-0f13a7fc14fd\nS3B_SL_1_RBT____20190515T000040_20190515T00034...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2019/05/15/S...\n{'type': 'Polygon', 'coordinates': [[[-8.40421...\n\n\n1\n169fda08-9928-576e-a556-97a6d3b9bacf\nS3B_SL_1_RBT____20190515T000040_20190515T00034...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2019/05/15/S...\n{'type': 'Polygon', 'coordinates': [[[-8.40421...\n\n\n2\n07c0c999-5f9d-553f-9b3d-f2b8ab013856\nS3B_SL_2_LST____20190515T000040_20190515T00034...\n/eodata/Sentinel-3/SLSTR/SL_2_LST/2019/05/15/S...\n{'type': 'Polygon', 'coordinates': [[[-8.40421...\n\n\n\n\n\n\n\n\n\n\nAs an example, for the Copernicus Contributions Mission Data (CCM):\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))') and ContentDate/Start gt 2021-05-20T00:00:00.000Z and ContentDate/Start lt 2021-07-21T00:00:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))') and ContentDate/Start gt 2021-05-20T00:00:00.000Z and ContentDate/Start lt 2021-07-21T00:00:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n118fb2be-ebcc-4e85-ac59-c81c3539d723\nSW00_OPT_MS4_1B_20210714T093148_20210714T09315...\n/eodata/CCM/VHR_IMAGE_2021/OPT_MS4_1B_07B6/202...\n{'type': 'Polygon', 'coordinates': [[[21.43089...\n\n\n1\n82d08aaa-596b-449a-b88e-12276e787ca7\nTR00_VHI_MS4_1B_20210714T081827_20210714T08183...\n/eodata/CCM/VHR_IMAGE_2021/VHI_MS4_1B_07B6/202...\n{'type': 'Polygon', 'coordinates': [[[22.70168...\n\n\n2\n86d33e23-1eb7-4a21-9bb0-d61a1d061f1b\nSW00_OPT_MS4_1C_20210713T102155_20210713T10215...\n/eodata/CCM/VHR_IMAGE_2021/OPT_MS4_1C_07B6/202...\n{'type': 'Polygon', 'coordinates': [[[21.64792...\n\n\n\n\n\n\n\n\n\n\nUsually, there are two parameters describing the ContentDate (Acquisition Dates) for a product - Start and End. Depending on what the user is looking for, these parameters can be mixed, e.g.:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=ContentDate/Start gt 2019-05-15T00:00:00.000Z and ContentDate/End lt 2019-05-15T00:05:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=ContentDate/Start gt 2019-05-15T00:00:00.000Z and ContentDate/End lt 2019-05-15T00:05:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n4725d436-3e90-5480-bee1-0f13a7fc14fd\nS3B_SL_1_RBT____20190515T000040_20190515T00034...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2019/05/15/S...\n{'type': 'Polygon', 'coordinates': [[[-8.40421...\n\n\n1\n169fda08-9928-576e-a556-97a6d3b9bacf\nS3B_SL_1_RBT____20190515T000040_20190515T00034...\n/eodata/Sentinel-3/SLSTR/SL_1_RBT/2019/05/15/S...\n{'type': 'Polygon', 'coordinates': [[[-8.40421...\n\n\n2\n07c0c999-5f9d-553f-9b3d-f2b8ab013856\nS3B_SL_2_LST____20190515T000040_20190515T00034...\n/eodata/Sentinel-3/SLSTR/SL_2_LST/2019/05/15/S...\n{'type': 'Polygon', 'coordinates': [[[-8.40421...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFiltering by ContentDate/Start is much faster than by ContentDate/End for big collections. Narrowing ContentDate/Start gives the best performance boost for SENTINEL-2 collection.\n\n\n\n\nQuery by Geographic Criteria\nTo search for products intersecting the specified polygon:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))') and ContentDate/Start gt 2022-05-20T00:00:00.000Z and ContentDate/Start lt 2022-05-21T00:00:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))') and ContentDate/Start gt 2022-05-20T00:00:00.000Z and ContentDate/Start lt 2022-05-21T00:00:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n0d3b8cf1-01fb-5f7e-820a-1d5442875575\nLC08_L1TP_180031_20220520_20220520_02_RT\n/eodata/Landsat-8-ESA/OLI_TIRS/L1TP/2022/05/20...\n{'type': 'Polygon', 'coordinates': [[[27.73522...\n\n\n1\nd3b5fbd9-9ffd-579b-b327-4de3e8502591\nLC08_L2SP_180031_20220520_20220525_02_T1\n/eodata/Landsat-8-ESA/OLI_TIRS/L2SP/2022/05/20...\n{'type': 'Polygon', 'coordinates': [[[27.73522...\n\n\n2\n0bfcd52a-1c10-5870-820a-7282032fe8e4\nLC08_L1TP_180031_20220520_20220525_02_T1\n/eodata/Landsat-8-ESA/OLI_TIRS/L1TP/2022/05/20...\n{'type': 'Polygon', 'coordinates': [[[27.73522...\n\n\n\n\n\n\n\n\n\n\nSimilarly, for the Copernicus Contributing Missions (CCM) data:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))')&$top=20\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((12.655118166047592 47.44667197521409,21.39065656328509 48.347694733853245,28.334291357162826 41.877123516783655,17.47086198383573 40.35854475076158,12.655118166047592 47.44667197521409))')&$top=20\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\nfeab5ac8-af02-492a-92a9-541b2c897cd8\nPH1B_PHR_MS__2A_20180819T094810_20180819T09481...\n/eodata/CCM/VHR_IMAGE_2018/PHR_MS__2A_E1F0/201...\n{'type': 'Polygon', 'coordinates': [[[20.44659...\n\n\n1\nf49b4ea5-0886-42ee-b01e-8938bf0af54b\nPH1A_PHR_MS___3_20180908T094354_20180908T09440...\n/eodata/CCM/VHR_IMAGE_2018/PHR_MS___3_E1F0/201...\n{'type': 'Polygon', 'coordinates': [[[19.62556...\n\n\n2\na05c293d-cdc1-4ebd-9504-c3533e26183d\nPH1A_PHR_MS__2A_20180818T095540_20180818T09554...\n/eodata/CCM/VHR_IMAGE_2018/PHR_MS__2A_E1F0/201...\n{'type': 'Polygon', 'coordinates': [[[17.09548...\n\n\n\n\n\n\n\n\n\n\nTo search for products intersecting the specified point:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POINT(-0.5319577002158441 28.65487836189358)') and Collection/Name eq 'SENTINEL-1'&$top=20\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POINT(-0.5319577002158441 28.65487836189358)') and Collection/Name eq 'SENTINEL-1'&$top=20\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n962384e9-c5c4-4b02-b1c2-e41f8a9805b1\nS1A_IW_GRDH_1SDV_20230417T060457_20230417T0605...\n/eodata/Sentinel-1/SAR/IW_GRDH_1S-COG/2023/04/...\n{'type': 'Polygon', 'coordinates': [[[-0.36347...\n\n\n1\n0491e34e-73f6-40be-bc6c-327d2c639a98\nS1A_IW_SLC__1SDV_20230417T060456_20230417T0605...\n/eodata/Sentinel-1/SAR/IW_SLC__1S/2023/04/17/S...\n{'type': 'Polygon', 'coordinates': [[[-0.38326...\n\n\n2\nb7af1ca3-b0d7-489d-a487-48bd2cf04a42\nS1A_IW_GRDH_1SDV_20230417T060457_20230417T0605...\n/eodata/Sentinel-1/SAR/IW_GRDH_1S/2023/04/17/S...\n{'type': 'Polygon', 'coordinates': [[[-0.36347...\n\n\n\n\n\n\n\n\n\n\nDisclaimers:\n\nPolygon must start and end with the same point.\nCoordinates must be given in EPSG 4326\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note that the geometry is validated using the Shapely library, and invalid geometries results in an error.\n\n\n\n\nQuery by attributes\nTo search for products by attributes, it is necessary to build a filter with the following structure:\nAttributes/OData.CSC.ValueTypeAttribute/any(att:att/Name eq ‘[Attribute.Name]’ and att/OData.CSC.ValueTypeAttribute/Value eq [Attribute.Value])\nwhere\n\nValueTypeAttribute can take the following values:\n\nDoubleAttribute\nIntegerAttribute\nDateTimeOffsetAttribute\nStringAttribute\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo search for products by StringAttribute, the filter query should be built with the following structure: Attributes/OData.CSC.StringAttribute/any(att:att/Name eq ‘[Attribute.Name]’ and att/OData.CSC.StringAttribute/Value eq ‘[Attribute.Value]’)\n\n\n\n[Attribute.Name] is the attribute name which can take multiple values depending on collection; acceptable values for the attribute name can be checked at the specified endpoints for each collection, as provided in List of OData query attributes.\neq before [Attribute.Value] can be substituted with le, lt, ge, gt in case of Integer, Double or DateTimeOffset Attributes\n[Attribute.Value] is the specific value that the user is searching for\n\nTo get Sentinel-2 products with CloudCover&lt;40% between two dates:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and att/OData.CSC.DoubleAttribute/Value le 40.00) and ContentDate/Start gt 2022-01-01T00:00:00.000Z and ContentDate/Start lt 2022-01-03T00:00:00.000Z&$top=10\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20%27SENTINEL-2%27%20and%20Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and att/OData.CSC.DoubleAttribute/Value le 40.00) and ContentDate/Start gt 2022-01-01T00:00:00.000Z and ContentDate/Start lt 2022-01-03T00:00:00.000Z&$top=10\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\nb7655921-29ab-5d6a-b0c6-b68b4ae1fe49\nS2B_MSIL2A_20220102T072309_N0301_R006_T37NHC_2...\n/eodata/Sentinel-2/MSI/L2A/2022/01/02/S2B_MSIL...\n{'type': 'Polygon', 'coordinates': [[[41.69700...\n\n\n1\n9ec93df8-d042-51a4-9e67-56f598b9796b\nS2B_MSIL2A_20220101T175739_N0301_R141_T12TYL_2...\n/eodata/Sentinel-2/MSI/L2A/2022/01/01/S2B_MSIL...\n{'type': 'Polygon', 'coordinates': [[[-107.622...\n\n\n2\ne4f917fd-b6ed-57c3-8c2c-65e182b6426c\nS2A_MSIL1C_20220101T002021_N0301_R059_T57UXA_2...\n/eodata/Sentinel-2/MSI/L1C/2022/01/01/S2A_MSIL...\n{'type': 'Polygon', 'coordinates': [[[161.6818...\n\n\n\n\n\n\n\n\n\n\nTo get products with cloudCover&lt; 10% and productType=S2MSI2A and ASCENDING orbitDirection between two dates:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and att/OData.CSC.DoubleAttribute/Value lt 10.00) and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A') and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'orbitDirection' and att/OData.CSC.StringAttribute/Value eq 'ASCENDING') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T04:00:00.000Z&$top=10\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2' and Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and att/OData.CSC.DoubleAttribute/Value lt 10.00) and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'S2MSI2A') and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'orbitDirection' and att/OData.CSC.StringAttribute/Value eq 'ASCENDING') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T04:00:00.000Z&$top=10\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n7e0f9557-d537-56bb-90a1-9b4a746f0f55\nS2B_MSIL2A_20220503T000139_N0400_R016_T08XMQ_2...\n/eodata/Sentinel-2/MSI/L2A/2022/05/03/S2B_MSIL...\n{'type': 'Polygon', 'coordinates': [[[-138.241...\n\n\n1\na3041799-63e6-5b61-a16a-cb5bfabce2aa\nS2B_MSIL2A_20220503T000139_N0400_R016_T09XVJ_2...\n/eodata/Sentinel-2/MSI/L2A/2022/05/03/S2B_MSIL...\n{'type': 'Polygon', 'coordinates': [[[-128.506...\n\n\n2\n716d55e7-ee2a-5985-afed-4ca073864ca9\nS2B_MSIL2A_20220503T000139_N0400_R016_T08XNQ_2...\n/eodata/Sentinel-2/MSI/L2A/2022/05/03/S2B_MSIL...\n{'type': 'Polygon', 'coordinates': [[[-135.001...\n\n\n\n\n\n\n\n\n\n\nTo query a subset of CCM data for a specific area of interest and time period, selecting a specific mission, e.g. only Worldview-3:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and OData.CSC.Intersects(area=geography'SRID=4326;POLYGON ((6.535492 50.600673, 6.535492 50.937662, 7.271576 50.937662, 7.271576 50.600673, 6.535492 50.600673))') and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'platformName' and att/OData.CSC.StringAttribute/Value eq 'WorldView-3') and ContentDate/Start gt 2022-05-20T00:00:00.000Z and ContentDate/Start lt 2022-07-21T00:00:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'CCM' and OData.CSC.Intersects(area=geography'SRID=4326;POLYGON ((6.535492 50.600673, 6.535492 50.937662, 7.271576 50.937662, 7.271576 50.600673, 6.535492 50.600673))') and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'platformName' and att/OData.CSC.StringAttribute/Value eq 'WorldView-3') and ContentDate/Start gt 2022-05-20T00:00:00.000Z and ContentDate/Start lt 2022-07-21T00:00:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n5df5dd79-2dba-4005-8bc8-26f331758e82\nEW03_WV3_MS4_SO_20220717T105040_20220717T10504...\n/eodata/CCM/VHR_IMAGE_2021/WV3_MS4_SO_07B6/202...\n{'type': 'Polygon', 'coordinates': [[[6.97417,...\n\n\n1\n9f1020e6-be24-4675-8b1a-82ce6c27913f\nEW03_WV3_MS4_SO_20220717T105040_20220717T10504...\n/eodata/CCM/VHR_IMAGE_2021/WV3_MS4_SO_07B6/202...\n{'type': 'Polygon', 'coordinates': [[[6.99509,...\n\n\n2\n1aad79fa-90c6-498a-b2de-a20a34d06db8\nEW03_WV3_MS4_OR_20220717T105040_20220717T10504...\n/eodata/CCM/VHR_IMAGE_2021/WV3_MS4_OR_07B6/202...\n{'type': 'Polygon', 'coordinates': [[[6.983405...\n\n\n\n\n\n\n\n\n\n\nTo search all products of a specific dataset under CCM (for example for the products belonging to VHR_IMAGE_2018):\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'datasetFull' and att/OData.CSC.StringAttribute/Value eq 'VHR_IMAGE_2018')\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'datasetFull' and att/OData.CSC.StringAttribute/Value eq 'VHR_IMAGE_2018')\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\nfeab5ac8-af02-492a-92a9-541b2c897cd8\nPH1B_PHR_MS__2A_20180819T094810_20180819T09481...\n/eodata/CCM/VHR_IMAGE_2018/PHR_MS__2A_E1F0/201...\n{'type': 'Polygon', 'coordinates': [[[20.44659...\n\n\n1\nf49b4ea5-0886-42ee-b01e-8938bf0af54b\nPH1A_PHR_MS___3_20180908T094354_20180908T09440...\n/eodata/CCM/VHR_IMAGE_2018/PHR_MS___3_E1F0/201...\n{'type': 'Polygon', 'coordinates': [[[19.62556...\n\n\n2\na05c293d-cdc1-4ebd-9504-c3533e26183d\nPH1A_PHR_MS__2A_20180818T095540_20180818T09554...\n/eodata/CCM/VHR_IMAGE_2018/PHR_MS__2A_E1F0/201...\n{'type': 'Polygon', 'coordinates': [[[17.09548...\n\n\n\n\n\n\n\n\n\n\n\nList of OData query attributes by collection\nTo check acceptable attribute names for all Collections:\n\nAll collections\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes\n\n\n\nTo check acceptable attribute names for Copernicus Sentinel Missions:\n\nSENTINEL-1SENTINEL-2SENTINEL-3SENTINEL-5PSENTINEL-6SENTINEL-1-RTC\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SENTINEL-1)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SENTINEL-2)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SENTINEL-3)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SENTINEL-5P)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SENTINEL-6)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SENTINEL-1-RTC)\n\n\n\nTo check acceptable attribute names for Copernicus Contributing Missions (CCM):\n\nCCM\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(CCM)\n\n\n\nTo check acceptable attribute names for Complementary data:\n\nGLOBAL-MOSAICSSMOSENVISATLANDSAT-5LANDSAT-7LANDSAT-8COP-DEMTERRAAQUAS2GLC\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(GLOBAL-MOSAICS)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(SMOS)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(ENVISAT)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(LANDSAT-5)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(LANDSAT-7)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(LANDSAT-8)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(COP-DEM)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(TERRAAQUA)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(S2GLC)"
  },
  {
    "objectID": "APIs/OData.html#orderby-option",
    "href": "APIs/OData.html#orderby-option",
    "title": "OData",
    "section": "Orderby option",
    "text": "Orderby option\nOrderby option can be used to order the products in an ascending (asc) or descending (desc) direction. If asc or desc is not specified, then the resources will be ordered in ascending order.\n\n\n\n\n\n\nTip\n\n\n\nUsing the orderby option will exclude potential duplicates from the search results.\n\n\nTo order products by ContentDate/Start in a descending direction:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'EW_GRDM_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T03:00:00.000Z&$orderby=ContentDate/Start desc\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'EW_GRDM_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T03:00:00.000Z&$orderby=ContentDate/Start desc\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n6928b379-4f9a-5473-a12a-7e7e4b83f776\nS1A_EW_GRDM_1SSH_20220503T024410_20220503T0244...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-105.464...\n\n\n1\n4824ead5-b35c-5b83-80fa-71219c069e1c\nS1A_EW_GRDM_1SSH_20220503T024310_20220503T0244...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-103.097...\n\n\n2\n0929f73c-902a-506b-9646-c908199bfa23\nS1A_EW_GRDM_1SSH_20220503T024206_20220503T0243...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-97.2686...\n\n\n\n\n\n\n\n\n\n\nBy default, if the orderby option is not used, the results are not ordered. If orderby option is used, additional orderby by id is also used, so that the results are fully ordered, and no products are lost while paginating through the results.\nThe acceptable arguments for this option: ContentDate/Start, ContentDate/End, PublicationDate, ModificationDate, in directions: asc, desc."
  },
  {
    "objectID": "APIs/OData.html#top-option",
    "href": "APIs/OData.html#top-option",
    "title": "OData",
    "section": "Top option",
    "text": "Top option\nTop option specifies the maximum number of items returned from a query.\nTo limit the number of results:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'EW_GRDM_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$top=100\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'EW_GRDM_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$top=100\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n3b46f46b-4862-5587-89cb-9c52a9cc106a\nS1A_EW_GRDM_1SDH_20220503T051020_20220503T0511...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[34.92659...\n\n\n1\nd1402094-d440-570c-9f55-07ffdd2fae19\nS1A_EW_GRDM_1SDH_20220503T064800_20220503T0649...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[15.66478...\n\n\n2\nc8532dc6-3967-52b8-8ee4-ea63eb1a8ba2\nS1A_EW_GRDM_1SSH_20220503T090752_20220503T0908...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-71.2011...\n\n\n\n\n\n\n\n\n\n\nThe default value is set to 20.\nThe acceptable arguments for this option: Integer &lt;0,1000&gt;"
  },
  {
    "objectID": "APIs/OData.html#skip-option",
    "href": "APIs/OData.html#skip-option",
    "title": "OData",
    "section": "Skip option",
    "text": "Skip option\nThe skip option can be used to skip a specific number of results. Exemplary application of this option would be paginating through the results, however, for performance reasons, we recommend limiting queries with small time intervals as a substitute for skipping in a more generic query.\nTo skip a specific number of results:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'EW_GRDM_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$skip=23\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'EW_GRDM_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$skip=23\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\na46c4820-96f4-55f7-9ee0-bb897597ad20\nS1A_EW_GRDM_1SDH_20220503T115007_20220503T1150...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-77.5491...\n\n\n1\n5203efdf-4dd8-536a-9222-01364242bf7f\nS1A_EW_GRDM_1SSH_20220503T090548_20220503T0906...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-66.5592...\n\n\n2\na63512e8-ca23-58e0-90fb-02f7b3cfbb39\nS1A_EW_GRDM_1SDH_20220503T083125_20220503T0831...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_EW_G...\n{'type': 'Polygon', 'coordinates': [[[-25.6077...\n\n\n\n\n\n\n\n\n\n\nThe default value is set to 0.\nWhenever a query results in more products than 20 (default top value), the API provides a nextLink at the bottom of the page:\n\"@OData.nextLink\":\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_GRDH_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$skip=20\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_GRDH_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$skip=20\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n665501b5-56a4-5ba1-92ca-b62a4571afa2\nS1A_IW_GRDH_1SDV_20220503T013322_20220503T0133...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_IW_G...\n{'type': 'Polygon', 'coordinates': [[[-114.503...\n\n\n1\n63a43876-a5a3-52a1-a401-04c2bbd93faf\nS1A_IW_GRDH_1SDV_20220503T013617_20220503T0136...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_IW_G...\n{'type': 'Polygon', 'coordinates': [[[-116.981...\n\n\n2\ndd677ca4-b6b2-509d-8820-0d14ab5f52d5\nS1A_IW_GRDH_1SDV_20220503T013646_20220503T0137...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_IW_G...\n{'type': 'Polygon', 'coordinates': [[[-117.353...\n\n\n\n\n\n\n\n\n\n\nThe acceptable arguments for this option: Integer &lt;0,10000&gt;"
  },
  {
    "objectID": "APIs/OData.html#count-option",
    "href": "APIs/OData.html#count-option",
    "title": "OData",
    "section": "Count option",
    "text": "Count option\nThe count option lets users get the exact number of products matching the query. This option is disabled by default to accelerate the query performance.\n\n\n\n\n\n\nTip\n\n\n\nDon’t use count option if not necessary, it slows down the execution of the request.\n\n\nTo get the exact number of products for a given query:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_GRDH_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$count=True\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_GRDH_1S') and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$count=True\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n2af72689-8608-5d24-a7bb-a143f667dbd1\nS1A_IW_GRDH_1SDV_20220503T002004_20220503T0020...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_IW_G...\n{'type': 'Polygon', 'coordinates': [[[91.09471...\n\n\n1\ncc319b60-b419-59b6-b063-ace3facc8e72\nS1A_IW_GRDH_1SDV_20220503T002033_20220503T0021...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_IW_G...\n{'type': 'Polygon', 'coordinates': [[[90.36774...\n\n\n2\na2176410-7175-5b89-90f9-66be98f65d92\nS1A_IW_GRDH_1SDV_20220503T002641_20220503T0027...\n/eodata/Sentinel-1/SAR/GRD/2022/05/03/S1A_IW_G...\n{'type': 'Polygon', 'coordinates': [[[84.51186...\n\n\n\n\n\n\n\n\n\n\nThe acceptable arguments for this option: True, true, 1, False, false, 0."
  },
  {
    "objectID": "APIs/OData.html#expand-option",
    "href": "APIs/OData.html#expand-option",
    "title": "OData",
    "section": "Expand option",
    "text": "Expand option\nExpand option allows users to speficy the type of information they would like to see in detail.\nThe acceptable arguments for this option: Attributes, Assets and Locations.\n\nExpand Attributes\nThe expand attributes enables users to see the full metadata of each returned result.\nTo see the metadata of the results:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$expand=Attributes\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start gt 2022-05-03T00:00:00.000Z and ContentDate/Start lt 2022-05-03T12:00:00.000Z&$expand=Attributes\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n1d42f2d3-2456-485f-a93e-92f08bdd5c51\nS1A_OPER_AUX_GNSSRD_POD__20220510T020122_V2022...\n/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S...\nNone\n\n\n1\n5c744d5c-c082-4a34-a181-81cde73cd25d\nS1B_OPER_AUX_GNSSRD_POD__20220510T023113_V2022...\n/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S...\nNone\n\n\n2\n30252d61-e607-5525-be8d-aad13defd2c8\nS1A_IW_SLC__1SDV_20220503T002004_20220503T0020...\n/eodata/Sentinel-1/SAR/SLC/2022/05/03/S1A_IW_S...\n{'type': 'Polygon', 'coordinates': [[[91.08319...\n\n\n\n\n\n\n\n\n\n\n\n\nExpand Assets\nExpand assets allows to list additional assets of products, including quicklooks:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-3' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'SL_2_FRP___')&$expand=Assets\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-3' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'SL_2_FRP___')&$expand=Assets\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n36d4637d-d424-4d70-85d5-d209e3e0164e\nS3B_SL_2_FRP____20201230T045236_20201230T04543...\n/eodata/Sentinel-3/SLSTR/SL_2_FRP___/2020/12/3...\n{'type': 'Polygon', 'coordinates': [[[-108.014...\n\n\n1\ndabbef2e-a7c2-4d59-a183-b89b9881010a\nS3B_SL_2_FRP____20201231T235616_20201231T23591...\n/eodata/Sentinel-3/SLSTR/SL_2_FRP___/2020/12/3...\n{'type': 'Polygon', 'coordinates': [[[134.188,...\n\n\n2\n4377868e-b20f-4d47-a384-55795c3b5fec\nS3B_SL_2_FRP____20201231T235316_20201231T23561...\n/eodata/Sentinel-3/SLSTR/SL_2_FRP___/2020/12/3...\n{'type': 'Polygon', 'coordinates': [[[137.866,...\n\n\n\n\n\n\n\n\n\n\n\n\nExpand Locations\nExpand Locations allows users to see full list of available products’ forms (compressed/uncompressed) and locations from which they can be downloaded:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_RAW__0S')&$orderby=ContentDate/Start desc&$top=10&$expand=Locations\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'IW_RAW__0S')&$orderby=ContentDate/Start desc&$top=10&$expand=Locations\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name','S3Path','GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nS3Path\nGeoFootprint\n\n\n\n\n0\n8a72bfa1-bda5-425d-b87b-73bc28f1f7c6\nS1A_IW_RAW__0SDV_20241203T114238_20241203T1142...\n/eodata/Sentinel-1/SAR/IW_RAW__0S/2024/12/03/S...\n{'type': 'Polygon', 'coordinates': [[[-88.9728...\n\n\n1\naf52c58c-4ece-4af3-adfc-3fe7ef3f848c\nS1A_IW_RAW__0SDV_20241203T114213_20241203T1142...\n/eodata/Sentinel-1/SAR/IW_RAW__0S/2024/12/03/S...\n{'type': 'Polygon', 'coordinates': [[[-88.7999...\n\n\n2\n243039c5-c937-4d01-8726-84b6fde1b659\nS1A_IW_RAW__0SDV_20241203T113855_20241203T1139...\n/eodata/Sentinel-1/SAR/IW_RAW__0S/2024/12/03/S...\n{'type': 'Polygon', 'coordinates': [[[-86.2812...\n\n\n\n\n\n\n\n\n\n\nThe information about data storage locations and storage forms (compressed/uncompressed) are specified under expand=Locations.\nTo access more information, please review Compressed products section within Sentinel-1 mission description.\n\n\nQuicklook\nFor example, a quicklook for product S3A_SL_2_FRP____20200821T042815_20200821T043115_20200822T092750_0179_062_033_2340_LN2_O_NT_004.SEN3 with ID of a quicklook f4a87522-dd81-4c40-856e-41d40510e3b6, can be downloaded with the request:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Assets(f4a87522-dd81-4c40-856e-41d40510e3b6)/$value\n\n\n\nDownload link is also available under DownloadLink parameter in Assets."
  },
  {
    "objectID": "APIs/OData.html#listing-product-nodes",
    "href": "APIs/OData.html#listing-product-nodes",
    "title": "OData",
    "section": "Listing product nodes",
    "text": "Listing product nodes\nProduct content can be listed by accessing the following URL patterns using Nodes:\nhttps://download.dataspace.copernicus.eu/odata/v1/Products(&lt;PRODUCT_UUID&gt;)/Nodes\nhttps://download.dataspace.copernicus.eu/odata/v1/Products(&lt;PRODUCT_UUID&gt;)/Nodes(&lt;NODE_NAME&gt;)/Nodes\nhttps://download.dataspace.copernicus.eu/odata/v1/Products(&lt;PRODUCT_UUID&gt;)/Nodes(&lt;NODE_NAME&gt;)/Nodes(&lt;NODE_NAME&gt;)/Nodes\nwhere:\n - is ID of the product obtained by search query,\n - is name of element inside product returned from previous listing response.\nOnly nodes that are folders can have their contents listed. Attempting to list Nodes for file results returning an empty list. The listing Nodes feature is available for both authorized and unauthorized users.\n\nExample nodes listing\nExample URL:\nhttps://download.dataspace.copernicus.eu/odata/v1/Products(db0c8ef3-8ec0-5185-a537-812dad3c58f8)/Nodes\nResponse:\n{\n   \"result\":[\n      {\n         \"Id\":\"S2A_MSIL1C_20180927T051221_N0206_R033_T42FXL_20180927T073143.SAFE\",\n         \"Name\":\"S2A_MSIL1C_20180927T051221_N0206_R033_T42FXL_20180927T073143.SAFE\",\n         \"ContentLength\":0,\n         \"ChildrenNumber\":9,\n         \"Nodes\":{\n            \"uri\":\"https://download.dataspace.copernicus.eu/odata/v1/Products(db0c8ef3-8ec0-5185-a537-812dad3c58f8)/Nodes(S2A_MSIL1C_20180927T051221_N0206_R033_T42FXL_20180927T073143.SAFE)/Nodes\"\n         }\n      }\n   ]\n}\nEvery Listed Node has “uri” field, which lists its children."
  },
  {
    "objectID": "APIs/OData.html#engineering-level-product-search",
    "href": "APIs/OData.html#engineering-level-product-search",
    "title": "OData",
    "section": "Engineering level product search",
    "text": "Engineering level product search\nIn order to search for engineering level products, you must perform authorization by providing access token to the query.\n\ncURL\n\n\ncurl --location \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq 'L1B_CA_SIR')\" --header \"Authorization: Bearer $ACCESS_TOKEN\""
  },
  {
    "objectID": "APIs/OData.html#product-download",
    "href": "APIs/OData.html#product-download",
    "title": "OData",
    "section": "Product Download",
    "text": "Product Download\nFor downloading products you need an authorization token as only authorized users are allowed to download data products.\nTo get the token you can use the following scripts:\n\ncURL\n\n\ncurl --location --request POST 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'grant_type=password' \\\n  --data-urlencode 'username=&lt;LOGIN&gt;' \\\n  --data-urlencode 'password=&lt;PASSWORD&gt;' \\\n  --data-urlencode 'client_id=cdse-public'\n\n\n\nor \n\ncURL\n\n\ncurl -d 'client_id=cdse-public' -d 'username=&lt;LOGIN&gt;' -d 'password=&lt;PASSWORD&gt;' -d 'grant_type=password' 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' | python3 -m json.tool | grep \"access_token\" | awk -F\\\" '{print $4}'\n\n\n\nAlong with the Access Token, you will be returned a Refresh Token, the latter is used to generate a new Access Token without the need to specify a Username or Password; this helps to make requests less vulnerable to your credentials being exposed.\nTo re-generate the Access Token from the Refresh Token, it can be done with the following request:\n\ncURL\n\n\ncurl --location --request POST 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'grant_type=refresh_token' \\\n  --data-urlencode 'refresh_token=&lt;REFRESH_TOKEN&gt;' \\\n  --data-urlencode 'client_id=cdse-public'\n\n\n\n\n\nOnce you have your token, you require a product Id which can be found in the response of the products search: https://catalogue.dataspace.copernicus.eu/odata/v1/Products\nFinally, you can download the product using this script:\n\ncURL\n\n\ncurl -H \"Authorization: Bearer $ACCESS_TOKEN\" 'https://catalogue.dataspace.copernicus.eu/odata/v1/Products(060882f4-0a34-5f14-8e25-6876e4470b0d)/$value' --location-trusted --output /tmp/product.zip\n\n\n\nor\n\nWget\n\n\nwget  --header \"Authorization: Bearer $ACCESS_TOKEN\" 'https://catalogue.dataspace.copernicus.eu/odata/v1/Products(db0c8ef3-8ec0-5185-a537-812dad3c58f8)/$value' -O example_odata.zip\n\n\n\n\nPython\n\n\nimport requests\n\n# Make sure access_token is defined\naccess_token = \"your_access_token\"  # Replace with your actual access token\n\nurl = f\"https://download.dataspace.copernicus.eu/odata/v1/Products(a5ab498a-7b2f-4043-ae2a-f95f457e7b3b)/$value\"\n\nheaders = {\"Authorization\": f\"Bearer {access_token}\"}\n\n# Create a session and update headers\nsession = requests.Session()\nsession.headers.update(headers)\n\n# Perform the GET request\nresponse = session.get(url, stream=True)\n\n# Check if the request was successful\nif response.status_code == 200:\n    with open(\"product.zip\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            if chunk:  # filter out keep-alive new chunks\n                file.write(chunk)\nelse:\n    print(f\"Failed to download file. Status code: {response.status_code}\")\n    print(response.text)\n\n\n\n\nCompressed Product Download\nFor downloading products in their native format (as zipped files) you need to proceed with the standard authorization as for Product Download.\nCurrently, users can access Sentinel-1 (RAW, GRD, SLC) data stored in native format and compressed for one month following their publication date within Data Space Catalogue.\nTo access more information about compressed products, please review Compressed products section within Sentinel-1 mission description.\nThe access to compressed products (stored in native format):\n\ncURL\n\n\ncurl -H \"Authorization: Bearer $ACCESS_TOKEN\" 'https://catalogue.dataspace.copernicus.eu/odata/v1/Products(002f0c9e-8a4c-465b-9e03-479475947630)/$zip' --location-trusted --output /tmp/product.zip\n\n\n\nor\n\nWget\n\n\nwget  --header \"Authorization: Bearer $ACCESS_TOKEN\" 'https://catalogue.dataspace.copernicus.eu/odata/v1/Products(002f0c9e-8a4c-465b-9e03-479475947630)/$zip' -O example_odata.zip\n\n\n\n\nPython\n\n\nimport requests\n\n# Make sure access_token is defined\naccess_token = \"your_access_token\"  # Replace with your actual access token\n\nurl = f\"https://download.dataspace.copernicus.eu/odata/v1/Products(002f0c9e-8a4c-465b-9e03-479475947630)/$zip\"\n\nheaders = {\"Authorization\": f\"Bearer {access_token}\"}\n\n# Create a session and update headers\nsession = requests.Session()\nsession.headers.update(headers)\n\n# Perform the GET request\nresponse = session.get(url, stream=True)\n\n# Check if the request was successful\nif response.status_code == 200:\n    with open(\"product.zip\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            if chunk:  # filter out keep-alive new chunks\n                file.write(chunk)\nelse:\n    print(f\"Failed to download file. Status code: {response.status_code}\")\n    print(response.text)"
  },
  {
    "objectID": "APIs/OData.html#odata-deletedproducts-endpoint",
    "href": "APIs/OData.html#odata-deletedproducts-endpoint",
    "title": "OData",
    "section": "OData DeletedProducts endpoint",
    "text": "OData DeletedProducts endpoint\nThe DeletedProducts OData endpoint allows users to access information about the deleted products in the Copernicus Data Space Ecosystem Catalog. This endpoint provides a convenient way to retrieve details about the products that have been deleted from the CDSE Catalog. By utilizing the supported operations and filtering options, users can efficiently access the required deleted products’ details. For the DeletedProducts OData endpoint, requests should be built the same way as for the OData Products endpoint OData Query structure with the change in the endpoint URL ‘Products’ to ‘DeletedProducts’.\n\nEndpoint URL\nThe DeletedProducts OData endpoint can be accessed using the following URL:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts\n\n\n\n\n\nQuery structure\nThe DeletedProducts OData endpoint supports the same searching options as a standard OData Products endpoint. For more information, please go to OData Query structure\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$orderby=DeletionDate&$top=20\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$orderby=DeletionDate&$top=20\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n81e390c0-4f9c-4a3c-8813-5bc6d7b48aa1\nS1A_EW_GRDM_1SSH_20220225T025010_20220225T0251...\nDuplicated product\n{'type': 'MultiPolygon', 'coordinates': [[[[-7...\n\n\n1\n1b797847-592f-4883-8cb0-e5fc9d875041\nS1A_EW_GRDM_1SSH_20220225T025010_20220225T0251...\nDuplicated product\n{'type': 'MultiPolygon', 'coordinates': [[[[-7...\n\n\n2\n90b6daea-016e-4277-9c2b-ed6e70158207\nS1B_IW_GRDH_1SDV_20180330T172340_20180330T1724...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[4.086337...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo accelerate the query performance, it is recommended to limit the query by specified dates, e.g.:\nDeletionDate gt 2022-05-03T00:00:00.000Z and DeletionDate lt 2023-05-03T00:00:00.000Z\n\n\n\n\nFilter option\nTo search for products by properties, a filter should be built as explained Filter option\nThe acceptable products’ properties for OData DeletedProducts endpoint are:\n\nName - search for a specific product by its exact name\nId - search for a specific product by its id\nDeletionDate - search by deletion date\nDeletionCause - search by deletion cause\nCollection/Name - search within a specific collection\nOriginDate - search by origin date\nContentDate/Start and ContentDate/End - search by sensing date\nFootprint - search by geographic criteria\nAttributes - search by product’s attributes\n\n\nQuery by name\nTo search for a deleted product by its exact name:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Name eq 'S2A_MSIL1C_20210404T112111_N0500_R037_T31VEG_20230209T101305.SAFE'\n\n\n\n\n\nQuery by Id\nTo search for a deleted product by its Id:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts(29008eb1-1a51-48a8-9aec-288b00f7debe)\n\n\n\n\n\nQuery by Deletion Date\nTo search for products deleted between two inclusive interval dates:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=DeletionDate ge 2023-04-26T00:00:00.000Z and DeletionDate le 2023-04-27T23:59:59.999Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=DeletionDate ge 2023-04-26T00:00:00.000Z and DeletionDate le 2023-04-27T23:59:59.999Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\nf1a5d39a-7600-4701-9e61-03347f63d526\nS1A_IW_GRDH_1SDV_20230224T230426_20230224T2304...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[10...\n\n\n1\n766c1738-3eba-4865-81e0-c5c51f5e29b6\nS1A_IW_GRDH_1SDV_20230224T231156_20230224T2312...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[99...\n\n\n2\n474adc52-3a3c-4cf4-b498-47c5e5e64d27\nS1A_IW_GRDH_1SDV_20230225T000647_20230225T0007...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[-9...\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Deletion Cause\nTo search for products deleted from specific reason:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=DeletionCause eq 'Duplicated product' or DeletionCause eq 'Corrupted product'\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=DeletionCause eq 'Duplicated product' or DeletionCause eq 'Corrupted product'\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n9737a58e-53ff-4c03-97b3-35b34d51d382\nS1A_EW_GRDH_1SDH_20141020T103857_20141020T1040...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-49.6468...\n\n\n1\n09bdd78d-b389-4628-87bd-e493a1536c7a\nS1A_EW_GRDH_1SDH_20141009T080342_20141009T0804...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-14.6560...\n\n\n2\na84e28ff-770d-40f8-bf5b-57f125a5e4b3\nS1A_EW_GRDH_1SDH_20141010T120113_20141010T1202...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-70.5086...\n\n\n\n\n\n\n\n\n\n\nAllowed values of the DelationCause parameter are:\n\nDuplicated product\nMissing checksum\nCorrupted product\nObsolete product or Other\n\n\n\nQuery by Collection of Products\nTo search for deleted products within a specific collection:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-2' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-09-30T23:59:59.999Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-2' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-09-30T23:59:59.999Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\nedeba5bf-ce73-486d-a3d8-ea094733691a\nS2A_MSIL2A_20200603T002611_N9999_R102_T01XDA_2...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[71...\n\n\n1\ndf0b407e-768e-4567-9f54-cb50690907e3\nS2A_MSIL1C_20210401T010651_N0500_R131_T55TDN_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[146.2119...\n\n\n2\n524ca315-7444-41b2-8e23-f3de06bc09bf\nS2A_MSIL1C_20210401T010651_N0500_R131_T55TCH_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[145.9033...\n\n\n\n\n\n\n\n\n\n\nFor available collections, please refer to Query Collection of Products. Also, please note that it is possible that none of the products have been deleted from the available collections.\n\n\nQuery by Sensing Date\nTo search for deleted products acquired between two dates:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=ContentDate/Start gt 2021-09-01T00:00:00.000Z and ContentDate/End lt 2021-09-01T00:05:00.000Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=ContentDate/Start gt 2021-09-01T00:00:00.000Z and ContentDate/End lt 2021-09-01T00:05:00.000Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n2b01765d-7d3c-5f8b-b69f-88d121c42c8b\nS1B_IW_GRDH_1SDV_20210901T000023_20210901T0000...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[99...\n\n\n1\n053f10da-3028-5ca6-9ccc-66c8c56fa439\nS1B_IW_GRDH_1SDV_20210901T000048_20210901T0001...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[98...\n\n\n2\n73699a9d-cc42-5469-88a9-ecd0a595e0d9\nS1B_IW_GRDH_1SDV_20210901T000113_20210901T0001...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[97...\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Geographic Criteria\nTo search for deleted products intersecting the specified polygon:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POLYGON ((-75.000244 -42.4521508418609, -75.000244 -43.4409190460844, -73.643585 -43.432873907284, -73.66513 -42.4443775132447, -75.000244 -42.4521508418609))') and ContentDate/Start gt 2021-01-01T00:00:00.000Z and ContentDate/End lt 2021-04-01T23:59:59.999Z\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=OData.CSC.Intersects(area=geography'SRID=4326;POLYGON ((-75.000244 -42.4521508418609, -75.000244 -43.4409190460844, -73.643585 -43.432873907284, -73.66513 -42.4443775132447, -75.000244 -42.4521508418609))') and ContentDate/Start gt 2021-01-01T00:00:00.000Z and ContentDate/End lt 2021-04-01T23:59:59.999Z\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n3a3e9685-ab43-41ad-9f4c-593302f4ba75\nS2A_MSIL2A_20210309T143731_N9999_R096_T18GWU_2...\nReprocessed product\n{'type': 'Polygon', 'coordinates': [[[-75.0002...\n\n\n1\nc677c050-c18a-4f87-97a0-989624ea0712\nS2A_MSIL1C_20210316T142731_N0500_R053_T18GWS_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-74.1524...\n\n\n2\na56d9773-eb3d-4be0-81d4-4a526ff4bbbc\nS2A_MSIL1C_20210309T143731_N0500_R096_T18GVU_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-75.9656...\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by attributes\nTo search for products by attributes, it is necessary to build a filter with the specified structure as defined Query Collection of Products.\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Attributes/OData.CSC.IntegerAttribute/any(att:att/Name eq 'orbitNumber' and att/OData.CSC.IntegerAttribute/Value eq 10844) and attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'orbitDirection' and att/OData.CSC.StringAttribute/Value eq 'ASCENDING')\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Attributes/OData.CSC.IntegerAttribute/any(att:att/Name eq 'orbitNumber' and att/OData.CSC.IntegerAttribute/Value eq 10844) and attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'orbitDirection' and att/OData.CSC.StringAttribute/Value eq 'ASCENDING')\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n9a595c3d-02ba-5ae4-811c-70f8ce642580\nS1B_EW_GRDH_1SDH_20180509T120906_20180509T1210...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-71.8987...\n\n\n1\nf67ce5c3-65ab-5cfe-9796-c62087dfef29\nS1B_EW_GRDM_1SDH_20180509T121206_20180509T1213...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-81.6544...\n\n\n2\nb847c833-ceb8-5e23-a54e-c80c5d4a5be2\nS1B_EW_GRDM_1SDH_20180509T130033_20180509T1301...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[96.64321...\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrderby option\nOrderby option works the same way as explained Orderby option.\n\n\n\n\n\n\nTip\n\n\n\nUsing the orderby option will exclude potential duplicates from the search results.\n\n\nFor OData DeletedProducts endpoint, acceptable arguments for this option are:\n\nContentDate/Start\nContentDate/End\nDeletionDate\n\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$orderby=DeletionDate desc\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$orderby=DeletionDate desc\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n17e63a3d-b68b-5286-9ed7-43f4260acb0a\nS1A_IW_GRDH_1SDV_20210830T060853_20210830T0609...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[1....\n\n\n1\nc59d69f3-59b3-5386-a4fc-ad8985d9ba37\nS1A_IW_GRDH_1SDV_20210829T233752_20210829T2338...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[96...\n\n\n2\nc1993b21-f1a0-5d57-a192-b35250fae50c\nS1A_IW_GRDH_1SDV_20210830T060418_20210830T0604...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[6....\n\n\n\n\n\n\n\n\n\n\n\n\nExpand option\nThe expand option enables users to see the full metadata of each returned result.\nThe acceptable argument for this option is:\n\nAttributes\n\nTo see the metadata of the results:\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$expand=Attributes\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$expand=Attributes\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n81e390c0-4f9c-4a3c-8813-5bc6d7b48aa1\nS1A_EW_GRDM_1SSH_20220225T025010_20220225T0251...\nDuplicated product\n{'type': 'MultiPolygon', 'coordinates': [[[[-7...\n\n\n1\n1b797847-592f-4883-8cb0-e5fc9d875041\nS1A_EW_GRDM_1SSH_20220225T025010_20220225T0251...\nDuplicated product\n{'type': 'MultiPolygon', 'coordinates': [[[[-7...\n\n\n2\n90b6daea-016e-4277-9c2b-ed6e70158207\nS1B_IW_GRDH_1SDV_20180330T172340_20180330T1724...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[4.086337...\n\n\n\n\n\n\n\n\n\n\n\n\nSkip option\nSkip option can be used as defined Skip option.\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-2' and ContentDate/Start ge 2021-04-01T00:00:00.000Z and ContentDate/Start le 2021-04-30T23:59:59.999Z&$skip=30\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-2' and ContentDate/Start ge 2021-04-01T00:00:00.000Z and ContentDate/Start le 2021-04-30T23:59:59.999Z&$skip=30\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n40fb185e-7dc9-4bfc-8e18-8796033514a6\nS2B_MSIL2A_20210401T001149_N0500_R059_T08XNQ_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-135.001...\n\n\n1\na425a14e-7534-4e46-a384-7ffd5dab8f97\nS2B_MSIL1C_20210401T001149_N0500_R059_T10XDR_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-129.378...\n\n\n2\n46a3452b-bd33-4fc8-8d70-0eed66d2d486\nS2B_MSIL1C_20210401T001149_N0500_R059_T07XEM_2...\nCorrupted product\n{'type': 'Polygon', 'coordinates': [[[-141.001...\n\n\n\n\n\n\n\n\n\n\n\n\nTop option\nTop option can be used as defined Top option.\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start ge 2021-09-01T00:00:00.000Z and ContentDate/Start le 2021-09-30T23:59:59.999Z&$top=40\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start ge 2021-09-01T00:00:00.000Z and ContentDate/Start le 2021-09-30T23:59:59.999Z&$top=40\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n2b01765d-7d3c-5f8b-b69f-88d121c42c8b\nS1B_IW_GRDH_1SDV_20210901T000023_20210901T0000...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[99...\n\n\n1\n053f10da-3028-5ca6-9ccc-66c8c56fa439\nS1B_IW_GRDH_1SDV_20210901T000048_20210901T0001...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[98...\n\n\n2\n73699a9d-cc42-5469-88a9-ecd0a595e0d9\nS1B_IW_GRDH_1SDV_20210901T000113_20210901T0001...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[97...\n\n\n\n\n\n\n\n\n\n\n\n\nCount option\nCount option can be used as defined Count option\n\nHTTPS RequestPython\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$orderby=DeletionDate desc&$count=True\n\n\n\n\nCode\njson = requests.get(\"https://catalogue.dataspace.copernicus.eu/odata/v1/DeletedProducts?$filter=Collection/Name eq 'SENTINEL-1' and DeletionDate gt 2023-04-01T00:00:00.000Z and DeletionDate lt 2023-05-30T23:59:59.999Z&$orderby=DeletionDate desc&$count=True\").json()\ndf = pd.DataFrame.from_dict(json['value'])\n\n# Print only specific columns\ncolumns_to_print = ['Id', 'Name', 'DeletionCause', 'GeoFootprint']  \ndf[columns_to_print].head(3)\n\n\n\n\n\n\n\n\n\nId\nName\nDeletionCause\nGeoFootprint\n\n\n\n\n0\n17e63a3d-b68b-5286-9ed7-43f4260acb0a\nS1A_IW_GRDH_1SDV_20210830T060853_20210830T0609...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[1....\n\n\n1\nc59d69f3-59b3-5386-a4fc-ad8985d9ba37\nS1A_IW_GRDH_1SDV_20210829T233752_20210829T2338...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[96...\n\n\n2\nc1993b21-f1a0-5d57-a192-b35250fae50c\nS1A_IW_GRDH_1SDV_20210830T060418_20210830T0604...\nCorrupted product\n{'type': 'MultiPolygon', 'coordinates': [[[[6...."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/RateLimiting.html",
    "href": "APIs/SentinelHub/Overview/RateLimiting.html",
    "title": "Rate limiting",
    "section": "",
    "text": "In order to ensure the stability of the system and to guarantee good performance for all users we have to protect it against deliberate attacks or runaway scripts. Every request which reaches our system will therefore go through a rate limiting filter. As long as the agreed upon rate limiting policies are conformed to, responses by our services shall be delivered in timely fashion. On the other hand, requests which violate any of the agreed upon policies will be responded to with a HTTP 429 response.\nWe are able to adjust rate limit policies for each individual user so do contact our Support for specific requirements.\n\nRate limiting policy\nA rate limiting policy defines either how many processing units or HTTP requests can be used per given time period or in total. Both processing units and requests are rate limited and the level of rate limiting depends on your account (see pricing plans).\nAn API is usually protected by multiple rate limiting policies. For example, Processing API has both a processing unit and request rate limiting policies. To conform to the rate limiter, all rate limiting policies have to be satisfied. For example, lets say you have a policy of 100 requests per minute and a policy of 100 processing units per minute. By issuing 100 requests from each every request is valued at 2 processing units in one minute, only 50 requests will pass, all others will fail with HTTP status 429. Even though you have a limit of 100 requests per minute, 50 requests would violate the 100 processing units per minute policy and thus be rate limited.\nUnused processing units and requests do not accumulate. If you have a rate limit policy with 100 request per minute and you don't consume any request for a longer period you are still able to do just 100 requests within the next minute.\n\n\nRate limiting ramp up\nFor all SH subscriptions, the rate limiting is configured also on a \"per minute\" basis (i.e. 600 requests per minute and 1000 processing units per minute for the Enterprise S subscription). For optimal performance, it is best to spread this number of requests over a whole minute, i.e. to send one request every 0.1 seconds. As we understand that this might be difficult to do, we allow some variation from this optimum. However, if you will burst the full number of requests at once, some of them will be rate limited. For such requests, we recommend that you simply resend them - the process should reach the optimal level in a few minutes.\n\n\nResponse Headers\nAll requests going through rate limiting include headers to allow for programmatic adaption to Rate Limiting:\n\nRetry-After: Time in milliseconds until the next request is available.\n\n\nExample:\n\nResponse code and message\n{\n  \"status\": 429,\n  \"reason\": \"Too Many Requests\",\n  \"message\": \"You have exceeded your rate limit\",\n  \"code\": \"RATE_LIMIT_EXCEEDED\"\n}\n\n\nResponse header\n{\n  \"Date\": \"Tue, 16 Aug 2022 13:15:02 GMT\",\n  \"retry-after\": \"3398\",\n  ...\n}\nThe HTTP status code in this example is 429 meaning that the request was rate limited. The value of the Retry-After header is 3398, which means that next request will be available in 3398 ms.\n\n\n\n\nTry it out\nWe have set up a test user with two very restrictive rate limiting policies:\n\n10 requests per minute and\n10 processing units per minute\n\nYou can use its instance (for OGC requests) or Oauth client credentials (for API requests) to test how our rate limiting works and for integration purposes.\nAn example of a WMS request using the test user's instance.\nThe test user's Oauth client credentials below can be used to get an access token, which can then be included in header of a process API requests (for examples of requests see here):\n\nClient id: sh-110136b6-2837-4160-bed8-33e4ddb27e40 Client secret: oZWELVkpljc8bUjtIOnHh8mCBKSfi0ll\n\nNote that many people may be using it at the same moment so there is a chance that it will be over the limit more or less all the time. Its purpose is to evaluate response headers anyway.\n\n\nTips to Avoid Being Rate Limited\n\nCaching\nStore API responses that you expect to use a lot. For example, don’t call same requests on every page load but try to store responses in local storage.\n\n\nRequest only what you need\nBe defensive in fetching and try to request only the data that you actually need.\n\n\nExponential backoff\nWhen your limits have been exceeded, we recommend implementing retries with a exponential backoff. An exponential backoff means that you wait for exponentially longer intervals between each retry of a single failing request."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ErrorHandling.html",
    "href": "APIs/SentinelHub/Overview/ErrorHandling.html",
    "title": "Error handling",
    "section": "",
    "text": "Whenever an error occurs, whether it be the fault of the user or an internal system, an error object will be returned. HTTP response codes of 4xx suggest a bad request. If you receive a 4xx response, we recommend reviewing the API docs for more context to help you troubleshoot. 5xx errors suggest a problem on Sentinel Hub's end, so if you receive a 5xx error, please contact support."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html",
    "href": "APIs/SentinelHub/BatchV2.html",
    "title": "Batch Processing V2 API",
    "section": "",
    "text": "The BatchV2 API is only available for users with Copernicus Service accounts. Please refer to our FAQ on account typology change and Submit A Request to our Copernicus Data Space Ecosystem Support Team to request your Copernicus Service account accordingly."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#migration-guide",
    "href": "APIs/SentinelHub/BatchV2.html#migration-guide",
    "title": "Batch Processing V2 API",
    "section": "Migration guide",
    "text": "Migration guide\nIf you're interested on how to migrate from Batch Processing API to BatchV2, please read the following guide:\nBatchV2 Migration Guide"
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#overview",
    "href": "APIs/SentinelHub/BatchV2.html#overview",
    "title": "Batch Processing V2 API",
    "section": "Overview",
    "text": "Overview\nBatchV2 Processing API (or shortly \"BatchV2 API\") enables you to request data for large areas and/or longer time periods for any Sentinel Hub supported collection, including BYOC (bring your own data). It is an asynchronous REST service, meaning data won't be returned immediately but delivered to your specified object storage instead.\n\nWorkflow\nThe Batch V2 Processing API comes with the set of REST APIs which support the execution of various workflows. The diagram below shows all possible statuses of a batch task:\n\nCREATED\nANALYSING\nANALYSIS_DONE\nPROCESSING\nDONE\nFAILED\nSTOPPED\n\nand user's actions:\n\nANALYSE\nSTART\nSTOP\n\nwhich trigger transitions among them.\n👤 START/ANALYSE👤 START👤 STOP👤 STOP👤 STARTCREATEDANALYSINGFAILEDANALYSIS_DONEPROCESSINGSTOPPEDDONE\nThe workflow starts when a user posts a new batch request. In this step the system:\n\ncreates a new batch task with the status CREATED,\nvalidates the user's input (except the evalscript),\nensures the user's account has at least 1000 PUs,\nuploads a JSON of the original request to the user's bucket,\nand returns the overview of the created task.\n\nThe user can then decide to either request an additional analysis of the task or start the processing. When an additional analysis is requested:\n\nthe status of the task changes to ANALYSING,\nthe evalscript is validated,\na feature manifest file is uploaded to the user's bucket,\nafter the analysis is finished, the status of the task changes to ANALYSIS_DONE.\n\nIf the user chooses to directly start processing, the system still executes the analysis but when the analysis is done it automatically proceeds with processing. This is not explicitly shown in the diagram in order to keep it simple.\nWhen the user starts the processing:\n\nthe status of the task changes to PROCESSING (this may take a while, depending on the load on the service),\nthe processing starts,\nan execution database is periodically uploaded to the user's bucket,\nspent processing units are billed periodically.\n\nWhen the processing is finished, the status of the task changes to DONE.\n\nStopping the request\nA task might be stopped for the following reasons:\n\nit's requested by a user (user action),\nuser is out of processing units,\nsomething is wrong with the processing of the task (e.g. the system is not able to process the data).\n\nA user may stop the request in following states: ANALYSING, ANALYSIS_DONE and PROCESSING. However:\n\nif the status is ANALYSING, the analysis will complete,\nif the status is PROCESSING, all features (polygons) that have been processed or are being processed at that moment are charged for,\nuser is not allowed to restart the task in the next 30 minutes."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#input-features",
    "href": "APIs/SentinelHub/BatchV2.html#input-features",
    "title": "Batch Processing V2 API",
    "section": "Input features",
    "text": "Input features\nBatchV2 API supports two ways of specifying the input features of your batch task:\n\nPre-defined Tiling Grid\nUser-defined GeoPackage\n\n\n1. Tiling grid\nFor more effective processing we divide the area of interest into tiles and process each tile separately. While process API uses grids which come together with each datasource for processing of the data, the batch API uses one of the predefined tiling grids. The predefined tiling grids 0-2 are based on the Sentinel-2 tiling in WGS84/UTM projection with some adjustments:\n\nThe width and height of tiles in the original Sentinel 2 grid is 100 km while the width and height of tiles in our grids are given in the table below.\nAll redundant tiles (i.e. fully overlapped tiles) are removed.\n\nAll available tiling grids can be requested with (NOTE: To run this example you need to first create an OAuth client as is explained here):\nurl = \"https://sh.dataspace.copernicus.eu/api/v2/batch/tilinggrids/\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\nThis will return the list of available grids and information about tile size and available resolutions for each grid. Currently, available grids are:\n\n\n\nname\nid\ntile size\nresolutions\ncoverage\noutput CRS\ndownload the grid [zip with shp file] **\n\n\n\n\nUTM 20km grid\n0\n20040 m\n10 m, 20 m, 30m*, 60 m\nWorld, latitudes from -80.7° to 80.7°\nUTM\nUTM 20km grid\n\n\nUTM 10km grid\n1\n10000 m\n10 m, 20 m\nWorld, latitudes from -80.6° to 80.6°\nUTM\nUTM 10km grid\n\n\nUTM 100km grid\n2\n100080 m\n30m*, 60 m, 120 m, 240 m, 360 m\nWorld, latitudes from -81° to 81°\nUTM\nUTM 100km grid\n\n\nWGS84 1 degree grid\n3\n1 °\n0.0001°, 0.0002°\nWorld, all latitudes\nWGS84\nWGS84 1 degree grid\n\n\nLAEA 100km grid\n6\n100000 m\n40 m, 50 m, 100 m\nEurope, including Turkey, Iceland, Svalbald, Azores, and Canary Islands\nEPSG:3035\nLAEA 100km grid\n\n\nLAEA 20km grid\n7\n20000 m\n10 m, 20 m\nEurope, including Turkey, Iceland, Svalbald, Azores, and Canary Islands\nEPSG:3035\nLAEA 20km grid\n\n\n\n** The geometries of the tiles are reprojected to WGS84 for download. Because of this and other reasons the geometries of the output rasters may differ from the tile geometries provided here.\nTo use 20km grid with 60 m resolution, for example, specify id and resolution parameters of the tilingGrid object when creating a new batch request (see an example of full request) as:\n{\n  ...\n  \"tilingGrid\": {\n    \"id\": 0,\n    \"resolution\": 60.0\n  },\n  ...\n}\n\n\n2. GeoPackage\nIn addition to the predefined tiling grids, BatchV2 API now also support user-defined features through GeoPackages. This allows you to specify features of any shape as long as the underlying geometry is a POLYGON or MULTIPOLYGON in an EPSG compliant CRS listed here. The GeoPackage can also have multiple layers, offering more flexibility in specifying features in multiple CRS.\nThe GeoPackage must adhere to the GeoPackage spec and contain at least one feature table with any name. The table must include a column that holds the geometry data. This column can be named arbitrarily, but it must be listed as the geometry column in the gpkg_geometry_columns table. The table schema should include the following columns:\n\n\n\nColumn\nType\nExample\n\n\n\n\nid\nINTEGER (UNIQUE)\n1000\n\n\nidentifier\nTEXT (UNIQUE)\nFEATURE_NAME\n\n\ngeometry\nPOLYGON or MULTIPOLYGON\nFeature geometry representation in GeoPackage WKB format\n\n\nwidth\nINTEGER\n1000\n\n\nheight\nINTEGER\n1000\n\n\nresolution\nREAL\n0.005\n\n\n\n\nCaveats\n\nYou must specify either both width and height, or alternatively, specify resolution. If both values are provided, width and height will be used, and resolution will be ignored.\nThe feature table must use a CRS that is EPSG compliant.\nBoth id and identifier values must not be null and unique across all feature tables.\nThere can be a maximum of 700,000 features in the GeoPackage.\n\nAn example of a batch task with GeoPackage input is available here."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#processing-results",
    "href": "APIs/SentinelHub/BatchV2.html#processing-results",
    "title": "Batch Processing V2 API",
    "section": "Processing results",
    "text": "Processing results\nThe outputs of a batch task will be stored to your object storage in either:\n\nGeoTIFF (and JSON for metadata) or\nZarr format\n\n\n1. GeoTIFF output format\nThe GeoTIFF format will be used if your request includes the output.type parameter set to raster, along with other relevant parameters specified in the BatchV2 API reference. An example of a batch task with GeoTIFF output is available here.\nBy default, the results will be organized in sub-folders where one sub-folder will be created for each feature. Each sub-folder might contain one or more images depending on how many outputs were defined in the evalscript of the request. For example:\nYou can also customize the sub-folder structure and file naming as described in the delivery parameter under output in BatchV2 API reference.\nYou can choose to return your GeoTIFF files as Cloud Optimized GeoTIFF (COG), by setting the cogOutput parameter under output in your request as true. Several advanced COG options can be selected as well - read about the parameter in BatchV2 API reference.\nThe output projection depends on the selected input, either tiling grid or GeoPackage:\n\nIf the input is a tiling grid, the results of batch processing will be in the projection of the selected tiling grid. For UTM-based grids, each part of the AOI (area of interest) is delivered in the UTM zone with which it intersects. In other words, in case your AOI intersects with more UTM zones, the results will be delivered as tiles in different UTM zones (and thus different CRSs).\nIf the input is a GeoPackage, the results will be in the same CRS as the input feature's CRS.\n\n\n\n2. Zarr output format\nThe Zarr format will be used if your request includes the output.type parameter set to zarr, along with other relevant parameters specified in the BatchV2 API reference. An example of a batch request with Zarr output is available here. Your request must only have one band per output and the application/json format in responses is not supported.\nThe outputs of batch processing will be stored as a single Zarr group containing one data array for each evalscript output and multiple coordinate arrays. The output will be stored in a subfolder named after the requestId that you pass to the API in the delivery.s3.url parameter under output."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#ingesting-results-into-byoc",
    "href": "APIs/SentinelHub/BatchV2.html#ingesting-results-into-byoc",
    "title": "Batch Processing V2 API",
    "section": "Ingesting results into BYOC",
    "text": "Ingesting results into BYOC\n\nPurpose\nEnables automatic ingestion of processing results into a BYOC collection, allowing you to:\n\nAccess data with Processing API, by using the collection ID\nCreate a configuration with custom layers\nMake OGC requests to a configuration\nView data in EO Browser\n\nIn order to enable this functionality, user needs to specify either id of an existing BYOC collection (collectionId) or set createCollection = true.\n{\n  ...\n  \"output\": {\n    ...\n    \"createCollection\": true,\n    \"collectionId\": \"&lt;byoc-collection-id&gt;\",\n    ...\n  },\n  ...\n}\nIf collectionId is provided, the existing collection will be used for data ingestion.\nIf createCollection is set to true and collectionId is not provided, a new BYOC collection will be created automatically and the collection bands will be set according to the request output responses definitions.\nRegardless of whether the user specifies an existing collection or requests a new one, processed data will still be uploaded to the users bucket, where they will be available for download and analysis.\nWhen creating a new batch collection, one has to be careful to:\n\nMake sure that cogOutput=true and that the output format is a image/tiff\nIf an existing BYOC collection is used, make sure that identifier and sampleType from the output definition(s) match the name and the type of the BYOC band(s). Single band and multi-band outputs are supported.\nIf multi-band output is used in the request, the additionally generated bands will be named using a numerical suffix in ascending order (e.g. 2, ... 99). For example, if the output: { id: \"result\", bands: 3 } is used in the evalscript setup function, the produced BYOC bands will be named: result for band 1, result2 for band 2 and result3 for band 3. Make sure that no other output band has any of these automatically generated names, as this will throw an error during the analysis phase. The output: [{ id: \"result\", bands: 3 },{ id: \"result2\", bands: 1 }] will throw an exception.\nKeep sampleType in mind, as the values the evalscript returns when creating a collection will be the values available when making a request to access it.\n\n\n\nMandatory bucket settings\nRegardless of the credentials provided on the request, your bucket needs to be configured to allow full access to Sentinel Hub. To do this, update your bucket policy to include the following statement (don't forget to replace &lt;bucket_name&gt; with your actual bucket name):\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Sentinel Hub permissions\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::ddf4c98b5e6647f0a246f0624c8341d9:root\"\n      },\n      \"Action\": [\n        \"s3:*\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::&lt;bucket_name&gt;\",\n        \"arn:aws:s3:::&lt;bucket_name&gt;/*\"\n      ]\n    }\n  ]\n}"
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#feature-manifest",
    "href": "APIs/SentinelHub/BatchV2.html#feature-manifest",
    "title": "Batch Processing V2 API",
    "section": "Feature Manifest",
    "text": "Feature Manifest\n\nPurpose\n\nProvides a detailed overview of features scheduled for processing during the PROCESSING step.\nEnables users to verify feature information and corresponding output paths prior to processing.\n\n\n\nKey Information\n\nFile Type: GeoPackage\nFile Name: featureManifest-&lt;requestId&gt;.gpkg\nLocation: Root folder of the specified output delivery path\nStructure:\n\nMay contain multiple feature tables, one per distinct CRS used by the features.\nTable names follow the format feature_&lt;crs-id&gt; (e.g. feature_4326).\n\n\nDuring task analysis, the system will upload a file to the user's bucket called the featureManifest-&lt;requestId&gt;.gpkg. This file is a GeoPackage that contains basic information about the features that will be processed during the PROCESSING step. It is intended to be used by users to check the features that will be processed and their corresponding output paths.\nIf the output type is set to raster, the output paths will be the paths to the GeoTIFF files. If the output type is zarr, the output paths will just be the root of the output folder.\nThe database may contain multiple feature tables, one feature table for each CRS of all features. The tables will be named feature_&lt;crs-id&gt;, e.g. feature_4326. The schema of feature tables inside the database is currently the following:\n\n\n\nName\nType\nDescription\n\n\n\n\nfid\nINTEGER\nAuto-incrementing ID\n\n\nid\nINTEGER\nNumerical ID of the feature\n\n\nidentifier\nTEXT\nTextual ID of the feature\n\n\npath\nTEXT\nThe object storage path URI where the feature will be uploaded to\n\n\ngeometry\nGEOMETRY\nFeature geometry representation in GeoPackage WKB format"
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#execution-database",
    "href": "APIs/SentinelHub/BatchV2.html#execution-database",
    "title": "Batch Processing V2 API",
    "section": "Execution database",
    "text": "Execution database\n\nPurpose\nThe Execution Database serves as a monitoring tool for tracking the progress of feature execution within a specific task. It provides users with insight into the status of each feature being processed.\n\n\nKey Information\n\nFile Type: SQLite\nFile Name: execution-&lt;requestId&gt;.sqlite\nLocation: Root folder of specified output delivery path\nStructure:\n\nContains a single table called features.\n\n\nYou can monitor the execution of your features for a specific task by checking the SQLite database that is uploaded to your bucket. The database contains the name and status of each feature. The database is updated periodically during the execution of the task.\nThe database can be found in your bucket in the root output folder and is named execution-&lt;requestId&gt;.sqlite.\nThe schema of the features table is currently the following:\n\n\n\nName\nType\nDescription\n\n\n\n\nid\nINTEGER\nNumerical ID of the feature\n\n\nname\nTEXT\nTextual ID of the feature\n\n\nstatus\nTEXT\nStatus of the feature (PENDING, DONE, FAILED, etc.)\n\n\nerror\nTEXT\nError message in case processing has failed\n\n\ndelivered\nBOOLEAN\nTrue if output delivered to delivery bucket, otherwise False"
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#bucket-settings-and-access",
    "href": "APIs/SentinelHub/BatchV2.html#bucket-settings-and-access",
    "title": "Batch Processing V2 API",
    "section": "Bucket settings and access",
    "text": "Bucket settings and access\nThe results will be delivered in your own bucket hosted at Copernicus Data Space Ecosystem. To access your bucket accessKey and secretAccessKey pair have to bo provided in your request.\ns3 = {\n    \"url\": \"s3://&lt;your-bucket&gt;/&lt;path&gt;\",\n    \"accessKey\": \"&lt;your-bucket-access-key&gt;\",\n    \"secretAccessKey\": \"&lt;your-bucket-secret-access-key&gt;\"\n}\nIf you do not yet have a bucket at Copernicus Data Space Ecosystem, please follow these steps to get one."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2.html#examples",
    "href": "APIs/SentinelHub/BatchV2.html#examples",
    "title": "Batch Processing V2 API",
    "section": "Examples",
    "text": "Examples\nExample of Batch Processing Workflow"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html",
    "href": "APIs/SentinelHub/Evalscript/Functions.html",
    "title": "Utility Functions",
    "section": "",
    "text": "Visualizers are JavaScript classes with a method process which evaluates the representation value for a pixel from pixel’s band values.\n\n\nSets the color from a discrete color map.\n\n\n\nvalColPairs Array&lt;[number, number]&gt;\n\n\n\n\nconst map = [\n  [200, 0xff0000],\n  [300, 0x0000ff ],\n];\n\nconst visualizer = new ColorMapVisualizer(map);\nvisualizer.process(199); // returns [ 1, 0, 0 ]\nvisualizer.process(200); // returns [ 1, 0, 0 ]\nvisualizer.process(250); // returns [ 1, 0, 0 ]\nvisualizer.process(299); // returns [ 1, 0, 0 ]\nvisualizer.process(300); // returns [ 0, 0, 1 ]\n\n\n\nReturns interpolated color for value.\n\n\n\nval number\n\nReturns [number, number, number] normalized RGB triplet.\n\n\n\n\nCreates ColorMapVisualizer with following valColPairs\n[\n  [-1.0, 0x000000],\n  [-0.2, 0xff0000],\n  [-0.1, 0x9a0000],\n  [0.0, 0x660000],\n  [0.1, 0xffff33],\n  [0.2, 0xcccc33],\n  [0.3, 0x666600],\n  [0.4, 0x33ffff],\n  [0.5, 0x33cccc],\n  [0.6, 0x006666],\n  [0.7, 0x33ff33],\n  [0.8, 0x33cc33],\n  [0.9, 0x006600]\n]\n\n\n\n\nProvides a way to map values to colors. This is done by defining a number of values and colors, with values between the defined input values mapping to their interpolated colors, respectively. Colors may be defined as hex color codes or a normalized (between 0 and 1) array representing RGB.\n\n\n\nramps\n\nminVal number Optional override of the minimum value in ramps. Other values will be adjusted linearly.\nmaxVal number Optional override of the maximum value in ramps. Other values will be adjusted linearly.\n\n\n\n\nconst ramps = [\n  [200, 0xFF0000],\n  [300, 0x0000FF]\n];\nor\nconst ramps = [\n  [200, [1,0,0]],\n  [300, [0,0,1]]\n];\n\nconst visualizer = new ColorRampVisualizer(ramps);\nvisualizer.process(199); // [ 1, 0, 0 ]\nvisualizer.process(200); // [ 1, 0, 0 ]\nvisualizer.process(250); // [ 0.5019607843137255, 0, 0.5019607843137255 ]\nvisualizer.process(299); // [ 0.011764705882352941, 0, 0.9882352941176471 ]\nvisualizer.process(300); // [ 0, 1, 0 ]\n\n\n\nReturns a new ColorRampVisualizer which is the inverse of the current one. This means the color scale goes in the opposite direction.\n\n\n\nReturns interpolated color for value.\n\n\n\nvalue number\n\nReturns [number, number, number] normalized RGB triplet.\n\n\n\n\nCreates ColorRampVisualizer with valColPairs redTemperature\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorRampVisualizer.createRedTemperature(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0.43137254901960786, 0, 0 ]\nvisualizer.process(0.5); // returns [ 0.7176470588235294, 0.047058823529411764, 0 ]\nvisualizer.process(0.8); // returns [ 1, 0.6196078431372549, 0.2 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorRampVisualizer\n\n\n\n\nCreates ColorRampVisualizer with valColPairs greenWhite\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorRampVisualizer.createWhiteGreen(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0, 0.2980392156862745, 0 ]\nvisualizer.process(0.5); // returns [ 0.16862745098039217, 0.5019607843137255, 0 ]\nvisualizer.process(0.8); // returns [ 0.6666666666666666, 0.8, 0.3333333333333333 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorRampVisualizer\n\n\n\n\nCreates ColorRampVisualizer with valColPairs blueRed\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorRampVisualizer.createBlueRed(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0.5019607843137255 ]\nvisualizer.process(0.3); // returns [ 0, 0.7019607843137254, 1 ]\nvisualizer.process(0.5); // returns [ 0.5019607843137255, 1, 0.5019607843137255 ]\nvisualizer.process(0.8); // returns [ 1, 0.2980392156862745, 0 ]\nvisualizer.process(1.0); // returns [ 0.5019607843137255, 0, 0 ]\nReturns ColorRampVisualizer\n\n\n\n\nCreates ColorRampVisualizer with valColPairs oceanColor\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\nReturns ColorRampVisualizer\n\n\n\n\n\n\n\n\nvalColPairs Array&lt;[number, number]&gt;\nminVal number (optional, default 0.0)\nmaxVal number (optional, default 1.0)\n\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nInterpolates a color based on interval.\n\n\n\n\nReturns interpolated color for value.\n\n\n\nval number\n\nReturns [number, number, number] normalized RGB triplet.\n\n\n\n\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorGradientVisualizer.createRedTemperature(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0.43137254901960786, 0, 0 ]\nvisualizer.process(0.5); // returns [ 0.7176470588235294, 0.047058823529411764, 0 ]\nvisualizer.process(0.8); // returns [ 1, 0.6196078431372549, 0.2 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorGradientVisualizer\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nCreates ColorGradientVisualizer with valColPairs redTemperature\n\n\n\n\n\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorGradientVisualizer.createWhiteGreen(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0, 0.2980392156862745, 0 ]\nvisualizer.process(0.5); // returns [ 0.16862745098039217, 0.5019607843137255, 0 ]\nvisualizer.process(0.8); // returns [ 0.6666666666666666, 0.8, 0.3333333333333333 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorGradientVisualizer\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nCreates ColorGradientVisualizer with valColPairs greenWhite\n\n\n\n\n\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorGradientVisualizer.createBlueRed(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0.5019607843137255 ]\nvisualizer.process(0.3); // returns [ 0, 0.7019607843137254, 1 ]\nvisualizer.process(0.5); // returns [ 0.5019607843137255, 1, 0.5019607843137255 ]\nvisualizer.process(0.8); // returns [ 1, 0.2980392156862745, 0 ]\nvisualizer.process(1.0); // returns [ 0.5019607843137255, 0, 0 ]\nReturns ColorGradientVisualizer\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nCreates ColorGradientVisualizer with valColPairs blueRed\n\n\n\n\n\n\nThis is a piecewise linear function which compresses highlights. The minValue and maxValue will be mapped inside the interval [ 0, 1 ]. However, if maxValue lies in (0, 1) a second function which increases much more slowly will be used to further map the values which are mapped to 0.92 and above (see the figure below). This increases the visualized dynamic range while keeping most of the interval of interest linear. Useful, for example, for true color, with a maxValue of 0.4 to still keep some detail in clouds.\n\n\n\nPiecewise linear function which compresses highlights\n\n\n\n\n\nminValue number the value which will be mapped to 0. All values smaller than minValue will also be mapped to 0. (optional, default 0.0)\nmaxValue number the value which controls the position of the boundary point between both linear functions. It will be mapped to approx. 0.9259, while values greater than or equal to (2*maxValue - minValue) will be mapped to 1 (see the figure above). (optional, default 1.0)\ngain (optional, default 1.0)\noffset (optional, default 0.0)\ngamma (optional, default 1.0)\n\n\n\n\nconst visualizer = new HighlightCompressVisualizer(0.1, 0.4)\n\nvisualizer.process(0); // will return 0\nvisualizer.process(0.1); // will return 0\nvisualizer.process(0.25); // will return 0.5\nvisualizer.process(0.376); // will return 0.92. Note: 0.376 = minValue + 0.92*(maxValue - minValue)\nvisualizer.process(0.4); // will return 0.9259\nvisualizer.process(0.7); // will return 1 Note: 0.7 is the smallest value mapped to 1.\nvisualizer.process(1.1); // will return 1\n\n\n\nReturns mapped value.\n\n\n\nval number the input value to be mapped.\ni number the index of val. This is EO Browser specific.\n\nReturns [number] mapped value."
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#visualizers",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#visualizers",
    "title": "Utility Functions",
    "section": "",
    "text": "Visualizers are JavaScript classes with a method process which evaluates the representation value for a pixel from pixel’s band values.\n\n\nSets the color from a discrete color map.\n\n\n\nvalColPairs Array&lt;[number, number]&gt;\n\n\n\n\nconst map = [\n  [200, 0xff0000],\n  [300, 0x0000ff ],\n];\n\nconst visualizer = new ColorMapVisualizer(map);\nvisualizer.process(199); // returns [ 1, 0, 0 ]\nvisualizer.process(200); // returns [ 1, 0, 0 ]\nvisualizer.process(250); // returns [ 1, 0, 0 ]\nvisualizer.process(299); // returns [ 1, 0, 0 ]\nvisualizer.process(300); // returns [ 0, 0, 1 ]\n\n\n\nReturns interpolated color for value.\n\n\n\nval number\n\nReturns [number, number, number] normalized RGB triplet.\n\n\n\n\nCreates ColorMapVisualizer with following valColPairs\n[\n  [-1.0, 0x000000],\n  [-0.2, 0xff0000],\n  [-0.1, 0x9a0000],\n  [0.0, 0x660000],\n  [0.1, 0xffff33],\n  [0.2, 0xcccc33],\n  [0.3, 0x666600],\n  [0.4, 0x33ffff],\n  [0.5, 0x33cccc],\n  [0.6, 0x006666],\n  [0.7, 0x33ff33],\n  [0.8, 0x33cc33],\n  [0.9, 0x006600]\n]\n\n\n\n\nProvides a way to map values to colors. This is done by defining a number of values and colors, with values between the defined input values mapping to their interpolated colors, respectively. Colors may be defined as hex color codes or a normalized (between 0 and 1) array representing RGB.\n\n\n\nramps\n\nminVal number Optional override of the minimum value in ramps. Other values will be adjusted linearly.\nmaxVal number Optional override of the maximum value in ramps. Other values will be adjusted linearly.\n\n\n\n\nconst ramps = [\n  [200, 0xFF0000],\n  [300, 0x0000FF]\n];\nor\nconst ramps = [\n  [200, [1,0,0]],\n  [300, [0,0,1]]\n];\n\nconst visualizer = new ColorRampVisualizer(ramps);\nvisualizer.process(199); // [ 1, 0, 0 ]\nvisualizer.process(200); // [ 1, 0, 0 ]\nvisualizer.process(250); // [ 0.5019607843137255, 0, 0.5019607843137255 ]\nvisualizer.process(299); // [ 0.011764705882352941, 0, 0.9882352941176471 ]\nvisualizer.process(300); // [ 0, 1, 0 ]\n\n\n\nReturns a new ColorRampVisualizer which is the inverse of the current one. This means the color scale goes in the opposite direction.\n\n\n\nReturns interpolated color for value.\n\n\n\nvalue number\n\nReturns [number, number, number] normalized RGB triplet.\n\n\n\n\nCreates ColorRampVisualizer with valColPairs redTemperature\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorRampVisualizer.createRedTemperature(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0.43137254901960786, 0, 0 ]\nvisualizer.process(0.5); // returns [ 0.7176470588235294, 0.047058823529411764, 0 ]\nvisualizer.process(0.8); // returns [ 1, 0.6196078431372549, 0.2 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorRampVisualizer\n\n\n\n\nCreates ColorRampVisualizer with valColPairs greenWhite\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorRampVisualizer.createWhiteGreen(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0, 0.2980392156862745, 0 ]\nvisualizer.process(0.5); // returns [ 0.16862745098039217, 0.5019607843137255, 0 ]\nvisualizer.process(0.8); // returns [ 0.6666666666666666, 0.8, 0.3333333333333333 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorRampVisualizer\n\n\n\n\nCreates ColorRampVisualizer with valColPairs blueRed\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorRampVisualizer.createBlueRed(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0.5019607843137255 ]\nvisualizer.process(0.3); // returns [ 0, 0.7019607843137254, 1 ]\nvisualizer.process(0.5); // returns [ 0.5019607843137255, 1, 0.5019607843137255 ]\nvisualizer.process(0.8); // returns [ 1, 0.2980392156862745, 0 ]\nvisualizer.process(1.0); // returns [ 0.5019607843137255, 0, 0 ]\nReturns ColorRampVisualizer\n\n\n\n\nCreates ColorRampVisualizer with valColPairs oceanColor\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\nReturns ColorRampVisualizer\n\n\n\n\n\n\n\n\nvalColPairs Array&lt;[number, number]&gt;\nminVal number (optional, default 0.0)\nmaxVal number (optional, default 1.0)\n\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nInterpolates a color based on interval.\n\n\n\n\nReturns interpolated color for value.\n\n\n\nval number\n\nReturns [number, number, number] normalized RGB triplet.\n\n\n\n\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorGradientVisualizer.createRedTemperature(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0.43137254901960786, 0, 0 ]\nvisualizer.process(0.5); // returns [ 0.7176470588235294, 0.047058823529411764, 0 ]\nvisualizer.process(0.8); // returns [ 1, 0.6196078431372549, 0.2 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorGradientVisualizer\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nCreates ColorGradientVisualizer with valColPairs redTemperature\n\n\n\n\n\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorGradientVisualizer.createWhiteGreen(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0 ]\nvisualizer.process(0.3); // returns [ 0, 0.2980392156862745, 0 ]\nvisualizer.process(0.5); // returns [ 0.16862745098039217, 0.5019607843137255, 0 ]\nvisualizer.process(0.8); // returns [ 0.6666666666666666, 0.8, 0.3333333333333333 ]\nvisualizer.process(1.0); // returns [ 1, 1, 1 ]\nReturns ColorGradientVisualizer\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nCreates ColorGradientVisualizer with valColPairs greenWhite\n\n\n\n\n\n\n\n\nminVal number min value of interval\nmaxVal number max value of interval\n\n\n\n\nconst visualizer = ColorGradientVisualizer.createBlueRed(0.0, 1.0);\nvisualizer.process(0.0); // returns [ 0, 0, 0.5019607843137255 ]\nvisualizer.process(0.3); // returns [ 0, 0.7019607843137254, 1 ]\nvisualizer.process(0.5); // returns [ 0.5019607843137255, 1, 0.5019607843137255 ]\nvisualizer.process(0.8); // returns [ 1, 0.2980392156862745, 0 ]\nvisualizer.process(1.0); // returns [ 0.5019607843137255, 0, 0 ]\nReturns ColorGradientVisualizer\nMeta\n\ndeprecated: Instead see ColorRampVisualizer\nCreates ColorGradientVisualizer with valColPairs blueRed\n\n\n\n\n\n\nThis is a piecewise linear function which compresses highlights. The minValue and maxValue will be mapped inside the interval [ 0, 1 ]. However, if maxValue lies in (0, 1) a second function which increases much more slowly will be used to further map the values which are mapped to 0.92 and above (see the figure below). This increases the visualized dynamic range while keeping most of the interval of interest linear. Useful, for example, for true color, with a maxValue of 0.4 to still keep some detail in clouds.\n\n\n\nPiecewise linear function which compresses highlights\n\n\n\n\n\nminValue number the value which will be mapped to 0. All values smaller than minValue will also be mapped to 0. (optional, default 0.0)\nmaxValue number the value which controls the position of the boundary point between both linear functions. It will be mapped to approx. 0.9259, while values greater than or equal to (2*maxValue - minValue) will be mapped to 1 (see the figure above). (optional, default 1.0)\ngain (optional, default 1.0)\noffset (optional, default 0.0)\ngamma (optional, default 1.0)\n\n\n\n\nconst visualizer = new HighlightCompressVisualizer(0.1, 0.4)\n\nvisualizer.process(0); // will return 0\nvisualizer.process(0.1); // will return 0\nvisualizer.process(0.25); // will return 0.5\nvisualizer.process(0.376); // will return 0.92. Note: 0.376 = minValue + 0.92*(maxValue - minValue)\nvisualizer.process(0.4); // will return 0.9259\nvisualizer.process(0.7); // will return 1 Note: 0.7 is the smallest value mapped to 1.\nvisualizer.process(1.1); // will return 1\n\n\n\nReturns mapped value.\n\n\n\nval number the input value to be mapped.\ni number the index of val. This is EO Browser specific.\n\nReturns [number] mapped value."
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#helper-functions",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#helper-functions",
    "title": "Utility Functions",
    "section": "Helper functions",
    "text": "Helper functions\nHelper functions that can be used in custom scripts.\n\nint2rgb\nTransforms a color as integer into RGB triplet.\n\nParameters\n\ncolor number as integer\n\n\n\nExamples\nint2rgb(255);    // returns [ 0, 0, 255 ]\nint2rgb(256);    // returns [ 0, 1, 0 ]\nint2rgb(65537);  // returns [ 1, 0, 1 ]\nReturns [number, number, number]\n\n\n\nrgb2int\nInverse of the int2rgb function. Transforms a RGB triplet into integer.\n\nParameters\n\ncolor [number, number, number] as RGB triplet\n\n\n\nExamples\nrgb2int([0, 0, 255]);  // returns 255\nrgb2int([0, 1, 0]);    // returns 256\nrgb2int([1, 0, 1]);    // returns 65537\nReturns number\n\n\n\nnormalizeRGB\nReturns a new, normalized array without modifying the input array. It does this by dividing by 255. The input range is expected between 0 and 255 giving an output between 0 and 1.\n\nParameters\n\nrgb255Array\n\n\n\n\ncombine\nCombines two colors.\n\nParameters\n\ncolor1 number The first color defined as an array of values.\ncolor2 number The second color defined as an array of values.\nalpha number A share of the first color defined as a floating point between 0 and\n\n\n\n\n\n\nExamples\ncombine([100, 0, 0], [0, 100, 0], 1);   // returns [ 100, 0, 0 ]\ncombine([100, 0, 0], [0, 100, 0], 0);   // returns [ 0, 100, 0 ]\ncombine([100, 0, 0], [0, 100, 0], 0.5); // returns [ 50, 50, 0 ]\nReturns number The combined color defined as an array of values.\n\n\n\nindex\nCalculate difference divided by sum\n\nParameters\n\nx number first value\ny number second value\n\n\n\nExamples\nindex(0.6, 0.4); // returns 0.2\nindex(0.5, -0.5); //returns 0.0\nReturns number (x - y) / (x + y), if sum is 0 returns 0\n\n\n\ninverse\nCalculate inverse value\n\nParameters\n\nx number value\n\n\n\nExamples\ninverse(2.0); // returns 0.5\ninverse(5.0); // returns 0.2\ninverse(0); // returns 1.7976931348623157E308\nReturns number inverse of value of x (1 / x), if x is 0 returns JAVA_DOUBLE_MAX_VAL\n\n\n\nvalueMap\nMaps a value to another value bound by an interval (from,to].\nintervals = [-10, -5, 0, 5, 10], values = [-100,-50, 0, 50, 100]\ndefines the following mapping:\n(-inf, -10]  =&gt; -100\n(-10, -5] =&gt; -50\n(-5,0] =&gt; 0\n(0, 5] =&gt; 50\n(5, +inf) =&gt; 100\n\nParameters\n\nvalue number input value\nintervals [number] array of numbers in ascending order defining intervals\nvalues [number] output value for the given interval\n\n\n\nExamples\nvalueMap(5, [1, 3, 5, 7, 10], [100, 300, 500, 700, 900]); // returns 500\nvalueMap(1, [1, 3, 5, 7, 10], [100, 300, 500, 700, 900]); // returns 100\nvalueMap(2, [1, 3, 5, 7, 10], [100, 300, 500, 700, 900]); // returns 300\nvalueMap(12, [1, 3, 5, 7, 10], [100, 300, 500, 700, 900]); // returns 900\nvalueMap(50); // returns 50\nReturns number\n\n\n\nvalueInterpolate\nInterpolates a value to another value bound by an interval (from,to]. Values at far ends of defined intervals are clamped to min/max value. This function is a replacement for the deprecated colorBlend function.\nintervals = [-10, -5, 0, 5, 10], values = [-1000,-50, 0, 50, 1000]\ndefines the following mapping:\n(-inf, -10]  =&gt; -1000\n(-10, -5] =&gt; (-1000, -50]\n(-5,0] =&gt; (-50,0]\n(0, 5] =&gt; (0,50]\n(5, 10] =&gt; (50,1000]\n(10, +inf) =&gt; 1000\n\nParameters\n\nvalue number input value\nintervals Array&lt;number&gt; array of numbers in ascending order defining intervals\nvalues (Array&lt;number&gt; | Array&lt;Array&lt;number&gt;&gt;) output interval for the given value/interval of the intervals array\n\n\n\nExamples\nvalueInterpolate(0, [-10, -5, 0, 5, 10], [-1000,-50, 0, 50, 1000]); // returns 0\nvalueInterpolate(-10, [-10, -5, 0, 5, 10], [-1000,-50, 0, 50, 1000]); // returns -1000\nvalueInterpolate(9, [-10, -5, 0, 5, 10], [-1000,-50, 0, 50, 1000]); // returns 810\nvalueInterpolate(50); // returns 50\nvalueInterpolate(0.1, [0, 0.2, 0.4, 0.6, 0.8, 1], [\n  [0, 0, 0],\n  [0.1, 0.2, 0.5],\n  [0.25, 0.4, 0.5],\n  [0.4, 0.6, 0.5],\n  [0.75, 0.8, 0.5],\n  [1, 1, 0.5]\n]); // return [0.05, 0.1, 0.25]\nReturns (number | Array&lt;number&gt;)"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#constants",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#constants",
    "title": "Utility Functions",
    "section": "Constants",
    "text": "Constants\n\nJAVA_DOUBLE_MAX_VAL\n const JAVA_DOUBLE_MAX_VAL = 1.7976931348623157E308;\nType: number\n\n\nblueRed\nconst blueRed = [\n  [1.000, 0x000080],\n  [0.875, 0x0000FF],\n  [0.625, 0x00FFFF],\n  [0.375, 0xFFFF00],\n  [0.125, 0xFF0000],\n  [0.000, 0x800000]\n]\nType: Array&lt;[number, number]&gt;\n\n\nredTemperature\nconst redTemperature = [\n  [1.000, 0x000000],\n  [0.525, 0xAE0000],\n  [0.300, 0xFF6E00],\n  [0.250, 0xFF8600],\n  [0.000, 0xFFFFFF]\n]\nType: Array&lt;[number, number]&gt;\n\n\ngreenWhite\nconst greenWhite = [\n  [1.000, 0x000000],\n  [0.600, 0x006600],\n  [0.300, 0x80B300],\n  [0.000, 0xFFFFFF]\n]\nType: Array&lt;[number, number]&gt;\n\n\noceanColor\nconst oceanColor = [\n[0.0000, normalizeRGB([147,0,108])],\n[0.0471, normalizeRGB([111,0,144])],\n[0.0980, normalizeRGB([72,0,183])],\n[0.1490, normalizeRGB([33,0,222])],\n[0.2000, normalizeRGB([0,10,255])],\n[0.2471, normalizeRGB([0,74,255])],\n[0.2980, normalizeRGB([0,144,255])],\n[0.3490, normalizeRGB([0,213,255])],\n[0.4000, normalizeRGB([0,255,215])],\n[0.4471, normalizeRGB([0,255,119])],\n[0.4980, normalizeRGB([0,255,15])],\n[0.5490, normalizeRGB([96,255,0])],\n[0.6000, normalizeRGB([200,255,0])],\n[0.6471, normalizeRGB([255,235,0])],\n[0.6980, normalizeRGB([255,183,0])],\n[0.7490, normalizeRGB([255,131,0])],\n[0.8000, normalizeRGB([255,79,0])],\n[0.8471, normalizeRGB([255,31,0])],\n[0.8980, normalizeRGB([230,0,0])],\n[0.9490, normalizeRGB([165,0,0])],\n[1.0000, normalizeRGB([105,0,0])]\n]\nType: Array&lt;[number, number]&gt;"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#colorblend",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#colorblend",
    "title": "Utility Functions",
    "section": "colorBlend",
    "text": "colorBlend\n\nParameters\n\nvalue number input value\nlimits Array&lt;number&gt; array of numbers in ascending order defining intervals\ncolors (Array&lt;number&gt; | Array&lt;Array&lt;number&gt;&gt;) output interval for the given value/interval of the intervals array\n\nReturns (number | Array&lt;number&gt;)\nMeta\n\ndeprecated: See valueInterpolate"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#landsat8c2qabandconditions",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#landsat8c2qabandconditions",
    "title": "Utility Functions",
    "section": "Landsat8C2QaBandConditions",
    "text": "Landsat8C2QaBandConditions\nCloud confidence, cloud shadow confidence, snow ice confidence and cirrus confidence represent levels of confidence that a condition exists:\n\n0 = “Not Determined”\n1 = “Low” = Low confidence.\n2 = “Medium / Reserved” = Medium only for cloud confidence.\n3 = “High” = High confidence.\n\nType: Object\n\nProperties\n\nfill number 0 for image data, 1 for fill data\ndilatedCloud number 0 for cloud is not dilated or no cloud, 1 for cloud dilation\ncirrus number 0 for no confidence level or low confidence, 1 for high confidence cirrus\ncloud number 0 for cloud confidence is not high, 1 for high confidence cloud\ncloudShadow number 0 for cloud shadow confidence is not high, 1 for high confidence cloud shadow\nsnow number 0 for snow/ice confidence is not high, 1 for high confidence snow cover\nclear number 0 if cloud or dilated cloud, or else 1\nwater number 0 for land or cloud, 1 for water\ncloudConfidence number\ncloudShadowConfidence number\nsnowIceConfidence number\ncirrusConfidence number"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#decodel8c2qa",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#decodel8c2qa",
    "title": "Utility Functions",
    "section": "decodeL8C2Qa",
    "text": "decodeL8C2Qa\nDecodes Landsat 8 Collection 2 Quality Assessment band conditions.\n\nParameters\n\nvalue integer band pixel (16-bit value)\n\n\n\nExamples\ndecodeL8C2Qa(55052);\n// returns {\n//   cirrus: 1, cirrusConfidence: 3,\n//   clear: 0,\n//   cloud: 1,\n//   cloudConfidence: 3,\n//   cloudShadow: 0,\n//   cloudShadowConfidence: 1,\n//   dilatedCloud: 0,\n//   fill: 0,\n//   snow: 0,\n//   snowIceConfidence: 1,\n//   water: 0\n// }\nReturns Landsat8C2QaBandConditions"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/Functions.html#decodes3olciqualityflags",
    "href": "APIs/SentinelHub/Evalscript/Functions.html#decodes3olciqualityflags",
    "title": "Utility Functions",
    "section": "decodeS3OLCIQualityFlags",
    "text": "decodeS3OLCIQualityFlags\nUnpacks bit-packed Sentinel 3 OLCI Quality Flags values.\n\nParameters\n\nvalue integer QUALITY_FLAGS band DN value (32-bit value)\n\nReturns object An object containing the following keys with either 0 or 1 values: land, coastline, fresh_inland_water, tidal_region, bright, straylight_risk, invalid, cosmetic, duplicated, sun_glint_risk, dubious, saturatedBxy (where xy is the band number, e.g. saturatedB01)."
  },
  {
    "objectID": "APIs/SentinelHub/Batch/Crs.html",
    "href": "APIs/SentinelHub/Batch/Crs.html",
    "title": "CRS",
    "section": "",
    "text": "Find the list of supported CRSs here.\nThe area of interest can be defined in any of these CRSs but the CRS of the output of batch API is defined with selected tiling grid."
  },
  {
    "objectID": "APIs/SentinelHub/Catalog.html",
    "href": "APIs/SentinelHub/Catalog.html",
    "title": "Catalog API",
    "section": "",
    "text": "Sentinel Hub Catalog API (or shortly \"Catalog\") is an API implementing the STAC Specification, describing geospatial information about different data used with Sentinel Hub."
  },
  {
    "objectID": "APIs/SentinelHub/Catalog.html#api-reference",
    "href": "APIs/SentinelHub/Catalog.html#api-reference",
    "title": "Catalog API",
    "section": "API Reference",
    "text": "API Reference\nAPI Reference for Sentinel Hub Catalog is available as an OpenAPI description.\nSimple search request for Sentinel-1 GRD with a bounding box (the coordinate reference system of the values is WGS84 longitude/latitude), available on 10th December 2019.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-10T23:59:59Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = requests.post(url, json=data)"
  },
  {
    "objectID": "APIs/SentinelHub/Catalog.html#authentication",
    "href": "APIs/SentinelHub/Catalog.html#authentication",
    "title": "Catalog API",
    "section": "Authentication",
    "text": "Authentication\nAuthentication for the Catalog API works completely the same as authentication for other Sentinel Hub services, see Authentication chapter."
  },
  {
    "objectID": "APIs/SentinelHub/Catalog.html#pagination",
    "href": "APIs/SentinelHub/Catalog.html#pagination",
    "title": "Catalog API",
    "section": "Pagination",
    "text": "Pagination\nExecuting the request specified above returns search context fields at the end of the response, looking like this:\n{\n  \"context\": {\n    \"next\": 5,\n    \"limit\": 5,\n    \"returned\": 5\n  }\n}\nThe presence of the next attribute indicates there is more data available for this query, but the server chose to only return 5 results, because the limit specified was 5 (if limit is not specified, default value is 10). To query the next page of items, our request needs to include the next attribute with its value in the query, like so:\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-10T23:59:59Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 5,\n    \"next\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = requests.post(url, json=data)\nThe response now includes the next page of items; in this case there is no next token in context, meaning no more items exist for this query."
  },
  {
    "objectID": "APIs/SentinelHub/Catalog.html#extensions",
    "href": "APIs/SentinelHub/Catalog.html#extensions",
    "title": "Catalog API",
    "section": "Extensions",
    "text": "Extensions\n\nFilter\nThe search endpoint by default only accepts the parameters described in OpenAPI. The Filter extension enables users to specify an additional parameter to filter on, while searching through data.\nThe syntax for filter is CQL2:\n{\n  \"filter\": {\n    \"op\": \"&lt;operator&gt;\",\n    \"args\": [\n      {\n        \"property\": \"&lt;property_name&gt;\"\n      },\n      \"&lt;value&gt;\"\n    ]\n  },\n  \"filter-lang\": \"cql2-json\"\n}\nIt is also possible to use simple cql2-text:\n{\n  \"filter\": \"eo:cloud_cover &gt; 90\"\n}\nThe available operators are eq, neq, lt, lte, gt, gte and between. Only and is currently supported as a logical operator. Be careful - different collections have different properties for the query filter available. The information describing this is available inside the documentation for each specific collection (ex. Sentinel-1 GRD).\n\n\nFields\nBy default, the search endpoint returns all the available attributes of each item. The fields extension provides a way for the client to specify which attributes should not be part of the output, making it easy for the client to not have to deal with unnecessary data.\nSyntax for the fields is:\n{\n  \"fields\": {\n    \"include\": [\n      \"&lt;property_name_1&gt;\",\n      \"&lt;property_name_2&gt;\"\n    ],\n    \"exclude\": [\n      \"&lt;property_name_3&gt;\",\n      \"&lt;property_name_4&gt;\",\n      \"&lt;property_name_5&gt;\"\n    ]\n  }\n}\n\nInclude/Exclude behaviour\n\nWhen no fields attribute is specified in the request, all the available attributes will be included in the response.\nIf the fields attribute is specified with an empty object, or both include and exclude are set to null or an empty array is returned, the attributes for each item will be as if include was set to a default set of [\"id\", \"type\", \"geometry\", \"bbox\", \"links\", \"assets\", \"properties.datetime\"].\nIf only include is specified, the attributes in include will be merged with the default set above.\nIf only exclude is specified, the attributes in exclude will be removed from the default set above.\nIf both include and exclude are specified, the rule is that an attribute must be included in and not excluded from the response.\n\n\n\n\nDistinct\nSometimes we don't want to search for product metadata, but want some general information about the product, such as for example, which acquisition dates are available for Sentinel-1 inside the specified bbox and time interval. distinct attribute inside a search request makes this possible.\nSyntax for distinct attribute is:\n{\n  \"distinct\": \"&lt;property_name&gt;\"\n}\nAs with the filter attribute, distinct is also a collection limited to some specific properties. Information describing these properties can be found inside each collection's documentation (ex. Sentinel-1 GRD)."
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Metadata.html",
    "href": "APIs/SentinelHub/UserGuides/Metadata.html",
    "title": "Working with metadata in evalscript",
    "section": "",
    "text": "This user guide will show you how to work with metadata in evalscripts. We will focus on using objects scenes, inputMetadata, and outputMetadata. Use cases, covered with the examples below, include accessing metadata and using it in processing, passing the metadata to an output file userdata.json, and adding your own metadata to the file.\nNote that metadata normally provided in raster format is available as bands in Sentinel Hub. Such metadata can be accessed and processed in evalscript in the same manner as any other input band. This is not covered in this guide, but you can find basic examples and such metadata listed in the Data section for each data collection e.g. sunAzimuthAngles.\nEach example below begins with a description that highlights the important points of the example. All examples output also processed satellite images (average values of NDVI or band B02) but we do not display them here, since the focus is on metadata. To run the examples, you only need to have Python installed on your machine and an active Sentinel Hub account. You will always need to run the code in the chapter \"Authentication\" while the rest of the examples can be run independently."
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Metadata.html#check-which-metadata-is-available",
    "href": "APIs/SentinelHub/UserGuides/Metadata.html#check-which-metadata-is-available",
    "title": "Working with metadata in evalscript",
    "section": "Check which metadata is available",
    "text": "Check which metadata is available\nThe metadata is stored in two objects, which we call inputMetadata and scenes. Their properties are documented here and here, respectively. However, the properties of the scenes object can be different depending on the selected:\n\nmosaicking (e.g. ORBIT or TILE),\ndata collection (Sentinel-2 L2A, Sentinel-1, Sentinel-5p, ...),\nfunction in the evalscript (evaluatePixel, preProcessScenes, updateOutputMetadata).\n\nA convenient way to check which metadata is for your request available in scenes is to dump (i.e. write) all properties of the object to userdata.json file. This can be achieved with the Processing API as shown in this basic example. The two examples below show few more tricks that can be used to explore scenes object.\n\nProperties of scenes object and mosaicking ORBIT\nThis example shows:\n\nHow to access metadata when mosaicking is ORBIT using scenes.orbits.\nHow to pass metadata from scenes to userdata.json file using outputMetadata.userData in updateOutputMetadata function.\n\nurl = 'https://sh.dataspace.copernicus.eu'\n\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"dataMask\"],\n    mosaicking: Mosaicking.ORBIT,\n    output: {\n      id: \"default\",\n      bands: 1\n    }\n  }\n}\n\nfunction evaluatePixel(samples, scenes, inputMetadata, customData, outputMetadata) {\n  //Average value of band B02 based on the requested scenes\n  var sumOfValidSamplesB02 = 0\n  var numberOfValidSamples = 0\n  for (i = 0; i &lt; samples.length; i++) {\n    var sample = samples[i]\n    if (sample.dataMask == 1){\n        sumOfValidSamplesB02 += sample.B02\n        numberOfValidSamples += 1\n    }\n  }\n  return [sumOfValidSamplesB02 / numberOfValidSamples]\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = {\n    \"inputMetadata\": inputMetadata\n  }\n  outputMetadata.userData[\"orbits\"] = scenes.orbits\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [13.8, 45.8, 13.9, 45.9]\n    },\n    \"data\": [{\n      \"type\": \"sentinel-2-l1c\",\n      \"dataFilter\": {\n        \"timeRange\": {\n          \"from\": \"2020-12-01T00:00:00Z\",\n          \"to\": \"2020-12-06T23:59:59Z\"\n        }\n      }\n    }]\n  },\n  \"output\": {\n    \"responses\": [{\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/tiff\"\n        }\n      },\n      {\n        \"identifier\": \"userdata\",\n        \"format\": {\n          \"type\": \"application/json\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/x-tar'\n}\n\nresponse = oauth.post(f\"{url}/api/v1/process\", headers=headers, json = request)\ntar = tarfile.open(fileobj=io.BytesIO(response.content))\nuserdata = json.load(tar.extractfile(tar.getmember('userdata.json')))\nuserdata\n{'inputMetadata': {'serviceVersion': '4.263.0', 'normalizationFactor': 0.0001},\n 'orbits': [{'tiles': [{'date': '2020-12-06T10:08:08Z',\n     'shId': 15161628,\n     'cloudCoverage': 100,\n     'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/UL/2020/12/6/0'},\n    {'date': '2020-12-06T10:08:05Z',\n     'shId': 15161463,\n     'cloudCoverage': 98.26,\n     'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/VL/2020/12/6/0'}],\n   'dateTo': '2020-12-06T23:59:59Z',\n   '__idx': 0,\n   'dateFrom': '2020-12-06T00:00:00Z'},\n  {'tiles': [{'date': '2020-12-04T10:18:05Z',\n     'shId': 15142759,\n     'cloudCoverage': 99.93,\n     'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/UL/2020/12/4/0'},\n    {'date': '2020-12-04T10:17:56Z',\n     'shId': 15142728,\n     'cloudCoverage': 98.5,\n     'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/VL/2020/12/4/0'}],\n   'dateTo': '2020-12-04T23:59:59Z',\n   '__idx': 1,\n   'dateFrom': '2020-12-04T00:00:00Z'},\n  {'tiles': [{'date': '2020-12-01T10:08:10Z',\n     'shId': 15117250,\n     'cloudCoverage': 22.85,\n     'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/UL/2020/12/1/0'},\n    {'date': '2020-12-01T10:08:06Z',\n     'shId': 15117286,\n     'cloudCoverage': 46.81,\n     'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/VL/2020/12/1/0'}],\n   'dateTo': '2020-12-01T23:59:59Z',\n   '__idx': 2,\n   'dateFrom': '2020-12-01T00:00:00Z'}]}\n\n\nProperties of scenes object and mosaicking TILE\nThis example shows how to:\n\nAccess scenes metadata when mosaicking is TILE using scenes.tiles and write it to userdata.json file.\nHow to calculate a maximum value of band B02 and write it to userdata.json file. Note that we use a global variable maxValueB02 so that we can assign a value to it in evaluatePixel function but write its value to metadata in updateOutputMetadata function. The advantage of this approach is that maxValueB02 is written to metadata only once and not for each output pixel.\n\nurl = 'https://sh.dataspace.copernicus.eu'\n\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"dataMask\"],\n    mosaicking: Mosaicking.TILE,\n    output: {\n      id: \"default\",\n      bands: 1\n    }\n  }\n}\n\nvar maxValueB02 = 0\n\nfunction evaluatePixel(samples, scenes, inputMetadata, customData, outputMetadata) {\n  //Average value of band B02 based on the requested tiles\n  var sumOfValidSamplesB02 = 0\n  var numberOfValidSamples = 0\n  for (i = 0; i &lt; samples.length; i++) {\n    var sample = samples[i]\n    if (sample.dataMask == 1){\n        sumOfValidSamplesB02 += sample.B02\n        numberOfValidSamples += 1\n        if (sample.B02 &gt; maxValueB02){\n            maxValueB02 = sample.B02\n        }\n    }\n  }\n  return [sumOfValidSamplesB02 / numberOfValidSamples]\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = { \"tiles\":  scenes.tiles }\n  outputMetadata.userData.maxValueB02 = maxValueB02\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [13.8, 45.8, 13.9, 45.9]\n    },\n    \"data\": [{\n      \"type\": \"sentinel-2-l1c\",\n      \"dataFilter\": {\n        \"timeRange\": {\n          \"from\": \"2020-12-01T00:00:00Z\",\n          \"to\": \"2020-12-06T23:59:59Z\"\n        }\n      }\n    }]\n  },\n  \"output\": {\n    \"responses\": [{\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/tiff\"\n        }\n      },\n      {\n        \"identifier\": \"userdata\",\n        \"format\": {\n          \"type\": \"application/json\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/x-tar'\n}\n\nresponse = oauth.post(f\"{url}/api/v1/process\", headers=headers, json = request)\ntar = tarfile.open(fileobj=io.BytesIO(response.content))\nuserdata = json.load(tar.extractfile(tar.getmember('userdata.json')))\nuserdata\n{'tiles': [{'date': '2020-12-06T10:08:08Z',\n   'shId': 15161628,\n   'cloudCoverage': 100,\n   '__idx': 0,\n   'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/UL/2020/12/6/0'},\n  {'date': '2020-12-06T10:08:05Z',\n   'shId': 15161463,\n   'cloudCoverage': 98.26,\n   '__idx': 1,\n   'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/VL/2020/12/6/0'},\n  {'date': '2020-12-04T10:18:05Z',\n   'shId': 15142759,\n   'cloudCoverage': 99.93,\n   '__idx': 2,\n   'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/UL/2020/12/4/0'},\n  {'date': '2020-12-04T10:17:56Z',\n   'shId': 15142728,\n   'cloudCoverage': 98.5,\n   '__idx': 3,\n   'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/VL/2020/12/4/0'},\n  {'date': '2020-12-01T10:08:10Z',\n   'shId': 15117250,\n   'cloudCoverage': 22.85,\n   '__idx': 4,\n   'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/UL/2020/12/1/0'},\n  {'date': '2020-12-01T10:08:06Z',\n   'shId': 15117286,\n   'cloudCoverage': 46.81,\n   '__idx': 5,\n   'dataPath': 's3://sentinel-s2-l1c/tiles/33/T/VL/2020/12/1/0'}],\n 'maxValueB02': 0.8795000000000001}"
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Metadata.html#output-metadata-into-userdatajson-file",
    "href": "APIs/SentinelHub/UserGuides/Metadata.html#output-metadata-into-userdatajson-file",
    "title": "Working with metadata in evalscript",
    "section": "Output metadata into userdata.json file",
    "text": "Output metadata into userdata.json file\nIn this example, we write several pieces of information to the userdata.json file:\n\nA version of the software with which the data was processed. We take this information from inputMetadata.\nDates when the data used for processing was acquired. We take this information from scene.tiles.\nValues set by user and used for processing, such as thresholds (e.g. ndviThreshold) and array of values (e.g. notAllowedDates).\nDates of all tiles available before we filtered out those acquired on dates given in notAllowedDates array. These dates are listed in tilesPPSDates property of userData. Note how we used a global variable tilesPPS: we assigned it a value in preProcessScenes and output it in updateOutputMetadata function.\nDates of all tiles available after the filtering. These dates are listed in tilesDates property of userData.\nDescription of the processing implemented in the evalscript and links to external resources.\n\nurl = 'https://sh.dataspace.copernicus.eu'\n\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B08\", \"B04\", \"dataMask\"],\n    mosaicking: Mosaicking.TILE,\n    output: {\n      id: \"default\",\n      bands: 1\n    }\n  }\n}\n\n// User's inputs\nvar notAllowedDates = [\"2020-12-06\", \"2020-12-09\"]\nvar ndviThreshold = 0.2\n\nvar tilesPPS = []\nfunction preProcessScenes(collections) {\n  tilesPPS = collections.scenes.tiles\n  collections.scenes.tiles = collections.scenes.tiles.filter(function(tile) {\n    var tileDate = tile.date.split(\"T\")[0];\n    return !notAllowedDates.includes(tileDate);\n  })\n  return collections\n}\n\nfunction evaluatePixel(samples, scenes, inputMetadata, customData, outputMetadata) {\n\n  var valid_ndvi_sum = 0\n  var numberOfValidSamples = 0\n  for (i = 0; i &lt; samples.length; i++) {\n    var sample = samples[i]\n    if (sample.dataMask == 1){\n        var ndvi = (sample.B08 - sample.B04)/(sample.B08 + sample.B04)\n        if (ndvi &lt;= ndviThreshold){\n          valid_ndvi_sum += ndvi\n          numberOfValidSamples += 1\n        }\n    }\n  }\n\n  return [valid_ndvi_sum / numberOfValidSamples]\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = {\n    \"inputMetadata.serviceVersion\": inputMetadata.serviceVersion\n  }\n\n  outputMetadata.userData.description = \"The evalscript calculates average ndvi \" +\n  \"in a requested time period. Data collected on notAllowedDates is excluded. \" +\n  \"ndvi values greater than ndviThreshold are excluded. \" +\n  \"More about ndvi: https://www.indexdatabase.de/db/i-single.php?id=58.\"\n\n  // Extract dates for all available tiles (before filtering)\n  var tilePPSDates = []\n  for (i = 0; i &lt; tilesPPS.length; i++){\n    tilePPSDates.push(tilesPPS[i].date)\n  }\n  outputMetadata.userData.tilesPPSDates = tilePPSDates\n\n  // Extract dates for tiles after filtering out tiles with \"notAllowedDates\"\n  var tileDates = []\n  for (i = 0; i &lt; scenes.tiles.length; i++){\n    tileDates.push(scenes.tiles[i].date)\n  }\n  outputMetadata.userData.tilesDates = tileDates\n\n  outputMetadata.userData.notAllowedDates = notAllowedDates\n  outputMetadata.userData.ndviThreshold = ndviThreshold\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [13.8, 45.8, 13.9, 45.9]\n    },\n    \"data\": [{\n      \"type\": \"sentinel-2-l1c\",\n      \"dataFilter\": {\n        \"timeRange\": {\n          \"from\": \"2020-12-01T00:00:00Z\",\n          \"to\": \"2020-12-15T23:59:59Z\"\n        }\n      }\n    }]\n  },\n  \"output\": {\n    \"responses\": [{\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/tiff\"\n        }\n      },\n      {\n        \"identifier\": \"userdata\",\n        \"format\": {\n          \"type\": \"application/json\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/x-tar'\n}\n\nresponse = oauth.post(f\"{url}/api/v1/process\", headers=headers, json = request)\ntar = tarfile.open(fileobj=io.BytesIO(response.content))\nuserdata = json.load(tar.extractfile(tar.getmember('userdata.json')))\nuserdata\n{'notAllowedDates': ['2020-12-06', '2020-12-09'],\n 'tilesDates': ['2020-12-11T10:08:07Z',\n  '2020-12-11T10:08:03Z',\n  '2020-12-04T10:18:05Z',\n  '2020-12-04T10:17:56Z',\n  '2020-12-01T10:08:10Z',\n  '2020-12-01T10:08:06Z'],\n 'inputMetadata.serviceVersion': '4.263.0',\n 'description': 'The evalscript calculates average ndvi in a requested time period. Data collected on notAllowedDates is excluded. ndvi values greater than ndviThreshold are excluded. More about ndvi: https://www.indexdatabase.de/db/i-single.php?id=58. ',\n 'tilesPPSDates': ['2020-12-11T10:08:07Z',\n  '2020-12-11T10:08:03Z',\n  '2020-12-09T10:18:04Z',\n  '2020-12-09T10:17:56Z',\n  '2020-12-06T10:08:08Z',\n  '2020-12-06T10:08:05Z',\n  '2020-12-04T10:18:05Z',\n  '2020-12-04T10:17:56Z',\n  '2020-12-01T10:08:10Z',\n  '2020-12-01T10:08:06Z'],\n 'ndviThreshold': 0.2}"
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Transparency.html",
    "href": "APIs/SentinelHub/UserGuides/Transparency.html",
    "title": "Transparency",
    "section": "",
    "text": "Parts of the image can be made fully or partially transparent by including the fourth output channel, also known as the alpha channel. The value 0 in the alpha channel makes a pixel fully transparent, while the maximum value in the alpha channel makes it fully opaque (not transparent). Values in between will make the pixel proportionally transparent. Maximum value in alpha channel depends on an image bit depth which is in SH specified by sampleType:\n\nfor sampleType AUTO or FLOAT32: values in alpha channel should be from the interval [0, 1]\nfor sampleType UINT8: values in alpha channel should be from the interval [0, 255]\nfor sampleType UINT16: values in alpha channel should be from the interval [0, 65535]\n\nOutput file formats which support transparency are PNG and TIFF. Note that the JPEG format does not support alpha channel.\n\n\nNo-data pixels are marked by the value 0 in the dataMask band. In the evalscript below, we return value 0 in the fourth band for such pixels, which made them transparent. This evalscript can be used as part of any request for Sentinel-2 data and it will display a true color image with transparent background.\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    if (samples.dataMask == 1){\n        return [samples.B04 * 3.5, samples.B03 * 3.5, samples.B02 * 3.5, 1]\n    } else if (samples.dataMask == 0) {\n        return [samples.B04 * 3.5, samples.B03 * 3.5, samples.B02 * 3.5, 0]\n    }\n}\nIn the example above we see that the value of dataMask band is exactly the value we want to use for the alpha channel (i.e. fourth channel) of the output. We can thus simplify the evalscript and return dataMask as the fourth band for all pixels.\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    return [samples.B04 * 3.5, samples.B03 * 3.5, samples.B02 * 3.5, samples.dataMask]\n}\nExamine in EO Browser\nElegant! This will work whenever you request sampleType AUTO or FLOAT32. However, for other sampleTypes, you will have to scale the output values to achieve the same transparency. Let us check how to do this in the examples below.\n\n\nWhen using sampleType UINT16 the range of output values in an image becomes [0, 65535] and we must return value 65535 in the alpha channel for pixels which should not be transparent. The above example, if using sampleType UINT16, would be:\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4, sampleType: \"UINT16\"}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    return [samples.B04 * 3.5 * 65535, samples.B03 * 3.5 * 65535, samples.B02 * 3.5 * 65535, samples.dataMask * 65535]\n}\n\n\n\nThe same logic applies also for sampleType UINT8 except that the range of output values in this case is [0, 255]. The same evalscript as above but for sampleType UINT8 is:\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4, sampleType: \"UINT8\"}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    return [samples.B04 * 3.5 * 255, samples.B03 * 3.5 * 255, samples.B02 * 3.5 * 255, samples.dataMask * 255]\n}\n\n\n\n\nTo use some other condition for turning pixels transparent, simply return the condition in the fourth channel, while also outputting four bands in the function setup(). In the example below, we are returning the Sentinel-2 L1C NDVI index larger than 0.6 as transparent. We also leave the no-data pixels non-transparent and thus do not need to use the the dataMask input band.\n//VERSION=3\nfunction setup () {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"B08\"],\n    output: {bands: 4}\n  }\n}\nfunction evaluatePixel(samples, scenes) {\n  var NDVI = (samples.B08 - samples.B04) / (samples.B08 + samples.B04)\n  return [samples.B04 * 2.5, samples.B03 * 2.5, samples.B02 * 2.5, NDVI &lt; 0.6]\n}\nExamine in EO Browser"
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Transparency.html#transparency-and-background-color",
    "href": "APIs/SentinelHub/UserGuides/Transparency.html#transparency-and-background-color",
    "title": "Transparency",
    "section": "",
    "text": "Parts of the image can be made fully or partially transparent by including the fourth output channel, also known as the alpha channel. The value 0 in the alpha channel makes a pixel fully transparent, while the maximum value in the alpha channel makes it fully opaque (not transparent). Values in between will make the pixel proportionally transparent. Maximum value in alpha channel depends on an image bit depth which is in SH specified by sampleType:\n\nfor sampleType AUTO or FLOAT32: values in alpha channel should be from the interval [0, 1]\nfor sampleType UINT8: values in alpha channel should be from the interval [0, 255]\nfor sampleType UINT16: values in alpha channel should be from the interval [0, 65535]\n\nOutput file formats which support transparency are PNG and TIFF. Note that the JPEG format does not support alpha channel.\n\n\nNo-data pixels are marked by the value 0 in the dataMask band. In the evalscript below, we return value 0 in the fourth band for such pixels, which made them transparent. This evalscript can be used as part of any request for Sentinel-2 data and it will display a true color image with transparent background.\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    if (samples.dataMask == 1){\n        return [samples.B04 * 3.5, samples.B03 * 3.5, samples.B02 * 3.5, 1]\n    } else if (samples.dataMask == 0) {\n        return [samples.B04 * 3.5, samples.B03 * 3.5, samples.B02 * 3.5, 0]\n    }\n}\nIn the example above we see that the value of dataMask band is exactly the value we want to use for the alpha channel (i.e. fourth channel) of the output. We can thus simplify the evalscript and return dataMask as the fourth band for all pixels.\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    return [samples.B04 * 3.5, samples.B03 * 3.5, samples.B02 * 3.5, samples.dataMask]\n}\nExamine in EO Browser\nElegant! This will work whenever you request sampleType AUTO or FLOAT32. However, for other sampleTypes, you will have to scale the output values to achieve the same transparency. Let us check how to do this in the examples below.\n\n\nWhen using sampleType UINT16 the range of output values in an image becomes [0, 65535] and we must return value 65535 in the alpha channel for pixels which should not be transparent. The above example, if using sampleType UINT16, would be:\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4, sampleType: \"UINT16\"}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    return [samples.B04 * 3.5 * 65535, samples.B03 * 3.5 * 65535, samples.B02 * 3.5 * 65535, samples.dataMask * 65535]\n}\n\n\n\nThe same logic applies also for sampleType UINT8 except that the range of output values in this case is [0, 255]. The same evalscript as above but for sampleType UINT8 is:\n//VERSION=3\nfunction setup () {\n    return {\n        input: [\"B04\", \"B03\", \"B02\", \"dataMask\"],\n        output: {bands: 4, sampleType: \"UINT8\"}\n    }\n}\n\nfunction evaluatePixel(samples, scenes) {\n    return [samples.B04 * 3.5 * 255, samples.B03 * 3.5 * 255, samples.B02 * 3.5 * 255, samples.dataMask * 255]\n}\n\n\n\n\nTo use some other condition for turning pixels transparent, simply return the condition in the fourth channel, while also outputting four bands in the function setup(). In the example below, we are returning the Sentinel-2 L1C NDVI index larger than 0.6 as transparent. We also leave the no-data pixels non-transparent and thus do not need to use the the dataMask input band.\n//VERSION=3\nfunction setup () {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"B08\"],\n    output: {bands: 4}\n  }\n}\nfunction evaluatePixel(samples, scenes) {\n  var NDVI = (samples.B08 - samples.B04) / (samples.B08 + samples.B04)\n  return [samples.B04 * 2.5, samples.B03 * 2.5, samples.B02 * 2.5, NDVI &lt; 0.6]\n}\nExamine in EO Browser"
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html",
    "href": "APIs/SentinelHub/Zarr.html",
    "title": "Zarr Import API",
    "section": "",
    "text": "Zarr Import API is only available for users with Copernicus Service accounts. Please refer to our FAQ on account typology change and Submit A Request to our Copernicus Data Space Ecosystem Support Team to request your Copernicus Service account accordingly."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#overview",
    "href": "APIs/SentinelHub/Zarr.html#overview",
    "title": "Zarr Import API",
    "section": "Overview",
    "text": "Overview\nZarr Import API (or shortly \"Zarr\") enables you to import your own Zarr data in Sentinel Hub and access it just like any other data when some conditions are met.\nThese are:\n\nStore your raster data in the Zarr format on your own S3 bucket in the supported region.\nZarr data must conform to Sentinel Hub data constraints.\nConfigure the bucket's permissions so that Sentinel Hub can read them."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#data-constraints",
    "href": "APIs/SentinelHub/Zarr.html#data-constraints",
    "title": "Zarr Import API",
    "section": "Data constraints",
    "text": "Data constraints\nSince Zarr is a generic data format, there are additional constraints in order to ingest the data to Sentinel Hub:\n\nData must be stored as a single Zarr group that contains coordinate arrays and data arrays.\nData array names should be valid JavaScript identifiers so they can be safely used in evalscripts; valid identifiers are case-sensitive, can contain Unicode letters, $, _, and digits (0-9), but may not start with a digit, and should not be one of the reserved JavaScript keywords.\nData arrays must have two or three dimensions. There must be exactly two spatial coordinate arrays, named either x and y or lat and lon, and an optional time coordinate array.\nData arrays must be stored in row-major order (\"order\": \"C\", i.e., the last dimension varies fastest). The ordering of the dimensions has to be [time, lat, lon] or [time, y, x] for 3 dimensional data, and [lat, lon] or [y, x] for 2 dimensional data.\nAll data, including the spatial and the optional time coordinate arrays, must consist of 32-bit or 64-bit integers and floats (Zarr data types u4, i4, i8, f4, f8).\nThe chunk size in the two spatial dimensions must be less than or equal to 3072, but does not need to be the same for all data arrays.\nFor 3 dimensional data:\n\nthe time array must include the units attribute in its zattrs, which has to be in the format &lt;unit&gt; since &lt;instant&gt;. Where supported units are days/hours/minutes/seconds/millis/micros/nanos and instant should either be in the format ISO8601 or should follow the definition of the time:units field of the CF time coordinate convention. As an example, unix epoch could be encoded as seconds since 1970-01-01 00:00:00,\nthe chunk size in time dimension must be the same for all data arrays and must be less than or equal to 50.\n\nData must not cross any of the two poles.\nData must use an equidistant spatial grid, that is, the two spatial coordinate arrays must be equidistant. The time coordinate array can be non-equidistant.\nThe projection needs to be one of: WGS84 (EPSG:4326), WebMercator (EPGS:3857), any UTM zone (EPSG:32601-32660, 32701-32760), or Europe LAEA (EPSG:3035).\nSubgroups within the Zarr group will be ignored, but may be ingested separately.\n\nPlease refer to Zarr specification for explanation of various Zarr format properties."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#bucket-settings",
    "href": "APIs/SentinelHub/Zarr.html#bucket-settings",
    "title": "Zarr Import API",
    "section": "Bucket settings",
    "text": "Bucket settings\nIf you do not yet have a bucket at Copernicus Data Space Ecosystem, please follow these steps to get one.\nYou will have to configure your bucket to allow read access to Sentinel Hub. To do this, update your bucket policy to include the following statement (don’t forget to replace &lt;bucket_name&gt; with your actual bucket name):\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Sentinel Hub permissions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::ddf4c98b5e6647f0a246f0624c8341d9:root\"\n            },\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\",\n                \"arn:aws:s3:::&lt;bucket_name&gt;/*\"\n            ]\n        }\n    ]\n}\nA python script to set a bucket policy can be downloaded here."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#creating-zarr-collections",
    "href": "APIs/SentinelHub/Zarr.html#creating-zarr-collections",
    "title": "Zarr Import API",
    "section": "Creating Zarr collections",
    "text": "Creating Zarr collections\nEach Zarr collection will correspond to a single Zarr group. When creating a collection, you need to provide:\n\nthe S3 bucket where you data is located,\nthe path in the bucket where the Zarr group resides, that is, the directory containing the .zgroup file,\nthe CRS in which your data is defined,\na name for the collection."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#ingesting-the-arrays",
    "href": "APIs/SentinelHub/Zarr.html#ingesting-the-arrays",
    "title": "Zarr Import API",
    "section": "Ingesting the arrays",
    "text": "Ingesting the arrays\nAfter a collection is created, the ingestion will start automatically. The service will try to ingest every data array found in the group in the given S3 bucket and path. If the Zarr data does not fulfill any of the above constraints, the ingestion will either fail entirely or the offending data arrays will be skipped.\nZarr service automatically configures collection bands named after the data arrays of the Zarr group, that is, the folder names of the arrays. For example, in a Zarr file that contains B1 and B2 array folders, the resulting arrays will be named B1 and B2.\nThe no data value will be read from data arrays' metadata, that is, from the fill_value property inside the array's .zarray file.\n\nQuerying ingestion status\nQuerying a collection will return the status of the ingestion as well as an error message if something went wrong. If the returned status is INGESTED you can start using your new zarr data with Sentinel Hub services."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#reingesting-the-zarr-collection",
    "href": "APIs/SentinelHub/Zarr.html#reingesting-the-zarr-collection",
    "title": "Zarr Import API",
    "section": "Reingesting the Zarr collection",
    "text": "Reingesting the Zarr collection\nWhen reingesting the Zarr, the data already ingested cannot be changed, but new chunks can be added to the existing data arrays and the temporal array can be expanded accordingly."
  },
  {
    "objectID": "APIs/SentinelHub/Zarr.html#examples",
    "href": "APIs/SentinelHub/Zarr.html#examples",
    "title": "Zarr Import API",
    "section": "Examples",
    "text": "Examples\nZarr Import API Examples"
  },
  {
    "objectID": "APIs/SentinelHub/Process.html",
    "href": "APIs/SentinelHub/Process.html",
    "title": "Processing API",
    "section": "",
    "text": "The Processing API (or shortly \"Process API\") is the most commonly used API in Sentinel Hub as it provides images based on satellite data. Users can request raw satellite data, simple band combinations such as false colour composites, calculations of simple remote sensing indices like NDVI, or more advanced processing such as calculation of Leaf area index (LAI).\nEven though satellite imagery data are often distributed in \"tiles\", we do not want users to be limited to these. Tiles are an artificially introduced entity to make data distribution easier to handle. However, users should not have to care about whether their AOI is on one tile or another, or perhaps on the border of two tiles. This is why Sentinel Hub API hides this complexity and simply makes the data available over chosen area of interest and temporal period of interest. Tiles are therefore automatically stitched together based on defined parameters (AOI, time period, cloud coverage, priority, etc., depending on the data type)."
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/S3OLCI.html",
    "href": "APIs/SentinelHub/Process/Examples/S3OLCI.html",
    "title": "Examples for S3OLCI",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nTrue Color\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B08\", \"B06\", \"B04\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\", // default value - scales the output values from [0,1] to [0,255].\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B08, 2.5 * sample.B06, 2.5 * sample.B04]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                8.3333,\n                41.3149,\n                9.7009,\n                43.0568,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B08\", \"B06\", \"B04\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\", // default value - scales the data from 0-255.\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B08, 2.5 * sample.B06, 2.5 * sample.B04]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color, resolution (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B08\", \"B06\", \"B04\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\", // default value - scales the output values from [0,1] to [0,255].\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B08, 2.5 * sample.B06, 2.5 * sample.B04]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    },\n                    \"processing\": {\"upsampling\": \"BILINEAR\"},\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 150,\n        \"resy\": 150,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color, multi-band GeoTiff\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B06\", \"B08\"],\n        units: \"REFLECTANCE\", // default value\n      },\n    ],\n    output: {\n      bands: 3,\n      sampleType: \"UINT16\", //floating point values are automatically rounded to the nearest integer by the service.\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  // Return reflectance multiplied by 10000 as integers to save processing units.\n  // To obtain reflectance values, simply divide the result pixel values by 10000.\n  return [10000 * sample.B08, 10000 * sample.B06, 10000 * sample.B04]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                8.3333,\n                41.3149,\n                9.7009,\n                43.0568,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/tiff\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"image/tiff\"})\n\n\nTrue color and metadata (multi-part response GeoTIFF and json)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B06\", \"B08\"],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    mosaicking: Mosaicking.SIMPLE,\n    output: {\n      id: \"default\",\n      bands: 3,\n      sampleType: \"UINT16\", //floating point values are automatically rounded to the nearest integer by the service.\n    },\n  }\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = { scenes: scenes.tiles }\n}\n\nfunction evaluatePixel(samples) {\n  // Return reflectance multiplied by 10000 as integers to save processing units.\n  // To obtain reflectance values, simply divide the result pixel values by 10000.\n  return [10000 * samples.B08, 10000 * samples.B06, 10000 * samples.B04]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                8.3333,\n                41.3149,\n                9.7009,\n                43.0568,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"userdata\",\n                \"format\": {\"type\": \"application/json\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nOTCI as jpeg image with bounds given as polygon\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B10\", \"B11\", \"B12\"],\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 3,\n      sampleType: \"AUTO\",\n    },\n  }\n}\n\n// Create a new visualiser to represent data\nvar cm = new ColorMapVisualizer([\n  [0, [0, 0, 0.5]],\n  [1, [0, 0.3, 0.8]],\n  [1.8, [1, 0.2, 0.2]],\n  [2.5, [1, 0.9, 0]],\n  [4, [0, 0.8, 0.1]],\n  [4.5, [0, 0.6, 0.2]],\n  [5, [1, 1, 1]],\n])\n\nfunction evaluatePixel(sample) {\n  let OTCI = (sample.B12 - sample.B11) / (sample.B11 - sample.B10)\n  return cm.process(OTCI)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            8.80279541015625,\n                            42.494377798972465,\n                        ],\n                        [\n                            8.6956787109375,\n                            42.370720143531976,\n                        ],\n                        [\n                            8.7890625,\n                            42.238685347536496,\n                        ],\n                        [\n                            8.60504150390625,\n                            42.20614200929954,\n                        ],\n                        [\n                            8.70391845703125,\n                            42.15322331239858,\n                        ],\n                        [\n                            8.83575439453125,\n                            41.97991089691236,\n                        ],\n                        [\n                            8.81378173828125,\n                            41.797935707842974,\n                        ],\n                        [\n                            8.9208984375,\n                            41.777456667491066,\n                        ],\n                        [\n                            8.94012451171875,\n                            41.68316883525891,\n                        ],\n                        [\n                            9.0472412109375,\n                            41.52297326747377,\n                        ],\n                        [\n                            9.35760498046875,\n                            41.70777900286713,\n                        ],\n                        [\n                            9.33013916015625,\n                            42.06764572379527,\n                        ],\n                        [\n                            9.48394775390625,\n                            42.261049162113856,\n                        ],\n                        [\n                            9.47021484375,\n                            42.51462626746592,\n                        ],\n                        [\n                            9.33837890625,\n                            42.62385465855651,\n                        ],\n                        [\n                            9.1900634765625,\n                            42.6844544397102,\n                        ],\n                        [\n                            8.80279541015625,\n                            42.494377798972465,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\n                    \"type\": \"image/jpeg\",\n                    \"quality\": 90,\n                },\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nOTCI image and value (multi-part response png and GeoTIFF containing floats)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B10\", \"B11\", \"B12\"],\n      },\n    ],\n    output: [\n      {\n        id: \"default\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"otci_image\",\n        bands: 3,\n        sampleType: \"AUTO\",\n      },\n    ],\n  }\n}\n\n// Create a new visualiser to represent data\nvar cm = new ColorMapVisualizer([\n  [0, [0, 0, 0.5]],\n  [1, [0, 0.3, 0.8]],\n  [1.8, [1, 0.2, 0.2]],\n  [2.5, [1, 0.9, 0]],\n  [4, [0, 0.8, 0.1]],\n  [4.5, [0, 0.6, 0.2]],\n  [5, [1, 1, 1]],\n])\n\nfunction evaluatePixel(sample) {\n  let OTCI = (sample.B12 - sample.B11) / (sample.B11 - sample.B10)\n  return {\n    default: [OTCI],\n    otci_image: cm.process(OTCI),\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            8.80279541015625,\n                            42.494377798972465,\n                        ],\n                        [\n                            8.6956787109375,\n                            42.370720143531976,\n                        ],\n                        [\n                            8.7890625,\n                            42.238685347536496,\n                        ],\n                        [\n                            8.60504150390625,\n                            42.20614200929954,\n                        ],\n                        [\n                            8.70391845703125,\n                            42.15322331239858,\n                        ],\n                        [\n                            8.83575439453125,\n                            41.97991089691236,\n                        ],\n                        [\n                            8.81378173828125,\n                            41.797935707842974,\n                        ],\n                        [\n                            8.9208984375,\n                            41.777456667491066,\n                        ],\n                        [\n                            8.94012451171875,\n                            41.68316883525891,\n                        ],\n                        [\n                            9.0472412109375,\n                            41.52297326747377,\n                        ],\n                        [\n                            9.35760498046875,\n                            41.70777900286713,\n                        ],\n                        [\n                            9.33013916015625,\n                            42.06764572379527,\n                        ],\n                        [\n                            9.48394775390625,\n                            42.261049162113856,\n                        ],\n                        [\n                            9.47021484375,\n                            42.51462626746592,\n                        ],\n                        [\n                            9.33837890625,\n                            42.62385465855651,\n                        ],\n                        [\n                            9.1900634765625,\n                            42.6844544397102,\n                        ],\n                        [\n                            8.80279541015625,\n                            42.494377798972465,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"otci_image\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nAll S3OLCI reflectance bands as a GeoTIFF (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\n          \"B01\",\n          \"B02\",\n          \"B03\",\n          \"B04\",\n          \"B05\",\n          \"B06\",\n          \"B07\",\n          \"B08\",\n          \"B09\",\n          \"B10\",\n          \"B11\",\n          \"B12\",\n          \"B13\",\n          \"B14\",\n          \"B15\",\n          \"B16\",\n          \"B17\",\n          \"B18\",\n          \"B19\",\n          \"B20\",\n          \"B21\",\n        ],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    output: {\n      bands: 21,\n      sampleType: \"UINT16\", //floating point values are automatically rounded to the nearest integer by the service.\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  // Return reflectance multiplied by 10000 as integers to save processing units.\n  // To obtain reflectance values, simply divide the result pixel values by 10000.\n  return [\n    10000 * sample.B01,\n    10000 * sample.B02,\n    10000 * sample.B03,\n    10000 * sample.B04,\n    10000 * sample.B05,\n    10000 * sample.B06,\n    10000 * sample.B07,\n    10000 * sample.B08,\n    10000 * sample.B09,\n    10000 * sample.B10,\n    10000 * sample.B11,\n    10000 * sample.B12,\n    10000 * sample.B13,\n    10000 * sample.B14,\n    10000 * sample.B15,\n    10000 * sample.B16,\n    10000 * sample.B17,\n    10000 * sample.B18,\n    10000 * sample.B19,\n    10000 * sample.B20,\n    10000 * sample.B21,\n  ]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-04-04T00:00:00Z\",\n                        \"to\": \"2020-04-05T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 300,\n        \"resy\": 300,\n        \"responses\": [{\"format\": {\"type\": \"image/tiff\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"image/tiff\"})"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/DataFusion.html",
    "href": "APIs/SentinelHub/Process/Examples/DataFusion.html",
    "title": "Examples of Data Fusion",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nPan-sharpen Sentinel-3 OLCI with Sentinel-2\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s2l1c\",\n        bands: [\"B02\", \"B03\", \"B04\"],\n      },\n      {\n        datasource: \"s3olci\",\n        bands: [\"B04\", \"B06\", \"B08\"],\n      },\n    ],\n    output: [\n      {\n        bands: 3,\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(\n  samples,\n  inputData,\n  inputMetadata,\n  customData,\n  outputMetadata\n) {\n  let s3 = samples.s3olci[0]\n  let s2 = samples.s2l1c[0]\n  let amount_s2 = 0.5\n  let gain = 3.0\n  return [\n    gain * (s3.B08 * (1 - amount_s2) + s2.B04 * amount_s2),\n    gain * (s3.B06 * (1 - amount_s2) + s2.B03 * amount_s2),\n    gain * (s3.B04 * (1 - amount_s2) + s2.B02 * amount_s2),\n  ]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                7.388827,\n                53.537043,\n                8.35627,\n                53.901102,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"id\": \"s2l1c\",\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-01T00:00:00Z\",\n                        \"to\": \"2020-06-01T23:59:00Z\",\n                    }\n                },\n            },\n            {\n                \"id\": \"s3olci\",\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-01T00:00:00Z\",\n                        \"to\": \"2020-06-01T23:59:00Z\",\n                    }\n                },\n            },\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 1024,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNDVI with Sentinel-1 and Sentinel-2\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s1\",\n        bands: [\"VV\", \"VH\"],\n      },\n      {\n        datasource: \"l2a\",\n        bands: [\"B08\", \"B04\", \"SCL\"],\n      },\n    ],\n    output: [\n      {\n        bands: 3,\n      },\n    ],\n  }\n}\n\nfunction toDb(linear) {\n  // Convert the linear backscatter to DB (Filgueiras et al. (2019), eq. 3)\n  return 10 * Math.LN10 * linear\n}\n\nfunction calc_s1_ndvi(sigmaVV, sigmaVH) {\n  // Convert sigma0 to Decibels\n  let vh_Db = toDb(sigmaVH)\n  let vv_Db = toDb(sigmaVV)\n  // Calculate NRPB (Filgueiras et al. (2019), eq. 4)\n  let NRPB = (vh_Db - vv_Db) / (vh_Db + vv_Db)\n  // Calculate NDVI_nc with approach A3 (Filgueiras et al. (2019), eq. 14)\n  let NDVInc = 2.572 - 0.05047 * vh_Db + 0.176 * vv_Db + 3.422 * NRPB\n  return NDVInc\n}\n\n// Create an NDVI visualiser\nvar viz = new ColorMapVisualizer([\n  [0.0, 0xa50026],\n  [0.0, 0xd73027],\n  [0.2, 0xf46d43],\n  [0.3, 0xfdae61],\n  [0.4, 0xfee08b],\n  [0.5, 0xffffbf],\n  [0.6, 0xd9ef8b],\n  [0.7, 0xa6d96a],\n  [0.8, 0x66bd63],\n  [0.9, 0x1a9850],\n  [1.0, 0x006837],\n])\n\nfunction evaluatePixel(samples) {\n  var s1 = samples.s1[0]\n  var s2 = samples.l2a[0]\n\n  // Use the S2-L2A classification to identify clouds\n  if ([7, 8, 9, 10].includes(s2.SCL)) {\n    // If clouds are present use S1 NDVI\n    let s1_ndvi = calc_s1_ndvi(s1.VV, s1.VH) // Calculate S1 NDVI\n    return viz.process(s1_ndvi)\n  } else {\n    // Otherwise use s2 NDVI\n    let ndvi = index(s2.B08, s2.B04) // Calculate S2 NDVI\n    return viz.process(ndvi)\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                -100.9204,\n                37.5718,\n                -100.4865,\n                37.864,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"id\": \"s1\",\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-26T00:00:00Z\",\n                        \"to\": \"2019-04-26T23:59:00Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"SIGMA0_ELLIPSOID\",\n                },\n            },\n            {\n                \"id\": \"l2a\",\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-26T00:00:00Z\",\n                        \"to\": \"2019-04-26T23:59:00Z\",\n                    }\n                },\n            },\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 1024,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nShip detection with Sentinel-1 and Sentinel-2\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s2l2a\",\n        bands: [\"B02\", \"B03\", \"B04\", \"B08\"],\n      },\n      {\n        datasource: \"s1grd\",\n        bands: [\"VV\", \"VH\"],\n      },\n    ],\n    output: [\n      {\n        bands: 3,\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(\n  samples,\n  inputData,\n  inputMetadata,\n  customData,\n  outputMetadata\n) {\n  var S2L2A = samples.s2l2a[0]\n  var S1 = samples.s1grd[0]\n\n  let ndwi = (S2L2A.B03 - S2L2A.B08) / (S2L2A.B03 + S2L2A.B08)\n  if (ndwi &gt; 0.1) {\n    if (S1.VV &gt; 0.3 || S1.VH &gt; 0.3) {\n      return [1, 1, 1]\n    }\n    return [4 * S2L2A.B04 - 0.2, 4 * S2L2A.B03 - 0.2, 5 * S2L2A.B02 - 0.2]\n  }\n  return [4 * S2L2A.B04 - 0.2, 4 * S2L2A.B03 - 0.2, 4 * S2L2A.B02 - 0.2]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                119.60987091064452,\n                32.176774851931214,\n                119.91474151611328,\n                32.3640132852233,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"id\": \"s1grd\",\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-05-23T00:00:00Z\",\n                        \"to\": \"2020-05-23T23:59:00Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"SIGMA0_ELLIPSOID\",\n                },\n            },\n            {\n                \"id\": \"s2l2a\",\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-05-23T00:00:00Z\",\n                        \"to\": \"2020-05-23T23:59:00Z\",\n                    }\n                },\n            },\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 742,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nBuilt up areas detection with Sentinel-1 and Sentinel-2\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s2l1c\",\n        bands: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\"],\n      },\n      {\n        datasource: \"s1grd\",\n        bands: [\"VV\", \"VH\"],\n      },\n      {\n        datasource: \"s2l2a\",\n        bands: [\"B02\", \"B03\", \"B04\"],\n      },\n    ],\n    output: [\n      {\n        bands: 3,\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(samples) {\n  var S2L1C = samples.s2l1c[0]\n  var S2L2A = samples.s2l2a[0]\n  var S1 = samples.s1grd[0]\n  let ndvi = (S2L1C.B08 - S2L1C.B04) / (S2L1C.B08 + S2L1C.B04)\n  if (ndvi &gt; 0.5) {\n    return [3 * S2L2A.B04, 3 * S2L2A.B03, 3 * S2L2A.B02]\n  }\n  let ndmi = (S2L1C.B08 - S2L1C.B11) / (S2L1C.B08 + S2L1C.B11)\n  if (ndmi &gt; 0) {\n    return [3 * S2L2A.B04, 3 * S2L2A.B03, 4 * S2L2A.B02]\n  }\n  if (S1.VH &gt; 0.2 || S1.VV &gt; 0.2) {\n    return [S1.VH * 5.5, S1.VV, S1.VH * 8]\n  }\n  return [3 * S2L1C.B04 - 0.2, 3 * S2L1C.B03 - 0.2, 3 * S2L1C.B02 - 0.2]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                12.280998229980469,\n                45.40206593659076,\n                12.43274688720703,\n                45.47361429775641,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"id\": \"s2l1c\",\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-12-10T00:00:00Z\",\n                        \"to\": \"2019-12-10T23:59:00Z\",\n                    }\n                },\n            },\n            {\n                \"id\": \"s1grd\",\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-12-10T00:00:00Z\",\n                        \"to\": \"2019-12-10T23:59:00Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"SIGMA0_ELLIPSOID\",\n                },\n            },\n            {\n                \"id\": \"s2l2a\",\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-12-10T00:00:00Z\",\n                        \"to\": \"2019-12-10T23:59:00Z\",\n                    }\n                },\n            },\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 1024,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nFire monitoring with Sentinel-1 and Sentinel-2\nevalscript = \"\"\"\n//VERSION=3\n// Multitemporal forest fire progression monitoring script utilizing a) Sentinel-2 data from 7 September 2019 for the visualization of burned areas\n// and b) Sentinel-1 SAR data to monitor forest fire progression in overcast conditions on 12 September 2019.\n\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s1_t1\",\n        bands: [\"VH\"],\n      }, // S1 data from 7 September 2019 (t1)\n      {\n        datasource: \"s1_t2\",\n        bands: [\"VV\", \"VH\"],\n      }, // S1 data from 12 September 2019 (t2)\n      {\n        datasource: \"l2a_t1\",\n        bands: [\"B03\", \"B04\", \"B08\", \"B11\", \"B12\"],\n      },\n    ], // S2 data from 7 September 2019 (t1)\n    output: [\n      {\n        bands: 3,\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(\n  samples,\n  inputData,\n  inputMetadata,\n  customData,\n  outputMetadata\n) {\n  var s1_1 = samples.s1_t1[0] //Assigns S1 data from t1\n  var s1_2 = samples.s1_t2[0] //Assigns S1 data from t2\n  var s2_1 = samples.l2a_t1[0] //Assigns S2 data from t1\n\n  // Calculate indices with S2 data from t1 for Burned Area visualization by Monja Sebela\n  var NDWI = index(s2_1.B03, s2_1.B08)\n  var NDVI = index(s2_1.B08, s2_1.B04)\n  var INDEX = (s2_1.B11 - s2_1.B12) / (s2_1.B11 + s2_1.B12) + s2_1.B08\n\n  // Calculate difference in S1 VH backscatter between second (t2) and first scene (t1) (Belenguer-Plomer et al. 2019)\n  var VH_diff = s1_2.VH - s1_1.VH\n\n  // Set classification threshholds\n  var thr_VH = 0.03\n  var thr_VH_diff = -0.015\n  var thr_VV = 0.2\n\n  if (NDWI &gt; 0.15 || NDVI &gt; 0.35 || INDEX &gt; 0.2) {\n    // If non-burned areas in S2 image from t1\n    if (s1_2.VH &lt; thr_VH && VH_diff &lt; thr_VH_diff) {\n      // are classified as burned in S1 image from t2 via thresholds for VH backscatter and the calculated difference layer\n      return [1, 0, 0] // Return red color\n    } else {\n      return [2.5 * s2_1.B12, 2.5 * s2_1.B08, 2.5 * s2_1.B04] // Else return SWIR composite\n    }\n  } else {\n    if (s1_2.VV &lt; thr_VV) {\n      // Else, if already burnt area is also burned in S1 image from t2\n      return [0.9, 0.9, 0.7] // Return beige color\n    } else {\n      return [0, 0, 1] // Else return blue for areas that are no longer burned in S1 image from t2\n    }\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                -59.75738525390625,\n                -19.919130502461016,\n                -58.7274169921875,\n                -19.062117883514652,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"id\": \"l2a_t1\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-09-06T00:00:00Z\",\n                        \"to\": \"2019-09-08T23:59:59Z\",\n                    }\n                },\n            },\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"id\": \"s1_t1\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-09-06T00:00:00Z\",\n                        \"to\": \"2019-09-08T23:59:59Z\",\n                    }\n                },\n            },\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"id\": \"s1_t2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-09-11T00:00:00Z\",\n                        \"to\": \"2019-09-13T23:59:59Z\",\n                    }\n                },\n            },\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 1024,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nMonitoring low pressure clouds with Sentinel-3 OLCI and Sentinel-5P\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"s3olci\",\n        bands: [\"B04\", \"B06\", \"B08\"],\n      },\n      {\n        datasource: \"s5pl2\",\n        bands: [\"CLOUD_TOP_PRESSURE\"],\n      },\n    ],\n    output: [\n      {\n        bands: 3,\n      },\n    ],\n  }\n}\n\nvar minVal = 10000.0\nvar maxVal = 110000.0\nvar diff = maxVal - minVal\nvar limits = [\n  minVal,\n  minVal + 0.125 * diff,\n  minVal + 0.375 * diff,\n  minVal + 0.625 * diff,\n  minVal + 0.875 * diff,\n  maxVal,\n]\nvar colors = [\n  [0, 0, 0.5],\n  [0, 0, 1],\n  [0, 1, 1],\n  [1, 1, 0],\n  [1, 0, 0],\n  [0.5, 0, 0],\n]\n\nfunction evaluatePixel(\n  samples,\n  inputData,\n  inputMetadata,\n  customData,\n  outputMetadata\n) {\n  var S5 = samples.s5pl2[0]\n  var S3 = samples.s3olci[0]\n  var CLOUD_TOP_PRESSURE = S5.CLOUD_TOP_PRESSURE\n\n  if (CLOUD_TOP_PRESSURE &gt; 0) {\n    return colorBlend(CLOUD_TOP_PRESSURE, limits, colors)\n  }\n  return [S3.B08 * 3, S3.B06 * 3, S3.B04 * 3.5]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                -154.82,\n                21.96,\n                -135.66,\n                13.56,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n        },\n        \"data\": [\n            {\n                \"id\": \"s3olci\",\n                \"type\": \"sentinel-3-olci\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-07-24T00:00:00Z\",\n                        \"to\": \"2020-07-24T23:59:59Z\",\n                    }\n                },\n            },\n            {\n                \"id\": \"s5pl2\",\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-07-24T00:00:00Z\",\n                        \"to\": \"2020-07-24T23:59:59Z\",\n                    }\n                },\n            },\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 449,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/S2L2A.html",
    "href": "APIs/SentinelHub/Process/Examples/S2L2A.html",
    "title": "Examples for S2L2A",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nTrue Color\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\", // default value - scales the output values from [0,1] to [0,255].\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color (EPSG 32633)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"},\n            \"bbox\": [\n                408553.58,\n                5078145.48,\n                466081.02,\n                5126576.61,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color, resolution (EPSG 32633)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"},\n            \"bbox\": [\n                408553.58,\n                5078145.48,\n                466081.02,\n                5126576.61,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 100,\n        \"resy\": 100,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color, multi-band GeoTIff\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"image/tiff\"})\n\n\nTrue Color, cloudy pixels masked out\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"SCL\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  if ([8, 9, 10].includes(sample.SCL)) {\n    return [1, 0, 0]\n  } else {\n    return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue color and metadata (multi-part response GeoTIFF and json)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    mosaicking: Mosaicking.ORBIT,\n    output: { id: \"default\", bands: 3 },\n  }\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = { scenes: scenes.orbits }\n}\n\nfunction evaluatePixel(samples) {\n  return [2.5 * samples[1].B04, 2.5 * samples[1].B03, 2.5 * samples[1].B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ]\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 200,\n        \"height\": 100,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"userdata\",\n                \"format\": {\"type\": \"application/json\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nTrue color multi-part-reponse (different formats and SampleType)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B03\", \"B02\"],\n        units: \"REFLECTANCE\", // default units\n      },\n    ],\n    output: [\n      {\n        id: \"default\",\n        bands: 3,\n        sampleType: \"AUTO\", // default  - scales the output values from input values [0,1] to [0,255].\n      },\n      {\n        id: \"true_color_8bit\",\n        bands: 3,\n        sampleType: \"UINT8\", //floating point values are automatically rounded to the nearest integer by the service.\n      },\n      {\n        id: \"true_color_16bit\",\n        bands: 3,\n        sampleType: \"UINT16\", //floating point values are automatically rounded to the nearest integer by the service.\n      },\n      {\n        id: \"true_color_32float\",\n        bands: 3,\n        sampleType: \"FLOAT32\",\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return {\n    // output band values are scaled from [0,1] to [0,255]. Multiply by 2.5 to increase brightness\n    default: [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02],\n\n    // Multiply input reflectance values by 2.5 to increase brighness and by 255 to return the band values clamped to [0, 255] unsigned 8 bit range.\n    true_color_8bit: [\n      2.5 * sample.B04 * 255,\n      2.5 * sample.B03 * 255,\n      2.5 * sample.B02 * 255,\n    ],\n\n    // Multiply input reflectance values by 2.5 to increase brightness and by 65535 to return the band values clamped to [0, 65535] unsigned 16 bit range.\n    true_color_16bit: [\n      2.5 * sample.B04 * 65535,\n      2.5 * sample.B03 * 65535,\n      2.5 * sample.B02 * 65535,\n    ],\n\n    // Returns band reflectance.\n    true_color_32float: [sample.B04, sample.B03, sample.B02],\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                12.206251,\n                41.627351,\n                12.594042,\n                41.856879,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/jpeg\"},\n            },\n            {\n                \"identifier\": \"true_color_8bit\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"true_color_16bit\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"true_color_32float\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nNDVI as jpeg image with bounds given as polygon\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 3,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n\n  if (ndvi &lt; -0.5) return [0.05, 0.05, 0.05]\n  else if (ndvi &lt; -0.2) return [0.75, 0.75, 0.75]\n  else if (ndvi &lt; -0.1) return [0.86, 0.86, 0.86]\n  else if (ndvi &lt; 0) return [0.92, 0.92, 0.92]\n  else if (ndvi &lt; 0.025) return [1, 0.98, 0.8]\n  else if (ndvi &lt; 0.05) return [0.93, 0.91, 0.71]\n  else if (ndvi &lt; 0.075) return [0.87, 0.85, 0.61]\n  else if (ndvi &lt; 0.1) return [0.8, 0.78, 0.51]\n  else if (ndvi &lt; 0.125) return [0.74, 0.72, 0.42]\n  else if (ndvi &lt; 0.15) return [0.69, 0.76, 0.38]\n  else if (ndvi &lt; 0.175) return [0.64, 0.8, 0.35]\n  else if (ndvi &lt; 0.2) return [0.57, 0.75, 0.32]\n  else if (ndvi &lt; 0.25) return [0.5, 0.7, 0.28]\n  else if (ndvi &lt; 0.3) return [0.44, 0.64, 0.25]\n  else if (ndvi &lt; 0.35) return [0.38, 0.59, 0.21]\n  else if (ndvi &lt; 0.4) return [0.31, 0.54, 0.18]\n  else if (ndvi &lt; 0.45) return [0.25, 0.49, 0.14]\n  else if (ndvi &lt; 0.5) return [0.19, 0.43, 0.11]\n  else if (ndvi &lt; 0.55) return [0.13, 0.38, 0.07]\n  else if (ndvi &lt; 0.6) return [0.06, 0.33, 0.04]\n  else return [0, 0.27, 0]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\n                    \"type\": \"image/jpeg\",\n                    \"quality\": 80,\n                },\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nExact NDVI values using a floating point GeoTIFF\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.FLOAT32,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n  return [ndvi]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"harmonizeValues\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNDVI values as INT16 raster\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.INT16, //floating point values are automatically rounded to the nearest integer by the service.\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n  // Return NDVI multiplied by 10000 as integers to save processing units. To obtain NDVI values, simply divide the resulting pixel values by 10000.\n  return [ndvi * 10000]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"harmonizeValues\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNDVI image and value (multi-part response png and GeoTIFF)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n      },\n    ],\n    output: [\n      {\n        id: \"default\",\n        bands: 1,\n        sampleType: SampleType.FLOAT32,\n      },\n      {\n        id: \"ndvi_image\",\n        bands: 3,\n        sampleType: SampleType.AUTO,\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n\n  if (ndvi &lt; -0.5) image = [0.05, 0.05, 0.05]\n  else if (ndvi &lt; -0.2) image = [0.75, 0.75, 0.75]\n  else if (ndvi &lt; -0.1) image = [0.86, 0.86, 0.86]\n  else if (ndvi &lt; 0) image = [0.92, 0.92, 0.92]\n  else if (ndvi &lt; 0.025) image = [1, 0.98, 0.8]\n  else if (ndvi &lt; 0.05) image = [0.93, 0.91, 0.71]\n  else if (ndvi &lt; 0.075) image = [0.87, 0.85, 0.61]\n  else if (ndvi &lt; 0.1) image = [0.8, 0.78, 0.51]\n  else if (ndvi &lt; 0.125) image = [0.74, 0.72, 0.42]\n  else if (ndvi &lt; 0.15) image = [0.69, 0.76, 0.38]\n  else if (ndvi &lt; 0.175) image = [0.64, 0.8, 0.35]\n  else if (ndvi &lt; 0.2) image = [0.57, 0.75, 0.32]\n  else if (ndvi &lt; 0.25) image = [0.5, 0.7, 0.28]\n  else if (ndvi &lt; 0.3) image = [0.44, 0.64, 0.25]\n  else if (ndvi &lt; 0.35) image = [0.38, 0.59, 0.21]\n  else if (ndvi &lt; 0.4) image = [0.31, 0.54, 0.18]\n  else if (ndvi &lt; 0.45) image = [0.25, 0.49, 0.14]\n  else if (ndvi &lt; 0.5) image = [0.19, 0.43, 0.11]\n  else if (ndvi &lt; 0.55) image = [0.13, 0.38, 0.07]\n  else if (ndvi &lt; 0.6) image = [0.06, 0.33, 0.04]\n  else image = [0, 0.27, 0]\n\n  return {\n    default: [ndvi],\n    ndvi_image: image,\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"ndvi_image\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nAll S2L2A raw bands, original data (no harmonization)\nLearn about harmonization here.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\n          \"B01\",\n          \"B02\",\n          \"B03\",\n          \"B04\",\n          \"B05\",\n          \"B06\",\n          \"B07\",\n          \"B08\",\n          \"B8A\",\n          \"B09\",\n          \"B11\",\n          \"B12\",\n        ],\n        units: \"DN\",\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 12,\n      sampleType: SampleType.UINT16,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [\n    sample.B01,\n    sample.B02,\n    sample.B03,\n    sample.B04,\n    sample.B05,\n    sample.B06,\n    sample.B07,\n    sample.B08,\n    sample.B8A,\n    sample.B09,\n    sample.B11,\n    sample.B12,\n  ]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"harmonizeValues\": \"false\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nOther S2L2A specific data (Aerosol Optical Thickness, Scene Classification, Snow and Cloud probabilities, Sun and View angles)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\n          \"B02\",\n          \"B03\",\n          \"B04\",\n          \"AOT\",\n          \"SCL\",\n          \"SNW\",\n          \"CLD\",\n          \"sunAzimuthAngles\",\n          \"sunZenithAngles\",\n          \"viewAzimuthMean\",\n          \"viewZenithMean\",\n        ],\n      },\n    ],\n    output: [\n      { id: \"TrueColor\", bands: 3, sampleType: SampleType.FLOAT32 },\n      { id: \"AOT\", bands: 1, sampleType: SampleType.UINT16 },\n      { id: \"SCL\", bands: 1, sampleType: SampleType.UINT8 },\n      { id: \"SNW\", bands: 1, sampleType: SampleType.UINT8 },\n      { id: \"CLD\", bands: 1, sampleType: SampleType.UINT8 },\n      { id: \"SAA\", bands: 1, sampleType: SampleType.FLOAT32 },\n      { id: \"SZA\", bands: 1, sampleType: SampleType.FLOAT32 },\n      { id: \"VAM\", bands: 1, sampleType: SampleType.FLOAT32 },\n      { id: \"VZM\", bands: 1, sampleType: SampleType.FLOAT32 },\n    ],\n  }\n}\n\nfunction evaluatePixel(sample) {\n  var truecolor = [sample.B04, sample.B03, sample.B02]\n  var aot = [sample.AOT]\n  var scl = [sample.SCL]\n  var snw = [sample.SNW]\n  var cld = [sample.CLD]\n  var saa = [sample.sunAzimuthAngles]\n  var sza = [sample.sunZenithAngles]\n  var vam = [sample.viewAzimuthMean]\n  var vzm = [sample.viewZenithMean]\n\n  return {\n    TrueColor: truecolor,\n    AOT: aot,\n    SCL: scl,\n    SNW: snw,\n    CLD: cld,\n    SAA: saa,\n    SZA: sza,\n    VAM: vam,\n    VZM: vzm,\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"TrueColor\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"AOT\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"SCL\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"SNW\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"CLD\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"SAA\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"SZA\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"VAM\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"VZM\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/S3SLSTR.html",
    "href": "APIs/SentinelHub/Process/Examples/S3SLSTR.html",
    "title": "Examples for S3SLSTR",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nFalse Color\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"S3\", \"S2\", \"S1\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\",\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2 * sample.S3, 2 * sample.S2, 2 * sample.S1]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                8.558382,\n                41.359678,\n                9.579525,\n                43.055688,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nFalse Color (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"S3\", \"S2\", \"S1\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\",\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2 * sample.S3, 2 * sample.S2, 2 * sample.S1]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nFalse Color, resolution (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"S3\", \"S2\", \"S1\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\",\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2 * sample.S3, 2 * sample.S2, 2 * sample.S1]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 250,\n        \"resy\": 250,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nThermal IR fire emission band, gradient visualizer (K)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"F1\"],\n    output: {\n      bands: 3,\n    },\n  }\n}\n\n// Create a Red gradient visualiser from 274-450 K\nvar viz = ColorGradientVisualizer.createRedTemperature(274, 450)\n\nfunction evaluatePixel(sample) {\n  return viz.process(sample.F1)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                -120.141,\n                37.5282,\n                -119.4131,\n                37.8716,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-08-06T00:00:00Z\",\n                        \"to\": \"2018-08-06T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/png\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nFalse Color and metadata (multi-part GeoTIFF and json)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"S3\", \"S2\", \"S1\"],\n    output: {\n      id: \"default\",\n      bands: 3,\n      sampleType: \"INT16\", //floating point values are automatically rounded to the nearest integer by the service.\n    },\n    mosaicking: \"TILE\",\n  }\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = { scenes: scenes.tiles }\n}\n\nfunction evaluatePixel(sample) {\n  // Return reflectance multiplied by 10000 as integers to save processing units.\n  // To obtain reflectance values, simply divide the result pixel values by 10000.\n  return [sample[0].S3 * 10000, sample[0].S2 * 10000, sample[0].S1 * 10000] //the values are multiplied by 10000 because output sampleType is UINT16\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                8.558382,\n                41.359678,\n                9.579525,\n                43.055688,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"userdata\",\n                \"format\": {\"type\": \"application/json\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nNDVI as jpeg image with bouds given as polygon\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"S2\", \"S3\"],\n    output: {\n      bands: 3,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let NDVI = index(sample.S3, sample.S2)\n  const viz = ColorGradientVisualizer.createWhiteGreen(-0.1, 1.0)\n  return viz.process(NDVI)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            535547.7290897672,\n                            4767538.771691742,\n                        ],\n                        [\n                            542559.6872296461,\n                            4744749.907737136,\n                        ],\n                        [\n                            550448.1401370098,\n                            4660606.41005859,\n                        ],\n                        [\n                            521523.8128100095,\n                            4570327.449007649,\n                        ],\n                        [\n                            474193.0953658272,\n                            4600128.271102134,\n                        ],\n                        [\n                            461045.67385355436,\n                            4630805.5879641045,\n                        ],\n                        [\n                            453157.22094619065,\n                            4698295.685060439,\n                        ],\n                        [\n                            497858.45408791833,\n                            4741243.928667196,\n                        ],\n                        [\n                            520647.3180425246,\n                            4744749.907737136,\n                        ],\n                        [\n                            525906.2866474338,\n                            4771044.750761681,\n                        ],\n                        [\n                            525906.2866474338,\n                            4771044.750761681,\n                        ],\n                        [\n                            525906.2866474338,\n                            4771044.750761681,\n                        ],\n                        [\n                            535547.7290897672,\n                            4767538.771691742,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [{\"format\": {\"type\": \"image/jpeg\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNDVI image and value (multi-part response png and GeoTIFF)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"S2\", \"S3\"],\n      },\n    ],\n    output: [\n      {\n        id: \"default\",\n        bands: 1,\n        sampleType: \"INT16\",\n      },\n      {\n        id: \"ndvi_image\",\n        bands: 3,\n        sampleType: \"AUTO\",\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let NDVI = index(sample.S3, sample.S2)\n  const viz = ColorGradientVisualizer.createWhiteGreen(-0.1, 1.0)\n  return {\n    default: [NDVI * 10000],\n    ndvi_image: viz.process(NDVI),\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            535547.7290897672,\n                            4767538.771691742,\n                        ],\n                        [\n                            542559.6872296461,\n                            4744749.907737136,\n                        ],\n                        [\n                            550448.1401370098,\n                            4660606.41005859,\n                        ],\n                        [\n                            521523.8128100095,\n                            4570327.449007649,\n                        ],\n                        [\n                            474193.0953658272,\n                            4600128.271102134,\n                        ],\n                        [\n                            461045.67385355436,\n                            4630805.5879641045,\n                        ],\n                        [\n                            453157.22094619065,\n                            4698295.685060439,\n                        ],\n                        [\n                            497858.45408791833,\n                            4741243.928667196,\n                        ],\n                        [\n                            520647.3180425246,\n                            4744749.907737136,\n                        ],\n                        [\n                            525906.2866474338,\n                            4771044.750761681,\n                        ],\n                        [\n                            525906.2866474338,\n                            4771044.750761681,\n                        ],\n                        [\n                            525906.2866474338,\n                            4771044.750761681,\n                        ],\n                        [\n                            535547.7290897672,\n                            4767538.771691742,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"ndvi_image\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nVNIR and SWIR bands as a GeoTIFF (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\"],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    output: {\n      bands: 6,\n      sampleType: \"UINT16\", //floating point values are automatically rounded to the nearest integer by the service.\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  // Return reflectance multiplied by 10000 as integers to save processing units.\n  // To obtain reflectance or BT values, simply divide the resulting pixel values by 10000.\n  return [\n    10000 * sample.S1,\n    10000 * sample.S2,\n    10000 * sample.S3,\n    10000 * sample.S4,\n    10000 * sample.S5,\n    10000 * sample.S6,\n  ]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 500,\n        \"resy\": 500,\n        \"responses\": [{\"format\": {\"type\": \"image/tiff\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTIR bands as a GeoTIFF (EPSG 32632)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"S7\", \"S8\", \"S9\", \"F1\", \"F2\"],\n      },\n    ],\n    output: {\n      bands: 5,\n      sampleType: \"UINT16\",\n    },\n  }\n}\n\nfunction multiplyband(sample) {\n  // Multiply by 100\n  return 100 * sample\n}\n\nfunction evaluatePixel(sample) {\n  // Return the bands multiplied by 100 as integers to save processing units.\n  // To obtain reflectance or BT values, simply divide the resulting pixel values by 100.\n  return [\n    multiplyband(sample.S7),\n    multiplyband(sample.S8),\n    multiplyband(sample.S9),\n    multiplyband(sample.F1),\n    multiplyband(sample.F2),\n  ]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n            \"bbox\": [\n                444170,\n                4574059,\n                557052,\n                4767386,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-3-slstr\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-20T00:00:00Z\",\n                        \"to\": \"2020-06-20T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"DESCENDING\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 500,\n        \"resy\": 500,\n        \"responses\": [{\"format\": {\"type\": \"image/tiff\"}}],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript.html",
    "href": "APIs/SentinelHub/Evalscript.html",
    "title": "Evalscript (custom script)",
    "section": "",
    "text": "An evalscript (or \"custom script\") is a piece of Javascript code which defines how the satellite data shall be processed by Sentinel Hub and what values the service shall return. It is a required part of any process, batch processing or OGC request.\nEvalscripts can use any JavaScript function or language structures, along with certain utility functions we provide for your convenience. For running evalscripts we use the Chrome V8 JavaScript engine.\nIn the Evalscript V3 section you will find a technical documentation with detailed explanations of parameters and functions you can use in your evalscripts.\n\nExamples\nExamples of various evalscritps can be found on our Custom Scripts Repository.\n\n\nTutorials and Other Related Materials\n\nA PDF tutorial on writing simple evalscripts for beginners: Custom scripts tutorial\nA webinar on writing evalscripts for beginners: Custom Scripts, September 28, 2020\nA webinar on multi-temporal scripts and data fusion: Multi-temporal Scripts and Data Fusion, March 3, 2021\nA blog on good scripting practices: Custom Scripts: Faster, Cheaper, Better!, November 18, 2019\nA blog post on color maps: PUCK - Perceptually Uniform Color Maps in Satellite Imagery, January 28, 2021\nA blog post on sampleType: SampleType: what’s all the fuss about?, February 15, 2022\nMore blog posts and useful links can be found on our Sentinel Hub website."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/Examples.html",
    "href": "APIs/SentinelHub/OGC/Examples.html",
    "title": "Examples of OGC API",
    "section": "",
    "text": "Below are examples for all our OGC APIs. To run the examples yourself, replace &lt;INSTANCE_ID&gt; in the URLs with your own configuration instance id and paste the url in any web browser. Your configuration must be based on the \"Simple WMS template\", which can be found when you create new configuration in the Dashboard in \"Configuration Utility\" tab.\nIf you want to check interactive example use this link.\n\nWMS #1\n\nURL\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?REQUEST=GetMap&BBOX=3238005,5039853,3244050,5045897&LAYERS=NATURAL-COLOR&MAXCC=20&WIDTH=320&HEIGHT=320&FORMAT=image/jpeg&TIME=2018-03-29/2018-05-29\n\n\nParameters\n\n\n\nParameters\nOptions\n\n\n\n\nLAYERS\nNATURAL-COLOR\n\n\nFORMAT\nimage/jpeg\n\n\nMAXCC\n20\n\n\nWIDTH\n320\n\n\nHEIGHT\n320\n\n\nTIME\n2018-03-29/2018-05-29\n\n\n\n\n\nResult\n\n\n\n\nWMS #2\n\nURL\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?REQUEST=GetMap&BBOX=3238005,5039853,3244050,5045897&FORMAT=image/jpeg&LAYERS=NATURAL-COLOR&MAXCC=20&WIDTH=320&HEIGHT=320&TIME=2017-01-29/2017-02-29\n\n\nParameters\n\n\n\nParameters\nOptions\n\n\n\n\nLAYERS\nNATURAL-COLOR\n\n\nFORMAT\nimage/jpeg\n\n\nMAXCC\n20\n\n\nWIDTH\n320\n\n\nHEIGHT\n320\n\n\nTIME\n2017-01-29/2017-02-29\n\n\n\n\n\nResult\n\n\n\n\nWCS\n\nURL\nhttps://sh.dataspace.copernicus.eu/ogc/wcs/&lt;INSTANCE_ID&gt;?SERVICE=WCS&REQUEST=GetCoverage&COVERAGE=NATURAL-COLOR&BBOX=3238005,5039853,3244050,5045897&MAXCC=20&WIDTH=320&HEIGHT=320&FORMAT=image/jpeg&TIME=2019-03-29/2019-05-29\n\n\nParameters\n\n\n\nParameters\nOptions\n\n\n\n\nLAYERS\nNATURAL-COLOR\n\n\nFORMAT\nimage/jpeg\n\n\nMAXCC\n20\n\n\nWIDTH\n320\n\n\nHEIGHT\n320\n\n\nTIME\n2019-03-29/2019-05-29\n\n\n\n\n\nResult\n\n\n\n\nWMTS\n\nURL\nhttps://sh.dataspace.copernicus.eu/ogc/wmts/&lt;INSTANCE_ID&gt;?REQUEST=GetTile&BBOX=3238005,5039853,3244050,5045897&RESOLUTION=10&TILEMATRIXSET=PopularWebMercator512&LAYER=FALSE-COLOR&MAXCC=20&TILEMATRIX=14&TILEROW=3065&TILECOL=4758&TIME=2018-03-29/2018-05-29\n\n\nParameters\n\n\n\nParameters\nOptions\n\n\n\n\nLAYERS\nFALSE-COLOR\n\n\nMAXCC\n20\n\n\nRESOLUTION\n10\n\n\nTILEMATRIXSET\nPopularWebMercator512\n\n\nTILEMATRIX\n14\n\n\nTILEROW\n3065\n\n\nTILECOL\n4758\n\n\nTIME\n2018-03-29/2018-05-29\n\n\n\n\n\nResult\n\n\n\n\nWFS\n\nURL\nhttps://sh.dataspace.copernicus.eu/ogc/wfs/&lt;INSTANCE_ID&gt;?REQUEST=GetFeature&srsName=EPSG:3857&TYPENAMES=DSS2&BBOX=3238005,5039853,3244050,5045897&TIME=2019-02-11/2019-02-12\n\n\nParameters\n\n\n\nParameters\nOptions\n\n\n\n\nREQUEST\nGetFeature\n\n\nsrsName\nEPSG:3857\n\n\nTYPENAMES\nDSS2\n\n\nBBOX\n3238005,5039853,3244050,5045897\n\n\nTIME\n2019-02-11/2019-02-12\n\n\n\n\n\nResult\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\n&lt;wfs:FeatureCollection xsi:schemaLocation=\"http://www.opengis.net/wfs/2.0 http://schemas.opengis.net/wfs/2.0/wfs.xsd http://www.opengis.net/gml/3.2 http://schemas.opengis.net/gml/3.2.1/gml.xsd\"\n    xmlns:sh=\"https://www.sentinel-hub.com/\" xmlns:gml=\"http://www.opengis.net/gml/3.2\" xmlns:wfs=\"http://www.opengis.net/wfs/2.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;\n  &lt;wfs:boundedBy&gt;\n    &lt;gml:Box srsName='urn:ogc:def:crs:EPSG::3857'&gt;\n      &lt;gml:coordinates&gt;\n        3137112.369571343,4944408.712920986 3285542.013115577,5093151.414429454\n      &lt;/gml:coordinates&gt;\n    &lt;/gml:Box&gt;\n  &lt;/wfs:boundedBy&gt;\n  &lt;wfs:member&gt;\n    &lt;DSS2&gt;\n      &lt;gml:boundedBy&gt;\n        &lt;gml:Box srsName='urn:ogc:def:crs:EPSG::3857'&gt;\n          &lt;gml:coordinates&gt;\n            3137112.369571343,4944408.712920986 3285542.013115577,5093151.414429454\n          &lt;/gml:coordinates&gt;\n        &lt;/gml:Box&gt;\n      &lt;/gml:boundedBy&gt;\n      &lt;id&gt;S2A_OPER_MSI_L2A_TL_SGS__20190212T133228_A019023_T35TPF_N02.11&lt;/id&gt;\n      &lt;date&gt;2019-02-12&lt;/date&gt;\n      &lt;time&gt;09:08:52&lt;/time&gt;\n      &lt;path&gt;s3://sentinel-s2-l2a/tiles/35/T/PF/2019/2/12/0&lt;/path&gt;\n      &lt;crs&gt;EPSG:32635&lt;/crs&gt;\n      &lt;mbr&gt;600000,4490220 709800,4600020&lt;/mbr&gt;\n      &lt;cloudCoverPercentage&gt;97.48&lt;/cloudCoverPercentage&gt;\n      &lt;geometryProperty&gt;\n        &lt;gml:MultiPolygon srsName='urn:ogc:def:crs:EPSG::3857'&gt;\n          &lt;gml:polygonMember&gt;\n            &lt;gml:Polygon&gt;\n              &lt;gml:outerBoundaryIs&gt;\n                &lt;gml:LinearRing&gt;\n                  &lt;gml:coordinates&gt;\n                    3139096.254297407,5093151.414429454 3137112.369571343,4947176.295365512 3272770.2640233915,4944408.712920986 3273149.797764646,4946283.966865066 3273655.8785869186,4947972.618733139 3274080.280822489,4949071.057870209 3275105.522264074,4952896.767191993 3275390.8923655148,4953708.980760984 3275718.486013052,4955098.598468996 3282365.302587746,4979008.234912587\n                    3285542.013115577,5089991.454384799 3139096.254297407,5093151.414429454\n                  &lt;/gml:coordinates&gt;\n                &lt;/gml:LinearRing&gt;\n              &lt;/gml:outerBoundaryIs&gt;\n            &lt;/gml:Polygon&gt;\n          &lt;/gml:polygonMember&gt;\n        &lt;/gml:MultiPolygon&gt;\n      &lt;/geometryProperty&gt;\n    &lt;/DSS2&gt;\n  &lt;/wfs:member&gt;\n&lt;/wfs:FeatureCollection&gt;"
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WMTS.html",
    "href": "APIs/SentinelHub/OGC/WMTS.html",
    "title": "Web Mapping Tile Service",
    "section": "",
    "text": "The Sentinel Hub WMTS (Web Map Tile Service) service conforms to the WMTS standard. It provides access to Sentinel-2's 13 unprocessed bands (B01 through B12, with B8A following B08) as well as processed products such as true color imagery and NDVI. Access to the service is done via a custom server instance URL which will be provided to you upon registration. Provides access to the same bands product and additional informational layers as the WMS request except only one layer can be specified at once, even when only raw Sentinel-2 bands are used. As with the WMS service, WMTS is also only available via a user-preconfigured custom server instance URL.\nSee our OGC API Webinar, which will guide you through different OGC services, including WMTS, help you understand the structure, show you how to run the requests in different environments and how they can be integrated with QGIS, ArcGIS and web applications.\nThe base URL for the WMTS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wmts/&lt;INSTANCE_ID&gt;\nThe service supports the same output formats as the WMS request and supports the standard WMTS requests GetTile, GetCapabilities. It supports WMTS version 1.0.0.\nIf you want to force a specific output format (e.g. float 32 or uint 16) set sampleType in your evalscript as explained here. Use dataMask band in your evalscript as explained here to make pixels transparent.\nCheck GetCapabilities for a list of supported coordinate reference systems and tile matrix sets which can be used for the TILEMATRIX and TILEMATRIXSET parameters."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WMTS.html#wmts-request",
    "href": "APIs/SentinelHub/OGC/WMTS.html#wmts-request",
    "title": "Web Mapping Tile Service",
    "section": "",
    "text": "The Sentinel Hub WMTS (Web Map Tile Service) service conforms to the WMTS standard. It provides access to Sentinel-2's 13 unprocessed bands (B01 through B12, with B8A following B08) as well as processed products such as true color imagery and NDVI. Access to the service is done via a custom server instance URL which will be provided to you upon registration. Provides access to the same bands product and additional informational layers as the WMS request except only one layer can be specified at once, even when only raw Sentinel-2 bands are used. As with the WMS service, WMTS is also only available via a user-preconfigured custom server instance URL.\nSee our OGC API Webinar, which will guide you through different OGC services, including WMTS, help you understand the structure, show you how to run the requests in different environments and how they can be integrated with QGIS, ArcGIS and web applications.\nThe base URL for the WMTS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wmts/&lt;INSTANCE_ID&gt;\nThe service supports the same output formats as the WMS request and supports the standard WMTS requests GetTile, GetCapabilities. It supports WMTS version 1.0.0.\nIf you want to force a specific output format (e.g. float 32 or uint 16) set sampleType in your evalscript as explained here. Use dataMask band in your evalscript as explained here to make pixels transparent.\nCheck GetCapabilities for a list of supported coordinate reference systems and tile matrix sets which can be used for the TILEMATRIX and TILEMATRIXSET parameters."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WMTS.html#wmts-parameters",
    "href": "APIs/SentinelHub/OGC/WMTS.html#wmts-parameters",
    "title": "Web Mapping Tile Service",
    "section": "WMTS Parameters",
    "text": "WMTS Parameters\nStandard common WMTS URL parameters (names are case insensitive):\n\n\n\nWMTS parameter\nInfo\n\n\n\n\nSERVICE\nRequired, must be \"WMTS\".\n\n\nVERSION\nWMTS version standard. Optional, default: \"1.0.0\". Supported values: \"1.0.0\".\n\n\nREQUEST\nWhat is requested, valid values: GetTile or GetCapabilities. Required.\n\n\nTIME\n(when REQUEST = GetTile) The time range for which to return the results. The result is based on all scenes between the specified times conforming to the cloud coverage criteria and stacked based on priority setting - e.g. most recent on top. It is written as two time values in ISO8601 format separated by a slash, for example: TIME=2016-01-01T09:02:44Z/2016-02-01T11:00:00Z. Reduced accuracy times, where parts of the time string are omitted, are also supported. For example, TIME=2016-07-15/2016-07-15 will be interpreted as \"TIME=2016-07-15T00:00:00Z/2016-07-15T23:59:59Z\" and TIME=2016-07/2016-08 will be interpreted as \"TIME=2016-07-01T00:00:00Z/2016-08-31T23:59:59Z\"  Optional, default: none (the last valid image is returned).  Note: Requesting a single value for TIME parameter is deprecated. Sentinel Hub interpreted it as a time interval [given time - 6 months, given time]. For vast majority of cases this resulted in unnecessary long processing time thus we strongly encourage you to always use the smallest possible time range instead.\n\n\n\nIn addition to the standard WMS URL parameters, the WMS service also supports many custom URL parameters. See Custom service URL parameters for details.\nStandard GetTile request URL parameters:\n\n\n\nWMTS parameter\nInfo\n\n\n\n\nTILEMATRIXSET\nThe matrix set to be used for the output tile. Check GetCapabilities for a list of supported matrix sets.\n\n\nTILEMATRIX\nThe matrix to be used for the output tile. Check GetCapabilities for a list of supported matrices.\n\n\nTILECOL\nThe column index of the output tile. Check GetCapabilities for a list of supported matrix widths.\n\n\nTILEROW\nThe row index of the output tile. Check GetCapabilities for a list of supported matrix heights.\n\n\nLAYER\nThe preconfigured (in the instance) layer for which to generate the output tile.\n\n\nFORMAT\nThe returned image format. Optional, default: \"image/png\", other options: \"image/jpeg\", \"image/tiff\". Detailed information about supported values."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WMS.html",
    "href": "APIs/SentinelHub/OGC/WMS.html",
    "title": "Web Mapping Service",
    "section": "",
    "text": "The Sentinel Hub WMS service conforms to the WMS standard. It not only provides access to raw satellite data but also to processed products such as true color imagery and NDVI. Access to the service is done via a custom server instance URL which will be provided to you upon registration.\nSee our OGC API Webinar, which will guide you through different OGC services, including WMS, help you understand the structure, show you how to run the requests in different environments and how it can be integrated with QGIS, ArcGIS and web applications.\nIt is possible to obtain multiple separate instances (which act as separate WMS services) each with their own configuration and list of layers which will likely be useful to advanced users.\nThe base URL for the WMS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;\nFor example, a GetCapabilities request can be done by changing the &lt;INSTANCE_ID&gt; to your provided instance ID and opening the following URL:\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?REQUEST=GetCapabilities\nSome of the most common provided products:\n\nTRUE_COLOR - a brightened RGB image\nFALSE_COLOR - uses near-infrared instead of the blue band\nNDVI - Normalized Difference Vegetation Index\nEVI - Enhanced Vegetation Index\n\nList of all available products.\nThe service supports standard WMS requests: GetMap, GetCapabilities, GetFeatureInfo, and also some custom requests. Supported WMS versions are 1.1.1 and 1.3.0.\nIf you want to force a specific output format (e.g. float 32 or uint 16) set sampleType in your evalscript as explained here. Use dataMask band in your evalscript as explained here to make pixels transparent.\nFor a list of supported coordinate reference systems check the GetCapabilities result."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WMS.html#wms-request",
    "href": "APIs/SentinelHub/OGC/WMS.html#wms-request",
    "title": "Web Mapping Service",
    "section": "",
    "text": "The Sentinel Hub WMS service conforms to the WMS standard. It not only provides access to raw satellite data but also to processed products such as true color imagery and NDVI. Access to the service is done via a custom server instance URL which will be provided to you upon registration.\nSee our OGC API Webinar, which will guide you through different OGC services, including WMS, help you understand the structure, show you how to run the requests in different environments and how it can be integrated with QGIS, ArcGIS and web applications.\nIt is possible to obtain multiple separate instances (which act as separate WMS services) each with their own configuration and list of layers which will likely be useful to advanced users.\nThe base URL for the WMS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;\nFor example, a GetCapabilities request can be done by changing the &lt;INSTANCE_ID&gt; to your provided instance ID and opening the following URL:\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?REQUEST=GetCapabilities\nSome of the most common provided products:\n\nTRUE_COLOR - a brightened RGB image\nFALSE_COLOR - uses near-infrared instead of the blue band\nNDVI - Normalized Difference Vegetation Index\nEVI - Enhanced Vegetation Index\n\nList of all available products.\nThe service supports standard WMS requests: GetMap, GetCapabilities, GetFeatureInfo, and also some custom requests. Supported WMS versions are 1.1.1 and 1.3.0.\nIf you want to force a specific output format (e.g. float 32 or uint 16) set sampleType in your evalscript as explained here. Use dataMask band in your evalscript as explained here to make pixels transparent.\nFor a list of supported coordinate reference systems check the GetCapabilities result."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WMS.html#wms-url-parameters",
    "href": "APIs/SentinelHub/OGC/WMS.html#wms-url-parameters",
    "title": "Web Mapping Service",
    "section": "WMS URL Parameters",
    "text": "WMS URL Parameters\nStandard common WMS URL parameters (parameter names are case insensitive, values are case sensitive):\n\n\n\nWMS parameter\nInfo\n\n\n\n\nSERVICE\nRequired, must be \"WMS\".\n\n\nVERSION\nWMS version standard. Optional, default: \"1.3.0\". Supported values: \"1.1.1\" and \"1.3.0\".\n\n\nREQUEST\nWhat is requested, valid values: GetMap, GetFeatureInfo, GetCapabilities or a custom request's name. Required.\n\n\nTIME\n(when REQUEST = GetTile) The time range for which to return the results. The result is based on all scenes between the specified times conforming to the cloud coverage criteria and stacked based on priority setting - e.g. most recent on top. It is written as two time values in ISO8601 format separated by a slash, for example: TIME=2016-01-01T09:02:44Z/2016-02-01T11:00:00Z. Reduced accuracy times, where parts of the time string are omitted, are also supported. For example, TIME=2016-07-15/2016-07-15 will be interpreted as \"TIME=2016-07-15T00:00:00Z/2016-07-15T23:59:59Z\" and TIME=2016-07/2016-08 will be interpreted as \"TIME=2016-07-01T00:00:00Z/2016-08-31T23:59:59Z\"  Optional, default: none (the last valid image is returned).  Note: Requesting a single value for TIME parameter is deprecated. Sentinel Hub interpreted it as a time interval [given time - 6 months, given time]. For vast majority of cases this resulted in unnecessary long processing time thus we strongly encourage you to always use the smallest possible time range instead.\n\n\n\nIn addition to the standard WMS URL parameters, the WMS service also supports many custom URL parameters. See Custom service URL parameters for details.\nStandard GetMap request URL parameters:\n\n\n\nWMS parameter\nInfo\n\n\n\n\nBBOX\nSpecifies the bounding box of the requested image. Coordinates must be in the specified coordinate reference system. The four coordinates representing the top-left and bottom-right of the bounding box must be separated by commas. Required. Example: BBOX=-13152499,4038942,-13115771,4020692\n\n\nCRS\n(when VERSION 1.3.0 or higher) the coordinate reference system in which the BBOX is specified and in which to return the image. Optional, default: \"EPSG:3857\". For a list of available CRSs see the GetCapabilities result.\n\n\nSRS\n(when VERSION 1.1.1 or lower) the coordinate reference system in which the BBOX is specified and in which to return the image. Optional, default: \"EPSG:3857\". For a list of available CRSs see the GetCapabilities result.\n\n\nFORMAT\nThe returned image format. Optional, default: \"image/png\", other options: \"image/jpeg\", \"image/tiff\". Detailed information about supported values.\n\n\nWIDTH\nReturned image width in pixels. Required, unless RESX is used. If WIDTH is used, HEIGHT is also required.\n\n\nHEIGHT\nReturned image height in pixels. Required, unless RESY is used. If HEIGHT is used, WIDTH is also required.\n\n\nRESX\nReturned horizontal image resolution in UTM units (if m is added, e.g. 10m, in metrical units). (optional instead of WIDTH). If used, RESY is also required.\n\n\nRESY\nReturned vertical image resolution in UTM units (if m is added, e.g. 10m, in metrical units). (optional instead of HEIGHT). If used, RESX is also required.\n\n\nLAYERS\nThe preconfigured layer (image) to be returned. You must specify exactly one layer and optionally add additional overlays. Required. Example: LAYERS=TRUE_COLOR,OUTLINE\n\n\nEXCEPTIONS\nThe exception format. Optional, default: \"XML\". Supported values: \"XML\", \"INIMAGE\", \"BLANK\" (all three for version &gt;= 1.3.0), \"application/vnd.ogc.se_xml\", \"application/vnd.ogc.se_inimage\", \"application/vnd.ogc.se_blank\" (all three for version &lt; 1.3.0).\n\n\n\nStandard GetFeatureInfo request URL parameters:\n\n\n\nWMS parameter\nInfo\n\n\n\n\nBBOX\nSpecifies the bounding box of the area which contains the queried point. Coordinates are in the specified CRS/SRS. Four coordinates representing the top-left and bottom-right of the bounding box must be separated by comma. Required. Example: BBOX=-13152499,4038942,-13115771,4020692\n\n\nCRS\n(when VERSION 1.3.0 or higher) the coordinate reference system in which the BBOX is specified. Optional, default: \"EPSG:3857\". For a list of available CRSs see the GetCapabilities result.\n\n\nSRS\n(when VERSION 1.1.1 or lower) the coordinate reference system in which the BBOX is specified. Optional, default: \"EPSG:3857\". For a list of available CRSs see the GetCapabilities result.\n\n\nWIDTH\nThe image-space width containing the queried point, in pixels. Required.\n\n\nHEIGHT\nThe image-space height containing the queried point, in pixels. Required.\n\n\nINFO_FORMAT\nThe output format of the feature info content. Check GetCapabilities for a list of supported formats.\n\n\nRESY\nThe layers for which the feature info is requested.\n\n\nI and J\n(when VERSION 1.3.0 or higher) The X and Y coordinates in the output image space in pixels of the feature queried.\n\n\nX and Y\n(when VERSION 1.1.1 or lower) The X and Y coordinates in the output image space in pixels of the feature queried."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/OutputFormats.html",
    "href": "APIs/SentinelHub/OGC/OutputFormats.html",
    "title": "Output Formats",
    "section": "",
    "text": "For the requests that provide image output, Sentinel-2 WMS/WMTS/WCS services can generate these output formats:\n\nimage/png - lossless image format for 1 (grayscale) or 3 (RGB) components\nimage/jpeg - lossy image format for 1 (grayscale) or 3 (RGB) components, without alpha channel. The quality can be controlled via the \"QUALITY\" URL parameter.\nimage/tiff - lossless image format for any number of the components.\n\nFind out more on how the values are reflected in the output.\n\n\nTo generate the output as jpeg, use the following example. Please replace &lt;INSTANCE_ID&gt; with your own.\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?SERVICE=WMS&REQUEST=GetMap&SHOWLOGO=false&VERSION=1.3.0&LAYERS=NDVI&MAXCC=20&WIDTH=640&HEIGHT=640&CRS=EPSG:4326&BBOX=46.697956,16.223885,46.699840,16.2276628&FORMAT=image/jpeg"
  },
  {
    "objectID": "APIs/SentinelHub/OGC/OutputFormats.html#output-image-formats",
    "href": "APIs/SentinelHub/OGC/OutputFormats.html#output-image-formats",
    "title": "Output Formats",
    "section": "",
    "text": "For the requests that provide image output, Sentinel-2 WMS/WMTS/WCS services can generate these output formats:\n\nimage/png - lossless image format for 1 (grayscale) or 3 (RGB) components\nimage/jpeg - lossy image format for 1 (grayscale) or 3 (RGB) components, without alpha channel. The quality can be controlled via the \"QUALITY\" URL parameter.\nimage/tiff - lossless image format for any number of the components.\n\nFind out more on how the values are reflected in the output.\n\n\nTo generate the output as jpeg, use the following example. Please replace &lt;INSTANCE_ID&gt; with your own.\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?SERVICE=WMS&REQUEST=GetMap&SHOWLOGO=false&VERSION=1.3.0&LAYERS=NDVI&MAXCC=20&WIDTH=640&HEIGHT=640&CRS=EPSG:4326&BBOX=46.697956,16.223885,46.699840,16.2276628&FORMAT=image/jpeg"
  },
  {
    "objectID": "APIs/SentinelHub/OGC/OutputFormats.html#output-vector-formats",
    "href": "APIs/SentinelHub/OGC/OutputFormats.html#output-vector-formats",
    "title": "Output Formats",
    "section": "Output Vector Formats",
    "text": "Output Vector Formats\nFor the requests that provide vector output, Sentinel-2 WMS/WMTS/WCS services can generate these output formats:\n\napplication/x-esri-shape - zip containing shape files\napplication/json - GeoJSON file\n\nBoth formats are returning polygons in vector format only in case when the image does not consists of more than 10 different values. Therefore, this formats only work with custom script layers.\n\nExample requests for vector formats:\nTo generate the output as GeoJSON file, follow the example below. Replace &lt;INSTANCE_ID&gt; with your own.\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?SERVICE=WMS&REQUEST=GetMap&SHOWLOGO=false&VERSION=1.3.0&LAYERS=NDVI&MAXCC=20&WIDTH=640&HEIGHT=640&CRS=EPSG:4326&BBOX=46.697956,16.223885,46.699840,16.2276628&FORMAT=application/json\n{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {\n        \"COLOR_HEX\": \"FFFFFF\",\n        \"ID\": 0\n      },\n      \"geometry\": {\n        \"type\": \"MultiPolygon\",\n        \"crs\": {\n            \"type\": \"name\",\n            \"properties\": {\n                \"name\": \"urn:ogc:def:crs:OGC::CRS84\"\n            }\n        },\n        \"coordinates\": [[[\n            [16.225567302, 46.698948044],\n            [16.225567302, 46.6989451],\n            [16.225561399, 46.6989451],\n            [16.225561399, 46.698942156],\n            ...\n        ]]]\n      }\n    },\n    ...\n  ]\n}\nTo generate the output as x-esri-shape, replace the FORMAT with application/x-esri-shape, which will enable you to get the zip containing shape files."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCI.html",
    "href": "APIs/SentinelHub/Data/S3OLCI.html",
    "title": "Sentinel-3 OLCI L1B",
    "section": "",
    "text": "General information about Sentinel-3 mission can be found here. Sentinel Hub offers Sentinel-3 OLCI Level 1B products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCI.html#about-sentinel-3-olci-l1b-data",
    "href": "APIs/SentinelHub/Data/S3OLCI.html#about-sentinel-3-olci-l1b-data",
    "title": "Sentinel-3 OLCI L1B",
    "section": "",
    "text": "General information about Sentinel-3 mission can be found here. Sentinel Hub offers Sentinel-3 OLCI Level 1B products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCI.html#accessing-sentinel-3-olci-l1b-data",
    "href": "APIs/SentinelHub/Data/S3OLCI.html#accessing-sentinel-3-olci-l1b-data",
    "title": "Sentinel-3 OLCI L1B",
    "section": "Accessing Sentinel-3 OLCI L1B Data",
    "text": "Accessing Sentinel-3 OLCI L1B Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the datasource you are querying. This chapter will help you understand the parameters for S3OLCI data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: sentinel-3-olci\nUse sentinel-3-olci (previously S3OLCI) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-3 OLCI L1B data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S3OLCI process API.\n\nmosaickingOrder\nSets the order of overlapping tiles from which the output result is mosaicked. The tiling is based on ESA's Product Dissemination Units for easier distribution.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\nthe pixel will be selected from the most recently acquired tile\nIf there are multiple products with the same timestamp then NTC will be used over NRT. Default value.\n\n\nleastRecent\nthe pixel will be selected from the oldest acquired tile\nIf there are multiple products with the same timestamp then NTC will be used over NRT.\n\n\n\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S3OLCI process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing, regardless of the resolution\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nNot used, use upsampling instead\nN/A\nIgnored\n\n\n\n\n\nAvailable Bands and Data\nThis chapter will explain the bands and data which can be set in the evalscript input object. Any string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nName\nDescription\nWavelength centre (nm)\nResolution (m)\n\n\n\n\nB01\nAerosol correction, improved water constituent retrieval\n400\n300\n\n\nB02\nYellow substance and detrital pigments (turbidity)\n412.5\n300\n\n\nB03\nChlorophyll absorption maximum, biogeochemistry, vegetation\n442.5\n300\n\n\nB04\nChlorophyll\n490\n300\n\n\nB05\nChlorophyll, sediment, turbidity, red tide\n510\n300\n\n\nB06\nChlorophyll reference (minimum)\n560\n300\n\n\nB07\nSediment loading\n620\n300\n\n\nB08\n2nd Chlorophyll absorption maximum, sediment, yellow substance / vegetation\n665\n300\n\n\nB09\nImproved fluorescence retrieval\n673.75\n300\n\n\nB10\nChlorophyll fluorescence peak, red edge\n681.25\n300\n\n\nB11\nChlorophyll fluorescence baseline, red edge transition\n708.75\n300\n\n\nB12\nO2 absorption / clouds, vegetation\n753.75\n300\n\n\nB13\nO2 absorption / aerosol correction\n761.25\n300\n\n\nB14\nAtmospheric correction\n764.375\n300\n\n\nB15\nO2 absorption used for cloud top pressure, fluorescence over land\n767.5\n300\n\n\nB16\nAtmospheric / aerosol correction\n778.75\n300\n\n\nB17\nAtmospheric / aerosol correction, clouds, pixel co-registration\n865\n300\n\n\nB18\nWater vapour absorption reference. Common reference band with SLSTR. Vegetation monitoring\n885\n300\n\n\nB19\nWater vapour absorption, vegetation monitoring (maximum REFLECTANCE)\n900\n300\n\n\nB20\nWater vapour absorption, atmospheric / aerosol correction\n940\n300\n\n\nB21\nAtmospheric / aerosol correction, snow grain size\n1020\n300\n\n\nQUALITY_FLAGS\nClassification and quality flags\nN/A\n300\n\n\nSAA\nSun azimuth angle\nN/A\n19000\n\n\nSZA\nSun zenith angle\nN/A\n19000\n\n\nVAA\nViewing (observation) azimuth angle\nN/A\n19000\n\n\nVZA\nViewing (observation) zenith angle\nN/A\n19000\n\n\nHUMIDITY\nRelative humidity (meteo band)\nN/A\n19000\n\n\nSEA_LEVEL_PRESSURE\nMean sea level pressure (meteo band)\nN/A\n19000\n\n\nTOTAL_COLUMN_OZONE\nTotal column ozone (meteo band)\nN/A\n19000\n\n\nTOTAL_COLUMN_WATER_VAPOUR\nTotal column water vapour (meteo band)\nN/A\n19000\n\n\ndataMask\nThe mask of data/no data pixels (more).\nN/A*\nN/A**\n\n\n\n*dataMask has no wavelength information, as it carries only boolean information on whether a pixel has data or not. See the chapter on Units for more.  **dataMask has no source resolution as it is calculated for each output pixel.\nFor more about Sentinel-3 OLCI bands, visit  this Copernicus website.\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data. Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\nNotes\n\n\n\n\nOptical bands  B01-B21\nReflectance (unitless)\nREFLECTANCE (default)\nUINT16\n0 - 0.4\nHigher values in infrared bands. Highly reflective pixels, such as clouds, can have reflectance values above 1.\n\n\nOptical bands  B01-B21\nRadiance (mW/m2/sr/nm)\nRADIANCE\nUINT16\n0 - 300\n\n\n\nVAA\nAngle (degrees)\nDEGREES\nINT16\n-180 - 180\n\n\n\nVZA\nAngle (degrees)\nDEGREES\nUINT16\n0 - 180\n\n\n\nSAA\nAngle (degrees)\nDEGREES\nINT16\n-180 - 180\n\n\n\nSZA\nAngle (degrees)\nDEGREES\nUINT16\n0 - 180\n\n\n\nHUMIDITY\nHumidity (percent)\nPERCENT\nFLOAT32\n0 - 100\n\n\n\nSEA_LEVEL_PRESSURE\nPressure (hectopascals)\nHECTOPASCALS\nFLOAT32\n980 - 1030\nExtreme weather can be outside this range\n\n\nTOTAL_COLUMN_OZONE\nTotal column ozone (kg/m2)\nKG_M2\nFLOAT32\n0.004 - 0.008\n\n\n\nTOTAL_COLUMN_WATER_VAPOUR\nTotal column water vapour (kg/m2)\nKG_M2\nFLOAT32\n0 - 70\n\n\n\nQUALITY_FLAGS\nN/A\nDN\nUINT32\n0 - 4,294,967,294\nBit packed value. Use decodeS3OLCIQualityFlags to unpack.\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\n\nMosaicking\nAll mosaicking types are supported.\n\n\nScenes Object\nscenes object stores metadata. An example of metadata available in scenes object for Sentinel-3 OLCI when mosaicking is ORBIT:\n\n\n\nProperty name\nValue\n\n\n\n\ntiles[i].date\n'2019-04-02T17:05:39Z'\n\n\ntiles[i].shId\n881338\n\n\ntiles[i].dataPath\n'http://data.cloudferro.com/EODATA/Sentinel-3/OLCI/OL_1_EFR/2020/04/04/S3A_OL_1_EFR____20200404T093158_20200404T093458_20200405T131253_0179_056_364_2160_LN1_O_NT_002.SEN3'\n\n\n\nProperties of a scenes object can differ depending on the selected mosaicking and in which evalscript function the object is accessed. Working with metadata in evalscript user guide explains all details and provides examples."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCI.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S3OLCI.html#catalog-api-capabilities",
    "title": "Sentinel-3 OLCI L1B",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel 3 OLCI product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request.\n\nCollection identifier: sentinel-3-olci\n\n\nDistinct extension\n\ndate"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCI.html#examples",
    "href": "APIs/SentinelHub/Data/S3OLCI.html#examples",
    "title": "Sentinel-3 OLCI L1B",
    "section": "Examples",
    "text": "Examples\nS3OLCI examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/Byoc.html",
    "href": "APIs/SentinelHub/Data/Byoc.html",
    "title": "Bring Your Own COG / Batch",
    "section": "",
    "text": "Bring Your Own COG (BYOC) and Batch data is data ingested by Sentinel Hub users using BYOC API and Batch API, respectively."
  },
  {
    "objectID": "APIs/SentinelHub/Data/Byoc.html#about-byoc--batch-data",
    "href": "APIs/SentinelHub/Data/Byoc.html#about-byoc--batch-data",
    "title": "Bring Your Own COG / Batch",
    "section": "",
    "text": "Bring Your Own COG (BYOC) and Batch data is data ingested by Sentinel Hub users using BYOC API and Batch API, respectively."
  },
  {
    "objectID": "APIs/SentinelHub/Data/Byoc.html#accessing-data",
    "href": "APIs/SentinelHub/Data/Byoc.html#accessing-data",
    "title": "Bring Your Own COG / Batch",
    "section": "Accessing data",
    "text": "Accessing data\nData is accessed using Sentinel Hub APIs, just like any other data you are used to. In all cases collection id is needed, which can be obtained from your dashboard. You also need to access data from the correct endpoint.\n\nData type identifier\nUse byoc-&lt;collectionId&gt; for BYOC, and batch-&lt;collectionId&gt; for Batch as the value of the input.data.type parameter in your API requests. For example, set it to byoc-017aa0ae-33a6-45d3-8548-0f7d1041b40c for BYOC collection with id 017aa0ae-33a6-45d3-8548-0f7d1041b40c.\n\n\nRequest resolution limit\nFor the limit, we take five times the median resolution of the lowest resolution overview of all tiles in the collection. If this resolution is higher than 500 meters per pixel, the limit is set at 500 meters per pixel. This is to allow for more zoomed out viewing. Note, only overviews which are at least 256 pixels in either width or height are used for this computation. Adding or removing tiles from a collection will recompute its limit.\nYou can see the computed resolution limit for a collection in the collection details in your dashboard. Alternatively, in the response if querying a collection using the BYOC API.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object.\n\nmosaickingOrder\nSets the order of overlapping tiles from which the output result is mosaicked. The tiling is defined by the user when ingesting the data in the collection.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\nThe pixel will be selected from the tile, which the most recent sensing time.\nIn case there are more tiles available with the same sensing time, the one, which was created later will be used.\n\n\nleastRecent\nSimilar to mostRecent but in reverse order.\n\n\n\n\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing when the pixel resolution is greater than the source resolution (e.g. 5m/px with a 10m/px source)\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nAs above except when the resolution is lower.\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\n\n\n\nAvailable Bands and Data\nSince collections contains your data this means that the available bands are the ones you have prepared. The band names to use in you evalscript are also listed in each collection in your dashboard.\ndataMask is also available. In BYOC, dataMask value equals 1 only when a pixel is contained within cover geometry of a BYOC tile and when a pixel has data at the same time. All other instances (e.g. a pixel with data outside tile's cover geometry or a pixel with no-data within tile's cover geometry) will result in dataMask 0. Note also that for floating point rasters, NaN is always treated as no data. See here for more information.\n\n\nUnits\nThe only units available are digital numbers (DN) so any unit conversions, if necessary, are the responsibility of your evalscript.\n\n\nOGC API\nTo access your data via OGC you need to create a layer in the Configuration utility in either an existing or new configuration. When adding a layer you should set Source to Bring Your Own COG and Collection id to the id of your collection. You should also enter a custom script in Data processing field. This should return the appropriate values based on bands that are defined in the collection.\nOnce this is done the layer can be used via OGC API in the usual way.\nFor example: https://sh.dataspace.copernicus.eu/ogc/wms/&lt;MyInstanceID&gt;?REQUEST=GetMap&BBOX=15959450,8695500,16059450,8795500&CRS=EPSG:3857&WIDTH=500&HEIGHT=500&LAYERS=&lt;MyLayerName&gt;\n\nWFS\nTo query your tiles using WFS you need to set the WFS feature type (TYPENAMES parameter) to byoc-&lt;MyCollectionID&gt; for BYOC, and batch-&lt;MyCollectionID&gt; for Batch, e.g. byoc-a550f5e9-84d0-441a-8338-bbb04d42a72e.\nHere is an example of a WFS GetFeature request:\nhttps://sh.dataspace.copernicus.eu/ogc/wfs/&lt;MyInstanceID&gt;?SERVICE=wfs&REQUEST=GetFeature&BBOX=-90,180,90,-180&SRSNAME=EPSG:4326&OUTPUTFORMAT=application/json&TYPENAMES=byoc-&lt;MyCollectionID&gt;"
  },
  {
    "objectID": "APIs/SentinelHub/Data/Byoc.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/Byoc.html#catalog-api-capabilities",
    "title": "Bring Your Own COG / Batch",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Landsat BYOC or Batch product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request.\n\nCollection identifier: &lt;collection id&gt;\n\n\nDistinct extension\n\ndate"
  },
  {
    "objectID": "APIs/SentinelHub/Data/Zarr.html",
    "href": "APIs/SentinelHub/Data/Zarr.html",
    "title": "Zarr",
    "section": "",
    "text": "Zarr data can be ingested by Sentinel Hub users using Zarr Import API."
  },
  {
    "objectID": "APIs/SentinelHub/Data/Zarr.html#about-zarr-data",
    "href": "APIs/SentinelHub/Data/Zarr.html#about-zarr-data",
    "title": "Zarr",
    "section": "",
    "text": "Zarr data can be ingested by Sentinel Hub users using Zarr Import API."
  },
  {
    "objectID": "APIs/SentinelHub/Data/Zarr.html#accessing-data",
    "href": "APIs/SentinelHub/Data/Zarr.html#accessing-data",
    "title": "Zarr",
    "section": "Accessing data",
    "text": "Accessing data\nZarr data is accessed using Sentinel Hub APIs, just like any other data you are used to. In all cases collection id is needed, which can be obtained using the Zarr Import API.\n\nData type identifier\nUse zarr-&lt;collectionId&gt; for accessing as the value of the input.data.type parameter in your API requests. For example, set it to: zarr-123e4567-e89b-12d3-a456-426614174000 for Zarr collection with id 123e4567-e89b-12d3-a456-426614174000. Note that each Zarr collection in Sentinel Hub contains data from a single Zarr group.\n\n\nRequest resolution limit\nThe maximum meters per pixel limit is set by the service and is approximately 3 times the resolution of the actual ingested data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object.\n\nmosaickingOrder\nSets the sensing time order of preference.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\n- In case of SIMPLE mosaicking, the values for most recent sensing time will be returned.  - For ORBIT and TILE mosaicking, samples in the evalscript will have values sorted by descending sensing times.\n\n\n\nleastRecent\nSimilar to mostRecent but in reverse order.\n\n\n\n\nNote that mosaicking works differently for Zarrs than for other collections.\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing when the pixel resolution is greater than the source resolution (e.g. 5m/px with a 10m/px source)\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nAs above except when the resolution is lower.\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\n\n\n\nAvailable Data Arrays\nSince Zarr collections contains your ingested data, this means that the available arrays (or bands) are the ones that the Zarr group contains. The band names to use in your evalscript are the ingested arrays names and can be listed using the Zarr Import API.\n\n\nData mask and mosaicking\nA Zarr collection only contains data arrays of a single Zarr group. Zarr metadata does not contain cover geometries (other than the envelope), thus all data arrays are considered to cover the full Zarr envelope. Thus, the value of dataMask is always 1 inside the Zarr's envelope and 0 outside. Areas for which an array has no data chunks are filled with the no data value.\nConsequently, mosaicking works differently than for other collections:\n\nTimeless (two-dimensional) Zarrs contain data for a single (unspecified) sensing time. This data will be returned for both SIMPLE and TILE mosaicking.\nThree-dimensional Zarrs contain data for multiple sensing times. The data returned will be:\n\nFor SIMPLE mosaicking, only the data for a single sensing time. As explained above, the data is considered to cover the full Zarr envelope, thus there are no missing areas where data from other sensing times would be mosaicked in.\nFor TILE mosaicking, an array of tiles corresponding to sensing times.\n\nORBIT mosaicking is not supported because Zarrs do not contain orbit metadata.\n\n\n\nUnits\nThe only units available are digital numbers (DN) so any unit conversions, if necessary, are the responsability of your evalscript."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L2A.html",
    "href": "APIs/SentinelHub/Data/S2L2A.html",
    "title": "Sentinel-2 L2A",
    "section": "",
    "text": "General information about Sentinel-2 mission can be found here. Sentinel Hub offers Sentinel-2 Level 2A products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L2A.html#about-sentinel-2-l2a-data",
    "href": "APIs/SentinelHub/Data/S2L2A.html#about-sentinel-2-l2a-data",
    "title": "Sentinel-2 L2A",
    "section": "",
    "text": "General information about Sentinel-2 mission can be found here. Sentinel Hub offers Sentinel-2 Level 2A products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L2A.html#accessing-sentinel-2-l2a-data",
    "href": "APIs/SentinelHub/Data/S2L2A.html#accessing-sentinel-2-l2a-data",
    "title": "Sentinel-2 L2A",
    "section": "Accessing Sentinel-2 L2A Data",
    "text": "Accessing Sentinel-2 L2A Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the datasource you are querying. This chapter will help you understand the parameters for S2L2A data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: sentinel-2-l2a\nUse sentinel-2-l2a (previously S2L2A) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-2 L2A data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S2L2A process API.\n\nmosaickingOrder\nSets the order of overlapping tiles from which the output result is mosaicked. Note that tiles will in most cases come from the same orbit/acquisition. The tiling is done by ESA for easier distribution.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\nselected by default. The pixel will be selected from the tile, which was acquired most recently\nin case there are more tiles available with the same timestamp (some tiles are processed by many ground stations, some are reprocessed, etc.), the one, which was downloaded from SciHub later will be used.\n\n\nleastRecent\nsimilar to mostRecent but in reverse order\n\n\n\nleastCC\npixel is selected from tile with the least cloud coverage metadata\nnote that \"per tile\" information is used here, each covering about a 12,000 sq. km area, so this information is only an estimate .\n\n\n\n\n\nmaxCloudCoverage\nSets the upper limit for cloud coverage in percent based on the precomputed cloud coverage estimate for each Sentinel-2 tile as present in the tile metadata. Satellite data will therefore not be retrieved for tiles with a higher cloud coverage estimate. For example, by setting the value to 20, only tiles with at most 20% cloud coverage will be used. Note that this parameter is set per tile and might not be directly applicable to the chosen area of interest.\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S2L2A process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing when the pixel resolution is greater than the source resolution (e.g. 5m/px with a 10m/px source)\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nAs above except when the resolution is lower.\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\n\nA visual representation of upsampling results using various algorithms is available here.\n\n\nAvailable Bands and Data\nThis chapter will explain the bands and data which can be set in the evalscript input object. Any string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nName\nDescription\nResolution\n\n\n\n\nB01\nCoastal aerosol, 442.7 nm (S2A), 442.3 nm (S2B)\n60m\n\n\nB02\nBlue, 492.4 nm (S2A), 492.1 nm (S2B)\n10m\n\n\nB03\nGreen, 559.8 nm (S2A), 559.0 nm (S2B)\n10m\n\n\nB04\nRed, 664.6 nm (S2A), 665.0 nm (S2B)\n10m\n\n\nB05\nVegetation red edge, 704.1 nm (S2A), 703.8 nm (S2B)\n20m\n\n\nB06\nVegetation red edge, 740.5 nm (S2A), 739.1 nm (S2B)\n20m\n\n\nB07\nVegetation red edge, 782.8 nm (S2A), 779.7 nm (S2B)\n20m\n\n\nB08\nNIR, 832.8 nm (S2A), 833.0 nm (S2B)\n10m\n\n\nB8A\nNarrow NIR, 864.7 nm (S2A), 864.0 nm (S2B)\n20m\n\n\nB09\nWater vapour, 945.1 nm (S2A), 943.2 nm (S2B)\n60m\n\n\nB11\nSWIR, 1613.7 nm (S2A), 1610.4 nm (S2B)\n20m\n\n\nB12\nSWIR, 2202.4 nm (S2A), 2185.7 nm (S2B)\n20m\n\n\nAOT\nAerosol Optical Thickness map, based on Sen2Cor processor\n10m\n\n\nSCL\nScene classification data, based on Sen2Cor processor, codelist\n20m\n\n\nSNW\nSnow probability, based on Sen2Cor processor\n20m\n\n\nCLD\nCloud probability, based on Sen2Cor processor\n20m\n\n\nsunAzimuthAngles\nSun azimuth angle\n5000m\n\n\nsunZenithAngles\nSun zenith angle\n5000m\n\n\nviewAzimuthMean\nViewing azimuth angle\n5000m\n\n\nviewZenithMean\nViewing zenith angle\n5000m\n\n\ndataMask\nThe mask of data/no data pixels (more).\nN/A*\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\n\n\n\n\n\nNote\n\n\n\nThe cirrus band B10 is excluded as it does not contain any \"bottom of the atmosphere\" information (Source: Sen2Cor Configuration and User Manual).\n\n\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data (See also Harmonize Values). Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\nFor Sentinel-2 optical data, the relation between DN and REFLECTANCE (default unit) is: DN = 10000 * REFLECTANCE. See also Harmonize Values.\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\n\n\n\n\nOptical bands\nReflectance (unitless)\nREFLECTANCE (default)\nUINT15\n0 - 0.4*\n\n\nOptical bands\nDigital numbers (unitless)\nDN\nUINT15\n0 - 4000*\n\n\nAOT\nAerosol optical thickness (unitless)\nOPTICAL_DEPTH (default) **\nUINT16\n0 - 0.6**\n\n\nAOT\nDigital numbers (unitless)\nDN**\nUINT16\n0 - 600**\n\n\nSCL\nScene classification mask (unitless)\nDN\nUINT8\n0 - No data 1 - Saturated / Defective  2 - Dark Area Pixels  3 - Cloud Shadows  4 - Vegetation  5 - Bare Soils  6 - Water  7 - Clouds low probability / Unclassified  8 - Clouds medium probability  9 - Clouds high probability  10 - Cirrus  11 - Snow / Ice\n\n\nSNW\nSnow probability (percent)\nPERCENT\nUINT8\n0 - 100\n\n\nCLD\nCloud probability (percent)\nPERCENT\nUINT8\n0 - 100\n\n\nsunAzimuthAngles\nAngle (degrees)\nDEGREES\nFLOAT32\n30 - 200\n\n\nviewAzimuthMean\nAngle (degrees)\nDEGREES\nFLOAT32\n90 - 300\n\n\nsunZenithAngles\nAngle (degrees)\nDEGREES\nFLOAT32\n15 - 80\n\n\nviewZenithMean\nAngle (degrees)\nDEGREES\nFLOAT32\n0 - 12\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n*Higher values are expected in infrared bands. Reflectance values can easily be above 1.  ** AOT = DN / 1000.\n\nHarmonize Values\nESA updated the Sentinel-2 processing baseline to version 04.00 in January, 2022, which introduced breaking changes to the interpretation of digital numbers (DN). The optional harmonizeValues parameter gives you extra control over the values which enter your evalscript.\nharmonizeValues can be true (default) or false, and it's behavior depends on the units chosen:\n\nREFLECTANCE:\n\nharmonizeValues = true: negative reflectance values are clamped to zero. In other words, pixels with negative reflectance return zero reflectance instead.\nharmonizeValues = false: negative reflectance values can be returned.\n\nDN:\n\nharmonizeValues = true: DN values are harmonized so they are comparable with data from previous baselines. Therefore it still holds that DN = 10000 * REFLECTANCE. In addition, negative values are clamped to zero.\nharmonizeValues = false: DN values are exactly as provided in the source files themselves. The \"true\" DN value, you could say. Don't forget that values have different definitions with different processing baselines, careful with mosaicking!\n\n\n\n\n\nMosaicking\nAll mosaicking types are supported.\n\n\nScenes Object\nscenes object stores metadata, for an example see scenes object for Sentinel-2 L1C.\n\n\nCollection Specific Constraints\n\nEach Sentinel-2 satellite takes 100.6 minutes for an entire orbit, with S2A and S2B orbiting 180 degree apart. By setting a time interval to less than 50 minutes, there should be no overlap possible between different acquisitions, even near the poles.\nFor atmospheric correction, ESA's official Sen2Cor is used. In most cases the processing is done by ESA itself. On users' request we can also process some archive data using the same processor."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L2A.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S2L2A.html#catalog-api-capabilities",
    "title": "Sentinel-2 L2A",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel 2 L2A product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request. This chapter will help with understanding Sentinel 2 L2A specific parameters for search request.\n\nCollection identifier: sentinel-2-l2a\n\n\nFilter extension\n\neo:cloud_cover cloud cover percentage\n\n\n\nDistinct extension\n\ndate"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L2A.html#examples",
    "href": "APIs/SentinelHub/Data/S2L2A.html#examples",
    "title": "Sentinel-2 L2A",
    "section": "Examples",
    "text": "Examples\nS2L2A examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCIL2.html",
    "href": "APIs/SentinelHub/Data/S3OLCIL2.html",
    "title": "Sentinel-3 OLCI L2",
    "section": "",
    "text": "General information about Sentinel-3 mission can be found here. Sentinel Hub offers Sentinel-3 OLCI Level 2 products.\n\n\nEU law grants free access to Copernicus Sentinel Data and Service Information for the purpose of the following use in so far as it is lawful: a) reproduction; b) distribution; c) communication to the public; d) adaptation, modification and combination with other data and information; e) any combination of points a to d.\nSee more details on the use of Copernicus Sentinel data and service information\nTracing based on Sentinel imagery is allowed for commercial purposes as well.\nAcknowledgment or credit: Contains modified Copernicus Sentinel data [Year] processed by Sentinel Hub."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCIL2.html#about-sentinel-3-olci-l2-data",
    "href": "APIs/SentinelHub/Data/S3OLCIL2.html#about-sentinel-3-olci-l2-data",
    "title": "Sentinel-3 OLCI L2",
    "section": "",
    "text": "General information about Sentinel-3 mission can be found here. Sentinel Hub offers Sentinel-3 OLCI Level 2 products.\n\n\nEU law grants free access to Copernicus Sentinel Data and Service Information for the purpose of the following use in so far as it is lawful: a) reproduction; b) distribution; c) communication to the public; d) adaptation, modification and combination with other data and information; e) any combination of points a to d.\nSee more details on the use of Copernicus Sentinel data and service information\nTracing based on Sentinel imagery is allowed for commercial purposes as well.\nAcknowledgment or credit: Contains modified Copernicus Sentinel data [Year] processed by Sentinel Hub."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCIL2.html#accessing-sentinel-3-olci-l2-data",
    "href": "APIs/SentinelHub/Data/S3OLCIL2.html#accessing-sentinel-3-olci-l2-data",
    "title": "Sentinel-3 OLCI L2",
    "section": "Accessing Sentinel-3 OLCI L2 Data",
    "text": "Accessing Sentinel-3 OLCI L2 Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the datasource you are querying. This chapter will help you understand the parameters for S3OLCI L2 data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: sentinel-3-olci-l2\nUse sentinel-3-olci-l2 as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-3 OLCI L2 data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S3OLCI L2 process API.\n\nmosaickingOrder\nSets the order of overlapping tiles from which the output result is mosaicked. The tiling is based on ESA's Product Dissemination Units for easier distribution.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\nthe pixel will be selected from the most recently acquired tile\nIf there are multiple products with the same timestamp then NTC will be used over NRT. Default value.\n\n\nleastRecent\nthe pixel will be selected from the oldest acquired tile\nIf there are multiple products with the same timestamp then NTC will be used over NRT.\n\n\nleastCC\npixel is selected from tile with the least cloud coverage metadata\nNote that \"per tile\" information is used here.\n\n\n\n\n\nmaxCloudCoverage\nSets the upper limit for cloud coverage in percent based on the precomputed cloud coverage estimate for each Sentinel-3 tile as present in the tile metadata. Satellite data will therefore not be retrieved for tiles with a higher cloud coverage estimate. For example, by setting the value to 20, only tiles with at most 20% cloud coverage will be used. Note that this parameter is set per tile and might not be directly applicable to the chosen area of interest.\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S3OLCI L2 process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing, regardless of the resolution\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nNot used, use upsampling instead\nN/A\nIgnored\n\n\n\n\n\nAvailable Bands and Data\nFor more about Sentinel-3 OLCI L2 bands, visit this Copernicus website.\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data. Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and REFLECTANCE source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\n\n\nSentinel-3 OLCI L2 LAND\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\n\n\n\n\nGIFAPAR\nGreen Instantaneous Fraction of Absorbed Photosynthetically Available Radiation (unitless)\nFRACTION\nUINT8\n0 - 1\n\n\nIWV_L\nIntegrated water vapour column (kg/m^2)\nKG_M2\nUINT8\n1 - 70\n\n\nOTCI\nOLCI Terrestrial Chlorophyll Index (unitless)\nINDEX\nUINT8\n0 - 4\n\n\nRC681\nRectified reflectance (unitless)\nREFLECTANCE\nUINT16\n0 - 1\n\n\nRC865\nRectified reflectance (unitless)\nREFLECTANCE\nUINT16\n0 - 1\n\n\nLQSF\nN/A\nDN\nUINT32\n0 - 30000000\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\nSentinel-3 OLCI L2 WATER\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\n\n\n\n\nIWV_W\nIntegrated water vapour column (kg/m^2)\nKG_M2\nUINT8\n1 - 70\n\n\nCHL_OC4ME\nAlgal pigment concentration (log10(mg/m^3))\nLOG10_MILLIGRAMS_M3\nUINT8\n-1.8 - 2\n\n\nTSM_NN\nTotal suspended matter concentration (log10(g/m^3))\nLOG10_GRAMS_M2\nUINT8\n-2.5 - 3\n\n\nPAR\nPhotosynthetically active radiation (microeinstein/s/m^2)\nMICROEINSTEINS_S_M2\nUINT8\n300 - 2100\n\n\nKD490_M07\nDiffuse attenuation coefficient (log10(1/m))\nLOG10_1_M\nUINT8\n-2.0 - 1.5\n\n\nA865\nAerosol Angstrom exponent (unitless)\nANGSTROM_EXPONENT\nUINT8\n-0.1 - 2\n\n\nT865\nAerosol optical thickness (unitless)\nOPTICAL_DEPTH\nUINT8\n-0.1 - 1\n\n\nCHL_NN\nAlgal pigment concentration (log10(mg/m^3))\nLOG10_MILLIGRAMS_M3\nUINT8\n-2 - 2\n\n\nADG443_NN\nCDM absorption coefficient (log10(1/m))\nLOG10_1_M\nUINT8\n-3 - 2\n\n\nOptical bands B01-B211\nReflectance (unitless)\nREFLECTANCE\nUINT16\n0 - 0.3\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\nMosaicking\nAll mosaicking types are supported.\n\n\nScenes Object\nscenes object stores metadata. An example of metadata available in scenes object for Sentinel-3 OLCI L2 when mosaicking is ORBIT:\n\n\n\nProperty name\nValue\n\n\n\n\ntiles[i].date\n'2019-04-02T17:05:39Z'\n\n\ntiles[i].shId\n881338\n\n\ntiles[i].dataPath\n'http://data.cloudferro.com/EODATA/Sentinel-3/OLCI/OL_2_LFR/2020/11/29/S3B_OL_2_LFR____20201129T040222_20201129T040522_20201130T074446_0179_046_161_2700_LN1_O_NT_002.SEN3'\n\n\n\nProperties of a scenes object can differ depending on the selected mosaicking and in which evalscript function the object is accessed. Working with metadata in evalscript user guide explains all details and provides examples."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCIL2.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S3OLCIL2.html#catalog-api-capabilities",
    "title": "Sentinel-3 OLCI L2",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel 3 OLCI L2 product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request.\n\nCollection identifier: sentinel-3-olci-l2\n\n\nDistinct extension\n\ndate"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3OLCIL2.html#footnotes",
    "href": "APIs/SentinelHub/Data/S3OLCIL2.html#footnotes",
    "title": "Sentinel-3 OLCI L2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that bands B13, B14, B15, B19, B20 are not available.↩︎"
  },
  {
    "objectID": "APIs/SentinelHub/Catalog/Examples.html",
    "href": "APIs/SentinelHub/Catalog/Examples.html",
    "title": "Catalog API examples",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nCatalog API Entry page\nCatalog API Entry page with link to other catalog API endpoints and available collections.\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/\"\nresponse = oauth.get(url)\n\n\nList collections\nList all available collections. The list will include deployment specific collections and collections available to users through BYOC, Batch or Third Party Data Import functionalities.\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/collections\"\nresponse = oauth.get(url)\n\n\nSentinel 2 L1C collection\nList single collection, in this case Sentinel 2 L1C collection.\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/collections/sentinel-2-l1c/\"\nresponse = oauth.get(url)\n\n\nSimple GET search\nSimple version of search available via GET request is also available. The only query parameters that can be specified in this simpler version are: bbox, datetime, collections, limit and next.\nquery = {\n    \"bbox\": \"13,45,14,46\",\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": \"sentinel-1-grd\",\n    \"limit\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.get(url, params=query)\n\n\nSimple POST search\nThe same parameters can also be specified a POST request, query parameters need to be specified as json formatted body and sent to server like:\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-10T23:59:59Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSimple POST search with pagination\nnext token can be specified in the request to get back the next page of results.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-10T23:59:59Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 5,\n    \"next\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch with GeoJSON\nInstead of bbox it is possible to add intersects attribute, which can be any type of GeoJSON object (Point, LineString, Polygon, MultiPoint, MultiPolygon).\ndata = {\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 5,\n    \"intersects\": {\n        \"type\": \"Point\",\n        \"coordinates\": [\n            13,\n            45,\n        ],\n    },\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch with Filter\nfilter object can be used to instruct server to only return a specific subset of data.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 5,\n    \"filter\": \"sat:orbit_state='ascending'\",\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nGet Filter parameters for collection\nList all available filter parameters represented as JSON Schema.\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/collections/sentinel-1-grd/queryables\"\nresponse = oauth.get(url)\n\n\nSearch with Fields: No fields\nDefault outputs from the server can be quite verbose for some collections. By default, all available item properties are included in the response.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": [\"sentinel-2-l1c\"],\n    \"limit\": 1,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch with Fields: Empty fields\nfields attribute can be specific to return less information. When fields object is empty only a default set of properties is included: id, type, geometry, bbox, links, assets.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": [\"sentinel-2-l1c\"],\n    \"limit\": 1,\n    \"fields\": {},\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch with Fields: Include\nBy specifying additional attributes in the include list, those attributes are added to the output along with the default ones.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": [\"sentinel-2-l1c\"],\n    \"limit\": 1,\n    \"fields\": {\"include\": [\"properties.gsd\"]},\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch with Fields: Exclude\nexlude list can be used to exclude even the default ones from the output.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": [\"sentinel-2-l1c\"],\n    \"limit\": 1,\n    \"fields\": {\"exclude\": [\"properties.datetime\"]},\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch with distinct\nUsing distinct it is possible to get some overview of the data available inside the specified query. For example specifying date as an option will return a list of dates where data is available.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-01T00:00:00Z/2020-01-01T00:00:00Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 100,\n    \"distinct\": \"date\",\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\nOr see different Sentinel 1 instrument modes used.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-01T00:00:00Z/2020-01-01T00:00:00Z\",\n    \"collections\": [\"sentinel-1-grd\"],\n    \"limit\": 100,\n    \"distinct\": \"sar:instrument_mode\",\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\n\n\nSearch on BYOC/BATCH collections\nYou can search for features on your own BYOC or Batch collections. The functionality described above regarding GET and POST search is the same. The only difference is that you have to specify the collection id with the appropriate prefix on collections parameter (e.g: byoc-&lt;your-collection-id&gt; for byoc or batch-&lt;your-collection-id&gt; for batch). Remember that you will have to use the appropriate deployment endpoint depending on where your collection is hosted.\ndata = {\n    \"bbox\": [13, 45, 14, 46],\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-10T23:59:59Z\",\n    \"collections\": [\"byoc-&lt;byoc-collection-id&gt;\"],\n    \"limit\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.post(url, json=data)\nOr using GET simple search endpoint:\nquery = {\n    \"bbox\": \"13,45,14,46\",\n    \"datetime\": \"2019-12-10T00:00:00Z/2019-12-11T00:00:00Z\",\n    \"collections\": \"batch-&lt;batch-collection-id&gt;\",\n    \"limit\": 5,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/catalog/1.0.0/search\"\nresponse = oauth.get(url, params=query)"
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess/Examples.html",
    "href": "APIs/SentinelHub/AsyncProcess/Examples.html",
    "title": "Async API examples",
    "section": "",
    "text": "The requests below are written in Python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nCreate an asynchronous processing request\nBefore running the example, you will have to replace placeholders &lt;your-bucket&gt;/&lt;path&gt;, &lt;your-bucket-access-key&gt;, and &lt;your-bucket-access-key-secret&gt; with your values.\nurl = 'https://sh.dataspace.copernicus.eu/api/v1/async/process'\n\nevalscript = \"\"\"\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\"\n      ]\n    }],\n    output: {\n      bands: 3\n    }\n  }\n}\n\nlet viz = ColorGradientVisualizer.createWhiteGreen();\n\nfunction evaluatePixel(samples) {\n    let ndvi = index(samples.B08, samples.B04);\n    vizualizedNdvi = viz.process(ndvi);\n    return vizualizedNdvi;\n}\n\n\"\"\"\n\npayload = {\n  \"input\" : {\n    \"bounds\" : {\n      \"bbox\" : [ 426000, 3960000, 462000, 3994000 ],\n      \"properties\" : {\n        \"crs\" : \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n      }\n    },\n    \"data\" : [ {\n      \"dataFilter\" : {\n        \"timeRange\" : {\n          \"from\" : \"2022-06-20T00:00:00Z\",\n          \"to\" : \"2022-06-30T23:59:59Z\"\n        }\n      },\n      \"type\" : \"S2L2A\"\n    } ]\n  },\n  \"output\" : {\n    \"resx\" : 10,\n    \"resy\" : 10,\n    \"responses\" : [ {\n      \"identifier\" : \"default\",\n      \"format\" : {\n        \"type\" : \"image/tiff\"\n      }\n    } ],\n    \"delivery\" : {\n      \"s3\" : {\n        \"url\": \"s3://&lt;your-bucket&gt;/&lt;path&gt;\",\n        \"accessKey\": \"&lt;your-bucket-access-key&gt;\",\n        \"secretAccessKey\": \"&lt;your-bucket-access-key-secret&gt;\"\n      }\n    }\n  },\n  \"evalscript\" : evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = oauth.post(url, headers=headers, json=payload)\nresponse.json()\nExtracting the asynchronous request id from the response:\nrequest_id = response.json()['id']\n\n\nGet information about your asynchronous processing request\nresponse = oauth.get(url=f\"{url}/{request_id}\")\nresponse.json()\n\n\nCloudless mosaic example\nurl = 'https://sh.dataspace.copernicus.eu/api/v1/async/process'\n\nevalscript = \"\"\"\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B03\",\n        \"B02\",\n        \"SCL\"\n      ]\n    }],\n    output: { bands: 3, sampleType: \"UINT16\" },\n    mosaicking: \"ORBIT\"\n  }\n}\nfunction preProcessScenes(collections) {\n  collections.scenes.orbits = collections.scenes.orbits.filter(function (orbit) {\n    var orbitDateFrom = new Date(orbit.dateFrom)\n    return orbitDateFrom.getTime() &gt;= (collections.to.getTime() - 3 * 31 * 24 * 3600 * 1000);\n  })\n  return collections\n}\nfunction getValue(values) {\n  values.sort(function (a, b) { return a - b; });\n  return getFirstQuartile(values);\n}\n\nfunction getFirstQuartile(sortedValues) {\n  var index = Math.floor(sortedValues.length / 4);\n  return sortedValues[index];\n}\nfunction getDarkestPixel(sortedValues) {\n  return sortedValues[0]; // darkest pixel\n}\nfunction validate(samples) {\n  var scl = samples.SCL;\n\n  if (scl === 3) { // SC_CLOUD_SHADOW\n    return false;\n  } else if (scl === 9) { // SC_CLOUD_HIGH_PROBA\n    return false;\n  } else if (scl === 8) { // SC_CLOUD_MEDIUM_PROBA\n    return false;\n  } else if (scl === 7) { // SC_CLOUD_LOW_PROBA / UNCLASSIFIED\n    // return false;\n  } else if (scl === 10) { // SC_THIN_CIRRUS\n    return false;\n  } else if (scl === 11) { // SC_SNOW_ICE\n    return false;\n  } else if (scl === 1) { // SC_SATURATED_DEFECTIVE\n    return false;\n  } else if (scl === 2) { // SC_DARK_FEATURE_SHADOW\n    // return false;\n  }\n  return true;\n}\n\nfunction evaluatePixel(samples, scenes) {\n  var clo_b02 = []; var clo_b03 = []; var clo_b04 = [];\n  var clo_b02_invalid = []; var clo_b03_invalid = []; var clo_b04_invalid = [];\n  var a = 0; var a_invalid = 0;\n\n  for (var i = 0; i &lt; samples.length; i++) {\n    var sample = samples[i];\n\n    if (sample.B02 &gt; 0 && sample.B03 &gt; 0 && sample.B04 &gt; 0) {\n      var isValid = validate(sample);\n\n      if (isValid) {\n        clo_b02[a] = sample.B02;\n        clo_b03[a] = sample.B03;\n        clo_b04[a] = sample.B04;\n        a = a + 1;\n      } else {\n        clo_b02_invalid[a_invalid] = sample.B02;\n        clo_b03_invalid[a_invalid] = sample.B03;\n        clo_b04_invalid[a_invalid] = sample.B04;\n        a_invalid = a_invalid + 1;\n      }\n    }\n  }\n\n  var rValue;\n  var gValue;\n  var bValue;\n  if (a &gt; 0) {\n    rValue = getValue(clo_b04);\n    gValue = getValue(clo_b03);\n    bValue = getValue(clo_b02);\n  } else if (a_invalid &gt; 0) {\n    rValue = getValue(clo_b04_invalid);\n    gValue = getValue(clo_b03_invalid);\n    bValue = getValue(clo_b02_invalid);\n  } else {\n    rValue = 0;\n    gValue = 0;\n    bValue = 0;\n  }\n  return [rValue * 10000,\n  gValue * 10000,\n  bValue * 10000]\n}\n\n\"\"\"\n\npayload = {\n  \"input\" : {\n    \"bounds\" : {\n      \"bbox\" : [\n        11.193762,\n        44.684277,\n        18.622922,\n        47.872144\n      ],\n    },\n    \"data\" : [ {\n      \"dataFilter\" : {\n        \"timeRange\" : {\n          \"from\" : \"2019-06-01T00:00:00Z\",\n          \"to\" : \"2019-10-31T23:59:59Z\"\n        },\n        \"mosaickingOrder\": \"leastCC\"\n      },\n      \"type\" : \"S2L2A\"\n    } ]\n  },\n  \"output\" : {\n    \"width\" : 5000,\n    \"height\" : 5000,\n    \"responses\" : [ {\n      \"identifier\" : \"default\",\n      \"format\" : {\n        \"type\" : \"image/tiff\"\n      }\n    } ],\n    \"delivery\" : {\n      \"s3\" : {\n        \"url\": \"s3://&lt;your-bucket&gt;/&lt;path&gt;\",\n        \"accessKey\": \"&lt;your-bucket-access-key&gt;\",\n        \"secretAccessKey\": \"&lt;your-bucket-access-key-secret&gt;\"\n      }\n    }\n  },\n  \"evalscript\" : evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = oauth.post(url, headers=headers, json=payload)\nresponse.json()\n\n\nLarge true color example\nurl = 'https://sh.dataspace.copernicus.eu/api/v1/async/process'\n\nevalscript = \"\"\"\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\" // default value - scales the output values from [0,1] to [0,255].\n     }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\npayload = {\n  \"input\" : {\n    \"bounds\" : {\n      \"bbox\" : [\n        11.193762,\n        44.684277,\n        18.622922,\n        47.872144\n      ],\n    },\n    \"data\" : [ {\n      \"dataFilter\" : {\n        \"timeRange\" : {\n          \"from\" : \"2022-10-01T00:00:00Z\",\n          \"to\" : \"2022-10-31T00:00:00Z\"\n        },\n      },\n      \"type\" : \"S2L1C\"\n    } ]\n  },\n  \"output\" : {\n    \"width\" : 10000,\n    \"height\" : 10000,\n    \"responses\" : [ {\n      \"identifier\" : \"default\",\n      \"format\" : {\n        \"type\" : \"image/tiff\"\n      }\n    } ],\n    \"delivery\" : {\n      \"s3\" : {\n        \"url\": \"s3://&lt;your-bucket&gt;/&lt;path&gt;\",\n        \"accessKey\": \"&lt;your-bucket-access-key&gt;\",\n        \"secretAccessKey\": \"&lt;your-bucket-access-key-secret&gt;\"\n      }\n    }\n  },\n  \"evalscript\" : evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = oauth.post(url, headers=headers, json=payload)\nresponse.json()\n\n\nMedian NDVI over three years, which excludes CLM\nurl = 'https://sh.dataspace.copernicus.eu/api/v1/async/process'\n\nevalscript = \"\"\"\n//VERSION=3\n\nfunction setup() {\n    return {\n      input: [{\n        bands: [\"B08\", \"B04\", \"B03\", \"B02\", \"CLM\", \"SCL\"],  //Requests required bands, s2cloudless mask and scene classification layer\n        units: \"DN\"\n      }],\n      output: {\n        bands: 4,\n        sampleType: SampleType.UINT16\n      },\n      mosaicking: \"ORBIT\"\n    };\n  }\n\n  function filterScenes (scenes, inputMetadata) {\n    return scenes.filter(function (scene) {\n      return scene.date.getTime()&gt;=(inputMetadata.to.getTime()-12*30*24*3600*1000); //Defines the time range, e.g. from 1st June until 31st October counts 5 months with 30 days, 24 hours...\n    });\n  }\n\n  function getValue(values) {\n    values.sort( function(a,b) {return a - b;} );\n    return getMedian(values);\n  }\n\n  function getMedian(sortedValues) {\n    var index = Math.floor(sortedValues.length / 2);\n    return sortedValues[index];\n  }\n  function getDarkestPixel(sortedValues) {\n    return sortedValues[0]; // darkest pixel\n  }\n\n  function validate (samples) {\n    var scl = samples.SCL;\n    var clm = samples.CLM;\n\n    if (clm === 1 || clm === 255) {\n          return false;\n    } else if (scl === 1) { // SC_SATURATED_DEFECTIVE\n          return false;\n    } else if (scl === 3) { // SC_CLOUD_SHADOW\n          return false;\n    } else if (scl === 7) { // SC_CLOUD_LOW_PROBA\n          return false;\n    } else if (scl === 8) { // SC_CLOUD_MEDIUM_PROBA\n          return false;\n    } else if (scl === 9) { // SC_CLOUD_HIGH_PROBA\n          return false;\n    } else if (scl === 10) { // SC_THIN_CIRRUS\n          return false;\n    } else if (scl === 11) { // SC_SNOW_ICE\n      return false;\n    }  else {\n    return true;\n    }\n  }\n\n  function evaluatePixel(samples, scenes) {\n    var clo_b02 = []; var clo_b03 = []; var clo_b04 = []; var clo_b08 = [];\n    var clo_b02_invalid = []; var clo_b03_invalid = []; var clo_b04_invalid = []; var clo_b08_invalid = [];\n    var a = 0; var a_invalid = 0;\n\n    for (var i = 0; i &lt; samples.length; i++) {\n      var sample = samples[i];\n      if (sample.B02 &gt; 0 && sample.B03 &gt; 0 && sample.B04 &gt; 0 && sample.B08 &gt; 0) {\n        var isValid = validate(sample);\n\n        if (isValid) {\n          clo_b02[a] = sample.B02;\n          clo_b03[a] = sample.B03;\n          clo_b04[a] = sample.B04;\n          clo_b08[a] = sample.B08;\n          a = a + 1;\n        } else {\n          clo_b02_invalid[a_invalid] = sample.B02;\n          clo_b03_invalid[a_invalid] = sample.B03;\n          clo_b04_invalid[a_invalid] = sample.B04;\n          clo_b08_invalid[a_invalid] = sample.B08;\n          a_invalid = a_invalid + 1;\n        }\n      }\n    }\n\n    var rValue;\n    var gValue;\n    var bValue;\n    var nirValue;\n\n    if (a &gt; 0) {\n      nirValue = getValue(clo_b08);\n      rValue = getValue(clo_b04);\n      gValue = getValue(clo_b03);\n      bValue = getValue(clo_b02);\n    } else if (a_invalid &gt; 0) {\n      nirValue = getValue(clo_b08_invalid);\n      rValue = getValue(clo_b04_invalid);\n      gValue = getValue(clo_b03_invalid);\n      bValue = getValue(clo_b02_invalid);\n    } else {\n      nirValue = 0;\n      rValue = 0;\n      gValue = 0;\n      bValue = 0;\n    }\n    return [nirValue, rValue, gValue, bValue]\n  }\n\"\"\"\n\npayload = {\n  \"input\" : {\n    \"bounds\" : {\n      \"bbox\" : [\n        11.193762,\n        44.684277,\n        18.622922,\n        47.872144\n      ],\n    },\n    \"data\" : [ {\n      \"dataFilter\" : {\n        \"timeRange\" : {\n          \"from\" : \"2019-10-01T00:00:00Z\",\n          \"to\" : \"2022-10-01T00:00:00Z\"\n        },\n      },\n      \"type\" : \"S2L2A\"\n    } ]\n  },\n  \"output\" : {\n    \"width\" : 5000,\n    \"height\" : 5000,\n    \"responses\" : [ {\n      \"identifier\" : \"default\",\n      \"format\" : {\n        \"type\" : \"image/tiff\"\n      }\n    } ],\n    \"delivery\" : {\n      \"s3\" : {\n        \"url\": \"s3://&lt;your-bucket&gt;/&lt;path&gt;\",\n        \"accessKey\": \"&lt;your-bucket-access-key&gt;\",\n        \"secretAccessKey\": \"&lt;your-bucket-access-key-secret&gt;\"\n      }\n    }\n  },\n  \"evalscript\" : evalscript\n}\n\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = oauth.post(url, headers=headers, json=payload)\nresponse.json()"
  },
  {
    "objectID": "APIs/SentinelHub/Zarr/Examples.html",
    "href": "APIs/SentinelHub/Zarr/Examples.html",
    "title": "Zarr Import API examples",
    "section": "",
    "text": "The following API requests are written in Python. To execute them, you need to create an OAuth client as is explained here. The client is named oauth in these examples. The examples are structured in a way to be as separable as possible, however in many cases doing all the steps in each chapter makes sense.\n\nCreating a collection\nTo create a collection with the name My Collection and S3 bucket my-bucket using Zarr data that resides inside s3://my-bucket/path-to-zarr.zarr/:\n\ncollection = {\n  'name': 'My Collection',\n  's3Bucket': 'my-bucket',\n  'path': 'path-to-zarr.zarr/',\n  \"crs\":\"http://www.opengis.net/def/crs/EPSG/0/4326\"\n}\n\nresponse = oauth.post('https://sh.dataspace.copernicus.eu/api/v1/zarr/collections', json=collection)\nresponse.raise_for_status()\nExtracting the collection id and status from the response:\ncollection = response.json()['data']\ncollection_id = collection['id']\ncollection_status = collection['status']\n\n# if the ingestion failed, you can access the error as follows:\n# collection_errors = collection['ingestionErrors']\nTo update the name of your Zarr collection:\n# Update name of the collection\nnew_col_name = {\n  name: 'My modified collection name'\n}\nresponse = oauth.put(f'https://sh.dataspace.copernicus.eu/api/v1/zarr/collections/{collection_id}', json=new_col_name)\nresponse.raise_for_status()\nTo delete the collection:\n# Delete the collection\nresponse = oauth.delete(f'https://sh.dataspace.copernicus.eu/api/v1/zarr/collections/{collection_id}')\nresponse.raise_for_status()\n\n\nListing arrays\nIf the ingestion was successful, you can query all ingested arrays and their properties. Arrays are paginated, that is, if there are more than 100 arrays you will only get the first 100 by default. All pages can be traversed in the same way as with other paged Sentinel Hub endpoints (see e.g. the example under listing tiles on BYOC). Retrieving the first page is shown in the following snippet:\nresponse = oauth.get(f'https://sh.dataspace.copernicus.eu/api/v1/zarr/collections/{collection_id}/arrays')\nresponse.raise_for_status()\n\narrays = response.json()['data']\nYou can also get a single array by adding the array name to the URL path. For example, to get the array named B1 of the above collection:\n\nb1_array_response = oauth.get(f'https://sh.dataspace.copernicus.eu/api/v1/zarr/collections/{collection_id}/arrays/B1')\nb1_array_response.raise_for_status()\n\nb1_array = b1_array_response.json()['data']"
  },
  {
    "objectID": "APIs/SentinelHub/Overview.html",
    "href": "APIs/SentinelHub/Overview.html",
    "title": "API Overview",
    "section": "",
    "text": "The Sentinel Hub API is a RESTful API interface to various satellite imagery archives. It provides access to raw satellite data, rendered images, statistical analysis and much more."
  },
  {
    "objectID": "APIs/SentinelHub/Overview.html#about-sentinel-hub-api",
    "href": "APIs/SentinelHub/Overview.html#about-sentinel-hub-api",
    "title": "API Overview",
    "section": "",
    "text": "The Sentinel Hub API is a RESTful API interface to various satellite imagery archives. It provides access to raw satellite data, rendered images, statistical analysis and much more."
  },
  {
    "objectID": "APIs/SentinelHub/Overview.html#sentinel-hub-api-reference",
    "href": "APIs/SentinelHub/Overview.html#sentinel-hub-api-reference",
    "title": "API Overview",
    "section": "Sentinel Hub API reference",
    "text": "Sentinel Hub API reference\nThe Sentinel Hub API is annotated via OpenAPI. You can browse reference docs here:\n\nWeb preview\nYAML"
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2/Migration.html",
    "href": "APIs/SentinelHub/BatchV2/Migration.html",
    "title": "Migration",
    "section": "",
    "text": "Due to a few breaking changes we've made between the old Batch Processing API and Batch Processing V2, we've prepared a migration guide on what has changed between the two APIs."
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2/Migration.html#changes-to-bucket-access",
    "href": "APIs/SentinelHub/BatchV2/Migration.html#changes-to-bucket-access",
    "title": "Migration",
    "section": "7. Changes to Bucket Access",
    "text": "7. Changes to Bucket Access\nBackground In the previous version of our Batch API, accessing buckets required configuring bucket policies to allow full access to Sentinel Hub. However, in an effort to enhance security, this feature has been disabled in Batch Processing V2.\nNew Approach Instead of using bucket policies, Batch Processing V2 now supports alternative approach for accessing bucket, similarly to Batch Statistical API. These approach is detailed in the Bucket settings and access sections.\nAction Required When migrating to Batch Processing V2, ensure that you update your bucket access mechanisms to align with one mentioned above."
  },
  {
    "objectID": "Data/ComplementaryData/MERIS.html",
    "href": "Data/ComplementaryData/MERIS.html",
    "title": "ENVISAT- Medium Resolution Imaging Spectrometer (MERIS)",
    "section": "",
    "text": "Access to ENVISAT MERIS data is possible via API:  - OpenSearch  - OData\nIn order to get access to data at specific processing level as well as for specific product types, you are advised to use queries provided in each section below.\nIf it is required to customize query in respect to spatial and time coverage, satellite features etc. please, follow instructions on:  - OpenSearch  - OData"
  },
  {
    "objectID": "Data/ComplementaryData/MERIS.html#medium-resolution-imaging-spectrometer-meris---envisat",
    "href": "Data/ComplementaryData/MERIS.html#medium-resolution-imaging-spectrometer-meris---envisat",
    "title": "ENVISAT- Medium Resolution Imaging Spectrometer (MERIS)",
    "section": "Medium Resolution Imaging Spectrometer (MERIS) - ENVISAT",
    "text": "Medium Resolution Imaging Spectrometer (MERIS) - ENVISAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Medium Resolution Imaging Spectrometer (MERIS) was a programmable spectrometer on board the Envisat mission, operating in the solar reflective spectral range. Although primarily dedicated to ocean colour observations, MERIS extended its objectives to atmospheric- and land-surface-related studies. MERIS had high spectral and radiometric resolution and a dual spatial resolution of 260m x 290m over land and coastal regions and reduced resolution of 1040m x 1160m over ocean.\nMERIS was operational throughout the Envisat mission lifetime, from 2002 to 2012, and the first data from the instrument were available from May 2002.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nMER_FRS_1P\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMay 2002 - Apr 2012\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://meris-ds.eo.esa.int/oads/access/\n\n\nMore Information: https://earth.esa.int/eogateway/instruments/meris"
  },
  {
    "objectID": "Data/ComplementaryData/CEMS_Events/Events2.html",
    "href": "Data/ComplementaryData/CEMS_Events/Events2.html",
    "title": "Events",
    "section": "",
    "text": "Mass Movement\n        Storm\n        Volcano\n        Wild fire\n        Forest fire\n        Flood\n        Wind storm\n        Earthquake\n        Industrial\n        Humanitarian\n        Epidemic\n        Others"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat5.html",
    "href": "Data/ComplementaryData/Landsat5.html",
    "title": "Landsat-5",
    "section": "",
    "text": "The Landsat programme is a joint USGS and NASA-led enterprise for Earth observation that represents the world’s longest running system of satellites for moderate-resolution optical remote sensing for land, coastal areas and shallow waters.\nLandsat products in the Copernicus Data Space Ecosystem originate from the ESA processing. For more information please visit here.\nLandsat-5 was launched on 1 March 1984 and ended its mission on 5 June 2013. It carried the Thematic Mapper (TM), a multispectral scanning radiometer operating in the visible and infrared regions of the electromagnetic spectrum. It was characterized by 185 km swath width and 30 m resolution for visible (VIS), near infrared (NIR) and shortwave infrared (SWIR), and 120 m for thermal infrared (TIR). The acquired Landsat TM scene covers an area of approximately 183 km x 172.8 km. A standard full scene is nominally centred on the intersection of a path and a row (the actual image centre can vary by up to 100 m). A full image consists of 6920 pixels x 5760 lines and each uncompressed band in the VIS, NIR, SWIR and TIR spectral regions requires 40 MB of storage space.\nThe objective of Landsat-5 and every Landsat mission has been to repeatedly image Earth’s land and coastal areas in order to monitor changes to these areas over time.\nAccess to Landsat-5 data is possible via API\nIn order to get access to data at specific processing level as well as specific product types, you are advised to use queries provided in each section below.\nIf it is required to customize query in respect to spatial and time coverage, satellite features etc. please, follow instructions on:\n• OpenSearch\n• OData\nLevel-1"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat5.html#landsat-5-tm-l1g",
    "href": "Data/ComplementaryData/Landsat5.html#landsat-5-tm-l1g",
    "title": "Landsat-5",
    "section": "Landsat-5 TM-L1G",
    "text": "Landsat-5 TM-L1G\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-5 TM-L1G stands for Landsat-5 Thematic Mapper Level-1 Ground data. The data is calibrated and corrected to remove distortions, and then orthorectified to provide systematic geometric accuracy. It is widely used for environmental monitoring, land-use mapping, and natural resource management.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nTM__GEO_1P\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nApr 1984 - Nov 2011\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LandsatTM\n\n\nMore Information: https://earth.esa.int/eogateway/catalog/landsat-tm-esa-archive"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat5.html#landsat-5-tm-l1t",
    "href": "Data/ComplementaryData/Landsat5.html#landsat-5-tm-l1t",
    "title": "Landsat-5",
    "section": "Landsat-5 TM-L1T",
    "text": "Landsat-5 TM-L1T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-5 TM-L1T stands for Landsat-5 Thematic Mapper Level-1 Terrain corrected data. The Level-1 terrain-corrected data refer to the correction of the topographic displacement effects in the images, also known as relief displacement or parallax. The L1T processing level provides more precise geolocation information, which is particularly important for applications such as land-cover mapping and change detection. This product allows for more accurate and consistent image interpretation and analysis, making it a valuable tool for scientific research and environmental management.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nTM__GTC_1P\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nApr 1984 - Nov 2011\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LandsatTM\n\n\nMore Information: https://earth.esa.int/eogateway/catalog/landsat-tm-esa-archive"
  },
  {
    "objectID": "Data/ComplementaryData/SMOS.html",
    "href": "Data/ComplementaryData/SMOS.html",
    "title": "Soil Moisture and Ocean Salinity (SMOS)",
    "section": "",
    "text": "The Soil Moisture and Ocean Salinity (SMOS) mission was launched on 2 November 2009. It is one of the European Space Agency’s Earth Explorer missions, which form the science and research element of ESA’s Living Planet Programme.\nThe SMOS payload consists of the Microwave Imaging Radiometer using Aperture Synthesis (MIRAS) instrument, a passive microwave 2-D interferometric radiometer operating in the L-band (1.413 GHz, 21 cm) within a protected wavelength/frequency band. The SMOS mission operates on a sun-synchronous orbit (dusk-dawn 6am/6pm). SMOS measurements are made over a range of incidence angles (0 to 55°) across a swath of approximately 1000 km with a spatial resolution of 35 to 50 km. MIRAS can provide measurements in dual and full polarisation, the latter being its current mode of operation.\nSMOS Level 1 data products are designed for scientific and operational users who need to work with calibrated MIRAS instrument measurements, while SMOS Level 2 data products are designed for scientific and operational users who need to work with geo-located estimates of soil moisture and sea surface salinity as retrieved from the Level 1 dataset.\nAccess to SMOS data is possible via API\nIn order to get access to data at specific processing level as well as specific product types, you are advised to use queries provided in each section below.\nIf it is required to customize query in respect to spatial and time coverage, satellite features etc. please, follow instructions on:\n• OpenSearch\n• OData\nLevel-1 Level-2"
  },
  {
    "objectID": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l1b",
    "href": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l1b",
    "title": "Soil Moisture and Ocean Salinity (SMOS)",
    "section": "Soil Moisture and Ocean Salinity - L1B",
    "text": "Soil Moisture and Ocean Salinity - L1B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nSoil Moisture and Ocean Salinity - L1B are processed data of the SMOS mission. These are geolocated brightness temperatures that have been calibrated and corrected to provide valuable input for further processing into higher-level products like soil moisture and ocean salinity maps.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nMIR_SC_F1B\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\nMIR_SC_D1B\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://smos-diss.eo.esa.int/oads/access/\n\n\nMore Information: https://earth.esa.int/eogateway/missions/smos#instruments-section"
  },
  {
    "objectID": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l1cl",
    "href": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l1cl",
    "title": "Soil Moisture and Ocean Salinity (SMOS)",
    "section": "Soil Moisture and Ocean Salinity - L1CL",
    "text": "Soil Moisture and Ocean Salinity - L1CL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Soil Moisture and Ocean Salinity (SMOS) The L1CL product is an intermediate SMOS soil moisture and ocean salinity product that is used as input to generate other higher-level SMOS products such as L2 soil moisture and ocean salinity products, which combine SMOS data with other satellite and ground-based observations.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nMIR_BWLF1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\nMIR_BWLD1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\nMIR_BWSF1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\nMIR_BWSD1C - SCLD1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://smos-diss.eo.esa.int/oads/access/\n\n\nMore Information: https://earth.esa.int/eogateway/missions/smos#instruments-section"
  },
  {
    "objectID": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l1cs",
    "href": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l1cs",
    "title": "Soil Moisture and Ocean Salinity (SMOS)",
    "section": "Soil Moisture and Ocean Salinity - L1CS",
    "text": "Soil Moisture and Ocean Salinity - L1CS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nSMOS L1CS is an intermediate product that measures soil moisture and ocean salinity derived from raw data collected by the microwave radiometer onboard the SMOS satellite. It has a spatial resolution of 40 km and provides brightness temperatures and scattering angles to calculate the essential values for understanding the global water cycle. The data is useful for weather forecasting, drought monitoring, crop management, and coastal ecosystem protection. The product is used to generate higher-level SMOS products to manage water resources and monitor ecological systems.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nMIR_SCLF1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\nMIR_SCLD1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\nMIR_SCSF1C/MIR_SCSD1C\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://smos-diss.eo.esa.int/oads/access/\n\n\nMore Information: https://earth.esa.int/eogateway/missions/smos#instruments-section"
  },
  {
    "objectID": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l2os",
    "href": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l2os",
    "title": "Soil Moisture and Ocean Salinity (SMOS)",
    "section": "Soil Moisture and Ocean Salinity - L2OS",
    "text": "Soil Moisture and Ocean Salinity - L2OS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe SMOS L1OS product is an intermediate data product derived from the raw data collected by the microwave radiometer on board the SMOS satellite. It is used to derive higher-level SMOS products, such as L2 soil moisture and ocean salinity products, which are used for various applications, including weather forecasting, climate monitoring, agricultural planning, and water management. The SMOS L1OS data product is important for understanding the earth’s water cycle and the impacts of climate change on water resources. The data is used by scientists, policymakers, and resource managers to better understand and manage water resources, monitor ecological systems, and improve weather and climate forecasting.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nMIR_OSUDP2\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://smos-diss.eo.esa.int/oads/access/\n\n\nMore Information: https://earth.esa.int/eogateway/missions/smos#instruments-section"
  },
  {
    "objectID": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l2sm",
    "href": "Data/ComplementaryData/SMOS.html#soil-moisture-and-ocean-salinity---l2sm",
    "title": "Soil Moisture and Ocean Salinity (SMOS)",
    "section": "Soil Moisture and Ocean Salinity - L2SM",
    "text": "Soil Moisture and Ocean Salinity - L2SM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Level 2 Soil Moisture (SM) product comprises soil moisture measurements geo-located in an equal-area grid system ISEA 4H9. The product contains not only the retrieved soil moisture, but also a series of ancillary data derived from the processing (nadir optical thickness, surface temperature, roughness parameter, dielectric constant and brightness temperature retrieved at top of atmosphere and on the surface) with the corresponding uncertainties.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nMIR_SMUDP2\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2010 - Present\n\n\nOpenSearch\n\n\nOData\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://smos-diss.eo.esa.int/oads/access/\n\n\nMore Information: https://earth.esa.int/eogateway/missions/smos#instruments-section"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat8.html",
    "href": "Data/ComplementaryData/Landsat8.html",
    "title": "Landsat-8",
    "section": "",
    "text": "The Landsat programme is a joint USGS and NASA-led enterprise for Earth observation that represents the world’s longest running system of satellites for moderate-resolution optical remote sensing for land, coastal areas and shallow waters.\nLandsat products in the Copernicus Data Space Ecosystem originate from the ESA processing. For more information please visit here.\nLandsat-8 carries the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS). OLI provides imagery in the VIS, NIR and SWIR spectral ranges. It acquires images with 15 m panchromatic and 30 m multi-spectral spatial resolutions, covering a wide 185 km swath. This allows it to capture extensive areas of the Earth’s landscape while maintaining enough resolution to identify features like urban centers, farms, forests, and other land uses. The entire Earth falls within view once every 16 days due to Landsat-8’s near-polar orbit. The TIRS instrument is a thermal imager operating in a pushbroom mode with two Infra-Red channels: 10.8 µm and 12 µm with 100 m spatial resolution.\nAccess to Landsat-8 data is possible via API\nIn order to get access to data at specific processing level as well as specific product types, you are advised to use queries provided in each section below.\nIf it is required to customize query in respect to spatial and time coverage, satellite features etc. please, follow instructions on:\n• OpenSearch\n• OData\nLevel-1 Level-2"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l1gt",
    "href": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l1gt",
    "title": "Landsat-8",
    "section": "Landsat-8 OLI/TIRS_L1GT",
    "text": "Landsat-8 OLI/TIRS_L1GT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-8 OLI/TIRS L1GT refers to the Level-1 Geocorrected and Terrain corrected product acquired by the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) instrument on board Landsat 8 satellite. This product is corrected for geometric distortions caused by satellite altitude, position, and attitude, as well as to correct for variations in terrain height. The corrected images are orthorectified to a cartographic projection, with radiometric and atmospheric corrections applied to produce accurate and calibrated reflectance values. The Landsat-8 OLI/TIRS L1GT product also includes a level of geolocation accuracy and precision that corresponds to 3-meters.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nOLI/TIRS_L1GT\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nMar 2013 - Present\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LANDSAT-8_L1\n\n\nMore Information: https://earth.esa.int/eogateway/missions/landsat-8"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l1t",
    "href": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l1t",
    "title": "Landsat-8",
    "section": "Landsat-8 OLI/TIRS_L1T",
    "text": "Landsat-8 OLI/TIRS_L1T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-8 OLI/TIRS L1T refers to the Level-1 Precision Terrain corrected product acquired by the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) instrument on board Landsat 8 satellite. This product is corrected for geometric distortions caused by the satellite’s altitude, position, and attitude, as well as to correct for variations in terrain height. The corrected images are orthorectified to a cartographic projection, with radiometric and atmospheric corrections applied to produce accurate and calibrated reflectance values. The Landsat-8 OLI/TIRS L1T product also includes a level of radiometric accuracy and precision that corresponds to 1/3 of a Landsat pixel.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\n(*) OLI/TIRS_L1T\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nMar 2013 - Aug 2020\n\n\n\n\n\n(*) not accessible by landsat-diss.eo.esa.int\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LANDSAT-8_L1\n\n\nMore Information: https://earth.esa.int/eogateway/missions/landsat-8"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l1tp",
    "href": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l1tp",
    "title": "Landsat-8",
    "section": "Landsat-8 OLI/TIRS_L1TP",
    "text": "Landsat-8 OLI/TIRS_L1TP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-8 OLI/TIRS L1TP refers to the Level-1 Precision Terrain corrected product acquired by the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) instrument on board Landsat 8 satellite. This product is similar to the Landsat-8 OLI/TIRS L1T product, but with the addition of a Landsat Quality Assessment (QA) band. This QA band flags areas of the image affected by clouds, cloud shadow, haze, or other factors that may affect the overall quality of the image. The Landsat-8 OLI/TIRS L1TP product also includes the same level of radiometric and geometric accuracy and precision as the Landsat-8 OLI/TIRS L1T product.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nOLI/TIRS_L1TP\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nMar 2013 - Present\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LANDSAT-8_L1\n\n\nMore Information: https://earth.esa.int/eogateway/missions/landsat-8"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l2sp",
    "href": "Data/ComplementaryData/Landsat8.html#landsat-8-olitirs_l2sp",
    "title": "Landsat-8",
    "section": "Landsat-8 OLI/TIRS_L2SP",
    "text": "Landsat-8 OLI/TIRS_L2SP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-8 OLI/TIRS L2SP refers to the Level-2 Surface Reflectance product acquired by the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) instrument on board Landsat 8 satellite. This product is derived from the Landsat-8 OLI/TIRS L1T or L1TP product and is generated using atmospheric correction algorithms to remove atmospheric effects and convert the top-of-atmosphere reflectance to surface reflectance values.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nOLI/TIRS_L2SP\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nJan 2015 - Present\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LANDSAT-8_L1\n\n\nMore Information: https://earth.esa.int/eogateway/missions/landsat-8"
  },
  {
    "objectID": "Data/ComplementaryData/CAMS.html",
    "href": "Data/ComplementaryData/CAMS.html",
    "title": "Copernicus Atmosphere Monitoring Service (CAMS)",
    "section": "",
    "text": "The Copernicus Atmosphere Monitoring Service (CAMS) is a service implemented by the European Centre for Medium-Range Weather Forecasts (ECMWF) which provides continuous, open, free, regular data and information on atmospheric composition. CAMS describes the current situation, forecasts, reanalysis and analyses consistently retrospective data records for recent years.\nCopernius Data Space Ecosystem provides data from Global Fire Assimilation System (GFAS), Global Atmospheric Composition Forecasts (GLOBAL) including analysis and forecast data on vertical level, Global Additional (GLOBAL_ADDITIOANL) and WMO Essential including data on cyclones."
  },
  {
    "objectID": "Data/ComplementaryData/CAMS.html#global-fire-assimilation-system-gfas",
    "href": "Data/ComplementaryData/CAMS.html#global-fire-assimilation-system-gfas",
    "title": "Copernicus Atmosphere Monitoring Service (CAMS)",
    "section": "Global Fire Assimilation System (GFAS)",
    "text": "Global Fire Assimilation System (GFAS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Global Fire Assimilation System (GFAS) uses data from satellite observations, such as fire detection and radiative power measurements, to estimate global wildfire emissions. This data is then assimilated into a global atmospheric chemistry transport model to provide tailored information on wildfire emissions to CAMS users.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nCatalogue\n\n\n\n\n\n\nAN (Analysis)\n\n\nWorld\n\n\n(*) Nov 2022 - Apr 2023\n\n\n/eodata/CAMS/GFAS/\n\n\n\n\n\n(*) Available ~7-14 days after product’s acquisition."
  },
  {
    "objectID": "Data/ComplementaryData/CAMS.html#global-atmospheric-composition-forecasts-global",
    "href": "Data/ComplementaryData/CAMS.html#global-atmospheric-composition-forecasts-global",
    "title": "Copernicus Atmosphere Monitoring Service (CAMS)",
    "section": "Global Atmospheric Composition Forecasts (GLOBAL)",
    "text": "Global Atmospheric Composition Forecasts (GLOBAL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe CAMS Global Atmospheric Composition Forecasts (GLOBAL) service provides short-term forecasts of atmospheric composition up to five days in advance. The GLOBAL service utilizes state-of-the-art satellite observations, measurement networks, and models to provide forecasts of a variety of atmospheric constituents, including ozone, nitrogen dioxide, carbon monoxide, and dust.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nCatalogue\n\n\n\n\n\n\nAN (Analysis)\nFC (Forecasts)\n\n\nWorld\n\n\n(*) Nov 2022 - Apr 2023\n\n\n/eodata/CAMS/GLOBAL/\n\n\n\n\n\n(*) Available ~7-14 days after product’s acquisition"
  },
  {
    "objectID": "Data/ComplementaryData/CAMS.html#global-additional",
    "href": "Data/ComplementaryData/CAMS.html#global-additional",
    "title": "Copernicus Atmosphere Monitoring Service (CAMS)",
    "section": "GLOBAL ADDITIONAL",
    "text": "GLOBAL ADDITIONAL\n\nOverview\nIn addition to the services provided by the CAMS Global Atmospheric Composition Forecasts (GLOBAL), the CAMS program offers several other services aimed at monitoring and forecasting air quality, greenhouse gas concentrations, and other atmospheric constituents.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nCatalogue\n\n\n\n\n\n\nAN (Analysis)\nFC (Forecasts)\n\n\nWorld\n\n\n(*) Nov 2022 - Apr 2023\n\n\n/eodata/CAMS/GLOBAL_ADDITIONAL/\n\n\n\n\n\n(*) Available ~7-14 days after product’s acquisition"
  },
  {
    "objectID": "Data/ComplementaryData/CAMS.html#wmo-essentials",
    "href": "Data/ComplementaryData/CAMS.html#wmo-essentials",
    "title": "Copernicus Atmosphere Monitoring Service (CAMS)",
    "section": "WMO ESSENTIALS",
    "text": "WMO ESSENTIALS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nWorld Meteorological Organization (WMO) Essential Climate Variables (ECVs). These ECVs are parameters that are used to monitor and understand climate change, and are recognized by the WMO as essential indicators of the Earth’s climate system. They related to atmospheric composition, including the concentration of aerosols and greenhouse gases, as well as the distribution of ozone, nitrogen dioxide, and sulfur dioxide.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nCatalogue\n\n\n\n\n\n\nGH (Geopotential Height),\nT (Temperature),\nMSL (Mean sea level pressure),\nU (U component of wind),\nV (V component of wind),\nTCT (Tropical Cyclone Trajectory)\n\n\nWorld\n\n\n(*) Mar 2018 - Oct 2022\n\n\n/eodata/CAMS/WMO_ESSENTIAL/\n\n\n\n\n\n(*) Available ~7-14 days after product’s acquisition"
  },
  {
    "objectID": "Data/Others/Sentinel2_L1C_baseline.html",
    "href": "Data/Others/Sentinel2_L1C_baseline.html",
    "title": "Sentinel-2 L1C Baselines",
    "section": "",
    "text": "The Sentinel-1 L1C collection within the Copernicus Data Space Ecosystem originates from the operational Copernicus processing, i.e. data previously accessible on the Copernicus Open Access Hub (formerly known as Sentinels Scientific Data Hub).\nThese products can be differentiated by the OData and OpenSearch processingBaseline attribute. Processing Baseline is presented on Copernicus Browser as a “Processor version” attribute.\nThe current state of the archive of the Sentinel-L1C data is as follows:\n\n\n\n\n\nAvailability of S-2 L1C products based on sensing date\n\n\nProcessing Baseline\n\n\n\n\n\n\n15/12/2015 - 03/05/2016\n\n\nProcessing Baseline 02.01 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 2.01)\n\n\n\n\n03/05/2016 - 15/06/2016\n\n\nProcessing Baseline 02.02 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.02 or 2.01)\n\n\n\n\n04/07/2015 - 26/05/2017\n\n\nProcessing Baseline 02.04 (To replace 02.03) (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.04 or 2.04)\n\n\n\n\n19/03/2017 - 23/10/2017\n\n\nProcessing Baseline 02.05 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.05 or 2.05)\n\n\n\n\n10/04/2016 - 06/11/2018\n\n\nProcessing Baseline 02.06 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.06 or 2.06)\n\n\n\n\n06/11/2018 - 22/07/2019\n\n\nProcessing Baseline 02.07 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.07 or 2.07)\n\n\n\n\n10/04/2016 - 04/02/2020\n\n\nProcessing Baseline 02.08 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 2.08)\n\n\n\n\n29/09/2015 – 30/03/2021\n\n\nProcessing Baseline 02.09 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 2.09)\n\n\n\n\n30/03/2021 - 30/06/2021\n\n\nProcessing Baseline 03.00 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 3.0)\n\n\n\n\n30/06/2021 - 25/01/2022\n\n\nProcessing Baseline 03.01 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 03.01 or 3.01)\n\n\n\n\n25/01/2022 - 06/12/2022\n\n\nProcessing Baseline 04.00 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 04.00 or 4.0)\n\n\n\n\n29/04/2022 – 13/12/2023\n\n\nProcessing Baseline 05.09 (originating from Copernicus Open Access Hub and Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.09 or 5.09)\n\n\n\n\n13/12/2023 - Now\n\n\nProcessing Baseline 05.10 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.10 or 5.10)\n\n\n\n\n23/07/2024 - Now\n\n\nProcessing Baseline 05.11 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.11 or 5.11)\n\n\n\n\n\nFor more information on baseline 05.11 see Deployment of Sentinel-2 Processing Baseline in version 05.11 on 23 July .\n\nProcessing Baseline 5.00 and 5.10 for Sentinel-2 L1C and L2A\n\n\n\n\n\n\nUpdated availability by sensing time period\n\n\nSentinel-2A\n\n\nSentinel-2B\n\n\n\n\n\n\nPublished (Processing baseline 05.00)\n\n\nFrom 4 July 2015 to 31 December 2021 included\n\n\nFrom 17 March 2017 to 31 December 2021 included\n\n\n\n\nNext period in list (Processing baseline 05.10)\n\n\nFrom 1 January 2022 then continuing in chronological order sensing time, up to 13 December 2023\n\n\nFrom January 1 2022 then continuing in chronological order of sensing time, up to 13 December 2023"
  },
  {
    "objectID": "Data/Others/Sentinel2_L2A_baseline.html",
    "href": "Data/Others/Sentinel2_L2A_baseline.html",
    "title": "Sentinel-2 L2A Baselines",
    "section": "",
    "text": "The Sentinel-2 L2A collection within the Copernicus Data Space Ecosystem originates from two sources:\n(1) operational Copernicus processing, i.e. data previously accessible on the Copernicus Open Access Hub (formerly known as Sentinels Scientific Data Hub);\n(2) reprocessing of the L2A archive from 2015 until the end of 2021 (Collection 1), and ;\nThese products can be differentiated by the OData and OpenSearch (origin) attribute (ESA or CloudFerro) and by the ‘processingBaseline’ attribute. Processing Baseline is presented on Copernicus Browser as a “Processor version” attribute.\nThe current state of the archive of the Sentinel-L2A data is as follows:\n\n\n\n\n\nAvailability of S-2 L2A products based on sensing date\n\n\nProcessing Baseline\n\n\n\n\n\n\n04/07/2015 - 21/05/2021\n\n\nProcessing Baselines 2.05 -2.13 (originating from Copernicus Open Access Hub and generated by CloudFerro)\n\n\n\n\n04/02/2020 - 30/03/2021\n\n\nProcessing Baseline 2.14 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.14)\n\n\n\n\n25/01/2022 - 06/12/2022\n\n\nProcessing Baseline 4.00 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 04.00 or 4.0)\n\n\n\n\n29/04/2022 - 13/12/2023\n\n\nProcessing Baseline 5.09 (originating from Copernicus Open Access Hub and Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.09 or 5.09)\n\n\n\n\n13/12/2023 - Now\n\n\nProcessing Baseline 5.10 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.10 or 5.10)\n\n\n\n\n23/07/2024 - Now\n\n\nProcessing Baseline 05.11 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.11 or 5.11)\n\n\n\n\n\nFor more information on baseline 05.11 see Deployment of Sentinel-2 Processing Baseline in version 05.11 on 23 July .\n\nProcessing Baseline 5.00 and 5.10 for Sentinel-2 L1C and L2A\n\n\n\n\n\n\nUpdated availability by sensing time period\n\n\nSentinel-2A\n\n\nSentinel-2B\n\n\n\n\n\n\nPublished (Processing baseline 05.00)\n\n\nFrom 4 July 2015 to 31 December 2021 included\n\n\nFrom 17 March 2017 to 31 December 2021 included\n\n\n\n\nNext period in list (Processing baseline 05.10)\n\n\nFrom 1 January 2022 then continuing in chronological order sensing time, up to 13 December 2023\n\n\nFrom January 1 2022 then continuing in chronological order of sensing time, up to 13 December 2023"
  },
  {
    "objectID": "Data/Others/Dashboard.html",
    "href": "Data/Others/Dashboard.html",
    "title": "Copernicus Operations Dashboard",
    "section": "",
    "text": "This dashboard keeps users and stakeholders up to date with the latest information about available satellite data.\nAccess Link : https://operations.dashboard.copernicus.eu/index\nIt aims at providing details on the status of the Copernicus operations, covering Sentinel-1, 2, 3 (Land) and Sentinel-5P.\nThe Homepage includes a high-level overview of the Copernicus Sentinel missions over the past 24 h, including the number of missions, time spent gathering data, data volumes, and the number of products delivered.\nThe Events tab provides details of events over the past three months that have impact on the completeness of the data production, such as planned calibration activities, manoeuvrers, or anomalies. The information of which data is affected is included.\n\n\n\n\n\n\n\n\n\n\nThe Data Takes tab hosts a real-time list of available collections delivered by the missions, enabling users to scan through these products to find data that meet their research and development requirements, and to assess their availability.\n\n\n\n\n\nThe Publication Statistics tab provides detailed information on Copernicus Sentinel data – such as number of products delivered – covering anywhere between the past 24h and the past three months. These insights are represented visually, with one graphical representation per mission that is subdivided by sensor.\n\n\n\n\n\nIn the coming year the Dashboard is planned to improve the emerging requirements.\nFor any inquiries on the Copernicus Operations Dashboard contact us."
  },
  {
    "objectID": "Data/Others/Sentinel1_Mosaic_Algorithm.html",
    "href": "Data/Others/Sentinel1_Mosaic_Algorithm.html",
    "title": "Algorithm",
    "section": "",
    "text": "Input data and preprocessing\nSentinel-1 GRD data, as offered and pre-processed in Sentinel Hub, serves as input for the generation of mosaics. The preprocessing steps made in Sentinel Hub are explained in detail in the Sentinel Hub documentation. Here, we are only listing the main processing steps applied to the input data before mosaicking:\n(1) Calibration to beta0 backscatter coefficient\n(2) Thermal noise removal\n(3) Radiometric terrain correction using area integration\n(4) Terrain Correction using Range-Doppler terrain correction\nFor steps 3 and 4, the Copernicus DEM was used with a spatial resolution of 10m over Europe and 30m for the rest of the world.\nGeneration of mosaics\nTwo different mosaics are being produced for each month: IW mosaic and DH mosaic\n\n\n\n\n\n\n\nIW mosaic\n\n\nDH mosaic\n\n\n\n\n\n\nPolarization\n\n\nVV + VH\n\n\nHH + HV\n\n\n\n\nAcquisition mode\n\n\nIW acquisition mode\n\n\nAll acquisition modes\n\n\n\n\nOrbit direction\n\n\nboth\n\n\n\n\nProcessing grid\n\n\nUTM grid with 100,08 x 100,08 km tiles (link)\n\n\n\n\nSpatial resolution\n\n\n20 m\n\n\n40 m\n\n\n\n\nOutput format\n\n\n16-bit cloud optimized GeoTIFF\n\n\n\n\n\nThe weighted sum of the flattened backscatter observations was used for mosaicking data in a monthly stack. Observations with the highest available local resolution receive the highest local weights. Therefore, the differences in backscatter between areas sloping towards the sensor and away from the sensor in individual orbits are largely corrected, and the resulting signal is mainly a product of the local surface properties.Observations in radar shadows are filtered out and are not used. The algorithm is described in detail in the paper by D. Small ‘SAR backscatter multitemporal compositing via local resolution weighting’ (pdf is available). The resulting mosaics have less noise and better spatial homogeneity when compared to each single Sentinel-1 GRD observation.\nAccess Sentinel-1 Level 3 Monthly Mosaics with Sentinel Hub\n\n\nAccess Sentinel-1 Level 3 Monthly Mosaics with Sentinel Hub\nSentinel-1 Level 3 Monthly Mosaics are onboarded to Sentinel Hub as a BYOC data collection. To access the data, you will need the specific pieces of information listed below, for general information about how to access BYOC collections visit our Data BYOC page.\nIW Mosaics\n\nData collection id: byoc-3c662330-108b-4378-8899-525fd5a225cb\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nVV\n\n\nVV polarization\n\n\n20 m\n\n\n\n\nVH\n\n\nVH polarization\n\n\n20 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more)\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\nExample of requesting mosaic over Sfântu Gheorghe with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\", \"VH\", \"dataMask\"],\n    output: { bands: 3 }\n  };\n}\n\nvar viz = new HighlightCompressVisualizer(0, 0.8);\nvar gain = 0.8;\n\n\nfunction evaluatePixel(sample) {\n  if (sample.dataMask == 0) {\n    return [0, 0, 0];\n  }\n  \n  let vals = [gain * sample.VV / 0.28,\n              gain * sample.VH / 0.06,\n              gain * sample.VH / sample.VV / 0.49];\n  \n  return viz.processList(vals);\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n          25.713501,\n          45.74836,\n          26.196213,\n          45.965231\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-09-01T00:00:00Z\",\n            \"to\": \"2023-09-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-3c662330-108b-4378-8899-525fd5a225cb\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 512,\n    \"height\": 330,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\nDH Mosaics\n\nData collection id: byoc-cc676fec-cb8d-4bc1-adce-1d9658da950b\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nHH\n\n\nHH polarization\n\n\n40 m\n\n\n\n\nHV\n\n\nHV polarization\n\n\n40 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more)\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\n\nExample of requesting mosaic over Reykjavík with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"HH\", \"HV\", \"dataMask\"],\n    output: { bands: 4 }\n  };\n}\n\nvar viz = new HighlightCompressVisualizer(0, 0.8);\nvar gain = 0.8;\n\n\nfunction evaluatePixel(sample) {\n  let vals = [gain * sample.HH / 0.28,\n              gain * sample.HV / 0.06,\n              gain * sample.HV / sample.HH / 0.49];\n  \n  let out = viz.processList(vals);\n  out.push(sample.dataMask);\n  return out;\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n          -22.486267,\n          63.959085,\n          -19.79187,\n          64.722572\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-01-01T00:00:00Z\",\n            \"to\": \"2023-01-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-cc676fec-cb8d-4bc1-adce-1d9658da950b\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 858,\n    \"height\": 553,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel2.html",
    "href": "Data/SentinelMissions/Sentinel2.html",
    "title": "Sentinel-2",
    "section": "",
    "text": "The Copernicus Sentinel-2 mission comprises a land monitoring constellation of two polar-orbiting satellites placed in the same sun-synchronous orbit, phased at 180° to each other. It aims at monitoring variability in land surface conditions, and its wide swath width (290 km) and high revisit time (10 days at the equator with one satellite, and 5 days with 2 satellites which results in 2-3 days at mid-latitudes) will support monitoring of Earth’s surface changes.\nEach Sentinel-2 products is composed of approximately 110x110 km tiles in cartographic geometry (UTM/WGS84 projection). Earth is subdivided on a predefined set of tiles, defined in UTM/WGS84 projection and using a 100 km step. However, each tile has a surface of 110x110 km² in order to provide large overlap with the neighbouring.\nLevel-1 Level-2 Level-3"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-2a-surface-reflectance",
    "href": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-2a-surface-reflectance",
    "title": "Sentinel-2",
    "section": "Sentinel-2 Level 2A Surface Reflectance",
    "text": "Sentinel-2 Level 2A Surface Reflectance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nSentinel-2 Level 2A Level 2A product provides atmospherically corrected Surface Reflectance (SR) images, derived from the associated Level-1C products. The atmospheric correction of Sentinel-2 images includes the correction of the scattering of air molecules (Rayleigh scattering), of the absorbing and scattering effects of atmospheric gases, in particular ozone, oxygen and water vapour and the correction of absorption and scattering due to aerosol particles. Level 2A product are considered an ARD product.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked (original ESA product)\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMarch 2017 - Present\n\n\nJan 2023\n\n\n\n\nPacked or Unpacked (Product generated by CloudFerro using sen2cor processor)\n\n\nImmediately available data(IAD)\n\n\nRandom coverage based on user request\n\n\nJuly 2015 - January 2023\n\n\nJan 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2015-07-04’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\nCommon Name\n\n\nGSD(m)\n\n\nCenter Wavelength(μm)\n\n\n\n\n\n\nB01\n\n\nCoastal aerosol\n\n\n60\n\n\n0.443\n\n\n\n\nB02\n\n\nBlue\n\n\n10\n\n\n0.49\n\n\n\n\nB03\n\n\nGreen\n\n\n10\n\n\n0.56\n\n\n\n\nB04\n\n\nRed\n\n\n10\n\n\n0.665\n\n\n\n\nB05\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.705\n\n\n\n\nB06\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.74\n\n\n\n\nB07\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.783\n\n\n\n\nB08\n\n\nNIR\n\n\n10\n\n\n0.842\n\n\n\n\nB8A\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.865\n\n\n\n\nB09\n\n\nWater vapour\n\n\n60\n\n\n0.945\n\n\n\n\nB10\n\n\nSWIR - Cirrus\n\n\n60\n\n\n1.375\n\n\n\n\nB11\n\n\nSWIR\n\n\n20\n\n\n1.61\n\n\n\n\nB12\n\n\nSWIR\n\n\n20\n\n\n2.19\n\n\n\n\nSCL\n\n\nScene Classification\n\n\n20\n\n\n\n\n\n\nSNW\n\n\nSnow probability\n\n\n20\n\n\n\n\n\n\nCLD\n\n\nCloud probability\n\n\n20\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSTAC: https://stac-extensions.github.io/eo/v1.0.0/schema.json\n\n\nWMTS: https://services.sentinel-hub.com/ogc/wmts/7d34803f-511c-4caf-9438-6d72f32c8174\n\n\n\n\n\nSentinel-2 L2A Baselines\nThe Sentinel-2 L2A collection within the Copernicus Data Space Ecosystem originates from two sources:\n(1) operational Copernicus processing, i.e. data previously accessible on the Copernicus Open Access Hub (formerly known as Sentinels Scientific Data Hub);\n(2) reprocessing of the L2A archive from 2015 until the end of 2021 (Collection 1), and ;\nThese products can be differentiated by the OData and OpenSearch (origin) attribute (ESA or CloudFerro) and by the ‘processingBaseline’ attribute. Processing Baseline is presented on Copernicus Browser as a “Processor version” attribute.\nThe current state of the archive of the Sentinel-L2A data is as follows:\n\n\n\n\n\nAvailability of S-2 L2A products based on sensing date\n\n\nProcessing Baseline\n\n\n\n\n\n\n04/07/2015 - 21/05/2021\n\n\nProcessing Baselines 2.05 -2.13 (originating from Copernicus Open Access Hub and generated by CloudFerro)\n\n\n\n\n04/02/2020 - 30/03/2021\n\n\nProcessing Baseline 2.14 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.14)\n\n\n\n\n25/01/2022 - 06/12/2022\n\n\nProcessing Baseline 4.00 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 04.00 or 4.0)\n\n\n\n\n29/04/2022 - 13/12/2023\n\n\nProcessing Baseline 5.09 (originating from Copernicus Open Access Hub and Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.09 or 5.09)\n\n\n\n\n13/12/2023 - Now\n\n\nProcessing Baseline 5.10 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.10 or 5.10)\n\n\n\n\n23/07/2024 - Now\n\n\nProcessing Baseline 05.11 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.11 or 5.11)\n\n\n\n\n\nFor more information on baseline 05.11 see Deployment of Sentinel-2 Processing Baseline in version 05.11 on 23 July .\n\n\nProcessing Baseline 5.00 and 5.10 for Sentinel-2 L1C and L2A\n\n\n\n\n\n\nUpdated availability by sensing time period\n\n\nSentinel-2A\n\n\nSentinel-2B\n\n\n\n\n\n\nPublished (Processing baseline 05.00)\n\n\nFrom 4 July 2015 to 31 December 2021 included\n\n\nFrom 17 March 2017 to 31 December 2021 included\n\n\n\n\nNext period in list (Processing baseline 05.10)\n\n\nFrom 1 January 2022 then continuing in chronological order sensing time, up to 13 December 2023\n\n\nFrom January 1 2022 then continuing in chronological order of sensing time, up to 13 December 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-1c-top-of-atmosphere-toa",
    "href": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-1c-top-of-atmosphere-toa",
    "title": "Sentinel-2",
    "section": "Sentinel-2 Level 1C Top of Atmosphere (TOA)",
    "text": "Sentinel-2 Level 1C Top of Atmosphere (TOA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nSentinel-2 Level 1C products are available globally from 2015 onwards. These products are resampled with a constant Ground Sampling Distance (GSD) of 10, 20 and 60 m, depending on the native resolution of the different spectral bands. Pixel coordinates refer to the upper left corner of the pixel.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJul 2015 - Present\n\n\nJan 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2015-07-04T00:00:00Z’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\nCommon Name\n\n\nGSD(m)\n\n\nCenter Wavelength(μm)\n\n\n\n\n\n\nB01\n\n\nCoastal aerosol\n\n\n60\n\n\n0.443\n\n\n\n\nB02\n\n\nBlue\n\n\n10\n\n\n0.49\n\n\n\n\nB03\n\n\nGreen\n\n\n10\n\n\n0.56\n\n\n\n\nB04\n\n\nRed\n\n\n10\n\n\n0.665\n\n\n\n\nB05\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.705\n\n\n\n\nB06\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.74\n\n\n\n\nB07\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.783\n\n\n\n\nB08\n\n\nNIR\n\n\n10\n\n\n0.842\n\n\n\n\nB8A\n\n\nVegetation Red Edge\n\n\n20\n\n\n0.865\n\n\n\n\nB09\n\n\nWater vapour\n\n\n60\n\n\n0.945\n\n\n\n\nB10\n\n\nSWIR - Cirrus\n\n\n60\n\n\n1.375\n\n\n\n\nB11\n\n\nSWIR\n\n\n20\n\n\n1.61\n\n\n\n\nB12\n\n\nSWIR\n\n\n20\n\n\n2.19\n\n\n\n\nSCL\n\n\nScene Classification\n\n\n20\n\n\n\n\n\n\nSNW\n\n\nSnow probability\n\n\n20\n\n\n\n\n\n\nCLD\n\n\nCloud probability\n\n\n20\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSTAC: https://stac-extensions.github.io/datacube/v2.2.0/schema.json\n\n\nWMTS: https://services.sentinel-hub.com/ogc/wmts/ef291c3e-77fd-43f2-a885-dced9ac1e6a7\n\n\n\n\n\nSentinel-2 L1C Baselines\nThe Sentinel-1 L1C collection within the Copernicus Data Space Ecosystem originates from the operational Copernicus processing, i.e. data previously accessible on the Copernicus Open Access Hub (formerly known as Sentinels Scientific Data Hub).\nThese products can be differentiated by the OData and OpenSearch processingBaseline attribute. Processing Baseline is presented on Copernicus Browser as a “Processor version” attribute.\nThe current state of the archive of the Sentinel-L1C data is as follows:\n\n\n\n\n\nAvailability of S-2 L1C products based on sensing date\n\n\nProcessing Baseline\n\n\n\n\n\n\n15/12/2015 - 03/05/2016\n\n\nProcessing Baseline 02.01 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 2.01)\n\n\n\n\n03/05/2016 - 15/06/2016\n\n\nProcessing Baseline 02.02 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.02 or 2.01)\n\n\n\n\n04/07/2015 - 26/05/2017\n\n\nProcessing Baseline 02.04 (To replace 02.03) (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.04 or 2.04)\n\n\n\n\n19/03/2017 - 23/10/2017\n\n\nProcessing Baseline 02.05 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.05 or 2.05)\n\n\n\n\n10/04/2016 - 06/11/2018\n\n\nProcessing Baseline 02.06 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.06 or 2.06)\n\n\n\n\n06/11/2018 - 22/07/2019\n\n\nProcessing Baseline 02.07 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 02.07 or 2.07)\n\n\n\n\n10/04/2016 - 04/02/2020\n\n\nProcessing Baseline 02.08 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 2.08)\n\n\n\n\n29/09/2015 – 30/03/2021\n\n\nProcessing Baseline 02.09 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 2.09)\n\n\n\n\n30/03/2021 - 30/06/2021\n\n\nProcessing Baseline 03.00 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 3.0)\n\n\n\n\n30/06/2021 - 25/01/2022\n\n\nProcessing Baseline 03.01 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 03.01 or 3.01)\n\n\n\n\n25/01/2022 - 06/12/2022\n\n\nProcessing Baseline 04.00 (originating from Copernicus Open Access Hub) (Attribute ‘processorVersion’: 04.00 or 4.0)\n\n\n\n\n29/04/2022 – 13/12/2023\n\n\nProcessing Baseline 05.09 (originating from Copernicus Open Access Hub and Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.09 or 5.09)\n\n\n\n\n13/12/2023 - Now\n\n\nProcessing Baseline 05.10 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.10 or 5.10)\n\n\n\n\n23/07/2024 - Now\n\n\nProcessing Baseline 05.11 (originating from Copernicus Data Space Ecosystem) (Attribute ‘processorVersion’: 05.11 or 5.11)\n\n\n\n\n\nFor more information on baseline 05.11 see Deployment of Sentinel-2 Processing Baseline in version 05.11 on 23 July .\n\n\nProcessing Baseline 5.00 and 5.10 for Sentinel-2 L1C and L2A\n\n\n\n\n\n\nUpdated availability by sensing time period\n\n\nSentinel-2A\n\n\nSentinel-2B\n\n\n\n\n\n\nPublished (Processing baseline 05.00)\n\n\nFrom 4 July 2015 to 31 December 2021 included\n\n\nFrom 17 March 2017 to 31 December 2021 included\n\n\n\n\nNext period in list (Processing baseline 05.10)\n\n\nFrom 1 January 2022 then continuing in chronological order sensing time, up to 13 December 2023\n\n\nFrom January 1 2022 then continuing in chronological order of sensing time, up to 13 December 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-1b",
    "href": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-1b",
    "title": "Sentinel-2",
    "section": "Sentinel-2 Level 1B",
    "text": "Sentinel-2 Level 1B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-2 Level 1B product provides radiometrically corrected imagery in Top-Of-Atmosphere (TOA) radiance values and in sensor geometry. Additionally, this product includes the refined geometric model which is used to generate the Level 1C product.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\n(*) EUP\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast two weeks\n\n\nQ3 2024\n\n\n\n\n\n(*) Access restrictions may apply."
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-3-quarterly-mosaics",
    "href": "Data/SentinelMissions/Sentinel2.html#sentinel-2-level-3-quarterly-mosaics",
    "title": "Sentinel-2",
    "section": "Sentinel-2 Level 3 Quarterly Mosaics",
    "text": "Sentinel-2 Level 3 Quarterly Mosaics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nSentinel-2 Quarterly Mosaics are mosaics generated from three months of Sentinel-2 level 2A.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\n(*) Jan 2022 - Present\n\n\nNov 2023\n\n\n\n\n\n(*) More will be added\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2023-01-01T00:00:00Z’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\nCommon Name\n\n\nGSD(m)\n\n\nCenter Wavelength(μm)\n\n\n\n\n\n\nB02\n\n\nBlue\n\n\n10\n\n\n0.49\n\n\n\n\nB03\n\n\nGreen\n\n\n10\n\n\n0.56\n\n\n\n\nB04\n\n\nRed\n\n\n10\n\n\n0.665\n\n\n\n\nB08\n\n\nNIR\n\n\n10\n\n\n0.842\n\n\n\n\nobservations\n\n\nValid pixels\n\n\n10\n\n\nN/A\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nThe following algorithm was run independently for each pixel:\n(1) For each pixel: Take the three-month stack of Sentinel-2 L2A observations. Only bands B02, B03, B04, B08 and SCL are used to create the mosaic. For bands B02-B08 transform the values to reflectance.\n(2) For each observation: Mark an observation as invalid if the value of the Sentinel-2 L2A scene classification band (SCL) has one of the following values:\n\n1-SATURATED_DEFECTIVE,\n3-CLOUD_SHADOW,\n7-CLOUD_LOW_PROBA / UNCLASSIFIED,\n8-CLOUD_MEDIUM_PROBA,\n9-CLOUD_HIGH_PROBA,\n10-THIN_CIRRUS\n\n(3) For each pixel: Discard all invalid observations, what remains is called valid observations. The number of valid observations is generally different for each pixel and is output as a positive integer in the observations output band.\n(4) For each pixel, for each band (B02, B03, B04, B08): Sort all valid observations for each band separately.\n(5) For each pixel, for each band (B02, B03, B04, B08): Take the value of the first quartile and multiply it by 10000 (to get a ‘digital number’). This is an output value.\n(6) For each pixel, for each band (B02, B03, B04, B08): If there are no valid observations, output the value -32768, which represents no data. For the observations band, output the value 0, which also represents no data.\n\n\n\n\n\n\nNote\n\n\n\n\nIf multiple Sentinel-2 observations from the same day are available, only the most recent observation on that day is used.\nNo pre-filtering (e.g. based on cloud coverage) was performed to preserve as many non-cloudy pixels as possible.\n\n\n\nAccess Sentinel-2 Level 3 Quarterly Mosaics with Sentinel Hub\n\n\n\nAccess Sentinel-2 Level 3 Quarterly Mosaics with Sentinel Hub\nSentinel-2 Level 3 Quarterly Mosaics are onboarded to Sentinel Hub as a BYOC data collection. To access the data, you will need the specific pieces of information listed below, for general information about how to access BYOC collections visit our Data BYOC page.\n\nData collection id: byoc-5460de54-082e-473a-b6ea-d5cbe3c17cca\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nB02\n\n\nBlue\n\n\n10 m\n\n\n\n\nB03\n\n\nGreen\n\n\n10 m\n\n\n\n\nB04\n\n\nRed\n\n\n10 m\n\n\n\n\nB08\n\n\nNear Infrared (NIR)\n\n\n10 m\n\n\n\n\nobservations\n\n\nNumber of valid observations\n\n\n10 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more).\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\nExample of requesting mosaic over Rome with Processing API request\nThe request below is written in python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 }\n  };\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04/10000, 2.5 * sample.B03/10000, 2.5 * sample.B02/10000];\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n        12.44693,\n        41.870072,\n        12.541001,\n        41.917096\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-01-01T00:00:00Z\",\n            \"to\": \"2023-01-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-5460de54-082e-473a-b6ea-d5cbe3c17cca\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 780,\n    \"height\": 523,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel2.html#sentinel-2-precise-orbit-determination-pod-products",
    "href": "Data/SentinelMissions/Sentinel2.html#sentinel-2-precise-orbit-determination-pod-products",
    "title": "Sentinel-2",
    "section": "Sentinel-2 Precise Orbit Determination (POD) products",
    "text": "Sentinel-2 Precise Orbit Determination (POD) products\n\nOverview\nThe Copernicus POD Service for SENTINEL-2 provides Precise Orbital products with NRT timeliness, including two product types. Near Real-Time (NRT) orbital products are created immediately using real-time GPS data from EGP, while Near Real Time Predicted (PRE) products are calculated in advance of specific astronomical events, such as ascending node crossings.\n\nOffered Data\n\n\n\n\n\n\nProduct ID\n\n\nContent\n\n\nEOF\n\n\nTGZ\n\n\nCatalog API\n\n\nS3 Path\n\n\n\n\n\n\nAUX_GNSSRD\n\n\nRINEX\n\n\n\n\nX\n\n\nOData\n\n\n/eodata/Sentinel-2/AUX/AUX_GNSSRD/\n\n\n\n\nAUX_PROQUA\n\n\nQuaternions\n\n\n\n\nX\n\n\nOData\n\n\n/eodata/Sentinel-2/AUX/AUX_PROQUA/\n\n\n\n\nAUX_POEORB*\n\n\nOrbit\n\n\nX\n\n\n\n\nOData\n\n\n/eodata/Sentinel-2/AUX/AUX_POEORB/\n\n\n\n\n\n*S-2 AUX_POEORB currently not available. The backlog will be generated by CPOD, and made available on CPODIP, but it will require coordination with ESA/Copernicus Data Space Ecosystem."
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel6.html",
    "href": "Data/SentinelMissions/Sentinel6.html",
    "title": "Sentinel-6",
    "section": "",
    "text": "Copernicus Sentinel-6 Michael Freilich includes two satellites that will fly sequentially, launched in 2020 and 2025, carrying a state-of-the art optimized payload.\nCopernicus Sentinel-6 Michael Freilich is an Earth Observation satellite mission developed to provide enhanced continuity to the very stable time series of mean sea level measurements and ocean sea state that started in 1992, with the TOPEX/Poseidon mission, then continued by the Jason-1, Jason-2 and Jason-3 satellite missions."
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel6.html#sentinel-6-precise-orbit-determination-pod-products",
    "href": "Data/SentinelMissions/Sentinel6.html#sentinel-6-precise-orbit-determination-pod-products",
    "title": "Sentinel-6",
    "section": "Sentinel-6 Precise Orbit Determination (POD) products",
    "text": "Sentinel-6 Precise Orbit Determination (POD) products\n\nOverview\nThe Copernicus Sentinel-6 satellites feature three main scientific instruments: a Ku/C-band Synthetic Aperture Radar (SAR) altimeter known as Poseidon-4, a multi-frequency Advanced Microwave Radiometer for Climate (AMR-C) with an experimental High-Resolution Microwave Radiometer (HRMR), and a suite for Precise Orbit Determination (POD) incorporating Global Navigation Satellite System (GNSS) receivers, a Laser Retroreflector Array (LRA), and a Doppler Orbitography Radio-positioning Integrated by Satellite (DORIS) system. Additionally, there are secondary instruments: a Global Navigation Satellite System Radio Occultation (GNSS-RO) device for atmospheric vertical profile data, and a Radiation Environment Monitor (REM) sensor for in-situ measurement of proton and electron fluxes in the challenging space radiation environment of low-earth orbit.\n\nOffered Data\n\n\n\n\n\n\nProduct ID\n\n\nContent\n\n\nTGZ\n\n\ntar\n\n\nsp3\n\n\nRolling Policy\n\n\nCatalog API\n\n\nS3 Path\n\n\n\n\n\n\nAX____ROE__AX\n\n\nOrbit\n\n\n\n\nX\n\n\n\n\n1 month\n\n\nOData\n\n\n/eodata/Sentinel-6/AUX/AX____ROE__AX/\n\n\n\n\nAUX_GNSSRD\n\n\nRINEX\n\n\nX\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-6/AUX/AUX_GNSSRD/\n\n\n\n\nAUX_PROQUA\n\n\nQuaternions\n\n\nX\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-6/AUX/AUX_PROQUA/\n\n\n\n\nAX____MOED_AX\n\n\nOrbit\n\n\n\n\nX\n\n\n\n\n1 month\n\n\nOData\n\n\n/eodata/Sentinel-6/AUX/AX____MOED_AX/\n\n\n\n\nAX____POE__AX\n\n\nOrbit\n\n\n\n\nX\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-6/AUX/AX____POE__AX/\n\n\n\n\nAUX_COMB\n\n\nOrbit\n\n\n\n\n\n\nX\n\n\n\n\nOData\n\n\n/eodata/Sentinel-6/AUX/AUX_COMB__/"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html",
    "href": "Data/SentinelMissions/Sentinel5P.html",
    "title": "Sentinel-5P",
    "section": "",
    "text": "The Copernicus Sentinel-5 Precursor mission is the first Copernicus mission dedicated to monitoring our atmosphere.\nThe main objective of the Copernicus Sentinel-5P mission is to perform atmospheric measurements with high spatio-temporal resolution, to be used for air quality, ozone & UV radiation, and climate monitoring & forecasting.\nThere are different data products associated with the three levels of TROPOMI processing: Level-0, Level-1B and Level-2.\nLevel-1 Level-2"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-aerosol-index",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-aerosol-index",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Aerosol Index",
    "text": "Sentinel-5P Level 2 Aerosol Index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 Aerosol Index (AER_AI) dataset provides high-resolution imagery of the UV Aerosol Index (UVAI), also called the Absorbing Aerosol Index (AAI). The AAI is based on wavelength-dependent changes in Rayleigh scattering in the UV spectral range for a pair of wavelengths. The difference between observed and modelled reflectance results in the AAI. When the AAI is positive, it indicates the presence of UV-absorbing aerosols like dust and smoke. It is useful for tracking the evolution of episodic aerosol plumes from dust outbreaks, volcanic ash, and biomass burning.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-carbon-monoxide",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-carbon-monoxide",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Carbon Monoxide",
    "text": "Sentinel-5P Level 2 Carbon Monoxide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 CO data refers to processed and derived datasets obtained from the Sentinel-5P satellite mission, specifically focusing on measuring and analyzing the concentration of carbon monoxide in the Earth’s atmosphere. It includes data on the total column carbon monoxide content, as well as vertical profiles that describe how the concentration changes with altitude.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-cloud",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-cloud",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Cloud",
    "text": "Sentinel-5P Level 2 Cloud\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 Cloud dataset provides high-resolution imagery of cloud parameters. The TROPOMI/S5P cloud properties retrieval is based on the OCRA and ROCINN algorithms currently being used in the operational GOME and GOME-2 products. OCRA retrieves the cloud fraction using measurements in the UV/VIS spectral regions and ROCINN retrieves the cloud height (pressure) and optical thickness (albedo) using measurements in and around the oxygen A-band at 760 nm. Additionally, the cloud parameters are also provided for a cloud model which assumes the cloud to be a Lambertian reflecting boundary.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-formaldehyde",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-formaldehyde",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Formaldehyde",
    "text": "Sentinel-5P Level 2 Formaldehyde\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 HCHO data refers to processed and derived datasets obtained from the Sentinel-5P satellite mission that focus on measuring and analyzing the concentration of formaldehyde in the Earth’s atmosphere. The Level 2 Formaldehyde data also incorporates auxiliary information, such as geolocation, cloud properties, and surface reflectance, which are crucial for contextualizing and interpreting the measurements.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-methane",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-methane",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Methane",
    "text": "Sentinel-5P Level 2 Methane\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 CH4 data from the Copernicus Sentinel-5P satellite shows the methane concentrations globally. This product provides processed and derived measurements of methane concentrations in the Earth’s atmosphere. It is a valuable resource for studying climate change, understanding methane emissions, and informing environmental policies and mitigation efforts.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-nitrogen-dioxide",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-nitrogen-dioxide",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Nitrogen Dioxide",
    "text": "Sentinel-5P Level 2 Nitrogen Dioxide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 NO2 data comes from the Copernicus Sentinel-5P satellite and shows the nitrogen dioxide concentrations across the globe. Concentrations of short-lived pollutants, such as nitrogen dioxide, are indicators of changes in economic slowdowns and are comparable to changes in emissions.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-ozone",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-ozone",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Ozone",
    "text": "Sentinel-5P Level 2 Ozone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 O3 data refers to processed and derived datasets obtained from the Sentinel-5P satellite mission that focuses on measuring and analyzing the concentration and distribution of ozone in the Earth’s atmosphere. Researchers and scientists utilize this data for various purposes, that includes monitoring and assessing ozone depletion, particularly in regions like the polar areas, where the ozone layer is crucial. Additionally, the data aids in air quality monitoring, enabling the evaluation of ozone pollution control measures and understanding of pollution sources.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-sulfur-dioxide",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-2-sulfur-dioxide",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 2 Sulfur Dioxide",
    "text": "Sentinel-5P Level 2 Sulfur Dioxide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 2 SO2 data refers to processed and derived datasets obtained from the Sentinel-5P satellite mission that focuses on measuring and analyzing the concentration and distribution of sulfur dioxide in the Earth’s atmosphere. It provides comprehensive information on atmospheric sulfur dioxide’s vertical distribution and spatial variations. It includes data on the total column sulfur dioxide content and vertical profiles that describe how the concentration changes with altitude. This data also incorporates auxiliary information, such as geolocation, cloud properties, and surface reflectance, which are crucial for contextualising and interpreting the measurements. It is a valuable resource for studying air quality, volcanic activity, atmospheric chemistry, and assessing the impacts of sulfur dioxide on human health and the environment.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-1b",
    "href": "Data/SentinelMissions/Sentinel5P.html#sentinel-5p-level-1b",
    "title": "Sentinel-5P",
    "section": "Sentinel-5P Level 1B",
    "text": "Sentinel-5P Level 1B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-5P Level 1B data refers to a processed and calibrated dataset derived from the raw measurements acquired by the Sentinel-5P satellite. This level of data undergoes initial processing steps to correct for instrument effects, atmospheric disturbances, and other artifacts.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nApr 2018 - Present\n\n\nJan 2023"
  },
  {
    "objectID": "Roadmap.html",
    "href": "Roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem is continually advancing, with its roadmap frequently revised to incorporate new improvements. Discover additional data offerings and upcoming services in the near future here."
  },
  {
    "objectID": "Roadmap.html#upcoming-improvements",
    "href": "Roadmap.html#upcoming-improvements",
    "title": "Roadmap",
    "section": "Upcoming improvements",
    "text": "Upcoming improvements"
  },
  {
    "objectID": "Home.html",
    "href": "Home.html",
    "title": "Documentation",
    "section": "",
    "text": "We’re delighted you signed up!\nNow, let’s explore our extensive documentation to gain insights into our comprehensive Earth Observation data collection and the array of data access and data processing capabilities.\nOur documentation is a living resource, continuously updated to provide you with the latest information.\nDiscover within this documentation:\n\nData: Explore large amounts of open and free Earth Observation datasets, including Sentinel Data, Copernicus Contributing Missions, Federated Datasets, and Complementary Data, with detailed information.\nAPIs: Find the perfect interface for your needs in our suite of APIs. Whether you seek catalog access, product downloads, data visualization, or processing capabilities, our offerings encompass a range of options, including S3, STAC, openEO, and Sentinel Hub APIs.\nApplications: Simplify your satellite data journey and engage with data using our user-friendly applications for searching, visualizing, modifying, and downloading data effortlessly.\nQuotas and Limitations: Know the quotas and limitations that come with your user type and plan your data download and processing pipelines accordingly.\n\nFor quick answers to common queries, check out our FAQ section. If you have any questions that remain unanswered on this portal, our Support team is here to assist you. Feel free to reach out at any time.\nWelcome aboard\nReady to get started? Explore our documentation now and unlock the full potential of the Copernicus Data Space Ecosystem.\nLet’s embark on an Earth Observation data-driven journey together!"
  },
  {
    "objectID": "Home.html#welcome-to-the-copernicus-data-space-ecosystem-documentation-portal",
    "href": "Home.html#welcome-to-the-copernicus-data-space-ecosystem-documentation-portal",
    "title": "Documentation",
    "section": "",
    "text": "We’re delighted you signed up!\nNow, let’s explore our extensive documentation to gain insights into our comprehensive Earth Observation data collection and the array of data access and data processing capabilities.\nOur documentation is a living resource, continuously updated to provide you with the latest information.\nDiscover within this documentation:\n\nData: Explore large amounts of open and free Earth Observation datasets, including Sentinel Data, Copernicus Contributing Missions, Federated Datasets, and Complementary Data, with detailed information.\nAPIs: Find the perfect interface for your needs in our suite of APIs. Whether you seek catalog access, product downloads, data visualization, or processing capabilities, our offerings encompass a range of options, including S3, STAC, openEO, and Sentinel Hub APIs.\nApplications: Simplify your satellite data journey and engage with data using our user-friendly applications for searching, visualizing, modifying, and downloading data effortlessly.\nQuotas and Limitations: Know the quotas and limitations that come with your user type and plan your data download and processing pipelines accordingly.\n\nFor quick answers to common queries, check out our FAQ section. If you have any questions that remain unanswered on this portal, our Support team is here to assist you. Feel free to reach out at any time.\nWelcome aboard\nReady to get started? Explore our documentation now and unlock the full potential of the Copernicus Data Space Ecosystem.\nLet’s embark on an Earth Observation data-driven journey together!"
  },
  {
    "objectID": "APIs.html",
    "href": "APIs.html",
    "title": "APIs",
    "section": "",
    "text": "This section gives an overview on the APIs provided by Copernicus Data Space Ecosystem."
  },
  {
    "objectID": "APIs.html#catalog-apis",
    "href": "APIs.html#catalog-apis",
    "title": "APIs",
    "section": "Catalog APIs",
    "text": "Catalog APIs\nThere are various interfaces providing capability to search the catalog, to serve various users’ needs and to ensure continuity over the existing Copernicus Hubs. All interfaces are connected to the same database to guarantee consistency.\n\n\n\n\n\n\n\nOData\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenSearch Catalog web service\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTAC product catalogue\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nCatalog APIs news\nThis section provides detailed information about upcoming changes and the changelog of the implemented updates. The releases include all Catalog APIs interfaces. To avoid disruption to your scripts or apps, we recommend reviewing the upcoming changes and the latest release notes described below:\n\n1.Upcoming Changes\nWe’ve put together a list of potential updates related to Catalog APIs as of September 2023. You can find a detailed comparison of these changes here.\n\n\n2. Release notes\nThe release notes document provides you with a comprehensive list of modifications made to the Catalog APIs for every release."
  },
  {
    "objectID": "APIs.html#streamlined-data-access",
    "href": "APIs.html#streamlined-data-access",
    "title": "APIs",
    "section": "Streamlined data access",
    "text": "Streamlined data access\nThe Streamlined Data Access APIs (SDA) enables users to access and retrieve Earth observation (EO) data from the Copernicus Data Space Ecosystem catalogue. These APIs also provide you with a set of tools and services to support data processing and analysiss.\n\n\n\n\n\n\n\nSentinel Hub\n\n\n\n\n\n\n\n\n\n\n\n\n\nopenEO\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "APIs.html#additionally",
    "href": "APIs.html#additionally",
    "title": "APIs",
    "section": "Additionally",
    "text": "Additionally\nAdditional in Copernicus Data Space Ecosystem, we provide you with the S3 API, Traceability API and On-Demand Production API. The S3 API is one of the main access methods for EO data. Whereas, the Traceability API allows you to verify and register traces for user level data available in the Copernicus Data Space. Similarly, the On-Demand Production API provides you with on-demand processing capability for CARD-BS, CARD-COH6/12.\n\n\n\n\n\n\n\nAccess to EO data via S3\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn-Demand Production API\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraceability Service\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html",
    "href": "Data/SentinelMissions/Sentinel1.html",
    "title": "Sentinel-1",
    "section": "",
    "text": "The Sentinel-1 radar imaging mission is composed of a constellation of two polar-orbiting satellites providing continuous all-weather, day and night imagery for Land and Maritime Monitoring. C-band synthetic aperture radar imaging has the advantage of operating at wavelengths that are not obstructed by clouds or lack of illumination and therefore can acquire data during day or night under all weather conditions.\nThe end of mission of the Sentinel-1B satellite has been declared in July 2022 On 23 December 2021, Copernicus Sentinel-1B experienced an anomaly related to the instrument electronics power supply provided by the satellite platform, leaving it unable to deliver radar data. Despite all investigations and recovery attempts, ESA and the European Commission had to announce that it is the end of the mission for Sentinel-1B. Copernicus Sentinel-1A remains fully operational. More information about the end of the mission for the Sentinel-1B satellite can be found on the webpage Mission ends for Copernicus Sentinel-1B satellite. In response to the loss of Sentinel-1B, the mission observation scenario of Sentinel-1A was adjusted, affecting the nominal global coverage frequency. An up-to-date overview of the observation scenario in place can be consulted on the webpage Sentinel-1 Observation Scenario. Some regions are currently not observed by Sentinel-1. Nevertheless, the regions that are still observed, now have a repeat cycle of 12 days under a one-satellite constellation scenario, which affects possible interferometric analyses.\nSentinel data products are made available systematically and free of charge to all data users including the general public, scientific and commercial users. These data products are available in single polarisation for Wave mode and dual polarisation or single polarisation for SM, IW and EW modes.\nLevel-0 Level-1 Level-2 Level-3"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-1-ground-range-detected-grd",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-1-ground-range-detected-grd",
    "title": "Sentinel-1",
    "section": "Sentinel-1 Level 1 Ground Range Detected (GRD)",
    "text": "Sentinel-1 Level 1 Ground Range Detected (GRD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel 1 Level 1 GRD products in this Collection consist of focused SAR data that has been detected, multi-looked and projected to ground range using the Earth ellipsoid model WGS84. The ellipsoid projection of the GRD products is corrected using the terrain height specified in the product general annotation. The terrain height used varies in azimuth but is constant in range (but can be different for each IW/EW sub-swath). Ground range coordinates are the slant range coordinates projected onto the ellipsoid of the Earth. Pixel values represent detected amplitude. Phase information is lost. The resulting product has approximately square resolution pixels and square pixel spacing with reduced speckle at a cost of reduced spatial resolution.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\n(*) Packed or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nOct 2014 - Present\n\n\nJan 2023\n\n\n\n\n(**) Packed or Unpacked, SAFE with Cloud optimized GeoTIFF\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nOct 2014 - Present\n\n\nJul 2023\n\n\n\n\n(***) Packed, original SAFE\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJul 2023\n\n\n\n\n\n(*) Packed means data are available in the original bundling (e.g. compressed zipping) (**) Conversion of Sentinel-1 GRD products to the SAFE with Cloud Optimized GeoTIFF (COG_SAFE) format was performed in June 2023. The newest products are converted and available first, and older products will be added gradually until the entire archive is converted. Please refer to Handling Sentinel-1 COG_SAFE products for more information regarding how COG_SAFE is created and how to search for such products. (***) COG_SAFE products will be available immediately (IAD). In case original Sentinel-1 GRD products would be needed with immediate access, users can convert COG_SAFE products to the original SAFE products using COG2GRD tool.\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2014-10-03T00:00:00Z’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\n\n\n\n\nVH\n\n\n\n\nVV\n\n\n\n\nHH\n\n\n\n\nHV\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSTAC: https://stac-extensions.github.io/datacube/v1.0.0/schema.json"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-1-single-look-complex-slc",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-1-single-look-complex-slc",
    "title": "Sentinel-1",
    "section": "Sentinel-1 Level 1 Single Look Complex (SLC)",
    "text": "Sentinel-1 Level 1 Single Look Complex (SLC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel 1 Level 1 SLC products are images in the slant range by azimuth imaging plane, in the image plane of satellite data acquisition. Each image pixel is represented by a complex (I and Q) magnitude value and therefore contains both amplitude and phase information. Each I and Q value is 16 bits per pixel. The processing for all SLC products results in a single look in each dimension using the full available signal bandwidth. The imagery is geo-referenced using orbit and attitude data from the satellite.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nOct 2014 - Present\n\n\nJan 2023\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nExcept Europe (RoW)\n\n\nFeb 2021 - Present\n\n\nJan 2023\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nOct 2014 - Present\n\n\nJul 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2014-10-03T00:00:00Z’, None]\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSTAC: https://stac-extensions.github.io/datacube/v2.2.0/schema.json"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-2-ocean-ocn",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-2-ocean-ocn",
    "title": "Sentinel-1",
    "section": "Sentinel-1 Level 2 Ocean (OCN)",
    "text": "Sentinel-1 Level 2 Ocean (OCN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-1 Level 2 OCN (Ocean) products are specifically processed radar data products for oceanographic applications. These products are derived from Sentinel-1 SAR data. They are tailored to meet the needs of oceanographic studies, such as monitoring sea surface conditions, detecting oil spills, tracking marine vessels, and studying ocean currents. The OCN products typically involve specialized processing techniques to extract relevant oceanographic information from the radar data. This can include surface wave analysis, wind speed and direction estimation, ocean surface current mapping, and identifying features such as oil slicks or marine traffic.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nDec 2014 - Present\n\n\nJan 2023\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nDec 2014 - Present\n\n\nJul 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-0",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-0",
    "title": "Sentinel-1",
    "section": "Sentinel-1 Level 0",
    "text": "Sentinel-1 Level 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-1 Level 0 products are unprocessed radar measurements obtained by the satellite’s SAR system, containing amplitude and phase information. They serve as the initial input for generating higher-level radar products with calibrated and corrected data.\n\nOffered Data\n\n\n\n\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nJan 2021 - Present\n\n\nJan 2023\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nOct 2014 - Present\n\n\nJul 2023\n\n\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nExcept Europe (RoW)\n\n\nLast one year\n\n\nJul 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-precise-orbit-determination-pod-products",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-precise-orbit-determination-pod-products",
    "title": "Sentinel-1",
    "section": "Sentinel-1 Precise Orbit Determination (POD) products",
    "text": "Sentinel-1 Precise Orbit Determination (POD) products\n\nOverview\nThe Precise Orbital products and auxiliary data from Copernicus POD for Sentinel-1 fall into three categories based on timeliness. Near Real-Time (NRT) products are created immediately using GPS L0 data and EGP’s near real-time GPS orbits and clocks. Near Real-Time Predicted (PRE) products are computed in advance of astronomical events, like ascending node crossings. Non Time Critical (NTC) products are generated after several days, incorporating highly accurate inputs like GPS orbits and clocks from IGS.\n\nOffered Data\n\n\n\n\n\n\nProduct ID\n\n\nContent\n\n\nEOF\n\n\nTGZ\n\n\nRolling Policy\n\n\nCatalog API\n\n\nS3 Path\n\n\n\n\n\n\nAUX_RESORB\n\n\nOrbit\n\n\nX\n\n\n\n\n3 months\n\n\nOData\n\n\n/eodata/Sentinel-1/AUX/AUX_RESORB/\n\n\n\n\nAUX_POEORB\n\n\nOrbit\n\n\nX\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-1/AUX/AUX_POEORB/\n\n\n\n\nAUX_PREORB\n\n\nOrbit\n\n\nX\n\n\n\n\n3 months\n\n\nOData\n\n\n/eodata/Sentinel-1/AUX/AUX_PREORB/\n\n\n\n\nAUX_GNSSRD\n\n\nRINEX\n\n\n\n\nX\n\n\n\n\nOData\n\n\n/eodata/Sentinel-1/AUX/AUX_GNSSRD/\n\n\n\n\nAUX_PROQUA\n\n\nQuaternions\n\n\n\n\nX\n\n\n\n\nOData\n\n\n/eodata/Sentinel-1/AUX/AUX_PROQUA/"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-3-monthly-mosaics",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-level-3-monthly-mosaics",
    "title": "Sentinel-1",
    "section": "Sentinel-1 Level 3 Monthly Mosaics",
    "text": "Sentinel-1 Level 3 Monthly Mosaics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nSentinel-1 monthly mosaics are generated from monthly stacks of Sentinel-1 GRD data by calculating the weighted sum of the terrain corrected backscatter observations. Two different Sentinel-1 mosaics are being produced for each month: IW mosaic and DH mosaic (more details below).\n\nOffered Data\n\n\n\n\n\n\nMosaic\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nS3 Path\n\n\n\n\n\n\nIW\n\n\nNon-polar landmasses, depends on availability of IW products\n\n\nJan - Dec 2023 *\n\n\ne.g., /eodata/Global-Mosaics/Sentinel-1/S1SAR_L3_IW_MCM/2023/01/01\n\n\n\n\nDH\n\n\nPolar regions, depends on availability of HH + HV polarised products\n\n\nJan - Dec 2023 *\n\n\ne.g., /eodata/Global-Mosaics/Sentinel-1/S1SAR_L3_DH_MCM/2023/01/01\n\n\n\n\n\nMore mosaics coming soon.\n\nAlgorithm\nInput data and preprocessing\nSentinel-1 GRD data, as offered and pre-processed in Sentinel Hub, serves as input for the generation of mosaics. The preprocessing steps made in Sentinel Hub are explained in detail in the Sentinel Hub documentation. Here, we are only listing the main processing steps applied to the input data before mosaicking:\n(1) Calibration to beta0 backscatter coefficient\n(2) Thermal noise removal\n(3) Radiometric terrain correction using area integration\n(4) Terrain Correction using Range-Doppler terrain correction\nFor steps 3 and 4, the Copernicus DEM was used with a spatial resolution of 10m over Europe and 30m for the rest of the world.\nGeneration of mosaics\nTwo different mosaics are being produced for each month: IW mosaic and DH mosaic\n\n\n\n\n\n\n\nIW mosaic\n\n\nDH mosaic\n\n\n\n\n\n\nPolarization\n\n\nVV + VH\n\n\nHH + HV\n\n\n\n\nAcquisition mode\n\n\nIW acquisition mode\n\n\nAll acquisition modes\n\n\n\n\nOrbit direction\n\n\nboth\n\n\n\n\nProcessing grid\n\n\nUTM grid with 100,08 x 100,08 km tiles (link)\n\n\n\n\nSpatial resolution\n\n\n20 m\n\n\n40 m\n\n\n\n\nOutput format\n\n\n16-bit cloud optimized GeoTIFF\n\n\n\n\n\nThe weighted sum of the flattened backscatter observations was used for mosaicking data in a monthly stack. Observations with the highest available local resolution receive the highest local weights. Therefore, the differences in backscatter between areas sloping towards the sensor and away from the sensor in individual orbits are largely corrected, and the resulting signal is mainly a product of the local surface properties.Observations in radar shadows are filtered out and are not used. The algorithm is described in detail in the paper by D. Small ‘SAR backscatter multitemporal compositing via local resolution weighting’ (pdf is available). The resulting mosaics have less noise and better spatial homogeneity when compared to each single Sentinel-1 GRD observation.\nAccess Sentinel-1 Level 3 Monthly Mosaics with Sentinel Hub\n\n\n\nAccess Sentinel-1 Level 3 Monthly Mosaics with Sentinel Hub\nSentinel-1 Level 3 Monthly Mosaics are onboarded to Sentinel Hub as a BYOC data collection. To access the data, you will need the specific pieces of information listed below, for general information about how to access BYOC collections visit our Data BYOC page.\nIW Mosaics\n\nData collection id: byoc-3c662330-108b-4378-8899-525fd5a225cb\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nVV\n\n\nVV polarization\n\n\n20 m\n\n\n\n\nVH\n\n\nVH polarization\n\n\n20 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more)\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\nExample of requesting mosaic over Sfântu Gheorghe with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\", \"VH\", \"dataMask\"],\n    output: { bands: 3 }\n  };\n}\n\nvar viz = new HighlightCompressVisualizer(0, 0.8);\nvar gain = 0.8;\n\n\nfunction evaluatePixel(sample) {\n  if (sample.dataMask == 0) {\n    return [0, 0, 0];\n  }\n  \n  let vals = [gain * sample.VV / 0.28,\n              gain * sample.VH / 0.06,\n              gain * sample.VH / sample.VV / 0.49];\n  \n  return viz.processList(vals);\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n          25.713501,\n          45.74836,\n          26.196213,\n          45.965231\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-09-01T00:00:00Z\",\n            \"to\": \"2023-09-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-3c662330-108b-4378-8899-525fd5a225cb\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 512,\n    \"height\": 330,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\nDH Mosaics\n\nData collection id: byoc-cc676fec-cb8d-4bc1-adce-1d9658da950b\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nHH\n\n\nHH polarization\n\n\n40 m\n\n\n\n\nHV\n\n\nHV polarization\n\n\n40 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more)\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\n\nExample of requesting mosaic over Reykjavík with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"HH\", \"HV\", \"dataMask\"],\n    output: { bands: 4 }\n  };\n}\n\nvar viz = new HighlightCompressVisualizer(0, 0.8);\nvar gain = 0.8;\n\n\nfunction evaluatePixel(sample) {\n  let vals = [gain * sample.HH / 0.28,\n              gain * sample.HV / 0.06,\n              gain * sample.HV / sample.HH / 0.49];\n  \n  let out = viz.processList(vals);\n  out.push(sample.dataMask);\n  return out;\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n          -22.486267,\n          63.959085,\n          -19.79187,\n          64.722572\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-01-01T00:00:00Z\",\n            \"to\": \"2023-01-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-cc676fec-cb8d-4bc1-adce-1d9658da950b\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 858,\n    \"height\": 553,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-1-rtc",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-1-rtc",
    "title": "Sentinel-1",
    "section": "Sentinel-1 RTC",
    "text": "Sentinel-1 RTC\nSentinel-1 RTC (Radiometric Terrain Correction) SAR Backscatter is a product processed from Sentinel-1 GRD data and compliant with CEOS Analysis Ready Data for Land (CARD4L) specifications for Normalised Radar Backscatter (NRB) products. Orthorectification is based on Copernicus DEM and no speckle filtering is applied. (Additional product information)"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#sentinel-hub-processing-options",
    "href": "Data/SentinelMissions/Sentinel1.html#sentinel-hub-processing-options",
    "title": "Sentinel-1",
    "section": "Sentinel Hub processing options",
    "text": "Sentinel Hub processing options\nSentinel Hub offers the following processing options in the Sentinel-1 GRD processing chain:\n\nBackscatter coefficients:\n\nbeta0 (ellipsoid)\nsigma0 (ellipsoid)\ngamma0 (ellipsoid)\ngamma0 (terrain) → this gamma0 RTC option can only be performed if orthorectification is enabled\n\nLee Speckle Filtering applied on source data after calibration and noise removal\nRadiometric Terrain Correction (RTC) can be enabled by setting the backscatter coefficient to gamma0 (terrain) and enabling orthorectification\nOrthorectification with Range-Doppler terrain correction using one of the following DEMs:\n\nCopernicus 10m/30m DEM (10m resolution inside 39 European states including islands and 30m elsewhere.)\nCopernicus 30m DEM\nCopernicus 90m DEM"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#openeo-processing-options",
    "href": "Data/SentinelMissions/Sentinel1.html#openeo-processing-options",
    "title": "Sentinel-1",
    "section": "openEO processing options",
    "text": "openEO processing options\nWhen working with the SENTINEL1_GRD data collection through openEO, SAR backscatter computation is automatically applied. Unfortunately, the default backscatter coefficient “gamma0-terrain” is not yet supported in the openEO backend implementation of Copernicus Data Space Ecosystem, typically resulting in an error like “Backscatter coefficient ‘gamma0-terrain’ is not supported.”\nAs a workaround, it is currently recommended to explicitly specify the sar_backscatter() process with the supported coefficient “sigma0-ellipsoid”.\n\nsigma0-ellipsoid: ground area computed with ellipsoid earth model\n\nFor example:\n\nsentinel1 = connection.load_collection(\n    \"SENTINEL1_GRD\",\n    temporal_extent = [\"2022-06-04\", \"2022-08-04\"],\n    bands = [\"VV\",\"VH\"]\n)\n\n  sentinel1 = sentinel1.sar_backscatter(\n      coefficient='sigma0-ellipsoid')\nThe product is orthorectified using the Copernicus 30m DEM. No RTC or speckle filtering is applied to this product."
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel1.html#on-demand-processing-options",
    "href": "Data/SentinelMissions/Sentinel1.html#on-demand-processing-options",
    "title": "Sentinel-1",
    "section": "On-demand processing options",
    "text": "On-demand processing options\nProcessing of CARD-BS and COH6/COH12 products can be requested on demand:\n\nSentinel-1 (CARD-BS) BackScatter\n\nThis processing option contains gamma0 geometric terrain correction (orthorectification) using Copernicus 30m DEM (identical to the gamma0 (ellipsoid) backscatter coefficient with enabled orthorectification option in Sentinel Hub processing options.) No RTC or speckle filtering is applied to this product.\nAdditional information\n\nSentinel-1 (CARD-COH) Coherence\n\nThe Sentinel-1 CARD COH (Copernicus Analysis Ready Data Coherence) processor generates a Sentinel-1 Level 2 product describing the coherence of a pair of images - 12 days apart. The product is orthorectified using Copernicus 30m DEM but no RTC or speckle filtering is applied.\nAdditional information."
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html",
    "href": "Data/SentinelMissions/Sentinel3.html",
    "title": "Sentinel-3",
    "section": "",
    "text": "The main objective of the Copernicus Sentinel-3 mission is to measure ocean and land surface colour, sea and land surface temperature, and sea surface topography with high accuracy and reliability to support ocean forecasting systems, environmental monitoring and climate monitoring. The mission definition is driven by the need for continuity in provision of ERS, ENVISAT and SPOT vegetation data, with improvements in instrument performance and coverage.\nLevel-1 Level-2"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-olci-level-1",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-olci-level-1",
    "title": "Sentinel-3",
    "section": "Sentinel-3 OLCI Level 1",
    "text": "Sentinel-3 OLCI Level 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-3 OLCI Level 1 products provides calibrated, geolocated, and orthorectified data from the Ocean and Land Colour Instrument (OLCI). These products are delivered not later than 1 month (commitment) after acquisition or from long-term archives.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJan 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2016-04-17T11:33:13Z’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\nCommon Name\n\n\nGSD(m)\n\n\nCenter Wavelength(μm)\n\n\n\n\n\n\nOa01\n\n\nAerosol correction\n\n\n300\n\n\n0.4000\n\n\n\n\nOa02\n\n\nYellow substance and detrital pigments (turbidity)\n\n\n300\n\n\n0.4125\n\n\n\n\nOa03\n\n\nChlorophyll absorption maximum\n\n\n300\n\n\n0.4425\n\n\n\n\nOa04\n\n\nChlorophyll\n\n\n300\n\n\n0.4900\n\n\n\n\nOa05\n\n\nChlorophyll\n\n\n300\n\n\n0.5100\n\n\n\n\nOa06\n\n\nChlorophyll reference (minimum)\n\n\n300\n\n\n0.5600\n\n\n\n\nOa07\n\n\nSediment loading\n\n\n300\n\n\n0.6200\n\n\n\n\nOa08\n\n\n2nd Chlorophyll absorption maximum\n\n\n300\n\n\n0.6650\n\n\n\n\nOa09\n\n\nImproved fluorescence retrieval\n\n\n300\n\n\n0.6737\n\n\n\n\nOa010\n\n\nChlorophyll fluorescence peak\n\n\n300\n\n\n0.6813\n\n\n\n\nOa11\n\n\nChlorophyll fluorescence baseline\n\n\n300\n\n\n0.7087\n\n\n\n\nOa12\n\n\nO2 absorption / clouds\n\n\n300\n\n\n0.7538\n\n\n\n\nOa16\n\n\nAtmospheric / aerosol correction\n\n\n300\n\n\n0.7788\n\n\n\n\nOa17\n\n\nAtmospheric / aerosol correction\n\n\n300\n\n\n0.8650\n\n\n\n\nOa18\n\n\nWater vapour absorption\n\n\n300\n\n\n0.8850\n\n\n\n\nOa19\n\n\nWater vapour absorption\n\n\n300\n\n\n0.9000\n\n\n\n\nOa21\n\n\nWater vapour absorption\n\n\n300\n\n\n1.0200"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-olci-level-2",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-olci-level-2",
    "title": "Sentinel-3",
    "section": "Sentinel-3 OLCI Level 2",
    "text": "Sentinel-3 OLCI Level 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-3 OLCI Level-2 product provides geophysical data that is derived from the Level-1 product. The level-2 land product provides land and atmospheric geophysical parameters computed for full and Reduced Resolution. The Level-2 product also includes data quality flags that provide information on the reliability of the geophysical parameters, as well as information on the atmospheric correction applied to the data. These flags can be used to filter out data that is not of sufficient quality for a particular application.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJan 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2016-04-17T11:33:13Z’, None]\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSTAC: https://stac-extensions.github.io/datacube/v1.0.0/schema.json"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-slstr-level-1",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-slstr-level-1",
    "title": "Sentinel-3",
    "section": "Sentinel-3 SLSTR Level 1",
    "text": "Sentinel-3 SLSTR Level 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-3 SLSTR Level-1 product provides a valuable source of processed and calibrated data that is suitable for a wide range of applications. The product includes key parameters and data quality flags that provide important information on the reliability and accuracy of the data, and the product is generated offline with a delay of a few days after the acquisition of the Level-0 data.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJan 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2016-04-17T11:33:13Z’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\nCommon Name\n\n\nGSD(m)\n\n\nCenter Wavelength(μm)\n\n\n\n\n\n\nS1\n\n\nCloud screening\n\n\n500\n\n\n0.5543\n\n\n\n\nS2\n\n\nVegetation monitoring\n\n\n500\n\n\n0.6595\n\n\n\n\nS3\n\n\nNDVI, cloud flagging\n\n\n500\n\n\n0.8680\n\n\n\n\nS4\n\n\nCirrus detection over land\n\n\n500\n\n\n1.3748\n\n\n\n\nS5\n\n\nCloud clearing\n\n\n500\n\n\n1.6134\n\n\n\n\nS6\n\n\nVegetation state and cloud clearing\n\n\n500\n\n\n2.2557\n\n\n\n\nS7\n\n\nSST, LST, Active fire\n\n\n500\n\n\n3.7420\n\n\n\n\nS8\n\n\nSST, LST, Active fire\n\n\n500\n\n\n10.8540\n\n\n\n\nS9\n\n\nSST, LST\n\n\n1000\n\n\n12.0225\n\n\n\n\nF1\n\n\nActive fire\n\n\n500\n\n\n3.7420\n\n\n\n\nF2\n\n\nActive fire\n\n\n1000\n\n\n3.9400\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSTAC: https://stac-extensions.github.io/datacube/v1.0.0/schema.json"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-slstr-level-2",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-slstr-level-2",
    "title": "Sentinel-3",
    "section": "Sentinel-3 SLSTR Level 2",
    "text": "Sentinel-3 SLSTR Level 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-3 SLSTR Level-2 product provides higher-level geophysical parameters, but with a longer processing time and coarser spatial resolution compared to the Level-1 product. The product also includes additional data quality flags to provide more information on the reliability and accuracy of the data.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-syn-level-2",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-syn-level-2",
    "title": "Sentinel-3",
    "section": "Sentinel-3 SYN Level 2",
    "text": "Sentinel-3 SYN Level 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-3 SYN Level 2 product is a higher-level processed product that contains information about the Earth’s atmosphere and its constituents. It is derived from the Level-1 and Level-2 products of the OLCI and SLSTR instruments on board the Sentinel-3 satellite.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nShort Time Critical (STC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\nCopernicus Sentinel data 2023  \n\n\n\n\n\nSpatial Extent\n\n[-180, -90, 180, 90]\n\n\n\nTemporal Interval\n\n[‘2016-04-17T11:33:13Z’, None]\n\n\n\n\nSpectral Bands\n\n\n\n\n\n\nBand Name\n\n\nCommon Name\n\n\nGSD(m)\n\n\nCenter Wavelength(μm)\n\n\n\n\n\n\nS1\n\n\nCloud screening\n\n\n500\n\n\n0.5543"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-sral-level-1",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-sral-level-1",
    "title": "Sentinel-3",
    "section": "Sentinel-3 SRAL Level 1",
    "text": "Sentinel-3 SRAL Level 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-3 SRAL Level-1 product provides corrected and validated geophysical parameters derived from the raw SRAL Level-0 data, along with metadata and data quality flags that enable the user to assess the reliability and suitability of the data for specific applications.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJan 2023\n\n\n\n\nShort Time Critical (STC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-sral-level-2",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-sral-level-2",
    "title": "Sentinel-3",
    "section": "Sentinel-3 SRAL Level 2",
    "text": "Sentinel-3 SRAL Level 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-3 SRAL Level-2 product is a higher-level processed product that contains more detailed and refined geophysical parameters suitable for scientific and research applications. It contains advanced geophysical parameters such as sea surface height, significant wave height, and wind speed, that are derived from the SRAL Level-1 products using advanced processing algorithms and quality control procedures.\n\nOffered Data\n\n\n\n\n\n\nTimeliness\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\nAvailable in Ecosystem from\n\n\n\n\n\n\nNon Time Critical (NTC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nMar 2016 - Present\n\n\nJan 2023\n\n\n\n\nNear Real Time (NRT)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one year\n\n\nJan 2023\n\n\n\n\nShort Time Critical (STC)\n\n\nPacked or Unpacked\n\n\nImmediately available data (IAD)\n\n\nWorld\n\n\nLast one month\n\n\nJan 2023"
  },
  {
    "objectID": "Data/SentinelMissions/Sentinel3.html#sentinel-3-precise-orbit-determination-pod-products",
    "href": "Data/SentinelMissions/Sentinel3.html#sentinel-3-precise-orbit-determination-pod-products",
    "title": "Sentinel-3",
    "section": "Sentinel-3 Precise Orbit Determination (POD) products",
    "text": "Sentinel-3 Precise Orbit Determination (POD) products\n\nOverview\nThe Copernicus POD Service for the Sentinel-3 mission categorizes Precise Orbital products into three types based on timeliness. Near Real-Time (NRT) products are generated immediately using real-time GPS data. Short Time Critical (STC) products use data delivered by EGP with a 1-day timeliness. Non Time Critical (NTC) products are computed after several days, incorporating precise inputs like GPS data from CODE, with current ambiguity resolution.\n\nOffered Data\n\n\n\n\n\n\nProduct ID\n\n\nContent\n\n\nEOF\n\n\nTGZ\n\n\nzip\n\n\nsp3\n\n\nRolling Policy\n\n\nCatalog API\n\n\nS3 Path\n\n\n\n\n\n\nSR___ROE_AX\n\n\nOrbit\n\n\n\n\n\n\nX\n\n\n\n\n1 month\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/SR___ROE_AX/\n\n\n\n\nAUX_MOEORB\n\n\nOrbit\n\n\nX\n\n\n\n\n\n\n\n\n1 month\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/AUX_MOEORB/\n\n\n\n\nAUX_POEORB\n\n\nOrbit\n\n\nX\n\n\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/AUX_POEORB/\n\n\n\n\nAUX_PRCPTF\n\n\nPlatform\n\n\nX\n\n\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/AUX_PRCPTF/\n\n\n\n\nAUX_GNSSRD\n\n\nRINEX\n\n\n\n\nX\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/AUX_GNSSRD/\n\n\n\n\nAUX_PROQUA\n\n\nQuaternions\n\n\n\n\nX\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/AUX_PROQUA/\n\n\n\n\nSR___MDO_AX\n\n\nOrbit\n\n\n\n\nX\n\n\n\n\n\n\n1 month\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/SR___MDO_AX/\n\n\n\n\nSR___POE_AX\n\n\nOrbit\n\n\nX\n\n\n\n\n\n\n\n\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/SR___POE_AX/\n\n\n\n\nAUX_COMB\n\n\nOrbit\n\n\n\n\n\n\n\n\nX\n\n\n\n\nOData\n\n\n/eodata/Sentinel-3/AUX/AUX_COMB__/"
  },
  {
    "objectID": "Data/Others/Sentinel1_Mosaic_access.html",
    "href": "Data/Others/Sentinel1_Mosaic_access.html",
    "title": "Access Sentinel-1 Level 3 Monthly Mosaics with Sentinel Hub",
    "section": "",
    "text": "Sentinel-1 Level 3 Monthly Mosaics are onboarded to Sentinel Hub as a BYOC data collection. To access the data, you will need the specific pieces of information listed below, for general information about how to access BYOC collections visit our Data BYOC page.\nIW Mosaics\n\nData collection id: byoc-3c662330-108b-4378-8899-525fd5a225cb\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nVV\n\n\nVV polarization\n\n\n20 m\n\n\n\n\nVH\n\n\nVH polarization\n\n\n20 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more)\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\nExample of requesting mosaic over Sfântu Gheorghe with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\", \"VH\", \"dataMask\"],\n    output: { bands: 3 }\n  };\n}\n\nvar viz = new HighlightCompressVisualizer(0, 0.8);\nvar gain = 0.8;\n\n\nfunction evaluatePixel(sample) {\n  if (sample.dataMask == 0) {\n    return [0, 0, 0];\n  }\n  \n  let vals = [gain * sample.VV / 0.28,\n              gain * sample.VH / 0.06,\n              gain * sample.VH / sample.VV / 0.49];\n  \n  return viz.processList(vals);\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n          25.713501,\n          45.74836,\n          26.196213,\n          45.965231\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-09-01T00:00:00Z\",\n            \"to\": \"2023-09-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-3c662330-108b-4378-8899-525fd5a225cb\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 512,\n    \"height\": 330,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\nDH Mosaics\n\nData collection id: byoc-cc676fec-cb8d-4bc1-adce-1d9658da950b\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nHH\n\n\nHH polarization\n\n\n40 m\n\n\n\n\nHV\n\n\nHV polarization\n\n\n40 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more)\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\n\nExample of requesting mosaic over Reykjavík with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"HH\", \"HV\", \"dataMask\"],\n    output: { bands: 4 }\n  };\n}\n\nvar viz = new HighlightCompressVisualizer(0, 0.8);\nvar gain = 0.8;\n\n\nfunction evaluatePixel(sample) {\n  let vals = [gain * sample.HH / 0.28,\n              gain * sample.HV / 0.06,\n              gain * sample.HV / sample.HH / 0.49];\n  \n  let out = viz.processList(vals);\n  out.push(sample.dataMask);\n  return out;\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n          -22.486267,\n          63.959085,\n          -19.79187,\n          64.722572\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-01-01T00:00:00Z\",\n            \"to\": \"2023-01-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-cc676fec-cb8d-4bc1-adce-1d9658da950b\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 858,\n    \"height\": 553,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "Data/Others/Sentinel1_COG.html",
    "href": "Data/Others/Sentinel1_COG.html",
    "title": "Handling Sentinel-1 COG_SAFE products",
    "section": "",
    "text": "The Sentinel-1 GRD COG_SAFE products can be filtered by the OData API query using three methods:\n\nFiltering ‘COG.SAFE’ substring in the product name:\n\nExample of a query:\n\nHTTP Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(name,%27COG%27)%20and%20ContentDate/Start%20gt%202022-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-21T00:00:00.000Z\n\n\n\n\nUsing proper data type with “-COG” suffix. One of: S1_GRDF_1S-COG,S2_GRDF_1S-COG,S3_GRDF_1S-COG,S4_GRDF_1S-COG,S5_GRDF_1S-COG,S6_GRDF_1S-COG,S1_GRDH_1S-COG,S2_GRDH_1S-COG,S3_GRDH_1S-COG,S4_GRDH_1S-COG,S5_GRDH_1S-COG,S6_GRDH_1S-COG,S1_GRDM_1S-COG,S2_GRDM_1S-COG,S3_GRDM_1S-COG,S4_GRDM_1S-COG,S5_GRDM_1S-COG,S6_GRDM_1S-COG,IW_GRDH_1S-COG,IW_GRDM_1S-COG,EW_GRDH_1S-COG,EW_GRDM_1S-COG,WV_GRDM_1S-COG\n\nExample of a query:\n\nHTTP Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27productType%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27IW_GRDH_1S-COG%27)%20and%20ContentDate/Start%20gt%202022-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-21T04:00:00.000Z&$top=10\n\n\n\n\nFiltering ‘GRD’ substring in product name and “origin” attribute equal “CLOUDFERRO”.\n\nExample of a query:\n\nHTTP Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(name,%27GRD%27)%20and%20Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27origin%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27CLOUDFERRO%27)%20and%20ContentDate/Start%20gt%202022-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-03T04:00:00.000Z&$top=10"
  },
  {
    "objectID": "Data/Others/Sentinel1_COG.html#how-to-search-for-cog_safe-products-with-odata-api",
    "href": "Data/Others/Sentinel1_COG.html#how-to-search-for-cog_safe-products-with-odata-api",
    "title": "Handling Sentinel-1 COG_SAFE products",
    "section": "",
    "text": "The Sentinel-1 GRD COG_SAFE products can be filtered by the OData API query using three methods:\n\nFiltering ‘COG.SAFE’ substring in the product name:\n\nExample of a query:\n\nHTTP Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(name,%27COG%27)%20and%20ContentDate/Start%20gt%202022-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-21T00:00:00.000Z\n\n\n\n\nUsing proper data type with “-COG” suffix. One of: S1_GRDF_1S-COG,S2_GRDF_1S-COG,S3_GRDF_1S-COG,S4_GRDF_1S-COG,S5_GRDF_1S-COG,S6_GRDF_1S-COG,S1_GRDH_1S-COG,S2_GRDH_1S-COG,S3_GRDH_1S-COG,S4_GRDH_1S-COG,S5_GRDH_1S-COG,S6_GRDH_1S-COG,S1_GRDM_1S-COG,S2_GRDM_1S-COG,S3_GRDM_1S-COG,S4_GRDM_1S-COG,S5_GRDM_1S-COG,S6_GRDM_1S-COG,IW_GRDH_1S-COG,IW_GRDM_1S-COG,EW_GRDH_1S-COG,EW_GRDM_1S-COG,WV_GRDM_1S-COG\n\nExample of a query:\n\nHTTP Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27productType%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27IW_GRDH_1S-COG%27)%20and%20ContentDate/Start%20gt%202022-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-21T04:00:00.000Z&$top=10\n\n\n\n\nFiltering ‘GRD’ substring in product name and “origin” attribute equal “CLOUDFERRO”.\n\nExample of a query:\n\nHTTP Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(name,%27GRD%27)%20and%20Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27origin%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27CLOUDFERRO%27)%20and%20ContentDate/Start%20gt%202022-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-03T04:00:00.000Z&$top=10"
  },
  {
    "objectID": "Data/Others/Sentinel1_COG.html#how-to-search-for-cog_safe-products-in-the-browser",
    "href": "Data/Others/Sentinel1_COG.html#how-to-search-for-cog_safe-products-in-the-browser",
    "title": "Handling Sentinel-1 COG_SAFE products",
    "section": "How to search for COG_SAFE products in the Browser?",
    "text": "How to search for COG_SAFE products in the Browser?\nThere are two separate options available for Sentinel-1 GRD products. Selecting the “Level-1 GRD COG” option under Sentinel-1 will return COG_SAFE products while option “Level-1 GRD” will return the original GRD products. If you would like to search for both type of products, select both options."
  },
  {
    "objectID": "Data/Others/Sentinel1_COG.html#how-were-original-sentinel-1-grd-products-converted-to-cog_safe-products",
    "href": "Data/Others/Sentinel1_COG.html#how-were-original-sentinel-1-grd-products-converted-to-cog_safe-products",
    "title": "Handling Sentinel-1 COG_SAFE products",
    "section": "How were original Sentinel-1 GRD products converted to COG_SAFE products?",
    "text": "How were original Sentinel-1 GRD products converted to COG_SAFE products?\nThe following changes were made during the conversion of original Sentinel-1 GRD products to COG_SAFE products:\n\nAll GeoTIFF files available in the measurements folder were converted to cloud optimized GeoTIFF format with the gdal command:\n\n\nCLI\n\n\ngdal_translate -of COG -a_nodata 0 -co OVERVIEW_COUNT=6 -co BLOCKSIZE=1024 -co BIGTIFF=NO -co OVERVIEW_RESAMPLING=RMS -co COMPRESS=ZSTD -co NUM_THREADS=ALL_CPUS -mo GRD_ORIGINAL_HEADER_SIZE=&lt;original_header_size&gt; -mo GRD_ORIGINAL_FOOTER_SIZE=&lt;original_footer_size&gt; &lt;input_tiff&gt;.tiff &lt;input_tiff&gt;-cog.tiff \n\n\n\nMore information about what these options mean can be found in the [GDAL official documentation](https://gdal.org/programs/gdal_translate.html){target='_blank'}. Note that the output filename has a suffix “-cog”, which indicates that the files were converted to COGs.\n\nA suffix “_COG” was added to the name of the product and a new CRC code was calculated. For example, the original product\nS1A_IW_GRDH_1SDV_20230206T165050_20230206T165115_047118_05A716_53C5.safe became\nS1A_IW_GRDH_1SDV_20230206T165050_20230206T165115_047118_05A716_74F9_COG.safe.\nManifest file was adjusted so that it reflects these changes:\n\nsafe:processing element with a name=“COG Conversion” was added. It contains metadata about the conversion and includes the name of the original product under safe:resource child element.\ndataObject elements, which describe the measurements files, have updated values for “size”, “href”, “checksum”."
  },
  {
    "objectID": "Data/Others/Sentinel2_Mosaic_access.html",
    "href": "Data/Others/Sentinel2_Mosaic_access.html",
    "title": "Access Sentinel-2 Level 3 Quarterly Mosaics with Sentinel Hub",
    "section": "",
    "text": "Sentinel-2 Level 3 Quarterly Mosaics are onboarded to Sentinel Hub as a BYOC data collection. To access the data, you will need the specific pieces of information listed below, for general information about how to access BYOC collection visit our Data BYOC page.\n\nData collection id: byoc-5460de54-082e-473a-b6ea-d5cbe3c17cca\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nB02\n\n\nBlue\n\n\n10 m\n\n\n\n\nB03\n\n\nGreen\n\n\n10 m\n\n\n\n\nB04\n\n\nRed\n\n\n10 m\n\n\n\n\nB08\n\n\nNear Infrared (NIR)\n\n\n10 m\n\n\n\n\nobservations\n\n\nNumber of valid observations\n\n\n10 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more).\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\nExample of requesting mosaic over Rome with Processing API request\nThe request below is written in Python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 }\n  };\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04/10000, 2.5 * sample.B03/10000, 2.5 * sample.B02/10000];\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n        12.44693,\n        41.870072,\n        12.541001,\n        41.917096\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-01-01T00:00:00Z\",\n            \"to\": \"2023-01-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-5460de54-082e-473a-b6ea-d5cbe3c17cca\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 780,\n    \"height\": 523,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "Data/Others/CCM.html",
    "href": "Data/Others/CCM.html",
    "title": "Copernicus Contributing Missions",
    "section": "",
    "text": "This section provides a high-level overview of the Copernicus Contributing Missions (CCMs), which supplement the Sentinel missions by delivering diverse Earth observation data. For detailed information on specific instruments and missions, please refer to the documentation provided by the respective data providers.\nThe tables provided in each mission section outline the product types delivered by that mission for systematic collections, and the relevant catalogue path."
  },
  {
    "objectID": "Data/Others/CCM.html#geoeye-1",
    "href": "Data/Others/CCM.html#geoeye-1",
    "title": "Copernicus Contributing Missions",
    "section": "GeoEye-1",
    "text": "GeoEye-1\n\nOverview\nGeoEye-1, launched in 2008, is a high-resolution Earth observation satellite operated by Maxar Technologies, distributor for Europe is EUSI. This satellite is equipped with a multispectral imaging system capable of capturing images in both panchromatic and multispectral modes. The panchromatic sensor delivers a ground resolution of 0.41 meters, while the multispectral sensor provides 1.65-meter resolution across four spectral bands: blue, green, red, and near-infrared. GeoEye-1 can collect imagery over 350,000 square kilometers of Earth surface’s each day, making it ideal for large-scale mapping and surveillance applications. It operates in a sun-synchronous orbit at an altitude of 681 kilometers, ensuring consistent lighting conditions for imaging. GeoEye-1’s high spatial resolution and large area coverage capabilities have made it a valuable asset for applications ranging from agriculture and forestry to urban planning and disaster response. For detailed specifications, visit the Maxar website and download the GeoEye-1 datasheet.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nGIS_MS4_OR\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_2015/GIS_MS4_OR*/\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR_IMAGE_2021/GIS_MS4_OR*/\n\n\n\n\nGIS_MS4_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_2015/GIS_MS4_SO*/\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR_IMAGE_2021/GIS_MS4_SO*/\n\n\n\n\nGIS_PM4_OR\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_2015/GIS_PM4_OR*/\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR_IMAGE_2021/GIS_PM4_OR*/\n\n\n\n\nGIS_PM4_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_2015/GIS_PM4_SO*/\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR_IMAGE_2021/GIS_ PM4_SO*/"
  },
  {
    "objectID": "Data/Others/CCM.html#geosat-1",
    "href": "Data/Others/CCM.html#geosat-1",
    "title": "Copernicus Contributing Missions",
    "section": "GeoSat-1",
    "text": "GeoSat-1\n\nOverview\nOriginally launched as Deimos-1 in 2009, Geosat-1 is a medium-resolution Earth observation satellite designed for wide-area monitoring. It carries a multispectral imager with a spatial resolution of 22 meters and a swath width of 600 kilometers, allowing for extensive area coverage in a single pass. The satellite captures imagery in three spectral bands: green, red, and near-infrared, which are particularly useful for applications such as vegetation monitoring, land use classification, and environmental management. Operating in a sun-synchronous orbit at an altitude of 686 kilometers, Geosat-1 revisits the same location every two to three days, providing regular updates that are crucial for time-sensitive monitoring tasks. The satellite’s ability to cover vast areas quickly makes it an important tool for large-scale agricultural assessments and natural resource management. Further details about the mission can be found on the GEOSAT website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nSL6_22P_1R\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/DWH_MG2_CORE_09/SL6_22P_1R*\n\n\n\n\n\n\n\n\n/eodata/CCM/EUR_HR2_MULTITEMP/SL6_22P_1R*\n\n\n\n\nSL6_22P_1T\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2_CORE_09/SL6_22P_1T*\n\n\n\n\n\n\n\n\n/eodata/CCM/EUR_HR2_MULTITEMP/SL6_22P_1T*\n\n\n\n\nSL6_22S_1R\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2_CORE_09/SL6_22S_1R*\n\n\n\n\n\n\n\n\n/eodata/CCM/EUR_HR2_MULTITEMP/SL6_22S_1R*\n\n\n\n\nSL6_22S_1T\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2_CORE_09/SL6_22S_1T*\n\n\n\n\n\n\n\n\n/eodata/CCM/EUR_HR2_MULTITEMP/SL6_22S_1T*"
  },
  {
    "objectID": "Data/Others/CCM.html#geosat-2",
    "href": "Data/Others/CCM.html#geosat-2",
    "title": "Copernicus Contributing Missions",
    "section": "GeoSat-2",
    "text": "GeoSat-2\n\nOverview\nLaunched in 2014, Geosat-2 (formerly known as Deimos-2) is a high-resolution satellite designed for detailed Earth observation tasks. It carries an advanced multispectral imager capable of capturing sub-meter resolution images, with a panchromatic resolution of 0.75 meters and a multispectral resolution of 3 meters. Geosat-2 operates in four spectral bands: blue, green, red, and near-infrared, enabling detailed analysis of land cover and vegetation health. The satellite has a swath width of 12 kilometers, which, combined with its high resolution, allows it to cover up to 150,000 square kilometers per day with a revisit time of two days globally. Geosat-2’s agile platform enables it to perform precise targeting and rapid response imaging, making it suitable for applications such as urban planning, disaster management, and environmental monitoring. For comprehensive technical details, download the GEOSAT-2 Imagery User Guide and visit the GEOSAT website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nHRS_MS4_1B\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_20nn/HRS_MS4_1B*\n\n\n\n\nHRS_MS4_1C\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/HRS_MS4_1C*\n\n\n\n\nHRS_PM4_1B\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/HRS_PM4_1B*\n\n\n\n\nHRS_PM4_1C\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/HRS_PM4_1C*"
  },
  {
    "objectID": "Data/Others/CCM.html#kompsat-3-3a",
    "href": "Data/Others/CCM.html#kompsat-3-3a",
    "title": "Copernicus Contributing Missions",
    "section": "Kompsat 3-3A",
    "text": "Kompsat 3-3A\n\nOverview\nKOMPSAT-3 and KOMPSAT-3A (Korean Multi-purpose Satellites) are Earth observation satellites operated by the Korea Aerospace Research Institute (KARI). KOMPSAT-3, launched in 2012, is equipped with a panchromatic camera with a ground resolution of 0.7 meters and a multispectral imager with a resolution of 2.8 meters, capturing images in red, green, blue, and near-infrared bands. It supports various applications, including urban planning, agricultural monitoring, and environmental management. KOMPSAT-3A, launched in 2015, enhances the capabilities of KOMPSAT-3, offering a higher panchromatic resolution of 0.55 meters and a multispectral resolution of 2.2 meters. It also carries an infrared sensor for nighttime observation, enabling thermal imaging. The addition of the infrared sensor expands its use to areas such as disaster monitoring, land surface temperature mapping, and resource exploration. Both satellites operate in sun-synchronous orbits, ensuring consistent revisit times for regular monitoring of specific areas. The data provided by these missions play a critical role in detailed analysis and mapping tasks, supporting a wide range of civilian and governmental applications.KOMPSAT is owned by the Korean government. For more details of the mission, visit the website of SI Imaging Services (SIIS).\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nAIS_MSP_1R\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_20nn/AIS_MSP_1G*\n\n\n\n\nAIS_MSP_1G\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/AIS_MSP_1G*"
  },
  {
    "objectID": "Data/Others/CCM.html#planetscope",
    "href": "Data/Others/CCM.html#planetscope",
    "title": "Copernicus Contributing Missions",
    "section": "PlanetScope",
    "text": "PlanetScope\n\nOverview\nPlanetScope is a satellite constellation operated by Planet, consisting of hundreds of small satellites known as Doves. Launched in multiple batches from 2014 onwards, these satellites are designed to provide high-resolution, near-daily imagery of the Earth’s entire landmass. Each Dove satellite carries an optical imaging system that captures imagery at a resolution of 3 to 5 meters across four spectral bands: red, green, blue, and near-infrared. The PlanetScope constellation operates in sun-synchronous orbits, ensuring consistent imaging conditions and enabling regular revisits to the same location. The high frequency of revisits makes PlanetScope ideal for monitoring dynamic environments, such as agricultural fields, forests, and urban areas. The constellation’s “always-on” capability eliminates the need for tasking requests, providing users with continuous and up-to-date imagery. This unique capability supports a wide range of applications, including crop health monitoring, disaster response, and land use planning. For detailed specifications, refer to the Combined Imagery Product Specification available on the Planet website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nDOV_MS_L1A\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_2018/DOV_MS_L1A*\n\n\n\n\nDOV_MS_L3A\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_2018/DOV_MS_L3A*"
  },
  {
    "objectID": "Data/Others/CCM.html#pleiades",
    "href": "Data/Others/CCM.html#pleiades",
    "title": "Copernicus Contributing Missions",
    "section": "Pleiades",
    "text": "Pleiades\n\nOverview\nThe Pléiades constellation, consisting of Pléiades 1A and 1B, was launched in 2011 and 2012, respectively, and is operated by Airbus Defence and Space. These high-resolution satellites capture images with a panchromatic resolution of 0.5 meters and a multispectral resolution of 2 meters, covering blue, green, red, and near-infrared bands. Operating in the same orbit as the SPOT 6 and SPOT 7 satellites, Pléiades 1A and 1B provide a daily revisit capability, making them well-suited for applications such as detailed cartography, precision agriculture, forestry management, and disaster response. The satellites’ agility allows for rapid tasking and collection of images with high geometric accuracy. Pléiades imagery supports a wide range of uses, from mapping and land management to monitoring environmental changes and urban development. For more detailed information, download the Pléiades imagery user guide and visit the relevant section at Airbus website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nPHR_BUN_1A\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_20nn/PHR_BUN_1A*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/PHR_BUN_1A*\n\n\n\n\nPHR_BUN__3\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/PHR_BUN__3*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/PHR_BUN__3*\n\n\n\n\nPHR_MS__2A\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/PHR_MS__2A*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/PHR_MS__2A*\n\n\n\n\nPHR_MS___3\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/PHR_MS___3*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/PHR_MS___3*"
  },
  {
    "objectID": "Data/Others/CCM.html#pleiades-neo",
    "href": "Data/Others/CCM.html#pleiades-neo",
    "title": "Copernicus Contributing Missions",
    "section": "Pleiades Neo",
    "text": "Pleiades Neo\n\nOverview\nPléiades Neo is Airbus next-generation constellation, launched in 2021, providing ultra-high-resolution Earth imagery with a resolution of 0.3 meters. The constellation consists of two identical satellites phased 180° apart in the same orbit, ensuring high frequency revisit capabilities over any point on Earth. Pléiades Neo’s advanced optics and on-board processing systems enable the capture of highly detailed imagery with a swath width of 14 kilometers. The satellites are equipped with smart tasking capabilities, allowing them to quickly respond to urgent imaging requests and cover large areas efficiently. This makes Pléiades Neo particularly valuable for monitoring sensitive sites, managing natural disasters, and conducting large-scale mapping projects. The constellation is designed to guarantee continuity with the original Pléiades mission while providing enhanced performance to meet the evolving needs of Earth observation users. For comprehensive technical details, visit the Airbus website or the specific Pléiades Neo pages\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nPNE_MS2__3\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_2024/PNE_MS2__3*"
  },
  {
    "objectID": "Data/Others/CCM.html#resourcesat-1-2",
    "href": "Data/Others/CCM.html#resourcesat-1-2",
    "title": "Copernicus Contributing Missions",
    "section": "Resourcesat 1-2",
    "text": "Resourcesat 1-2\n\nOverview\nThe Resourcesat series, developed by the Indian Space Research Organisation (ISRO), consists of two satellites: Resourcesat-1 (launched in 2003) and Resourcesat-2 (launched in 2011). These satellites are equipped with three main sensors: the Linear Imaging Self-Scanning Sensor (LISS-IV), which provides 5.8-meter resolution multispectral imagery; LISS-III, which captures imagery at 23.5-meter resolution; and the Advanced Wide Field Sensor (AWiFS), offering 56-meter resolution with a wide swath of 740 kilometers. These sensors operate in multiple spectral bands, including green, red, near-infrared, and shortwave infrared, enabling comprehensive monitoring of land and water resources. The satellites operate in sun-synchronous orbits, allowing for regular and consistent observations that are crucial for long-term environmental monitoring and agricultural assessments. For more detailed technical information, consult the key parameters of the IRS-R2 sensors. Visit the ISRO website for an overview of the mission.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nAWF_XA__1O\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/AWF_XA__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/AWF_XA__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/AWF_XA__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/AWF_XA__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/AWF_XA__1O*\n\n\n\n\nAWF_XA__3O\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/AWF_XA__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/AWF_XA__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/AWF_XA__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/AWF_XA__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/AWF_XA__3O*\n\n\n\n\nAWF_XB__1O\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/AWF_XB__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/AWF_XB__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/AWF_XB__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/AWF_XB__1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/AWF_XB__1O*\n\n\n\n\nAWF_XB__3O\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/AWF_XB__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/AWF_XB__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/AWF_XB__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/AWF_XB__3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/AWF_XB__3O*\n\n\n\n\nLI3_X___1O\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/LI3_X___1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/LI3_X___1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/LI3_X___1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/LI3_X___1O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/LI3_X___1O*\n\n\n\n\nLI3_X___3O\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/LI3_X___3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/LI3_X___3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/LI3_X___3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/LI3_X___3O*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_02/LI3_X___3O*\n\n\n\n\nAWF_XA__3T\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/AWF_XA__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/AWF_XA__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/AWF_XA__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/AWF_XA__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/AWF_XA__3T*\n\n\n\n\nAWF_XB__3T\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/AWF_XB__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/AWF_XB__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/AWF_XB__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2_CORE_01/AWF_XB__3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/AWF_XB__3T*\n\n\n\n\nLI3_X___3T\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG2b_CORE_30/LI3_X___3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/MR_IMAGE_2015/LI3_X___3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/HR_IMAGE_2015/LI3_X___3T*\n\n\n\n\n\n\n\n\n//eodata/CCM/DWH_MG2_CORE_01/LI3_X___3T*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2-3_CORE_08/LI3_X___3T*"
  },
  {
    "objectID": "Data/Others/CCM.html#spot-6-7",
    "href": "Data/Others/CCM.html#spot-6-7",
    "title": "Copernicus Contributing Missions",
    "section": "Spot 6-7",
    "text": "Spot 6-7\n\nOverview\nSPOT 6 and SPOT 7, launched in 2012 and 2014 respectively, are high-resolution Earth observation satellites operated by Airbus Defence and Space. These twin satellites provide 1.5-meter resolution imagery in panchromatic mode and 6-meter resolution in multispectral mode, with a swath width of 60 kilometers. The satellites are designed for wide-area monitoring and are particularly effective for applications such as agriculture, forestry, land management, and mapping. SPOT 6/7 operate in the same orbit as the Pléiades constellation, enabling complementary use of high-resolution and very-high-resolution data. This orbital synergy allows for daily revisit capabilities, making SPOT imagery highly valuable for time-sensitive monitoring tasks. The satellites’ high geometric accuracy and large-area coverage make them ideal for creating detailed land cover maps and large-scale mosaics. For more information, download the SPOT User Guide and visit the Airbus website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nNAO_BUN__3\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_BUN__3*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_BUN__3*\n\n\n\n\nNAO_BUN_1A\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_BUN_1A*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_BUN_1A*\n\n\n\n\nNAO_MS___3\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_MS___3*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_MS___3*\n\n\n\n\nNAO_MS__1A\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_MS__1A*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_MS__1A*\n\n\n\n\nNAO_MS4__3\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_MS4__3*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_MS4__3*\n\n\n\n\nNAO_MS4_2A\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_MS4_2A*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_MS4_2A*\n\n\n\n\nNAO_PMS__3\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/NAO_PMS__3*\n\n\n\n\n\n\n\n\n/eodata/CCM/DWH_MG2b_CORE_03/NAO_PMS__3*"
  },
  {
    "objectID": "Data/Others/CCM.html#vision-1",
    "href": "Data/Others/CCM.html#vision-1",
    "title": "Copernicus Contributing Missions",
    "section": "Vision-1",
    "text": "Vision-1\n\nOverview\nVision-1, launched by Airbus in 2019, is a sub-meter resolution Earth observation satellite that delivers imagery across five optical bands: panchromatic, blue, green, red, and near-infrared. With a ground resolution of 0.87 meters in panchromatic mode and 3.48 meters in multispectral mode, Vision-1 is well-suited for high-precision monitoring applications. The satellite operates in a sun-synchronous orbit, ensuring consistent imaging conditions with daily revisit capabilities. Vision-1’s agility and high-resolution imaging capabilities make it an ideal complement to the SPOT and Pléiades satellites, supporting various applications such as urban monitoring, infrastructure management, and environmental monitoring. The satellite’s advanced imaging system allows for rapid response to tasking requests, providing timely data that is crucial for decision-making in dynamic situations. For more technical details, visit the Airbus dedicated page Vision-1.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nS14_MS4_2A\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_2021/S14_MS4_2A*\n\n\n\n\nS14_MS4__3\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_2021/S14_MS4__3*"
  },
  {
    "objectID": "Data/Others/CCM.html#worldview-2-3",
    "href": "Data/Others/CCM.html#worldview-2-3",
    "title": "Copernicus Contributing Missions",
    "section": "WorldView 2-3",
    "text": "WorldView 2-3\n\nOverview\nWorldView-2 and WorldView-3, launched in 2009 and 2014 respectively, are high-capacity Earth observation satellites operated by Maxar Technologies with data distributed in Europe by EUSI. WorldView-2 captures imagery with a panchromatic resolution of 0.46 meters and an 8-band multispectral resolution of 1.85 meters, covering coastal, yellow, red, red edge, and near-infrared bands in addition to the traditional RGB bands. WorldView-3 improves upon this with a panchromatic resolution of 0.31 meters and a 16-band multispectral resolution of 1.24 meters, including additional shortwave infrared (SWIR) and CAVIS (Cloud, Aerosol, Water Vapor, Ice, and Snow) bands. These capabilities allow for detailed analysis of a wide range of environmental and man-made features, supporting applications such as urban planning, agriculture, and disaster response. WorldView-3’s ability to capture data in SWIR bands makes it particularly useful for material identification and geological surveys. Both satellites operate in sun-synchronous orbits, providing daily revisit capabilities. For more information visit the Maxar website for an overview of the WorldView Series or access the specific missions datasheets at WorldView-2 and WorldView-3.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nWV1_MS4_OR\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV1_MS4_OR*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV1_MS4_OR*\n\n\n\n\nWV1_MS4_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV1_MS4_SO*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV1_MS4_SO*\n\n\n\n\nWV1_PM4_OR\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV1_PM4_OR*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV1_PM4_OR*\n\n\n\n\nWV1_PM4_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV1_PM4_SO*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV1_PM4_SO*\n\n\n\n\nWV1_PM8_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV1_PM8_SO*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV1_PM8_SO*\n\n\n\n\nWV3_MS4_OR\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV3_MS4_OR*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV3_MS4_OR*\n\n\n\n\nWV3_MS4_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV3_MS4_SO*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV3_MS4_SO*\n\n\n\n\nWV3_PM4_OR\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV3_PM4_OR*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV3_PM4_OR*\n\n\n\n\nWV3_PM4_SO\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/VHR_IMAGE_20nn/WV3_PM4_SO*\n\n\n\n\n\n\n\n\n/eodata/CCM/VHR1-2_Urban_Atlas_2012/WV3_PM4_SO*"
  },
  {
    "objectID": "Data/Others/CCM.html#cosmo-skymed",
    "href": "Data/Others/CCM.html#cosmo-skymed",
    "title": "Copernicus Contributing Missions",
    "section": "Cosmo Skymed",
    "text": "Cosmo Skymed\n\nOverview\nCOSMO-SkyMed is a constellation of four X-band synthetic aperture radar (SAR) satellites, launched between 2007 and 2010, and operated by the Italian Space Agency (ASI) and the Italian Ministry of Defense. Designed to provide high-resolution radar imagery with a ground resolution of up to 1 meter, COSMO-SkyMed is capable of acquiring data regardless of weather conditions, day or night. The constellation’s short revisit times-ranging from a few hours to a couple of days-make it highly effective for applications such as emergency response, maritime surveillance, and environmental monitoring. COSMO-SkyMed’s SAR sensors operate in multiple modes, including spotlight, stripmap, and scanSAR, allowing for flexible imaging configurations that can be tailored to specific mission requirements. The constellation supports both civilian and defense applications, offering valuable data for managing ice monitoring, natural disasters and infrastructure monitoring. For more detailed information, visit the COSMO-SkyMed mission and products description at e-GEOS website and more specifically at Cosmo-SkyMed web page.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nSAR_HIM_1B\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_HIM_1B*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_HIM_1B*\n\n\n\n\nSAR_HIM_1C\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_HIM_1C*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_HIM_1C*\n\n\n\n\nSAR_HIM_AB\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_HIM_AB*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_HIM_AB*\n\n\n\n\nSAR_SCH_1B\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCH_1B*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCH_1B*\n\n\n\n\nSAR_SCH_1C\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCH_1C*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCH_1C*\n\n\n\n\nSAR_SCH_AB\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCH_AB*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCH_AB*\n\n\n\n\nSAR_SCW_1B\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCW_1B*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCW_1B*\n\n\n\n\nSAR_SCW_1C\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCW_1C*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCW_1C*\n\n\n\n\nSAR_SCW_1D\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCW_1D*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCW_1D*\n\n\n\n\nSAR_SCW_AB\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SCW_AB*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SCW_AB*\n\n\n\n\nSAR_SPP_1C\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SPP_1C*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SPP_1C*"
  },
  {
    "objectID": "Data/Others/CCM.html#iceye",
    "href": "Data/Others/CCM.html#iceye",
    "title": "Copernicus Contributing Missions",
    "section": "Iceye",
    "text": "Iceye\n\nOverview\nICEYE is a pioneering small satellite constellation operator, providing synthetic aperture radar (SAR) imagery with rapid revisit times and global coverage. The constellation, which began launching in 2018, consists of multiple small satellites equipped with high-frequency X-band SAR sensors capable of imaging at resolutions down to 0.5 meters. ICEYE’s Dwell Fine mode enhances image clarity by extending the radar illumination time over a target area, making it ideal for applications requiring detailed analysis, such as infrastructure monitoring and disaster assessment. The constellation’s agile design and multiple revisit capabilities-up to several times per day-enable near real-time monitoring of dynamic events, including ice monitoring, floods, forest fires, and urban development. ICEYE’s small satellites are designed for fast deployment and can be quickly added to the constellation, ensuring that users receive the most current and reliable data. For more information and detailed product specifications, visit the ICEYE product documentation portal.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nSAR_SC__GR\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/SAR_SEA_ICE/SAR_SC__GR*"
  },
  {
    "objectID": "Data/Others/CCM.html#paz",
    "href": "Data/Others/CCM.html#paz",
    "title": "Copernicus Contributing Missions",
    "section": "PAZ",
    "text": "PAZ\n\nOverview\nPAZ is an X-band synthetic aperture radar (SAR) satellite launched in 2018 as part of a public-private partnership between Hisdesat and Airbus Defence and Space. The satellite is capable of capturing up to 100 images daily, covering more than 300,000 square kilometers with a resolution of up to 25 centimeters in high-resolution modes. PAZ operates in multiple imaging modes, including spotlight, stripmap, and scanSAR, offering flexibility in data acquisition for different applications. The satellite’s SAR capabilities allow for all-weather, day-and-night imaging, making it particularly valuable for maritime surveillance, border control, emergency response and ice monitoring. PAZ also carries an Automatic Identification System (AIS) for tracking ship movements, enhancing its utility for monitoring maritime traffic and ensuring security in coastal areas. For detailed technical information, visit the Hisdesat website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nSAR_SC_GEC\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SC_GEC*\n\n\n\n\nSAR_SC_MGD\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SC_MGD*\n\n\n\n\nSAR_SM_EEC\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SM_EEC*\n\n\n\n\nSAR_SM_MGD\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SM_MGD*\n\n\n\n\nSAR_WS_GEC\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_WS_GEC*"
  },
  {
    "objectID": "Data/Others/CCM.html#radarsat-2",
    "href": "Data/Others/CCM.html#radarsat-2",
    "title": "Copernicus Contributing Missions",
    "section": "Radarsat-2",
    "text": "Radarsat-2\n\nOverview\nRADARSAT-2 is a synthetic aperture radar (SAR) satellite launched in 2007, operated by MDA in collaboration with the Canadian Space Agency (CSA). The satellite provides all-weather, day-and-night imaging capabilities with resolutions as fine as 1 meter in spotlight mode. RADARSAT-2 operates in a variety of imaging modes, including standard, fine, wide, and ultrafine, allowing for customizable data collection depending on user needs. The satellite’s high-resolution imaging capabilities make it suitable for a wide range of applications, including environmental monitoring, resource management, disaster response, maritime surveillance and ice monitoring. RADARSAT-2’s polarimetric imaging capability, which allows for the differentiation of various surface types, is particularly useful for monitoring changes in the environment, such as deforestation, ice movement, and oil spills. For more information and detailed product specifications, visit the Canadian Space Agency website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nSAR_SN_SCN\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SN_SCN*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SN_SCN*\n\n\n\n\nSAR_SN_SGF\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SN_SGF*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SN_SGF*\n\n\n\n\nSAR_SW_SCW\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SW_SCW*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SW_SCW*\n\n\n\n\nSAR_SW_SGF\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SW_SGF*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SW_SGF*\n\n\n\n\nSAR_Wx_SGF\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_Wx_SGF*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_Wx_SGF*"
  },
  {
    "objectID": "Data/Others/CCM.html#terrasar-x",
    "href": "Data/Others/CCM.html#terrasar-x",
    "title": "Copernicus Contributing Missions",
    "section": "TerraSar-X",
    "text": "TerraSar-X\n\nOverview\nTerraSAR-X, launched in 2007, is an X-band synthetic aperture radar (SAR) satellite operated by Airbus Defence and Space. The satellite is capable of delivering high-resolution radar imagery with a ground resolution of up to 1 meter in spotlight mode and 3 meters in stripmap mode. TerraSAR-X’s radar system operates at a frequency of 9.65 GHz, enabling it to capture detailed images of the Earth’s surface regardless of weather conditions or time of day. The satellite’s multiple imaging modes allow for versatile data acquisition, making it suitable for a wide range of applications, including maritime surveillance, urban mapping, and environmental monitoring. TerraSAR-X’s ability to detect subtle changes in the Earth’s surface over time is particularly valuable for ice monitoring, tracking natural disasters, and infrastructure monitoring. For more technical details and imagery samples, visit the Airbus website.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nSAR_SC_GEC\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SC_GEC*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SC_GEC*\n\n\n\n\nSAR_SC_MGD\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SC_MGD*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SC_MGD*\n\n\n\n\nSAR_SC_SSC\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_SC_SSC*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_SC_SSC*\n\n\n\n\nSAR_WS_EEC\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_WS_EEC*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_WS_EEC*\n\n\n\n\nSAR_WS_GEC\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_WS_GEC*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_WS_GEC*\n\n\n\n\nSAR_WS_MGD\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/DWH_MG1_CORE_11/SAR_WS_MGD*\n\n\n\n\n\n\n\n\n/eodata/CCM/SAR_SEA_ICE/SAR_WS_MGD"
  },
  {
    "objectID": "Data/Others/CCM.html#alos-1",
    "href": "Data/Others/CCM.html#alos-1",
    "title": "Copernicus Contributing Missions",
    "section": "ALOS-1",
    "text": "ALOS-1\n\nOverview\nThe ALOS-1 mission was a Japanese Earth-imaging satellite from the Japan Aerospace Exploration Agency (JAXA) that launched on 24 January 2006 and completed its operational phase on 12 May 2011 due to a power anomaly. It carried three instruments (2 optical and one SAR)."
  },
  {
    "objectID": "Data/Others/CCM.html#quickbird",
    "href": "Data/Others/CCM.html#quickbird",
    "title": "Copernicus Contributing Missions",
    "section": "QuickBird",
    "text": "QuickBird\n\nOverview\nQuickBird-2 was an Earth-imaging satellite of Maxar which launched in October 2001 and ceased operations in 2015. ESA has an agreement with European Space Imaging (EUSI) to distribute data products from the mission. The satellite collected panchromatic imagery at 61 centimeter resolution and multispectral imagery at 2.44 to 1.63-metre."
  },
  {
    "objectID": "Data/Others/CCM.html#formosat-2",
    "href": "Data/Others/CCM.html#formosat-2",
    "title": "Copernicus Contributing Missions",
    "section": "Formosat-2",
    "text": "Formosat-2\n\nOverview\nFormosat-2 was an earth image satellite of Taiwan Space Agency, operated until August 2016 when it was dismissed. Its data for Copernicus, 2 meters panchromatic and 8 meters multispectral, has been processed and distributed by Airbus.\n\nOffered Data\n\n\n\n\n\n\nProduct Type\n\n\nFile Description\n\n\nData Access Type\n\n\nCatalogue\n\n\n\n\n\n\nAV2_OBS_3A (ALOS)\n\n\nUnpacked\n\n\nIAD (Immediately available data)\n\n\n/eodata/CCM/DAP_MG2b_01/AV2_OBS_3A*\n\n\n\n\n\n\n\n\n/eodata/CCM/DAP_MG2b_02/AV2_OBS_3A*\n\n\n\n\nPSM_OB1_3A (ALOS)\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/ DAP_MG2b_01/PSM_OB1_3A*\n\n\n\n\n\n\n\n\n/eodata/CCM/ DAP_MG2b_02/PSM_OB1_3A*\n\n\n\n\nBGI_PM4_SO (QuickBird)\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/ DAP_MG2b_01/BGI_PM4_SO*\n\n\n\n\nRSI_FUS__3 (Formosat)\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/ DAP_MG2b_01/RSI_FUS__3*\n\n\n\n\nA3D_DEM_18 (Delivered by Airbus)\n\n\nUnpacked\n\n\nIAD\n\n\n/eodata/CCM/ DEM_VHR_2018/A3D_DEM_18*"
  },
  {
    "objectID": "Data/Others/CCM.html#examples-of-odata-queries",
    "href": "Data/Others/CCM.html#examples-of-odata-queries",
    "title": "Copernicus Contributing Missions",
    "section": "Examples of OData queries",
    "text": "Examples of OData queries\nBelow are some (not exhaustive) examples of OData queries that you can use and modify according to your needs.\nList of acceptable expand attribute names for CCM:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Attributes(CCM)\n\n\n\nTo search the entire CCM collection:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20%27CCM%27\n\n\n\nTo search for CCM collection and products acquired between two dates:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20%27CCM%27%20and%20ContentDate/Start%20gt%202005-05-03T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-05-03T00:11:00.000Z\n\n\n\nTo search for CCM collection products with footprints intersecting specific coordinates:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20%27CCM%27%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;POLYGON((12.655118166047592%2047.44667197521409,21.39065656328509%2048.347694733853245,28.334291357162826%2041.877123516783655,17.47086198383573%2040.35854475076158,12.655118166047592%2047.44667197521409))%27)&$top=20\n\n\n\nTo search for CCM collection products acquired between two dates with footprints intersecting specific coordinates:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20%27CCM%27%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;POLYGON((12.655118166047592%2047.44667197521409,21.39065656328509%2048.347694733853245,28.334291357162826%2041.877123516783655,17.47086198383573%2040.35854475076158,12.655118166047592%2047.44667197521409))%27)%20and%20ContentDate/Start%20gt%202021-05-20T00:00:00.000Z%20and%20ContentDate/Start%20lt%202021-07-21T00:00:00.000Z\n\n\n\nTo query a subset of CCM data for a specific area of interest and time period, selecting a specific mission (e.g., only Worldview-3):\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20%27CCM%27%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;POLYGON%20((6.535492%2050.600673,%206.535492%2050.937662,%207.271576%2050.937662,%207.271576%2050.600673,%206.535492%2050.600673))%27)%20and%20Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27platformName%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27WorldView-3%27)%20and%20ContentDate/Start%20gt%202022-05-20T00:00:00.000Z%20and%20ContentDate/Start%20lt%202022-07-21T00:00:00.000Z\n\n\n\nTo search all products of a specific CCM dataset (e.g., products belonging to VHR_IMAGE_2018):\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27datasetFull%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27VHR_IMAGE_2018%27)&$expand=Attributes\n\n\n\nTo search all products of a specific CCM sub-dataset (e.g., products belonging to VHR_IMAGE_2018/IT_N/Level_3):\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27dataset%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27VHR_IMAGE_2018/IT_N/Level_1%27)&$expand=Attributes\n\n\n\nTo search for CCM specific product by name / CPP filename:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Name%20eq%20%27SP07_NAO_MS4_2A_20210729T064948_20210729T064958_TOU_1234_90f0.DIMA%27&$expand=Attributes\n\n\n\nMore details on the structure of OData queries and available functionalities can be found in the OData documentation.\n\n\n\n\n\n\nNote\n\n\n\nCOGified data, processed to enable the VISUALISE service for the optical Europe VHR collections, will only be available for download via the OData interface and not through the Copernicus Browser.\nThe product/image names will be updated as follows:\nNative name:\nSP05_HRG_J___3_20060504T114303_20060504T114303_CDS_0000.EEEE_42289e36\nCOGified data name:\nSP05_HRG_J___3_20060504T114303_20060504T114303_CDS_0000.EEEE_42289e36-COG"
  },
  {
    "objectID": "Data/Others/CCM.html#filters-panel-and-values-in-the-copernicus-browser",
    "href": "Data/Others/CCM.html#filters-panel-and-values-in-the-copernicus-browser",
    "title": "Copernicus Contributing Missions",
    "section": "Filters panel and values in the Copernicus Browser",
    "text": "Filters panel and values in the Copernicus Browser\nThis section reports the attributes that are available for searching specific CCM data (e.g., mission, product type) through the “Filters” panel in the Copernicus Browser for the three CCM collections (Optical, SAR, and DEM).\n\nOptical data\n\n\n\n\n\n\nEOP IDENTIFIER\nThis is the unique identifier for a metadata item, including the ground segment namespace to ensure uniqueness within EOP. Example value:\nurn:eop:SPOT:MULTISPECTRAL_4m:DS_SPOT7_202107290649485_FR1_FR1_SV1_SV1_E045S13_01871_projected_90f0\n\nPLATFORM SHORT NAME and PLATFORM NAME\nThese are the mission identifiers, with a four-character code for the CCM ID and its long name. The full list of available values for Optical collections is provided in the following table:\n\n\n\n\n\n\n\nCCM ID four chars code (platformShortName)\nFull mission name (platformName)\n\n\n\n\nAL01\nALOS\n\n\nAR3D\nALOS REF 3d DEM\n\n\nBJ01\nBeijing-1\n\n\nDC00\nDMC Constellation\n\n\nDE01\nGEOSAT-1\n\n\nDEM1\nCopernicus DEM\n\n\nDM01\nGEOSAT-1\n\n\nDM02\nGEOSAT-2\n\n\nEW02\nWorldView-2\n\n\nEW03\nWorldView-3\n\n\nFO02\nFORMOSAT2\n\n\nGY01\nGeoEye-1\n\n\nIR06\nIRS-P6 Resourcesat-1\n\n\nIR07\nIRS-R2 Resourcesat-2\n\n\nKS03\nKOMPSAT3\n\n\nKS04\nKOMPSAT3A\n\n\nNG01\nNigeriaSat 1\n\n\nPH00\nPleiades Constellation\n\n\nPH1A\nPleiades 1A\n\n\nPH1B\nPleiades 1B\n\n\nPL00\nPlanetscope\n\n\nPN00\nPleiades Neo Constellation\n\n\nPN03\nPleiades Neo\n\n\nPN04\nPleiades Neo\n\n\nQB02\nQuickBird-2\n\n\nRE00\nRapidEye\n\n\nS20A\nSentinel 2A\n\n\nSP00\nSpot 6/7 Constellation\n\n\nSP04\nSpot-4\n\n\nSP05\nSpot-5\n\n\nSP06\nSpot-6\n\n\nSP07\nSpot-7\n\n\nSW00\nSUPERVIEW-1 Constellation\n\n\nTR00\nTripleSat Constellation\n\n\nUK01\nUK DMC1\n\n\nUK02\nUK DMC2\n\n\nVS01\nVision -1\n\n\n\n\nDataset and SUB-DATASET\nCCM data are organized in a dataset-subdataset hierarchy. The dataset can be used as a search parameter directly in the Copernicus Browser search panel, according to the dataset-collection mapping reported in the table below. The search for data belonging to a specific sub-dataset can be performed through the SUB-DATASET field in the Filters panel (e.g., HR_IMAGE_2015/AL/Coverage_2/L3_European …).\n\n\n\n\n\n\n\n\nCCM collection\nCollection name\nDataset\n\n\n\n\nCCM Optical\nHR Europe (2006, 2009)\nDAP_MG2-3_01\n\n\n\n\nDWH_MG2_CORE_02\n\n\n\nVHR Urban Atlas (2006, 2009)\nDAP_MG2b_01\n\n\n\n\nDAP_MG2b_02\n\n\n\nHR Europe (2011–2013)\nDWH_MG2_CORE_01\n\n\n\nVHR 2018 DEM\nDEM_VHR_2018\n\n\n\nHR Sub-Saharan (2011–2013)\nDWH_MG2_CORE_09\n\n\n\nMR Europe monthly (2011-2012)\nDWH_MG2-3_CORE_08\n\n\n\nVHR Europe (2011-2013)\nDWH_MG2b_CORE_03\n\n\n\nHR Europe monthly (Apr-Oct 2015)\nEUR_HR2_MULTITEMP\n\n\n\nHR Europe (2014-2015)\nHR_IMAGE_2015\n\n\n\nMR Europe monthly (Mar-Oct 2014)\nMR_IMAGE_2015\n\n\n\nVHR Europe (2014–2016)\nVHR_IMAGE_2015\n\n\n\nVHR Europe (2017–2019)\nVHR_IMAGE_2018\n\n\n\n\nVHR_IMAGE_2018_ENHANCED\n\n\n\nVHR Europe (2020–2022)\nVHR_IMAGE_2021\n\n\n\nVHR Europe (2023–2025)\nVHR_IMAGE_2024\n\n\n\nVHR Urban Atlas (2011-2013)\nVHR1-2_Urban_Atlas_2012\n\n\n\n\nPRODUCT TYPE\nThis describes the product type, specific to the mission, instrument, and processing level. Access rights at CDSE are granted at the product type level. For CCM data, the same product types can exist in different datasets with different access rights. To ensure uniqueness within a specific dataset and to implement the correct access rights, the product types have been updated according to the following algorithm:\n&lt;productType&gt; = &lt;CCM_native_productType&gt;_&lt;CRC-16/ARC_of_dataset&gt;\n\nHereinafter the full table according to the data currently present:\n\n\n\n\n\n\n\n\nDataset\nCCM native product type\nproductType\n\n\n\n\nDAP_MG2b_01\nHRG_THX__3\nHRG_THX__3_56FB\n\n\nDAP_MG2b_01\nMSI_IMG_3A\nMSI_IMG_3A_56FB\n\n\nDAP_MG2b_01\nHRG_T____3\nHRG_T____3_56FB\n\n\nDAP_MG2b_01\nHRG_J____3\nHRG_J____3_56FB\n\n\nDAP_MG2b_01\nPSM_OB1_3A\nPSM_OB1_3A_56FB\n\n\nDAP_MG2b_01\nAV2_OBS_3A\nAV2_OBS_3A_56FB\n\n\nDAP_MG2b_01\nBGI_PM4_SO\nBGI_PM4_SO_56FB\n\n\nDAP_MG2b_01\nRSI_FUS__3\nRSI_FUS__3_56FB\n\n\nDAP_MG2b_02\nAV2_OBS_3A\nAV2_OBS_3A_57BB\n\n\nDAP_MG2b_02\nHRG_J____3\nHRG_J____3_57BB\n\n\nDAP_MG2b_02\nHRG_T____3\nHRG_T____3_57BB\n\n\nDAP_MG2b_02\nHRG_THX__3\nHRG_THX__3_57BB\n\n\nDAP_MG2b_02\nPSM_OB1_3A\nPSM_OB1_3A_57BB\n\n\nDAP_MG2-3_01\nHIR_I____3\nHIR_I____3_44AF\n\n\nDAP_MG2-3_01\nHIR_M____3\nHIR_M____3_44AF\n\n\nDAP_MG2-3_01\nHIR_MI___3\nHIR_MI___3_44AF\n\n\nDAP_MG2-3_01\nHRG_A____3\nHRG_A____3_44AF\n\n\nDAP_MG2-3_01\nHRG_J____3\nHRG_J____3_44AF\n\n\nDAP_MG2-3_01\nHRVI_I___3O\nHRVI_I___3O_44AF\n\n\nDAP_MG2-3_01\nLI3_X___3O\nLI3_X___3O_44AF\n\n\nDAP_MG2_25\nSL6_22P_1T\nSL6_22P_1T_6485\n\n\nDAP_MG2_25\nSL6_22S_1T\nSL6_22S_1T_6485\n\n\nDAP_MG2_25\nSL6_32T_1T\nSL6_32T_1T_6485\n\n\nDAP_MG2_25\nMSI_IMG_3A\nMSI_IMG_3A_6485\n\n\nDAP_MG2_25\nSL6_32S_1T\nSL6_32S_1T_6485\n\n\nDAP_MG2_25\nSL6_32P_1T\nSL6_32P_1T_6485\n\n\nDAP_MG2_25\nSL6_22T_1T\nSL6_22T_1T_6485\n\n\nDEM_VHR_2018\nA3D_DEM_18\nA3D_DEM_18_7854\n\n\nDWH_MG2_CORE_01\nMSI_IMG_3A\nMSI_IMG_3A_7BC7\n\n\nDWH_MG2_CORE_01\nMSI_IMG_1B\nMSI_IMG_1B_7BC7\n\n\nDWH_MG2_CORE_01\nHRG_J___1A\nHRG_J___1A_7BC7\n\n\nDWH_MG2_CORE_01\nHIR_I____3\nHIR_I____3_7BC7\n\n\nDWH_MG2_CORE_01\nHIR_I___1A\nHIR_I___1A_7BC7\n\n\nDWH_MG2_CORE_01\nHRG_J____3\nHRG_J____3_7BC7\n\n\nDWH_MG2_CORE_01\nLI3_X___3O\nLI3_X___3O_7BC7\n\n\nDWH_MG2_CORE_01\nLI3_X___1O\nLI3_X___1O_7BC7\n\n\nDWH_MG2_CORE_02\nHRG_J____3\nHRG_J____3_7A87\n\n\nDWH_MG2_CORE_02\nHIR_I____3\nHIR_I____3_7A87\n\n\nDWH_MG2_CORE_02\nHRV_X____3\nHRV_X____3_7A87\n\n\nDWH_MG2_CORE_02\nLI3_X___3O\nLI3_X___3O_7A87\n\n\nDWH_MG2_CORE_09\nSL6_22P_1T\nSL6_22P_1T_BDC6\n\n\nDWH_MG2_CORE_09\nSL6_22S_1R\nSL6_22S_1R_BDC6\n\n\nDWH_MG2_CORE_09\nSL6_22S_1T\nSL6_22S_1T_BDC6\n\n\nDWH_MG2_CORE_09\nSL6_22P_1R\nSL6_22P_1R_BDC6\n\n\nDWH_MG2-3_CORE_08\nAWF_XB__1O\nAWF_XB__1O_5597\n\n\nDWH_MG2-3_CORE_08\nAWF_XA__3O\nAWF_XA__3O_5597\n\n\nDWH_MG2-3_CORE_08\nAWF_XA__1O\nAWF_XA__1O_5597\n\n\nDWH_MG2-3_CORE_08\nAWF_XB__3O\nAWF_XB__3O_5597\n\n\nDWH_MG2b_CORE_03\nHRG_THX__3\nHRG_THX__3_54FA\n\n\nDWH_MG2b_CORE_03\nNAO_PMS__3\nNAO_PMS__3_54FA\n\n\nDWH_MG2b_CORE_03\nPHR_MS___3\nPHR_MS___3_54FA\n\n\nEUR_HR2_MULTITEMP\nSL6_22P_1T\nSL6_22P_1T_C98A\n\n\nEUR_HR2_MULTITEMP\nSL6_22S_1T\nSL6_22S_1T_C98A\n\n\nHR_IMAGE_2015\nHRG_J____3\nHRG_J____3_62E1\n\n\nHR_IMAGE_2015\nLI3_X___3T\nLI3_X___3T_62E1\n\n\nHR_IMAGE_2015\nHRG_J___1A\nHRG_J___1A_62E1\n\n\nHR_IMAGE_2015\nNAO_MS___3\nNAO_MS___3_62E1\n\n\nHR_IMAGE_2015\nNAO_MS__1A\nNAO_MS__1A_62E1\n\n\nHR_IMAGE_2015\nMSI_IM__1C\nMSI_IM__1C_62E1\n\n\nHR_IMAGE_2015\nLI3_X___1O\nLI3_X___1O_62E1\n\n\nMR_IMAGE_2015\nAWF_XB__1O\nAWF_XB__1O_67ED\n\n\nMR_IMAGE_2015\nAWF_XA__3T\nAWF_XA__3T_67ED\n\n\nMR_IMAGE_2015\nAWF_XB__3T\nAWF_XB__3T_67ED\n\n\nMR_IMAGE_2015\nAWF_XA__1O\nAWF_XA__1O_67ED\n\n\nVHR_IMAGE_2015\nPHR_BUN_1A\nPHR_BUN_1A_71F4\n\n\nVHR_IMAGE_2015\nPHR_BUN__3\nPHR_BUN__3_71F4\n\n\nVHR_IMAGE_2015\nNAO_BUN__3\nNAO_BUN__3_71F4\n\n\nVHR_IMAGE_2015\nWV1_PM4_OR\nWV1_PM4_OR_71F4\n\n\nVHR_IMAGE_2015\nNAO_BUN_1A\nNAO_BUN_1A_71F4\n\n\nVHR_IMAGE_2015\nGIS_PM4_SO\nGIS_PM4_SO_71F4\n\n\nVHR_IMAGE_2015\nWV1_PM4_SO\nWV1_PM4_SO_71F4\n\n\nVHR_IMAGE_2015\nWV3_PM4_OR\nWV3_PM4_OR_71F4\n\n\nVHR_IMAGE_2015\nHRS_PM4_1C\nHRS_PM4_1C_71F4\n\n\nVHR_IMAGE_2015\nGIS_PM4_OR\nGIS_PM4_OR_71F4\n\n\nVHR_IMAGE_2015\nWV3_PM4_SO\nWV3_PM4_SO_71F4\n\n\nVHR_IMAGE_2015\nSAR_SC_MGD\nSAR_SC_MGD_71F4\n\n\nVHR_IMAGE_2015\nHRS_PM4_1B\nHRS_PM4_1B_71F4\n\n\nVHR_IMAGE_2018\nAIS_MSP_1G\nAIS_MSP_1G_E1F0\n\n\nVHR_IMAGE_2018\nAIS_MSP_1R\nAIS_MSP_1R_E1F0\n\n\nVHR_IMAGE_2018\nNAO_MS4__3\nNAO_MS4__3_E1F0\n\n\nVHR_IMAGE_2018\nNAO_MS4_2A\nNAO_MS4_2A_E1F0\n\n\nVHR_IMAGE_2018\nDOV_MS_L1A\nDOV_MS_L1A_E1F0\n\n\nVHR_IMAGE_2018\nDOV_MS_L3A\nDOV_MS_L3A_E1F0\n\n\nVHR_IMAGE_2018\nPHR_MS__2A\nPHR_MS__2A_E1F0\n\n\nVHR_IMAGE_2018\nOPT_MS4_1B\nOPT_MS4_1B_E1F0\n\n\nVHR_IMAGE_2018\nPHR_MS___3\nPHR_MS___3_E1F0\n\n\nVHR_IMAGE_2018\nOPT_MS4_1C\nOPT_MS4_1C_E1F0\n\n\nVHR_IMAGE_2018\nVHI_MS4_1C\nVHI_MS4_1C_E1F0\n\n\nVHR_IMAGE_2018\nVHI_MS4_1B\nVHI_MS4_1B_E1F0\n\n\nVHR_IMAGE_2018\nHRS_MS4_1C\nHRS_MS4_1C_E1F0\n\n\nVHR_IMAGE_2018\nHRS_MS4_1B\nHRS_MS4_1B_E1F0\n\n\nVHR_IMAGE_2018_ENHANCED\nPHR_MS__2A\nPHR_MS__2A_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nPHR_MS___3\nPHR_MS___3_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nAIS_MSP_1G\nAIS_MSP_1G_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nAIS_MSP_1R\nAIS_MSP_1R_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nOPT_MS4_1B\nOPT_MS4_1B_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nOPT_MS4_1C\nOPT_MS4_1C_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nNAO_MS4__3\nNAO_MS4__3_B34B\n\n\nVHR_IMAGE_2018_ENHANCED\nNAO_MS4_2A\nNAO_MS4_2A_B34B\n\n\nVHR_IMAGE_2021\nWV3_MS4_SO\nWV3_MS4_SO_07B6\n\n\nVHR_IMAGE_2021\nPHR_MS__2A\nPHR_MS__2A_07B6\n\n\nVHR_IMAGE_2021\nPHR_MS___3\nPHR_MS___3_07B6\n\n\nVHR_IMAGE_2021\nNAO_MS4__3\nNAO_MS4__3_07B6\n\n\nVHR_IMAGE_2021\nNAO_MS4_2A\nNAO_MS4_2A_07B6\n\n\nVHR_IMAGE_2021\nOPT_MS4_1B\nOPT_MS4_1B_07B6\n\n\nVHR_IMAGE_2021\nOPT_MS4_1C\nOPT_MS4_1C_07B6\n\n\nVHR_IMAGE_2021\nAIS_MSP_1G\nAIS_MSP_1G_07B6\n\n\nVHR_IMAGE_2021\nAIS_MSP_1R\nAIS_MSP_1R_07B6\n\n\nVHR_IMAGE_2021\nWV1_MS4_SO\nWV1_MS4_SO_07B6\n\n\nVHR_IMAGE_2021\nGIS_MS4_SO\nGIS_MS4_SO_07B6\n\n\nVHR_IMAGE_2021\nWV1_MS4_OR\nWV1_MS4_OR_07B6\n\n\nVHR_IMAGE_2021\nWV3_MS4_OR\nWV3_MS4_OR_07B6\n\n\nVHR_IMAGE_2021\nGIS_MS4_OR\nGIS_MS4_OR_07B6\n\n\nVHR_IMAGE_2021\nS14_MS4_2A\nS14_MS4_2A_07B6\n\n\nVHR_IMAGE_2021\nS14_MS4__3\nS14_MS4__3_07B6\n\n\nVHR_IMAGE_2021\nVHI_MS4_1B\nVHI_MS4_1B_07B6\n\n\nVHR_IMAGE_2021\nVHI_MS4_1C\nVHI_MS4_1C_07B6\n\n\nVHR_IMAGE_2021\nHRS_MS4_1B\nHRS_MS4_1B_07B6\n\n\nVHR_IMAGE_2021\nHRS_MS4_1C\nHRS_MS4_1C_07B6\n\n\nVHR_IMAGE_2024\nHRS_MS4_1C\nHRS_MS4_1C_0476\n\n\nVHR_IMAGE_2024\nHRS_MS2_1D\nHRS_MS2_1D_0476\n\n\nVHR_IMAGE_2024\nPHR_MS___3\nPHR_MS___3_0476\n\n\nVHR_IMAGE_2024\nNAO_MS4__3\nNAO_MS4__3_0476\n\n\nVHR_IMAGE_2024\nS14_MS4__3\nS14_MS4__3_0476\n\n\nVHR_IMAGE_2024\nS14_MS2__3\nS14_MS2__3_0476\n\n\nVHR_IMAGE_2024\nPNE_MS2__3\nPNE_MS2__3_0476\n\n\nVHR1-2_Urban_Atlas_2012\nWV1_PM8_SO\nWV1_PM8_SO_5558\n\n\n\n\nMAX ACROSS TRACK ANGLE (ONA)\nThe maximum across track angle can be specified in this field. Note that the Max Across Track Angle/ONA is not populated for every product, so the query results might not include all available catalogue entries. \n\n\nSAR data\n\n\n\n\n\n\nEOP IDENTIFIER\nThis is the unique identifier for a metadata item, including the ground segment namespace to ensure uniqueness within EOP. Example value:\nurn:eop:RS02:SAR_SEA_ICE:RS2_20161224_155421_0077_SCWA_HHHV_SGF_527951_1381_14621589\n\nPLATFORM SHORT NAME and PLATFORM NAME\nThese are the mission identifiers, with a four-character code for the CCM ID and its long name. The full list of available values for SAR collections is provided in the following table:\n\n\n\n\n\n\n\nCCM ID four chars code (platformShortName)\nFull mission name (platformName)\n\n\n\n\nCS00\nCOSMO-SkyMed Constellation\n\n\nCS01\nCOSMO-SkyMed 1\n\n\nCS02\nCOSMO-SkyMed 2\n\n\nCS03\nCOSMO-SkyMed 3\n\n\nCS04\nCOSMO-SkyMed 4\n\n\nIE00\nICEYE\n\n\nPAZ1\nPAZ (HISDESAT)\n\n\nRS02\nRADARSAT-2\n\n\nTX01\nTerraSAR-X\n\n\n\n\nDataset and SUB-DATASET\nCCM data are organized in a dataset-subdataset hierarchy. The dataset can be used as a search parameter directly in the Copernicus Browser search panel, according to the dataset-collection mapping reported in the table below. The search for data belonging to a specific sub-dataset can be performed through the SUB-DATASET field in the Filters panel (e.g. SAR_SEA_ICE/RS02/Eurarctic …).\n\n\n\n\n\n\n\n\nCCM collection\nCollection name\nDataset\n\n\n\n\nCCM SAR\nHR-MR sea ice monitoring (2011-2014)\nDWH_MG1_CORE_11\n\n\n\nHR-MR sea ice monitoring (2015–2024)\nSAR_SEA_ICE\n\n\n\n\nPRODUCT TYPE\nThis describes the product type, specific to the mission, instrument, and processing level. Access rights at CDSE are granted at the product type level. For CCM data, the same product types can exist in different datasets with different access rights. To ensure uniqueness within a specific dataset and to implement the correct access rights, the product types have been updated according to the following algorithm:\n&lt;productType&gt; = &lt;CCM_native_productType&gt;_&lt;CRC-16/ARC_of_dataset&gt;\n\n\n\n\n\nDataset\nCCM native product type\nproductType\n\n\n\n\nDWH_MG1_CORE_11\nSAR_SW_SCW\nSAR_SW_SCW_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SW_SGF\nSAR_SW_SGF_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SN_SGF\nSAR_SN_SGF_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SN_SCN\nSAR_SN_SCN_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_Wx_SGF\nSAR_Wx_SGF_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SCH_1B\nSAR_SCH_1B_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SCW_AB\nSAR_SCW_AB_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SCH_1C\nSAR_SCH_1C_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_HIM_1C\nSAR_HIM_1C_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SCW_1C\nSAR_SCW_1C_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SCW_1B\nSAR_SCW_1B_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SC_GEC\nSAR_SC_GEC_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SC_MGD\nSAR_SC_MGD_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_HIM_1B\nSAR_HIM_1B_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SCW_1D\nSAR_SCW_1D_1BD2\n\n\nDWH_MG1_CORE_11\nSAR_SPP_1C\nSAR_SPP_1C_1BD2\n\n\nSAR_SEA_ICE\nSAR_SCH_1C\nSAR_SCH_1C_6F15\n\n\nSAR_SEA_ICE\nSAR_SCH_1B\nSAR_SCH_1B_6F15\n\n\nSAR_SEA_ICE\nSAR_SW_SCW\nSAR_SW_SCW_6F15\n\n\nSAR_SEA_ICE\nSAR_WS_MGD\nSAR_WS_MGD_6F15\n\n\nSAR_SEA_ICE\nSAR_HIM_1C\nSAR_HIM_1C_6F15\n\n\nSAR_SEA_ICE\nSAR_SC_GEC\nSAR_SC_GEC_6F15\n\n\nSAR_SEA_ICE\nSAR_HIM_1B\nSAR_HIM_1B_6F15\n\n\nSAR_SEA_ICE\nSAR_SC_MGD\nSAR_SC_MGD_6F15\n\n\nSAR_SEA_ICE\nSAR_SC_SSC\nSAR_SC_SSC_6F15\n\n\nSAR_SEA_ICE\nSAR_SCW_1C\nSAR_SCW_1C_6F15\n\n\nSAR_SEA_ICE\nSAR_HIM_AB\nSAR_HIM_AB_6F15\n\n\nSAR_SEA_ICE\nSAR_SCH_AB\nSAR_SCH_AB_6F15\n\n\nSAR_SEA_ICE\nSAR_SPP_1C\nSAR_SPP_1C_6F15\n\n\nSAR_SEA_ICE\nSAR_WS_GEC\nSAR_WS_GEC_6F15\n\n\nSAR_SEA_ICE\nSAR_WS_EEC\nSAR_WS_EEC_6F15\n\n\nSAR_SEA_ICE\nSAR_SM_MGD\nSAR_SM_MGD_6F15\n\n\nSAR_SEA_ICE\nSAR_SM_EEC\nSAR_SM_EEC_6F15\n\n\nSAR_SEA_ICE\nSAR_SC__GR\nSAR_SC__GR_6F15\n\n\n\n\n\n\nCopernicus DEM\n\n\n\n\n\n\nEOP IDENTIFIER\nThis is the unique identifier for a metadata item, including the ground segment namespace to ensure uniqueness within EOP. Example value:\nurn:eop:DLR:CDEM90:Copernicus_DSM_30_N37_00_E051_00:V3388\n\nDELIVERY\nThe delivery ID allows searching for a specific COP DEM delivery. The most recent delivery contains all the latest corrections and improvements to the baseline, but previous full deliveries can also be searched, selected and downloaded if needed. This attribute is specific to COP DEM data.\nGRID ID\nThis attribute allows selection of available data for a specific grid ID. The syntax is as follows:\n&lt;N/S&gt;&lt;cell_id_latitude&gt;_&lt;E/W&gt;&lt;cell_id_longitude&gt;\n\nExample: N28_W115\nThe equivalent OData query to retrieve all COP DEM products for that grid cell is:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27gridId%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27N28_W115%27)&$expand=Attributes\n\n\n\nIt is specific and available only for COP DEM data.\nOther attributes for COP DEM data\nEven though Dataset, Sub-dataset, and Product Type are not available in the “Filters” panel, they can still be used to query the CDSE catalog for COP DEM data, through OData interface.\nThe search of data belonging to specific sub-dataset can be done through the “dataset” OData attribute, as well as for the other CCM collections:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27dataset%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27COP-DEM_GLO-30-DGED/2024_1%27)&$expand=Attributes\n\n\n\nRegarding the Product Type, the same considerations as for Optical and SAR data apply to COP DEM. Below is a full table according to the data currently available:\n\n\n\n\n\n\n\n\nDataset\nCCM native product type\nproductType\n\n\n\n\nCOP-DEM_EEA-10-DGED\nSAR_DGE_10\nSAR_DGE_10_9319\n\n\nCOP-DEM_EEA-10-INSP\nSAR_INS_10\nSAR_INS_10_52C5\n\n\nCOP-DEM_GLO-30-DGED\nSAR_DGE_30\nSAR_DGE_30_A4AD\n\n\nCOP-DEM_GLO-30-DTED\nSAR_DTE_30\nSAR_DTE_30_615C\n\n\nCOP-DEM_GLO-90-DGED\nSAR_DGE_90\nSAR_DGE_90_A407\n\n\nCOP-DEM_GLO-90-DTED\nSAR_DTE_90\nSAR_DTE_90_61F6\n\n\n\n\nAnd hereinafter a sample OData query on productType attribute:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27productType%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27SAR_DGE_30_A4AD%27)&$expand=Attributes"
  },
  {
    "objectID": "Data/Others/Sentinel2_Mosaic_Algorithm.html",
    "href": "Data/Others/Sentinel2_Mosaic_Algorithm.html",
    "title": "Algorithm",
    "section": "",
    "text": "The following algorithm was run independently for each pixel:\n(1) For each pixel: Take the three-month stack of Sentinel-2 L2A observations. Only bands B02, B03, B04, B08 and SCL are used to create the mosaic. For bands B02-B08 transform the values to reflectance.\n(2) For each observation: Mark an observation as invalid if the value of the Sentinel-2 L2A scene classification band (SCL) has one of the following values:\n\n1-SATURATED_DEFECTIVE,\n3-CLOUD_SHADOW,\n7-CLOUD_LOW_PROBA / UNCLASSIFIED,\n8-CLOUD_MEDIUM_PROBA,\n9-CLOUD_HIGH_PROBA,\n10-THIN_CIRRUS\n\n(3) For each pixel: Discard all invalid observations, what remains is called valid observations. The number of valid observations is generally different for each pixel and is output as a positive integer in the observations output band.\n(4) For each pixel, for each band (B02, B03, B04, B08): Sort all valid observations for each band separately.\n(5) For each pixel, for each band (B02, B03, B04, B08): Take the value of the first quartile and multiply it by 10000 (to get a ‘digital number’). This is an output value.\n(6) For each pixel, for each band (B02, B03, B04, B08): If there are no valid observations, output the value -32768, which represents no data. For the observations band, output the value 0, which also represents no data.\n\n\n\n\n\n\nNote\n\n\n\n\nIf multiple Sentinel-2 observations from the same day are available, only the most recent observation on that day is used.\nNo pre-filtering (e.g. based on cloud coverage) was performed to preserve as many non-cloudy pixels as possible.\n\n\n\nAccess Sentinel-2 Level 3 Quarterly Mosaics with Sentinel Hub\n\n\nAccess Sentinel-2 Level 3 Quarterly Mosaics with Sentinel Hub\nSentinel-2 Level 3 Quarterly Mosaics are onboarded to Sentinel Hub as a BYOC data collection. To access the data, you will need the specific pieces of information listed below, for general information about how to access BYOC collections visit our Data BYOC page.\n\nData collection id: byoc-5460de54-082e-473a-b6ea-d5cbe3c17cca\nAvailable Bands and Data:\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nResolution\n\n\n\n\n\n\nB02\n\n\nBlue\n\n\n10 m\n\n\n\n\nB03\n\n\nGreen\n\n\n10 m\n\n\n\n\nB04\n\n\nRed\n\n\n10 m\n\n\n\n\nB08\n\n\nNear Infrared (NIR)\n\n\n10 m\n\n\n\n\nobservations\n\n\nNumber of valid observations\n\n\n10 m\n\n\n\n\ndataMask\n\n\nThe mask of data/no data pixels (more).\n\n\nN/A*\n\n\n\n\n\n*dataMask has no source resolution as it is calculated for each output pixel.\n\nExample of requesting mosaic over Rome with Processing API request\nThe request below is written in python. To execute it, you need to create an OAuth client as is explained here. It is named oauth in this example.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 }\n  };\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04/10000, 2.5 * sample.B03/10000, 2.5 * sample.B02/10000];\n}\n\"\"\"\n\nrequest = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [\n        12.44693,\n        41.870072,\n        12.541001,\n        41.917096\n      ]\n    },\n    \"data\": [\n      {\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2023-01-01T00:00:00Z\",\n            \"to\": \"2023-01-02T23:59:59Z\"\n          }\n        },\n        \"type\": \"byoc-5460de54-082e-473a-b6ea-d5cbe3c17cca\"\n      }\n    ]\n  },\n  \"output\": {\n    \"width\": 780,\n    \"height\": 523,\n    \"responses\": [\n      {\n        \"identifier\": \"default\",\n        \"format\": {\n          \"type\": \"image/jpeg\"\n        }\n      }\n    ]\n  },\n  \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "Data/ComplementaryData/CMEMS.html",
    "href": "Data/ComplementaryData/CMEMS.html",
    "title": "Copernicus Marine Environment Monitoring Service (CMEMS)",
    "section": "",
    "text": "The Copernicus Marine Environment Monitoring Service (CMEMS) provides open, free, regular and systematic reference data on the blue (physical), white (sea ice), and green (biogeochemical) state of the marine environment, as well as data on variability and dynamics across the global ocean and European seas.\nCopernicus Data Space Ecosystem data catalogue provides resources for water analysis in all dimensions, from local to global and from visible to radar techniques. The CMEMS products available via Copernicus Data Space Ecosystem platform are particularly dedicated to water application.\nTwo main kinds of products are offered by Copernius Data Space Ecosystem: Near Real Time (NRT) and Reprocessed including climate analysis (REP)."
  },
  {
    "objectID": "Data/ComplementaryData/CMEMS.html#copernicus-marine-environment-monitoring-service-cmems---near-real-time-nrt",
    "href": "Data/ComplementaryData/CMEMS.html#copernicus-marine-environment-monitoring-service-cmems---near-real-time-nrt",
    "title": "Copernicus Marine Environment Monitoring Service (CMEMS)",
    "section": "Copernicus Marine Environment Monitoring Service (CMEMS) - Near Real Time (NRT)",
    "text": "Copernicus Marine Environment Monitoring Service (CMEMS) - Near Real Time (NRT)\n\nOverview\nThe CMEMS Near Real Time (NRT) service provides oceanic and marine data in near real-time, with a delay of typically less than three hours from the time of observation. The NRT service is particularly important for emergency response operations and weather forecasting, as well as for maritime transport, fisheries, oil and gas exploration, and other applications where near real-time data is critical. The CMEMS NRT service utilizes a network of in situ (buoys, drifters, etc.) and satellite-based sensors to collect high-quality oceanic and marine data in near real-time.\n\nOffered Data\n\n\nGLO - Global\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nGLOBAL_ANALYSIS_FORECAST_BIO_001_028\n\n\nJan 2018 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/BIO/GLOBAL_ANALYSIS_FORECAST_BIO_001_028/\n\n\nDetails\n\n\n\n\nGLOBAL_ANALYSIS_FORECAST_BIO_001_014\n\n\nDec 2011 - Jan 2021\n\n\n/eodata/CMEMS/NRT/GLO/BIO/GLOBAL_ANALYSIS_FORECAST_BIO_001_014/\n\n\nN/A\n\n\n\n\nCAR (Carbon)\n\n\nINSITU_GLO_CARBON_NRT_OBSERVATIONS_013_049\n\n\nMar 2019 - Dec 2021\n\n\n/eodata/CMEMS/NRT/GLO/CAR/INSITU_GLO_CARBON_NRT_OBSERVATIONS_013_049/\n\n\nN/A\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_GLO_CHL_L3_NRT_OBSERVATIONS_009_032\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/GLO/CHL/OCEANCOLOUR_GLO_CHL_L3_NRT_OBSERVATIONS_009_032/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_CHL_L4_NRT_OBSERVATIONS_009_033\n\n\nJan 2016 - Jan 2022\n\n\n/eodata/CMEMS/NRT/GLO/CHL/OCEANCOLOUR_GLO_CHL_L4_NRT_OBSERVATIONS_009_033/\n\n\nN/A\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_GLO_NRT_OBSERVATIONS_013_030\n\n\nFeb 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/GLO/OBS/INSITU_GLO_NRT_OBSERVATIONS_013_030/\n\n\nDetails\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_GLO_OPTICS_L3_NRT_OBSERVATIONS_009_030\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/GLO/OPT/OCEANCOLOUR_GLO_OPTICS_L3_NRT_OBSERVATIONS_009_030/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_OPTICS_L4_NRT_OBSERVATIONS_009_083\n\n\nJan 2016 - Jun 2022\n\n\n/eodata/CMEMS/NRT/GLO/OPT/OCEANCOLOUR_GLO_OPTICS_L4_NRT_OBSERVATIONS_009_083/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nGLOBAL_ANALYSIS_FORECAST_PHY_001_024\n\n\nAug 2018 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/PHY/GLOBAL_ANALYSIS_FORECAST_PHY_001_024/\n\n\nDetails\n\n\n\n\nGLOBAL_ANALYSIS_FORECAST_PHYS_001_015\n\n\nDec 2015 - Jan 2022\n\n\n/eodata/CMEMS/NRT/GLO/PHY/GLOBAL_ANALYSIS_FORECAST_PHYS_001_015/\n\n\nN/A\n\n\n\n\nMULTIOBS_GLO_PHY_NRT_015_001\n\n\nJan 2018 - Mar 2023\n\n\n/eodata/CMEMS/NRT/GLO/PHY/MULTIOBS_GLO_PHY_NRT_015_001/\n\n\nN/A\n\n\n\n\nMULTIOBS_GLO_PHY_NRT_015_003\n\n\nMay 2019 - Aug 2019\n\n\n/eodata/CMEMS/NRT/GLO/PHY/MULTIOBS_GLO_PHY_NRT_015_003/\n\n\nDetails\n\n\n\n\nSEALEVEL_GLO_PHY_L4_NRT_OBSERVATIONS_008_046\n\n\nApr 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/PHY/SEALEVEL_GLO_PHY_L4_NRT_OBSERVATIONS_008_046/\n\n\nDetails\n\n\n\n\nSI (Sea Ice)\n\n\nSEAICE_GLO_SEAICE_L4_NRT_OBSERVATIONS_011_001\n\n\nMar 2005 Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/SI/SEAICE_GLO_SEAICE_L4_NRT_OBSERVATIONS_011_001/\n\n\nDetails\n\n\n\n\nSEAICE_GLO_SEAICE_L4_NRT_OBSERVATIONS_011_006\n\n\nMar 2010 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/SI/SEAICE_GLO_SEAICE_L4_NRT_OBSERVATIONS_011_006/\n\n\nDetails\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_GLO_SST_L3S_NRT_OBSERVATIONS_010_010\n\n\nJan 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/GLO/SST/SST_GLO_SST_L3S_NRT_OBSERVATIONS_010_010/\n\n\nDetails\n\n\n\n\nSST_GLO_SST_L4_NRT_OBSERVATIONS_010_001\n\n\nJan 2007 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/SST/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001/\n\n\nDetails\n\n\n\n\nSST_GLO_SST_L4_NRT_OBSERVATIONS_010_005\n\n\nSep 2017 - Dec 2022\n\n\n/eodata/CMEMS/NRT/GLO/SST/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_005/\n\n\nN/A\n\n\n\n\nSST_GLO_SST_L4_NRT_OBSERVATIONS_010_014\n\n\nFeb 2015 - Dec 2022\n\n\n/eodata/CMEMS/NRT/GLO/SST/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_014/\n\n\nN/A\n\n\n\n\nTS_OA (Real time in-situ observations objective analysis)\n\n\nINSITU_GLO_TS_OA_NRT_OBSERVATIONS_013_002_a\n\n\nJan 2015 - Dec 2022\n\n\n/eodata/CMEMS/NRT/GLO/TS_OA/INSITU_GLO_TS_OA_NRT_OBSERVATIONS_013_002_a/\n\n\nDetails\n\n\n\n\nUV (water velocity)\n\n\nINSITU_GLO_UV_NRT_OBSERVATIONS_013_048\n\n\nJan 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/GLO/UV/INSITU_GLO_UV_NRT_OBSERVATIONS_013_048/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nGLOBAL_ANALYSIS_FORECAST_WAV_001_023\n\n\nApr 2015 - Aug 2019\n\n\n/eodata/CMEMS/NRT/GLO/WAV/GLOBAL_ANALYSIS_FORECAST_WAV_001_023/\n\n\nN/A\n\n\n\n\nGLOBAL_ANALYSIS_FORECAST_WAV_001_027\n\n\nMay 2017 - Mar 2023\n\n\n/eodata/CMEMS/NRT/GLO/WAV/GLOBAL_ANALYSIS_FORECAST_WAV_001_027/\n\n\nDetails\n\n\n\n\nGLOBAL_ANALYSIS_FORECAST_WAV_001_028\n\n\nAug 2019 - Aug 2019\n\n\n/eodata/CMEMS/NRT/GLO/WAV/GLOBAL_ANALYSIS_FORECAST_WAV_001_028/\n\n\nDetails\n\n\n\n\nSEALEVEL_GLO_WAV_L3_NRT_OBSERVATIONS_008_052\n\n\nJul 2017 - Nov 2018\n\n\n/eodata/CMEMS/NRT/GLO/WAV/SEALEVEL_GLO_WAV_L3_NRT_OBSERVATIONS_008_052/\n\n\nN/A\n\n\n\n\nWAVE_GLO_WAV_L3_SPC_NRT_OBSERVATIONS_014_002\n\n\nApr 2018 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/WAV_SPC/WAVE_GLO_WAV_L3_SPC_NRT_OBSERVATIONS_014_002/\n\n\nDetails\n\n\n\n\nWAVE_GLO_WAV_L3_SWH_NRT_OBSERVATIONS_014_001\n\n\nOct 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/WAV_SWH/WAVE_GLO_WAV_L3_SWH_NRT_OBSERVATIONS_014_001/\n\n\nDetails\n\n\n\n\nWAVE_GLO_WAV_L4_SWH_NRT_OBSERVATIONS_014_003\n\n\nJun 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/WAV_SWH/WAVE_GLO_WAV_L4_SWH_NRT_OBSERVATIONS_014_003/\n\n\nDetails\n\n\n\n\nWIN (Wind)\n\n\nWIND_GLO_WIND_L3_NRT_OBSERVATIONS_012_002\n\n\nJan 2016 - Apr 2023\n\n\n/eodata/CMEMS/NRT/GLO/WIN/WIND_GLO_WIND_L3_NRT_OBSERVATIONS_012_002/\n\n\nDetails\n\n\n\n\nWIND_GLO_WIND_L4_NRT_OBSERVATIONS_012_004\n\n\nJan 2018 - Nov 2021\n\n\n/eodata/CMEMS/NRT/GLO/WIN/WIND_GLO_WIND_L4_NRT_OBSERVATIONS_012_004/\n\n\nDetails\n\n\n\n\n\n\nANT - Antarctic Ocean\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nSI (Sea Ice)\n\n\nSEAICE_ANT_SEAICE_L4_NRT_OBSERVATIONS_011_012\n\n\nJan 2011 - Mar 2022\n\n\n/eodata/CMEMS/NRT/ANT/SI/SEAICE_ANT_SEAICE_L4_NRT_OBSERVATIONS_011_012/\n\n\nDetails\n\n\n\n\n\n\nARC - Arctic Ocean\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nARCTIC_ANALYSISFORECAST_BGC_002_004\n\n\nJan 2019 - May 2023\n\n\n/eodata/CMEMS/NRT/ARC/BGC/ARCTIC_ANALYSISFORECAST_BGC_002_004/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_ARC_BGC_HR_L3_NRT_009_201\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/BGC/OCEANCOLOUR_ARC_BGC_HR_L3_NRT_009_201/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_ARC_BGC_HR_L4_NRT_009_207\n\n\nJan 2020 - Mar 2023\n\n\n/eodata/CMEMS/NRT/ARC/BGC/OCEANCOLOUR_ARC_BGC_HR_L4_NRT_009_207/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nARCTIC_ANALYSIS_FORECAST_BIO_002_004\n\n\nDec 2011 - Jul 2021\n\n\n/eodata/CMEMS/NRT/ARC/BIO/ARCTIC_ANALYSIS_FORECAST_BIO_002_004/\n\n\nDetails\n\n\n\n\nARCTIC_ANALYSIS_FORECAST_WAV_002_006\n\n\nJul 2019 - Aug 2019\n\n\n/eodata/CMEMS/NRT/ARC/BIO/ARCTIC_ANALYSIS_FORECAST_WAV_002_006/\n\n\nN/A\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_ARC_CHL_L3_NRT_OBSERVATIONS_009_047\n\n\nMar 2016 - Aug 2022\n\n\n/eodata/CMEMS/NRT/ARC/CHL/OCEANCOLOUR_ARC_CHL_L3_NRT_OBSERVATIONS_009_047/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ARC_CHL_L4_NRT_OBSERVATIONS_009_087\n\n\nApr 2016 - May 2022\n\n\n/eodata/CMEMS/NRT/ARC/CHL/OCEANCOLOUR_ARC_CHL_L4_NRT_OBSERVATIONS_009_087/\n\n\nN/A\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_ARC_NRT_OBSERVATIONS_013_031\n\n\nJan 1990 - Jan 2023\n\n\n/eodata/CMEMS/NRT/ARC/OPT/INSITU_ARC_NRT_OBSERVATIONS_013_031/\n\n\nDetails\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_ARC_OPTICS_L3_NRT_OBSERVATIONS_009_046\n\n\nMar 2016 - Aug 2022\n\n\n/eodata/CMEMS/NRT/ARC/OPT/OCEANCOLOUR_ARC_OPTICS_L3_NRT_OBSERVATIONS_009_046/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ARC_OPTICS_L4_NRT_OBSERVATIONS_009_089\n\n\nApr 2016 - May 2022\n\n\n/eodata/CMEMS/NRT/ARC/OPT/OCEANCOLOUR_ARC_OPTICS_L4_NRT_OBSERVATIONS_009_089/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nARCTIC_ANALYSIS_FORECAST_PHYS_002_001_a\n\n\nOct 2016 - May 2023\n\n\n/eodata/CMEMS/NRT/ARC/PHY/ARCTIC_ANALYSIS_FORECAST_PHYS_002_001_a/\n\n\nDetails\n\n\n\n\nARCTIC_ANALYSISFORECAST_PHY_ICE_002_011\n\n\nNov 2018 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/PHY/ARCTIC_ANALYSISFORECAST_PHY_ICE_002_011/\n\n\nDetails\n\n\n\n\nSEAICE_ARC_PHY_AUTO_L4_NRT_011_015\n\n\nDec 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/PHY/SEAICE_ARC_PHY_AUTO_L4_NRT_011_015/\n\n\nDetails\n\n\n\n\nSI (Sea Ice)\n\n\nSEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_002\n\n\nJan 2010 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/SI/SEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_002/\n\n\nDetails\n\n\n\n\nSEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_003\n\n\nJun 2008 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/SI/SEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_003/\n\n\nN/A\n\n\n\n\nSEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_007\n\n\nApr 2010 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/SI/SEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_007/\n\n\nDetails\n\n\n\n\nSEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_008\n\n\nJan 1982 - Apr 2023\n\n\n/eodata/CMEMS/NRT/ARC/SI/SEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_008/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nARCTIC_ANALYSIS_FORECAST_WAV_002_006\n\n\nDec 2016 - Feb 2020\n\n\n/eodata/CMEMS/NRT/ARC/WAV/ARCTIC_ANALYSIS_FORECAST_WAV_002_006/\n\n\nN/A\n\n\n\n\nARCTIC_ANALYSIS_FORECAST_WAV_002_010\n\n\nDec 2016 - Feb 2020\n\n\n/eodata/CMEMS/NRT/ARC/WAV/ARCTIC_ANALYSIS_FORECAST_WAV_002_010/\n\n\nN/A\n\n\n\n\n\n\nATL - Atlantic North\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_ATL_CHL_L3_NRT_OBSERVATIONS_009_036\n\n\nFeb 2016 - Aug 2022\n\n\n/eodata/CMEMS/NRT/ATL/CHL/OCEANCOLOUR_ATL_CHL_L3_NRT_OBSERVATIONS_009_036/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ATL_CHL_L4_NRT_OBSERVATIONS_009_037\n\n\nMay 2019 - Aug 2021\n\n\n/eodata/CMEMS/NRT/ATL/CHL/OCEANCOLOUR_ATL_CHL_L4_NRT_OBSERVATIONS_009_037/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ATL_CHL_L4_NRT_OBSERVATIONS_009_090\n\n\nApr 2016 - May 2022\n\n\n/eodata/CMEMS/NRT/ATL/CHL/OCEANCOLOUR_ATL_CHL_L4_NRT_OBSERVATIONS_009_090/\n\n\nN/A\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_ATL_OPTICS_L3_NRT_OBSERVATIONS_009_034\n\n\nApr 2016 - Aug 2022\n\n\n/eodata/CMEMS/NRT/ATL/OPT/OCEANCOLOUR_ATL_OPTICS_L3_NRT_OBSERVATIONS_009_034/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ATL_OPTICS_L4_NRT_OBSERVATIONS_009_092\n\n\nApr 2016 - May 2022\n\n\n/eodata/CMEMS/NRT/ATL/OPT/OCEANCOLOUR_ATL_OPTICS_L4_NRT_OBSERVATIONS_009_092/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_ATL_SST_L4_NRT_OBSERVATIONS_010_025\n\n\nJan 2018 - Nov 2022\n\n\n/eodata/CMEMS/NRT/ATL/SST/SST_ATL_SST_L4_NRT_OBSERVATIONS_010_025/\n\n\nDetails\n\n\n\n\n\n\nBAL - Baltic Sea\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nOCEANCOLOUR_BAL_BGC_HR_L3_NRT_009_202\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BAL/BGC/OCEANCOLOUR_BAL_BGC_HR_L3_NRT_009_202/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_BAL_BGC_HR_L4_NRT_009_208\n\n\nFeb 2020 - Jan 2023\n\n\n/eodata/CMEMS/NRT/BAL/BGC/OCEANCOLOUR_BAL_BGC_HR_L4_NRT_009_208/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nBALTICSEA_ANALYSIS_FORECAST_BIO_003_007\n\n\nMar 2016 - Feb 2021\n\n\n/eodata/CMEMS/NRT/BAL/BIO/BALTICSEA_ANALYSIS_FORECAST_BIO_003_007/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_BAL_CHL_L3_NRT_OBSERVATIONS_009_049\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/BAL/CHL/OCEANCOLOUR_BAL_CHL_L3_NRT_OBSERVATIONS_009_049/\n\n\nN/A\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_BAL_NRT_OBSERVATIONS_013_032\n\n\nJan 2015 - Jan 2023\n\n\n/eodata/CMEMS/NRT/BAL/OBS/INSITU_BAL_NRT_OBSERVATIONS_013_032/\n\n\nDetails\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_BAL_OPTICS_L3_NRT_OBSERVATIONS_009_048\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/BAL/OPT/OCEANCOLOUR_BAL_OPTICS_L3_NRT_OBSERVATIONS_009_048/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nBALTICSEA_ANALYSIS_FORECAST_PHY_003_006\n\n\nMar 2016 - Feb 2021\n\n\n/eodata/CMEMS/NRT/BAL/PHY/BALTICSEA_ANALYSIS_FORECAST_PHY_003_006/\n\n\nDetails\n\n\n\n\nSI (Sea Ice)\n\n\nSEAICE_BAL_SEAICE_L4_NRT_OBSERVATIONS_011_004\n\n\nJan 2010 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BAL/SI/SEAICE_BAL_SEAICE_L4_NRT_OBSERVATIONS_011_004/\n\n\nDetails\n\n\n\n\nSEAICE_BAL_SEAICE_L4_NRT_OBSERVATIONS_011_011\n\n\nJan 2011 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BAL/SI/SEAICE_BAL_SEAICE_L4_NRT_OBSERVATIONS_011_011/\n\n\nDetails\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_BAL_SST_L3S_NRT_OBSERVATIONS_010_032\n\n\nMar 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BAL/SST/SST_BAL_SST_L3S_NRT_OBSERVATIONS_010_032/\n\n\nDetails\n\n\n\n\nSST_BAL_SST_L4_NRT_OBSERVATIONS_010_007_b\n\n\nJan 2016 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BAL/SST/SST_BAL_SST_L4_NRT_OBSERVATIONS_010_007_b/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nBALTICSEA_ANALYSIS_FORECAST_WAV_003_010\n\n\nJul 2017 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BAL/WAV/BALTICSEA_ANALYSIS_FORECAST_WAV_003_010/\n\n\nDetails\n\n\n\n\n\n\nBLA, BLK and BS (Black Sea)\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nOCEANCOLOUR_BLK_BGC_HR_L3_NRT_009_206\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BLK/BGC/OCEANCOLOUR_BLK_BGC_HR_L3_NRT_009_206/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_BLK_BGC_HR_L4_NRT_009_212\n\n\nJan 2020 - Mar 2023\n\n\n/eodata/CMEMS/NRT/BLK/BGC/OCEANCOLOUR_BLK_BGC_HR_L4_NRT_009_212/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nBLKSEA_ANALYSIS_FORECAST_BIO_007_009\n\n\nMay 2019 - Aug 2019\n\n\n/eodata/CMEMS/NRT/BLA/BIO/BLKSEA_ANALYSIS_FORECAST_BIO_007_009/\n\n\nN/A\n\n\n\n\nBLKSEA_ANALYSIS_FORECAST_BIO_007_010\n\n\nJan 2018 - Feb 2023\n\n\n/eodata/CMEMS/NRT/BLA/BIO/BLKSEA_ANALYSIS_FORECAST_BIO_007_010/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_BS_CHL_L3_NRT_OBSERVATIONS_009_044\n\n\nJun 2013 - Sep 2022\n\n\n/eodata/CMEMS/NRT/BLA/CHL/OCEANCOLOUR_BS_CHL_L3_NRT_OBSERVATIONS_009_044/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_BS_CHL_L4_NRT_OBSERVATIONS_009_045\n\n\nJun 2013 - Sep 2022\n\n\n/eodata/CMEMS/NRT/BLA/CHL/OCEANCOLOUR_BS_CHL_L4_NRT_OBSERVATIONS_009_045/\n\n\nN/A\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_BS_NRT_OBSERVATIONS_013_034\n\n\nJan 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/BLA/OBS/INSITU_BS_NRT_OBSERVATIONS_013_034/\n\n\nDetails\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_BS_OPTICS_L3_NRT_OBSERVATIONS_009_042\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/BLA/OPT/OCEANCOLOUR_BS_OPTICS_L3_NRT_OBSERVATIONS_009_042/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_BS_OPTICS_L4_NRT_OBSERVATIONS_009_043\n\n\nJan 2016 - May 2022\n\n\n/eodata/CMEMS/NRT/BLA/OPT/OCEANCOLOUR_BS_OPTICS_L4_NRT_OBSERVATIONS_009_043/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nBLKSEA_ANALYSISFORECAST_PHY_007_001\n\n\nJan 2019 - Feb 2023\n\n\n/eodata/CMEMS/NRT/BLA/PHY/BLKSEA_ANALYSISFORECAST_PHY_007_001/\n\n\nDetails\n\n\n\n\nSEALEVEL_BS_PHY_L3_NRT_OBSERVATIONS_008_039\n\n\nMar 2017 - Jun 2019\n\n\n/eodata/CMEMS/NRT/BLA/PHY/SEALEVEL_BS_PHY_L3_NRT_OBSERVATIONS_008_039/\n\n\nN/A\n\n\n\n\nSEALEVEL_BS_PHY_L4_NRT_OBSERVATIONS_008_041\n\n\nJan 2017 - Jun 2019\n\n\n/eodata/CMEMS/NRT/BLA/PHY/SEALEVEL_BS_PHY_L4_NRT_OBSERVATIONS_008_041/\n\n\n\n\n\n\nSEALEVEL_BLK_PHY_MDT_L4_STATIC_008_067\n\n\nJan 1970 - Jan 1970\n\n\n/eodata/CMEMS/NRT/BLK/PHY/SEALEVEL_BLK_PHY_MDT_L4_STATIC_008_067/\n\n\nDetails\n\n\n\n\nSST_BS_PHY_SUBSKIN_L4_NRT_010_035\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/BS/PHY/SST_BS_PHY_SUBSKIN_L4_NRT_010_035/\n\n\nDetails\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_BS_SST_L3S_NRT_OBSERVATIONS_010_013\n\n\nJan 2008 -Apr 2023\n\n\n/eodata/CMEMS/NRT/BLA/SST/SST_BS_SST_L3S_NRT_OBSERVATIONS_010_013/\n\n\nDetails\n\n\n\n\nSST_BS_SST_L4_NRT_OBSERVATIONS_010_006\n\n\nJan 2008 -Apr 2023\n\n\n/eodata/CMEMS/NRT/BLA/SST/SST_BS_SST_L4_NRT_OBSERVATIONS_010_006/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nBLKSEA_ANALYSIS_FORECAST_WAV_007_003\n\n\nJul 2018 -Feb 2021\n\n\n/eodata/CMEMS/NRT/BLA/WAV/BLKSEA_ANALYSIS_FORECAST_WAV_007_003/\n\n\nDetails\n\n\n\n\n\n\nEUR - EUROPE\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_EUR_CHL_L3_NRT_OBSERVATIONS_009_050\n\n\nJan 2017 - Jun 2022\n\n\n/eodata/CMEMS/NRT/EUR/CHL/OCEANCOLOUR_EUR_CHL_L3_NRT_OBSERVATIONS_009_050/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nSEALEVEL_EUR_PHY_L3_NRT_OBSERVATIONS_008_059\n\n\nApr 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/EUR/PHY/SEALEVEL_EUR_PHY_L3_NRT_OBSERVATIONS_008_059/\n\n\nDetails\n\n\n\n\nSEALEVEL_EUR_PHY_L4_NRT_OBSERVATIONS_008_060\n\n\nApr 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/EUR/PHY/SEALEVEL_EUR_PHY_L4_NRT_OBSERVATIONS_008_060/\n\n\nDetails\n\n\n\n\nPHY_ASS (Physics_assimilation)\n\n\nSEALEVEL_EUR_PHY_ASSIM_L3_NRT_OBSERVATIONS_008_043\n\n\nMar 2017 - Jun 2019\n\n\n/eodata/CMEMS/NRT/EUR/PHY_ASS/SEALEVEL_EUR_PHY_ASSIM_L3_NRT_OBSERVATIONS_008_043/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_EUR_SST_L3C_NRT_OBSERVATIONS_010_009_b\n\n\nAug 2010 - Jan 2022\n\n\n/eodata/CMEMS/NRT/EUR/SST/SST_EUR_SST_L3C_NRT_OBSERVATIONS_010_009_b/\n\n\nN/A\n\n\n\n\nSST_EUR_SST_L3S_NRT_OBSERVATIONS_010_009_a\n\n\nJan 2016 - Jan 2022\n\n\n/eodata/CMEMS/NRT/EUR/SST/SST_EUR_SST_L3S_NRT_OBSERVATIONS_010_009_a/\n\n\nN/A\n\n\n\n\n\n\nIBI - Atlantic: Iberia-Biscay-Ireland\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nOCEANCOLOUR_IBI_BGC_HR_L3_NRT_009_204\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/IBI/BGC/OCEANCOLOUR_IBI_BGC_HR_L3_NRT_009_204/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_IBI_BGC_HR_L4_NRT_009_210\n\n\nJan 2020 - Mar 2023\n\n\n/eodata/CMEMS/NRT/IBI/BGC/OCEANCOLOUR_IBI_BGC_HR_L4_NRT_009_210/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nIBI_ANALYSIS_FORECAST_BIO_005_004\n\n\nDec 2015 - Feb 2021\n\n\n/eodata/CMEMS/NRT/IBI/BIO/IBI_ANALYSIS_FORECAST_BIO_005_004/\n\n\nDetails\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_IBI_NRT_OBSERVATIONS_013_033\n\n\nJan 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/IBI/OBS/INSITU_IBI_NRT_OBSERVATIONS_013_033/\n\n\nDetails\n\n\n\n\nPHY (Physics)\n\n\nIBI_ANALYSIS_FORECAST_PHYS_005_001\n\n\nDec 2011 - Feb 2021\n\n\n/eodata/CMEMS/NRT/IBI/PHY/IBI_ANALYSIS_FORECAST_PHYS_005_001/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nIBI_ANALYSIS_FORECAST_WAV_005_005\n\n\nJan 2015 - Apr 2023\n\n\n/eodata/CMEMS/NRT/IBI/WAV/IBI_ANALYSIS_FORECAST_WAV_005_005/\n\n\nDetails\n\n\n\n\n\n\nMED - Mediterranean Sea\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nMEDSEA_ANALYSISFORECAST_BGC_006_014\n\n\nNov 2019 - Feb 2023\n\n\n/eodata/CMEMS/NRT/MED/BGC/MEDSEA_ANALYSISFORECAST_BGC_006_014/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_MED_BGC_HR_L3_NRT_009_205\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/MED/BGC/OCEANCOLOUR_MED_BGC_HR_L3_NRT_009_205/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_MED_BGC_HR_L4_NRT_009_211\n\n\nJan 2020 - Mar 2023\n\n\n/eodata/CMEMS/NRT/MED/BGC/OCEANCOLOUR_MED_BGC_HR_L4_NRT_009_211/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nMEDSEA_ANALYSIS_FORECAST_BIO_006_014\n\n\nJul 2017 - Aug 2019\n\n\n/eodata/CMEMS/NRT/MED/BIO/MEDSEA_ANALYSIS_FORECAST_BIO_006_014/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_MED_CHL_L3_NRT_OBSERVATIONS_009_040\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/MED/CHL/OCEANCOLOUR_MED_CHL_L3_NRT_OBSERVATIONS_009_040/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_MED_CHL_L4_NRT_OBSERVATIONS_009_041\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/MED/CHL/OCEANCOLOUR_MED_CHL_L4_NRT_OBSERVATIONS_009_041/\n\n\nN/A\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_MED_NRT_OBSERVATIONS_013_035\n\n\nJan 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/MED/OBS/INSITU_MED_NRT_OBSERVATIONS_013_035/\n\n\nDetails\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_MED_OPTICS_L3_NRT_OBSERVATIONS_009_038\n\n\nApr 2016 - Sep 2022\n\n\n/eodata/CMEMS/NRT/MED/OPT/OCEANCOLOUR_MED_OPTICS_L3_NRT_OBSERVATIONS_009_038/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_MED_OPTICS_L4_NRT_OBSERVATIONS_009_039\n\n\nApr 2016 - May 2022\n\n\n/eodata/CMEMS/NRT/MED/OPT/OCEANCOLOUR_MED_OPTICS_L4_NRT_OBSERVATIONS_009_039/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nMEDSEA_ANALYSIS_FORECAST_PHY_006_013\n\n\nJul 2017 - Aug 2019\n\n\n/eodata/CMEMS/NRT/MED/PHY/MEDSEA_ANALYSIS_FORECAST_PHY_006_013/\n\n\nDetails\n\n\n\n\nMEDSEA_ANALYSISFORECAST_PHY_006_013\n\n\nMay 2019 - Feb 2023\n\n\n/eodata/CMEMS/NRT/MED/PHY/MEDSEA_ANALYSISFORECAST_PHY_006_013/\n\n\nDetails\n\n\n\n\nSEALEVEL_MED_PHY_L4_NRT_OBSERVATIONS_008_050\n\n\nJan 2017 -Jun 2019\n\n\n/eodata/CMEMS/NRT/MED/PHY/SEALEVEL_MED_PHY_L4_NRT_OBSERVATIONS_008_050/\n\n\nN/A\n\n\n\n\nSEALEVEL_MED_PHY_MDT_L4_STATIC_008_066\n\n\nJan 1970 - Jan 1970\n\n\n/eodata/CMEMS/NRT/MED/PHY/SEALEVEL_MED_PHY_MDT_L4_STATIC_008_066/\n\n\nDetails\n\n\n\n\nSST_MED_PHY_SUBSKIN_L4_NRT_010_036\n\n\nJan 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/MED/PHY/SST_MED_PHY_SUBSKIN_L4_NRT_010_036/\n\n\nDetails\n\n\n\n\nPHY_ASS (Physics_assimilation)\n\n\nSEALEVEL_MED_PHY_ASSIM_L3_NRT_OBSERVATIONS_008_048\n\n\nMar 2017 - Jun 2019\n\n\n/eodata/CMEMS/NRT/MED/PHY_ASS/SEALEVEL_MED_PHY_ASSIM_L3_NRT_OBSERVATIONS_008_048/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_MED_SST_L3S_NRT_OBSERVATIONS_010_012\n\n\nJan 2008 - Apr 2023\n\n\n/eodata/CMEMS/NRT/MED/SST/SST_MED_SST_L3S_NRT_OBSERVATIONS_010_012/\n\n\nDetails\n\n\n\n\nSST_MED_SST_L4_NRT_OBSERVATIONS_010_004\n\n\nJan 2008 - Apr 2023\n\n\n/eodata/CMEMS/NRT/MED/SST/SST_MED_SST_L4_NRT_OBSERVATIONS_010_004/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nMEDSEA_ANALYSIS_FORECAST_WAV_006_011\n\n\nJul 2017 - May 2018\n\n\n/eodata/CMEMS/NRT/MED/WAV/MEDSEA_ANALYSIS_FORECAST_WAV_006_011/\n\n\nN/A\n\n\n\n\nMEDSEA_ANALYSIS_FORECAST_WAV_006_017\n\n\nJul 2017 - Aug 2019\n\n\n/eodata/CMEMS/NRT/MED/WAV/MEDSEA_ANALYSIS_FORECAST_WAV_006_017/\n\n\nDetails\n\n\n\n\nMEDSEA_ANALYSISFORECAST_WAV_006_017\n\n\nNov 2019 - Nov 2022\n\n\n/eodata/CMEMS/NRT/MED/WAV/MEDSEA_ANALYSISFORECAST_WAV_006_017/\n\n\nDetails\n\n\n\n\n\n\nNWS (Atlantic: NW European Shelf)\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nNWSHELF_ANALYSISFORECAST_BGC_004_002\n\n\nMay 2019 - Apr 2023\n\n\n/eodata/CMEMS/NRT/NWS/BGC/NWSHELF_ANALYSISFORECAST_BGC_004_002/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_NWS_BGC_HR_L3_NRT_009_203\n\n\nJan 2020 - Apr 2023\n\n\n/eodata/CMEMS/NRT/NWS/BGC/OCEANCOLOUR_NWS_BGC_HR_L3_NRT_009_203/\n\n\nDetails\n\n\n\n\nOCEANCOLOUR_NWS_BGC_HR_L4_NRT_009_209\n\n\nJan 2020 - Mar 2023\n\n\n/eodata/CMEMS/NRT/NWS/BGC/OCEANCOLOUR_NWS_BGC_HR_L4_NRT_009_209/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nNORTHWESTSHELF_ANALYSIS_FORECAST_BIO_004_002_b\n\n\nJul 2018 - Aug 2021\n\n\n/eodata/CMEMS/NRT/NWS/BIO/NORTHWESTSHELF_ANALYSIS_FORECAST_BIO_004_002_b/\n\n\nDetails\n\n\n\n\nOBS (In-situ observation)\n\n\nINSITU_NWS_NRT_OBSERVATIONS_013_036\n\n\nJan 2016 - Jan 2023\n\n\n/eodata/CMEMS/NRT/NWS/OBS/INSITU_NWS_NRT_OBSERVATIONS_013_036/\n\n\nDetails\n\n\n\n\nPHY (Physics)\n\n\nNORTHWESTSHELF_ANALYSIS_FORECAST_PHY_004_013\n\n\nAug 2018 -Apr 2023\n\n\n/eodata/CMEMS/NRT/NWS/PHY/NORTHWESTSHELF_ANALYSIS_FORECAST_PHY_004_013/\n\n\nDetails\n\n\n\n\nNORTHWESTSHELF_ANALYSIS_FORECAST_PHYS_004_001_b\n\n\nJan 2018 - Aug 2021\n\n\n/eodata/CMEMS/NRT/NWS/PHY/NORTHWESTSHELF_ANALYSIS_FORECAST_PHYS_004_001_b/\n\n\nN/A\n\n\n\n\nNWSHELF_ANALYSISFORECAST_PHY_LR_004_001\n\n\nAug 2021 - Apr 2023\n\n\n/eodata/CMEMS/NRT/NWS/PHY/NWSHELF_ANALYSISFORECAST_PHY_LR_004_001/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nNORTHWESTSHELF_ANALYSIS_FORECAST_WAV_004_014\n\n\nAug 2018 - Apr 2023\n\n\n/eodata/CMEMS/NRT/NWS/WAV/NORTHWESTSHELF_ANALYSIS_FORECAST_WAV_004_014/\n\n\nDetails\n\n\n\n\n\nN/A - data no longer provided by CMEMS"
  },
  {
    "objectID": "Data/ComplementaryData/CMEMS.html#copernicus-marine-environment-monitoring-service-cmems---reprocessed-rep",
    "href": "Data/ComplementaryData/CMEMS.html#copernicus-marine-environment-monitoring-service-cmems---reprocessed-rep",
    "title": "Copernicus Marine Environment Monitoring Service (CMEMS)",
    "section": "Copernicus Marine Environment Monitoring Service (CMEMS) - Reprocessed (REP)",
    "text": "Copernicus Marine Environment Monitoring Service (CMEMS) - Reprocessed (REP)\n\nOverview\nThe CMEMS Reprocessed (REP) service provides historical oceanic and marine data that has been reprocessed using new versions of algorithms, as well as new reference data and quality assessment procedures. The REP service provides access to high-quality, consistent and homogenized data over long time series, allowing for long-term monitoring and trend analysis. The service is particularly useful for scientists seeking to better understand the impacts of climate change on the ocean, as well as for those involved in marine resource management and policy decisions. The CMEMS REP service utilizes a range of in situ and satellite-based data sources, including historical records of ocean observations, satellite measurements, and model simulations.\n\nOffered Data\n\n\nGLO - Global\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nGLOBAL_MULTIYEAR_BGC_001_033\n\n\nJan 1998 - May 2020\n\n\n/eodata/CMEMS/REP/GLO/BGC/GLOBAL_MULTIYEAR_BGC_001_033/\n\n\nDetails\n\n\n\n\nINSITU_GLO_BGC_REP_OBSERVATIONS_013_046\n\n\nJan 1990 - Dec 2021\n\n\n/eodata/CMEMS/REP/GLO/BGC/INSITU_GLO_BGC_REP_OBSERVATIONS_013_046/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nGLOBAL_REANALYSIS_BIO_001_029\n\n\nJan 1992 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/BIO/GLOBAL_REANALYSIS_BIO_001_029/\n\n\nDetails\n\n\n\n\nGLOBAL_REANALYSIS_BIO_001_033\n\n\nJan 2001 - Jun 2019\n\n\n/eodata/CMEMS/REP/GLO/BIO/GLOBAL_REANALYSIS_BIO_001_033/\n\n\nN/A\n\n\n\n\nMULTIOBS_GLO_BIO_REP_015_005\n\n\nJan 1985 - Dec 2019\n\n\n/eodata/CMEMS/REP/GLO/BIO/MULTIOBS_GLO_BIO_REP_015_005/\n\n\nDetails\n\n\n\n\nMULTIOBS_GLO_BIO_REP_015_006\n\n\nSep 2002 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/BIO/MULTIOBS_GLO_BIO_REP_015_006/\n\n\n\n\n\n\nCAR (Carbon)\n\n\nINSITU_GLO_CARBON_REP_OBSERVATIONS_013_050\n\n\n\n\n/eodata/CMEMS/REP/GLO/CAR/INSITU_GLO_CARBON_REP_OBSERVATIONS_013_050/\n\n\nDetails\n\n\n\n\nMULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008\n\n\nJan 1985 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/CAR/MULTIOBS_GLO_BIO_CARBON_SURFACE_REP_015_008/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_GLO_CHL_L3_REP_OBSERVATIONS_009_065\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/GLO/CHL/OCEANCOLOUR_GLO_CHL_L3_REP_OBSERVATIONS_009_065/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_CHL_L3_REP_OBSERVATIONS_009_085\n\n\nSep 1997 - Jun 2018\n\n\n/eodata/CMEMS/REP/GLO/CHL/OCEANCOLOUR_GLO_CHL_L3_REP_OBSERVATIONS_009_085/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082\n\n\nSep 1997 - Jun 2018\n\n\n/eodata/CMEMS/REP/GLO/CHL/OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_093\n\n\nSep 1997 - Jun 2018\n\n\n/eodata/CMEMS/REP/GLO/CHL/OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_093/\n\n\nN/A\n\n\n\n\nOHC (Ocean Heat Content)\n\n\nGLOBAL_OMI_OHC_anomalies (anomalies in respect to various years)\n\n\nAnomalies in respect to various years\n\n\n/eodata/CMEMS/REP/GLO/OHC/GLOBAL_OMI_OHC_anomalies/\n\n\n\n\n\n\nGLOBAL_OMI_OHC_area_averaged_anomalies (anomalies in respect to various years)\n\n\nAnomalies in respect to various years\n\n\n/eodata/CMEMS/REP/GLO/OHC/GLOBAL_OMI_OHC_area_averaged_anomalies/\n\n\n\n\n\n\nGLOBAL_OMI_OHC_trend (trends in respect to various years)\n\n\nTrends in respect to various years\n\n\n/eodata/CMEMS/REP/GLO/OHC/GLOBAL_OMI_OHC_trend/\n\n\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_GLO_OPTICS_L3_REP_OBSERVATIONS_009_064\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/GLO/OPT/OCEANCOLOUR_GLO_OPTICS_L3_REP_OBSERVATIONS_009_064/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_OPTICS_L3_REP_OBSERVATIONS_009_086\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/GLO/OPT/OCEANCOLOUR_GLO_OPTICS_L3_REP_OBSERVATIONS_009_086/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/GLO/OPT/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nGLOBAL_REANALYSIS_PHY_001_025\n\n\nJan 1993 - Dec 2015\n\n\n/eodata/CMEMS/REP/GLO/PHY/GLOBAL_REANALYSIS_PHY_001_025/\n\n\nN/A\n\n\n\n\nGLOBAL_REANALYSIS_PHY_001_026\n\n\nJan 1993 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/PHY/GLOBAL_REANALYSIS_PHY_001_026/\n\n\nDetails\n\n\n\n\nGLOBAL_REANALYSIS_PHY_001_030\n\n\nJan 1993 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/PHY/GLOBAL_REANALYSIS_PHY_001_030/\n\n\nDetails\n\n\n\n\nGLOBAL_REANALYSIS_PHY_001_031\n\n\nJan 1993 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/PHY/GLOBAL_REANALYSIS_PHY_001_031/\n\n\nDetails\n\n\n\n\nGLOBAL_REANALYSIS_PHY_001_017\n\n\nJan 1979 - Dec 2013\n\n\n/eodata/CMEMS/REP/GLO/PHY/GLOBAL_REANALYSIS_PHYS_001_017/\n\n\nN/A\n\n\n\n\nMULTIOBS_GLO_PHY_REP_015_002\n\n\nJan 1993 - Dec 2018\n\n\n/eodata/CMEMS/REP/GLO/PHY/MULTIOBS_GLO_PHY_REP_015_002/\n\n\nN/A\n\n\n\n\nMULTIOBS_GLO_PHY_REP_015_004\n\n\nJan 1993 - Dec 2021\n\n\n/eodata/CMEMS/REP/GLO/PHY/MULTIOBS_GLO_PHY_REP_015_004/\n\n\nN/A\n\n\n\n\nMULTIOBS_GLO_PHY_W_3D_REP_015_007\n\n\nJan 1993 - Dec 2018\n\n\n/eodata/CMEMS/REP/GLO/PHY/MULTIOBS_GLO_PHY_W_3D_REP_015_007/\n\n\nDetails\n\n\n\n\nSEALEVEL_GLO_PHY_L3_REP_OBSERVATIONS_008_062\n\n\nJan 1993 - Jun 2020\n\n\n/eodata/CMEMS/REP/GLO/PHY/SEALEVEL_GLO_PHY_L3_REP_OBSERVATIONS_008_062/\n\n\nDetails\n\n\n\n\nSEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047\n\n\nJan 1993 - Feb 2022\n\n\n/eodata/CMEMS/REP/GLO/PHY/SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047/\n\n\nDetails\n\n\n\n\nPHY_CLIM (Physics climate)\n\n\nSEALEVEL_GLO_PHY_CLIMATE_L4_REP_OBSERVATIONS_008_057\n\n\nJan 1993 - Feb 2022\n\n\n/eodata/CMEMS/REP/GLO/PHY_CLIM/SEALEVEL_GLO_PHY_CLIMATE_L4_REP_OBSERVATIONS_008_057/\n\n\nDetails\n\n\n\n\nSI (Sea Ice)\n\n\nSEAICE_GLO_SEAICE_L4_REP_OBSERVATIONS_011_009\n\n\nOct 1978 - Dec 2019\n\n\n/eodata/CMEMS/REP/GLO/SI/SEAICE_GLO_SEAICE_L4_REP_OBSERVATIONS_011_009/\n\n\nDetails\n\n\n\n\nSL (Sea level)\n\n\nGLOBAL_OMI_SL_regional_trends (anomalies in respect to various years)\n\n\nJan 1993 - Jan 1993\n\n\n/eodata/CMEMS/REP/GLO/SL/GLOBAL_OMI_SL_regional_trends/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_GLO_SST_L4_REP_OBSERVATIONS_010_011\n\n\nOct 1981 - May 2022\n\n\n/eodata/CMEMS/REP/GLO/SST/SST_GLO_SST_L4_REP_OBSERVATIONS_010_011/\n\n\nDetails\n\n\n\n\nSST_GLO_SST_L4_REP_OBSERVATIONS_010_024\n\n\nNov 1991 - Dec 2010\n\n\n/eodata/CMEMS/REP/GLO/SST/SST_GLO_SST_L4_REP_OBSERVATIONS_010_024/\n\n\nDetails\n\n\n\n\nTS (Temperature and salinity)\n\n\nINSITU_GLO_TS_OA_REP_OBSERVATIONS_013_002_b\n\n\nJan 1990 - Jun 2021\n\n\n/eodata/CMEMS/REP/GLO/TS/INSITU_GLO_TS_OA_REP_OBSERVATIONS_013_002_b/\n\n\nDetails\n\n\n\n\nINSITU_GLO_TS_REP_OBSERVATIONS_013_001_b\n\n\nJan 1950 - Dec 2018\n\n\n/eodata/CMEMS/REP/GLO/TS/INSITU_GLO_TS_REP_OBSERVATIONS_013_001_b/\n\n\nDetails\n\n\n\n\nUV (water velocity)\n\n\nINSITU_GLO_UV_L2_REP_OBSERVATIONS_013_044\n\n\nJan 2012 - Dec 2021\n\n\n/eodata/CMEMS/REP/GLO/UV/INSITU_GLO_UV_L2_REP_OBSERVATIONS_013_044/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nGLOBAL_REANALYSIS_WAV_001_032\n\n\nJan 1993 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/WAV/GLOBAL_REANALYSIS_WAV_001_032/\n\n\nDetails\n\n\n\n\nINSITU_GLO_WAVE_REP_OBSERVATIONS_013_045\n\n\nJan 1990 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/WAV/INSITU_GLO_WAVE_REP_OBSERVATIONS_013_045/\n\n\nDetails\n\n\n\n\nWIN (Wind)\n\n\nSST_NWS_SST_L4_REP_OBSERVATIONS_010_023\n\n\nMar 1992 - Nov 2021\n\n\n/eodata/CMEMS/REP/GLO/WIN/SST_NWS_SST_L4_REP_OBSERVATIONS_010_023/\n\n\nN/A\n\n\n\n\nWIND_GLO_WIND_L3_REP_OBSERVATIONS_012_005\n\n\nMar 1992 - Dec 2017\n\n\n/eodata/CMEMS/REP/GLO/WIN/WIND_GLO_WIND_L3_REP_OBSERVATIONS_012_005/\n\n\nDetails\n\n\n\n\nWIND_GLO_WIND_L4_REP_OBSERVATIONS_012_003\n\n\nMay 2007 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/WIN/WIND_GLO_WIND_L4_REP_OBSERVATIONS_012_003/\n\n\nDetails\n\n\n\n\nWIND_GLO_WIND_L4_REP_OBSERVATIONS_012_006\n\n\nJan 1992 - Dec 2020\n\n\n/eodata/CMEMS/REP/GLO/WIN/WIND_GLO_WIND_L4_REP_OBSERVATIONS_012_006/\n\n\nDetails\n\n\n\n\n\n\nANT - Antarctic Ocean\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nSI (Sea Ice)\n\n\nANTARCTIC_OMI_SI_extent (anomalies in respect to various years)\n\n\nJan 1993 - Mar 2022\n\n\n/eodata/CMEMS/REP/ANT/SI/ANTARCTIC_OMI_SI_extent/\n\n\nN/A\n\n\n\n\n\n\nARC - Arctic Ocean\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nARCTIC_MULTIYEAR_BGC_002_005\n\n\nJan 2007 - Dec 2020\n\n\n/eodata/CMEMS/REP/ARC/BGC/ARCTIC_MULTIYEAR_BGC_002_005/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nARCTIC_REANALYSIS_BIO_002_005\n\n\nJan 2007 - Dec 2010\n\n\n/eodata/CMEMS/REP/ARC/BIO/ARCTIC_REANALYSIS_BIO_002_005/\n\n\nN/A\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_ARC_CHL_L3_REP_OBSERVATIONS_009_069\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/ARC/CHL/OCEANCOLOUR_ARC_CHL_L3_REP_OBSERVATIONS_009_069/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ARC_CHL_L4_REP_OBSERVATIONS_009_088\n\n\nAug 1997 - Dec 2018\n\n\n/eodata/CMEMS/REP/ARC/CHL/OCEANCOLOUR_ARC_CHL_L4_REP_OBSERVATIONS_009_088/\n\n\nN/A\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_ARC_OPTICS_L3_REP_OBSERVATIONS_009_068\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/ARC/OPT/OCEANCOLOUR_ARC_OPTICS_L3_REP_OBSERVATIONS_009_068/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nARCTIC_REANALYSIS_PHYS_002_003\n\n\nJan 1991 - Dec 2019\n\n\n/eodata/CMEMS/REP/ARC/PHY/ARCTIC_REANALYSIS_PHYS_002_003/\n\n\nDetails\n\n\n\n\nSEAICE_ARC_PHY_CLIMATE_L4_MY_011_016\n\n\nJan 1982 - May 2021\n\n\n/eodata/CMEMS/REP/ARC/PHY/SEAICE_ARC_PHY_CLIMATE_L4_MY_011_016/\n\n\nDetails\n\n\n\n\nSI (Sea Ice)\n\n\nARCTIC_OMI_SI_extent (Anomalies in respect to various years)\n\n\nAnomalies in respect to various years\n\n\n/eodata/CMEMS/REP/ARC/SI/ARCTIC_OMI_SI_extent/\n\n\nN/A\n\n\n\n\nSEAICE_ARC_SEAICE_L3_REP_OBSERVATIONS_011_010\n\n\nSep 1999 - May 2022\n\n\n/eodata/CMEMS/REP/ARC/SI/SEAICE_ARC_SEAICE_L3_REP_OBSERVATIONS_011_010/\n\n\nDetails\n\n\n\n\n\n\nATL - Atlantic North\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_ATL_CHL_L3_REP_OBSERVATIONS_009_067\n\n\nSep 1997 - Dec 2018\n\n\n/eodata/CMEMS/REP/ATL/CHL/OCEANCOLOUR_ATL_CHL_L3_REP_OBSERVATIONS_009_067/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ATL_CHL_L4_REP_OBSERVATIONS_009_091\n\n\nAug 1997 - Dec 2018\n\n\n/eodata/CMEMS/REP/ATL/CHL/OCEANCOLOUR_ATL_CHL_L4_REP_OBSERVATIONS_009_091/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_ATL_CHL_L4_REP_OBSERVATIONS_009_098\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/ATL/CHL/OCEANCOLOUR_ATL_CHL_L4_REP_OBSERVATIONS_009_098/\n\n\nN/A\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_ATL_OPTICS_L3_REP_OBSERVATIONS_009_066\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/ATL/OPT/OCEANCOLOUR_ATL_OPTICS_L3_REP_OBSERVATIONS_009_066/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_ATL_SST_L4_REP_OBSERVATIONS_010_026\n\n\nJan 1982 - Dec 2018\n\n\n/eodata/CMEMS/REP/ATL/SST/SST_ATL_SST_L4_REP_OBSERVATIONS_010_026/\n\n\nDetails\n\n\n\n\n\n\nBAL - Baltic Sea\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nBALTICSEA_REANALYSIS_BIO_003_012\n\n\nJan 1993 - Dec 2021\n\n\n/eodata/CMEMS/REP/BAL/BIO/BALTICSEA_REANALYSIS_BIO_003_012/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_BAL_CHL_L3_REP_OBSERVATIONS_009_080\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/BAL/CHL/OCEANCOLOUR_BAL_CHL_L3_REP_OBSERVATIONS_009_080/\n\n\nN/A\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_BAL_OPTICS_L3_REP_OBSERVATIONS_009_097\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/BAL/OPT/OCEANCOLOUR_BAL_OPTICS_L3_REP_OBSERVATIONS_009_097/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nBALTICSEA_REANALYSIS_PHY_003_011\n\n\nSep 1997 - Dec 2021\n\n\n/eodata/CMEMS/REP/BAL/PHY/BALTICSEA_REANALYSIS_PHY_003_011/\n\n\nDetails\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSEALEVEL_MED_PHY_L4_REP_OBSERVATIONS_008_051\n\n\nJul 1982 - Dec 2011\n\n\n/eodata/CMEMS/REP/BAL/SST/SEALEVEL_MED_PHY_L4_REP_OBSERVATIONS_008_051/\n\n\nN/A\n\n\n\n\nSST_BAL_SST_L4_REP_OBSERVATIONS_010_016\n\n\nJan 1982 - Dec 2011\n\n\n/eodata/CMEMS/REP/BAL/SST/SST_BAL_SST_L4_REP_OBSERVATIONS_010_016/\n\n\nDetails\n\n\n\n\n\n\nBLA (Black Sea)\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nBLKSEA_REANALYSIS_BIO_007_005\n\n\nJan 1992 - Oct 2022\n\n\n/eodata/CMEMS/REP/BLA/BIO/BLKSEA_REANALYSIS_BIO_007_005/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_BS_CHL_L3_REP_OBSERVATIONS_009_071\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/BLA/CHL/OCEANCOLOUR_BS_CHL_L3_REP_OBSERVATIONS_009_071/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_BS_CHL_L4_REP_OBSERVATIONS_009_079\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/BLA/CHL/OCEANCOLOUR_BS_CHL_L4_REP_OBSERVATIONS_009_079/\n\n\nN/A\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_BS_OPTICS_L3_REP_OBSERVATIONS_009_096\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/BLA/OPT/OCEANCOLOUR_BS_OPTICS_L3_REP_OBSERVATIONS_009_096/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nBLKSEA_REANALYSIS_PHYS_007_004\n\n\nJan 1992 - Jun 2019\n\n\n/eodata/CMEMS/REP/BLA/PHY/BLKSEA_REANALYSIS_PHYS_007_004/\n\n\nDetails\n\n\n\n\nSEALEVEL_BS_PHY_L4_REP_OBSERVATIONS_008_042\n\n\nJan 1993 - Jun 2020\n\n\n/eodata/CMEMS/REP/BLA/PHY/SEALEVEL_BS_PHY_L4_REP_OBSERVATIONS_008_042/\n\n\nN/A\n\n\n\n\nPHY_CLIM (Physics climate)\n\n\nSEALEVEL_BS_PHY_CLIMATE_L4_REP_OBSERVATIONS_008_058\n\n\nJan 1993 - Jun 2020\n\n\n/eodata/CMEMS/REP/BLA/PHY_CLIM/SEALEVEL_BS_PHY_CLIMATE_L4_REP_OBSERVATIONS_008_058/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_BS_SST_L4_REP_OBSERVATIONS_010_022\n\n\nAug 1981 - Jun 2022\n\n\n/eodata/CMEMS/REP/BLA/SST/SST_BS_SST_L4_REP_OBSERVATIONS_010_022/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nBLKSEA_REANALYSIS_WAV_007_006\n\n\nJan 2002 - Jun 2019\n\n\n/eodata/CMEMS/REP/BLA/WAV/BLKSEA_REANALYSIS_WAV_007_006/\n\n\nDetails\n\n\n\n\n\n\nEUR - EUROPE\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nPHY (Physics)\n\n\nSEALEVEL_EUR_PHY_L3_REP_OBSERVATIONS_008_061\n\n\nJan 1993 - Jun 2020\n\n\n/eodata/CMEMS/REP/EUR/PHY/SEALEVEL_EUR_PHY_L3_REP_OBSERVATIONS_008_061/\n\n\nDetails\n\n\n\n\n\n\nIBI - Atlantic: Iberia-Biscay-Ireland\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nIBI_REANALYSIS_BIO_005_003\n\n\nJan 1992 - Dec 2019\n\n\n/eodata/CMEMS/REP/IBI/BIO/IBI_REANALYSIS_BIO_005_003/\n\n\nDetails\n\n\n\n\nIBI_REANALYSIS_PHYS_005_002\n\n\nJan 2014 - Dec 2019\n\n\n/eodata/CMEMS/REP/IBI/BIO/IBI_REANALYSIS_PHYS_005_002/\n\n\nDetails\n\n\n\n\nPHY (Physics)\n\n\nIBI_REANALYSIS_PHYS_005_002\n\n\nJan 1992 - Dec 2019\n\n\n/eodata/CMEMS/REP/IBI/PHY/IBI_REANALYSIS_PHYS_005_002/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nIBI_REANALYSIS_WAV_005_006\n\n\nJan 1992 - Dec 2019\n\n\n/eodata/CMEMS/REP/IBI/WAV/IBI_REANALYSIS_WAV_005_006/\n\n\nDetails\n\n\n\n\n\n\nMED - Mediterranean Sea\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBGC (Biogeochemistry)\n\n\nMEDSEA_MULTIYEAR_BGC_006_008\n\n\nJan 1999 - Dec 2019\n\n\n/eodata/CMEMS/REP/MED/BGC/MEDSEA_MULTIYEAR_BGC_006_008/\n\n\nDetails\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nMEDSEA_REANALYSIS_BIO_006_008\n\n\nJan 1999 - Dec 2018\n\n\n/eodata/CMEMS/REP/MED/BIO/MEDSEA_REANALYSIS_BIO_006_008/\n\n\nDetails\n\n\n\n\nCHL (Ocean colour)\n\n\nOCEANCOLOUR_MED_CHL_L3_REP_OBSERVATIONS_009_073\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/MED/CHL/OCEANCOLOUR_MED_CHL_L3_REP_OBSERVATIONS_009_073/\n\n\nN/A\n\n\n\n\nOCEANCOLOUR_MED_CHL_L4_REP_OBSERVATIONS_009_078\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/MED/CHL/OCEANCOLOUR_MED_CHL_L4_REP_OBSERVATIONS_009_078/\n\n\nN/A\n\n\n\n\nOPT (Optics)\n\n\nOCEANCOLOUR_MED_OPTICS_L3_REP_OBSERVATIONS_009_095\n\n\nSep 1997 - Jun 2021\n\n\n/eodata/CMEMS/REP/MED/OPT/OCEANCOLOUR_MED_OPTICS_L3_REP_OBSERVATIONS_009_095/\n\n\nN/A\n\n\n\n\nPHY (Physics)\n\n\nMEDSEA_REANALYSIS_PHYS_006_004\n\n\nJan 1987 - Dec 2018\n\n\n/eodata/CMEMS/REP/MED/PHY/MEDSEA_REANALYSIS_PHYS_006_004/\n\n\nDetails\n\n\n\n\nSEALEVEL_MED_PHY_L4_REP_OBSERVATIONS_008_051\n\n\nJan 1993 - Dec 2020\n\n\n/eodata/CMEMS/REP/MED/PHY/SEALEVEL_MED_PHY_L4_REP_OBSERVATIONS_008_051/\n\n\nN/A\n\n\n\n\nPHY_CLIM (Physics climate)\n\n\nSEALEVEL_MED_PHY_CLIMATE_L4_REP_OBSERVATIONS_008_056\n\n\nJan 1993 - Jun 2020\n\n\n/eodata/CMEMS/REP/MED/PHY_CLIM/SEALEVEL_MED_PHY_CLIMATE_L4_REP_OBSERVATIONS_008_056/\n\n\nN/A\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_MED_SST_L4_REP_OBSERVATIONS_010_021\n\n\nAug 1981 - Jun 2022\n\n\n/eodata/CMEMS/REP/MED/SST/SST_MED_SST_L4_REP_OBSERVATIONS_010_021/\n\n\nDetails\n\n\n\n\nWAV (Waves)\n\n\nMEDSEA_HINDCAST_WAV_006_012\n\n\nFeb 2006 - Jan 2020\n\n\n/eodata/CMEMS/REP/MED/WAV/MEDSEA_HINDCAST_WAV_006_012/\n\n\nDetails\n\n\n\n\nMEDSEA_MULTIYEAR_WAV_006_012\n\n\nJan 1993 - Dec 2019\n\n\n/eodata/CMEMS/REP/MED/WAV/MEDSEA_MULTIYEAR_WAV_006_012/\n\n\nDetails\n\n\n\n\n\n\nNWS (Atlantic: NW European Shelf)\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBIO (Biochemistry, chemistry)\n\n\nNORTHWESTSHELF_REANALYSIS_BIO_004_009\n\n\nJan 1993 - July 2022\n\n\n/eodata/CMEMS/REP/NWS/BIO/NORTHWESTSHELF_REANALYSIS_BIO_004_009/\n\n\nDetails\n\n\n\n\nNORTHWESTSHELF_REANALYSIS_BIO_004_011\n\n\nJan 1993 - Jun 2022\n\n\n/eodata/CMEMS/REP/NWS/BIO/NORTHWESTSHELF_REANALYSIS_BIO_004_011/\n\n\nDetails\n\n\n\n\nPHY (Physics)\n\n\nNORTHWESTSHELF_REANALYSIS_PHY_004_009\n\n\nJan 1992 - Dec 2017\n\n\n/eodata/CMEMS/REP/NWS/PHY/NORTHWESTSHELF_REANALYSIS_PHY_004_009/\n\n\nDetails\n\n\n\n\nSST (Sea Surface Temperature)\n\n\nSST_NWS_SST_L4_REP_OBSERVATIONS_010_023\n\n\nJan 1982 - Nov 2021\n\n\n/eodata/CMEMS/REP/NWS/SST/SST_NWS_SST_L4_REP_OBSERVATIONS_010_023/\n\n\nN/A\n\n\n\n\n\nN/A - data no longer provided by CMEMS\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://data.marine.copernicus.eu/products"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat7.html",
    "href": "Data/ComplementaryData/Landsat7.html",
    "title": "Landsat-7",
    "section": "",
    "text": "The Landsat programme is a joint USGS and NASA-led enterprise for Earth observation that represents the world’s longest running system of satellites for moderate-resolution optical remote sensing for land, coastal areas and shallow waters.\nLandsat products in the Copernicus Data Space Ecosystem originate from the ESA processing. For more information please visit here.\nLandsat-7 has continued the goal of the Landsat programme to repeatedly image Earth’s land and coastal areas in order to monitor changes to these areas over time. The satellite has continued to provide data continuity for the Thematic Mapper aboard Landsat-4 and 5, utilising an enhanced version of the instrument.\nThe Enhanced Thematic Mapper Plus (ETM+) is the main instrument on board Landsat-7 and has been operational since 1999. It provides 30 m resolution for visible (VIS), near-infrared (NIR) and shortwave infrared (SWIR) as well as 60 m resolution for thermal infrared. Moreover, it adds a 15 m resolution panchromatic band (PAN).\nAccess to Landsat-7 data is possible via API\nIn order to get access to data at specific processing level as well as specific product types, you are advised to use queries provided in each section below.\nIf it is required to customize query in respect to spatial and time coverage, satellite features etc. please, follow instructions on:\n• OpenSearch\n• OData\nLevel-1"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat7.html#landsat-7-etmgtc-1p",
    "href": "Data/ComplementaryData/Landsat7.html#landsat-7-etmgtc-1p",
    "title": "Landsat-7",
    "section": "Landsat-7 ETM+GTC-1P",
    "text": "Landsat-7 ETM+GTC-1P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-7 ETM+ GTC (Global Land Survey) 1-arc second Panchromatic (1P) product is particularly useful for applications such as detailed land-cover mapping, change detection, and mapping of urban areas, as it enables the ability to discriminate between objects with higher detail. However, the Landsat-7 satellite experienced a hardware malfunction that caused a loss of data in every image acquired after May 2003. Therefore, the Landsat-7 ETM+ GTC 1-arc second Panchromatic (1P) product is limited to images acquired before the malfunction occurred.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\n(*)ETM-GTC-1P\n\n\n(*) Unpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nSep 1999 - Dec 2003\n\n\n\n\n\n(*) Landsat ETM+ ESA archive\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LandsatETM\n\n\nMore Information: https://earth.esa.int/eogateway/catalog/landsat-etm-esa-archive"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat7.html#landsat-7-etml1g",
    "href": "Data/ComplementaryData/Landsat7.html#landsat-7-etml1g",
    "title": "Landsat-7",
    "section": "Landsat-7 ETM+L1G",
    "text": "Landsat-7 ETM+L1G\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-7 ETM+ Level-1 Georeferenced (L1G) product is suitable for applications such as land-use mapping, change detection, and ecological monitoring, where the spatial accuracy may not be critical. The data is provided in an unprocessed, uncalibrated format. However, it also includes georeferencing information, allowing for easy integration into geospatial analysis systems.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nETM-L1G\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nSep 1999 - Nov 2015\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LandsatETM\n\n\nMore Information: https://earth.esa.int/eogateway/catalog/landsat-etm-esa-archive"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat7.html#landsat-7-etml1gt",
    "href": "Data/ComplementaryData/Landsat7.html#landsat-7-etml1gt",
    "title": "Landsat-7",
    "section": "Landsat-7 ETM+L1GT",
    "text": "Landsat-7 ETM+L1GT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-7 ETM+ L1GT refers to the Level-1 Geocorrected and Terrain corrected product acquired by the Enhanced Thematic Mapper Plus (ETM+) instrument on board Landsat 7 satellite. This product is corrected for geometric distortions caused by the satellite’s altitude, position, and attitude, as well as to correct for variations in terrain height. The corrected images are orthorectified to a cartographic projection, with radiometric and atmospheric corrections applied to produce accurate and calibrated reflectance values. This product is widely used for various applications including crop management, forest management, geological studies, land-use planning, and environmental monitoring.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nETM-L1GT\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nSep 1999 - Jan 2017\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LandsatETM\n\n\nMore Information: https://earth.esa.int/eogateway/catalog/landsat-etm-esa-archive"
  },
  {
    "objectID": "Data/ComplementaryData/Landsat7.html#landsat-7-etml1t",
    "href": "Data/ComplementaryData/Landsat7.html#landsat-7-etml1t",
    "title": "Landsat-7",
    "section": "Landsat-7 ETM+L1T",
    "text": "Landsat-7 ETM+L1T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nLandsat-7 ETM+ L1T refers to the Level-1 Precision Terrain corrected product acquired by the Enhanced Thematic Mapper Plus (ETM+) instrument on board Landsat 7 satellite. This product is corrected for geometric distortions caused by the satellite’s altitude, position, and attitude, as well as to correct for variations in terrain height. The corrected images are orthorectified to a cartographic projection, with radiometric and atmospheric corrections applied to produce accurate and calibrated reflectance values. In addition to the correction for terrain effects, this product also has geometric accuracy maintained to 1/3 of a Landsat pixel. The Landsat-7 ETM+ L1T product is mostly used for precision mapping and monitoring of natural resources, such as land cover classification, vegetation change detection, and urban growth analysis.\n\nOffered Data\n\n\n\n\n\n\nProduct\n\n\nArchive Status\n\n\nAccess Type\n\n\nSpatial Extent\n\n\nTemporal Extent\n\n\n\n\n\n\nETM-L1T\n\n\nUnpacked\n\n\nImmediately available data (IAD)\n\n\nEurope\n\n\nSep 1999 - Jan 2017\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nSource: https://landsat-diss.eo.esa.int/socat/LandsatETM\n\n\nMore Information: https://earth.esa.int/eogateway/catalog/landsat-etm-esa-archive"
  },
  {
    "objectID": "Data/ComplementaryData/CLMS.html",
    "href": "Data/ComplementaryData/CLMS.html",
    "title": "Copernicus Land Monitoring Service (CLMS)",
    "section": "",
    "text": "Copernicus program priorities are to gain from Earth Observation techniques and make research, administration, agriculture, economy, environmental protection of our lands easier, cheaper and more effective. Copernicus Land Monitoring Service (CLMS) constitute rich data hub with archival and near real time environmental resources. Copernicus Data Space Ecosystem platform makes CLMS products accessible over S3 or NFS protocol . Each User can make his own contribution to expanding data land applications using Copernius Data Space Ecosystem resources.\nCLMS provides three kinds of data related to its coverage: Global, Pan-European and Local. It also provides imagery and reference data (IAR)."
  },
  {
    "objectID": "Data/ComplementaryData/CLMS.html#copernicus-land-monitoring-service-clms---global",
    "href": "Data/ComplementaryData/CLMS.html#copernicus-land-monitoring-service-clms---global",
    "title": "Copernicus Land Monitoring Service (CLMS)",
    "section": "Copernicus Land Monitoring Service (CLMS) - Global",
    "text": "Copernicus Land Monitoring Service (CLMS) - Global\n\nOverview\nThe Copernicus Global Land Service continuously provides a series of well-qualified bio-geophysical products on the state and evolution of the land surface on a global scale. The data are provided at medium to low spatial resolution and, for most products, cover the period from 1998 or 1999 till today. It is ready-to-use data that allows comprehensive and immediate analysis for the entire Earth.\n\nOffered Data\n\n\nEnergy\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nLAND SURFACE TEMPERATURE\n\n\nHOURLY_LST_V1,\nHOURLY_LST_V2,\n10DAY_LST_V1,\n10DAY_LST_V2,\n10DAY_TCI_V1,\n10DAY_TCI-V2\n\n\nWorld\n\n\n(*) Jun 2010 - Present\n\n\n/eodata/CLMS/Global/Energy/Land_Surface_Temperature/\n\n\nDetails\n\n\n\n\nSURFACE ALBEDO\n\n\nDIRECTIONAL_1.4_ALDH,\nDIRECTIONAL_1.5_ALDH,\nHEMPISPHERICAL_1.4_ALBH,\nHEMPISPHERICAL_1.5_ALBH\n\n\nWorld\n\n\n(**) Jan 1999 - Jun 2020\n\n\n/eodata/CLMS/Global/Energy/Surface_Albedo/\n\n\nDetails\n\n\n\n\nTOC (TOP OF THE CANOPY) REFLECTANCE\n\n\nTOC_REFLECTANCE\n\n\nWorld\n\n\nJan 1990 - Sep 2018\n\n\n/eodata/CLMS/Global/Energy/Top_Of_Canopy_Reflectances/\n\n\nDetails\n\n\n\n\n\n\nVegetation\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nBURNT AREA\n\n\nBURNT_AREA_V3\n\n\nWorld\n\n\nJul 2020 - Jul 2021\n\n\n/eodata/CLMS/Global/Vegetation/Burnt_Area/\n\n\nDetails\n\n\n\n\nDRY MATTER PRODUCTIVITY\n\n\nDMP_300,\nGDMP_300,\nDMP_1000_V2,\nGDMP_1000_V2\n\n\nWorld\n\n\n(*) Jan 1999 - Jun 2020\n\n\n/eodata/CLMS/Global/Vegetation/Dry_Matter_Productivity/\n\n\nDetails\n\n\n\n\nFAPAR\n\n\nFAPAR_300,\nFAPAR_1000_V1.5,\nFAPAR_1000_V2\n\n\nWorld\n\n\n(**) Jan 1999 - Present\n\n\n/eodata/CLMS/Global/Vegetation/FAPAR/\n\n\nDetails\n\n\n\n\nFCOVER\n\n\nFCOVER_300,\nFCOVER_1000_V1.5,\nFCOVER_1000_V2\n\n\nWorld\n\n\n(**) Jan 1999 - Present\n\n\n/eodata/CLMS/Global/Vegetation/FCOVER/\n\n\nDetails\n\n\n\n\nLEAF AREA INDEX\n\n\nLAI_300,\nLAI_1000_V2\n\n\nWorld\n\n\n(*) Jan 1999 - Present\n\n\n/eodata/CLMS/Global/Vegetation/Leaf_Area_Index/\n\n\nDetails\n\n\n\n\nLAND COVER\n\n\nGLOBAL_LAND_COVER,\nGLOBAL_LAND_COVER_COG\n\n\nWorld\n\n\nAnnual 2015-2019\n\n\n/eodata/CLMS/Global/Vegetation/Global_Land_Cover_COG/\n\n\nDetails\n\n\n\n\nNDVI\n\n\nNDVI_300_V1,\nNDVI_300_V2,\nNDVI_1000_V2.2,\nNDVI_1000_V2.2_LTS,\nNDVI_1000_V3\n\n\nWorld\n\n\n(**) Apr 1998 - Present\n\n\n/eodata/CLMS/Global/Vegetation/NDVI/\n\n\nDetails\n\n\n\n\nSOIL WATER INDEX\n\n\nSWI_1000_EUROPE,\nSWI_GLOBAL,\nSWI10_GLOBAL,\nSTATIC_LAYERS\n\n\nWorld\n\n\n(**) Jan 2007 - Present\n\n\n/eodata/CLMS/Global/Vegetation/Soil_Water_Index/\n\n\nDetails\n\n\n\n\nSURFACE SOIL MOISTURE\n\n\nSSM\n\n\nWorld\n\n\n(**) Oct 2014 - Present\n\n\n/eodata/CLMS/Global/Vegetation/Surface_Soil_Moisture/\n\n\nDetails\n\n\n\n\n\n\nWater\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nLAKE WATER QUALITY\n\n\nLWQ_100,\nLWQ_300_SENTINEL3\n\n\nWorld\n\n\n(**) Apr 2016 - Present\n\n\n/eodata/CLMS/Global/Water/Lake_Water_Quality/\n\n\nDetails\n\n\n\n\nWATER BODIES\n\n\nWB_300_V1,\nWB_1000_PROBA_V1,\nWB_1000_PROBA_V2,\nWB_1000_SPOT_V1\n\n\nWorld (WB_1000_PROBA_V1 - AFRICA)\n\n\n(*) Apr 1998 - Sep 2021\n\n\n/eodata/CLMS/Global/Water/Water_Bodies/\n\n\nDetails\n\n\n\n\n\n(*) Available ~7-14 days after product’s acquisition.\n(**) Depending on specific product.\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nMore Information: https://land.copernicus.eu/global/"
  },
  {
    "objectID": "Data/ComplementaryData/CLMS.html#copernicus-land-monitoring-service-clms--pan-european",
    "href": "Data/ComplementaryData/CLMS.html#copernicus-land-monitoring-service-clms--pan-european",
    "title": "Copernicus Land Monitoring Service (CLMS)",
    "section": "Copernicus Land Monitoring Service (CLMS)- Pan-European",
    "text": "Copernicus Land Monitoring Service (CLMS)- Pan-European\n\nOverview\nThe production of pan-European data is coordinated by the European Environment Agency (EEA). Copernius Data Space Ecosystem provides access to pan-European datasets such as: CORINE Land Cover datasets, High Resolution Layers and related pan-European products.\nThe CORINE Land Cover (CLC) inventory was initiated in 1985 (reference year 1990). It was updated in 2000, 2006, 2012, and 2018. The CLC database contains an inventory of land cover in 44 classes. The Minimum Mapping Unit (MMU) for the status layer is defined as 25 hectares (ha) for areal phenomena and 100 meters in width for linear phenomena. The time series are complemented by change layers which highlight changes in land cover between the most recent and the previous status layers with an MMU of 5 ha.\nHigh Resolution Layers (HRL) provide information on specific land cover characteristics, including status and changes. The HRLs are complementary to the CLC datasets. The HRLs are produced from satellite imagery through a combination of automatic processing and interactive rule-based classification.\nThe European Settlement Map (ESM) is a raster dataset mapping human settlements in Europe, produced from SPOT-5 and SPOT-6 satellite imagery. It has been produced with Global Human Settlement Layer (GHSL) technology by the European Commission, Joint Research Centre, Institute for the Protection and Security of the Citizen, Global Security and Crisis Management Unit.\n\nOffered Data\n\n\nCORINE LAND COVER (CLC)\n\n\n\n\n\n\nProduct Type\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nSTATUS\n\n\nCLC_1990,\nCLC_2000,\nCLC_2006,\nCLC_2012,\nCLC_2018\n\n\nEurope (*)\n\n\n/eodata/CLMS/Pan-European/CORINE_Land_Cover/\n\n\nDetails\n\n\n\n\nCHANGES\n\n\nCHA_1990_2000,\nCHA_2000_2006,\nCHA_2006_2012, CHA_2012_2018\n\n\nEurope (*)\n\n\n/eodata/CLMS/Pan-European/CORINE_Land_Cover/\n\n\nDetails\n\n\n\n\n\n\nHIGH RESOLUTION LAYERS (HRL)\n\n\n\n\n\n\nProduct Type\n\n\nProducts\n\n\nSub-Product\n\n\nSpecific Products\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nIMPERVIOUSNESS\n\n\nSTATUS\n\n\n\n\nIMP_STATUS_2006,\nIMP_STATUS_2009,\nIMP_STATUS_2012,\nIMP_STATUS_2015,\nIMP_STATUS_2018\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Imperviousness/Status_Maps/\n\n\nDetails\n\n\n\n\nSTATUS (BUILD-UP AREAS)\n\n\n\n\nIMP_BU_STATUS_2018\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Imperviousness/Status_Maps/Impervious_Built-up_2018/\n\n\nDetails\n\n\n\n\nCHANGES\n\n\n\n\nCHA_2006_2009,\nCHA_2006_2012,\nCHA_2009_2012,\nCHA_2012_2015\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Imperviousness/Change_Maps/\n\n\nDetails\n\n\n\n\nFORESTS\n\n\nSTATUS\n\n\nTREE COVER DENSITY (TCD)\n\n\nTCD_STATUS_2012,\nTCD_STATUS_2015,\nTCD_STATUS_2018\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Forests/Tree_Cover_Density/Status_Maps/\n\n\nDetails\n\n\n\n\nSTATUS\n\n\nDOMINANT LEAF TYPE (DMT)\n\n\nDLT_STATUS_2012,\nDLT_STATUS_2015,\nDLT_STATUS_2018\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Forests/Dominant_Leaf_Type/Status_Maps/\n\n\nDetails\n\n\n\n\nSTATUS\n\n\nFOREST TYPE (FT)\n\n\nFT_STATUS_2012,\nFT_STATUS_2015,\nFT_STATUS_2018\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Forests/Forest_Type/Status_Maps/\n\n\nDetails\n\n\n\n\nCHANGES\n\n\nTREE COVER DENSITY (TCD)\n\n\nTCD_CHA_2012_2015\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Forests/Tree_Cover_Density/Change_Maps/\n\n\nDetails\n\n\n\n\nEXPERT PRODUCTS\n\n\n\n\nForest_Additional_Support_Layer_2012,\nForest_Additional_Support_Layer_2015\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Forests/Forest_Type/Expert_Products/Forest_Additional_Support_Layer/\n\n\nDetails\n\n\n\n\nGRASSLANDS\n\n\nSTATUS\n\n\n\n\nGRS_STATUS_2015,\nGRS_STATUS_2018\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Grassland/Status_Maps/\n\n\nDetails\n\n\n\n\nEXPERT PRODUCTS\n\n\n\n\nPloughing_Indicator,\nGrassland_Vegetation_Probability_Index\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Grassland/Expert_Products/\n\n\nDetails\n\n\n\n\nWATER AND WETNESS\n\n\nSTATUS\n\n\n\n\nWAT_STATUS_2015\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Water_Wetness/Status_Maps/\n\n\nDetails\n\n\n\n\nEXPERT PRODUCTS\n\n\n\n\nWater_Wetness_Probability_Index\n\n\n/eodata/CLMS/Pan-European/High_Resolution_Layers/Water_Wetness/Expert_Products\n\n\nDetails\n\n\n\n\n\n\nRELATED PAN-EUROPEAN\n\n\n\n\n\n\nProduct Type\n\n\nProducts\n\n\nSpecific Products\n\n\nSpatial\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nSTATUS\n\n\nEuropean Settlement Map\n\n\nESM_2012_V2016,\nESM_2012_V2017\n\n\nEurope\n\n\n/eodata/CLMS/Pan-European/Related_Pan-European_Products/European_Settlement_Map/\n\n\nDetails\n\n\n\n\n\n(*) Number of involved countires (CLC_1990 - 27; CLC_2000 - 39, CLC_2006 - 39; CLC_2012 - 39; CLC 2018 - 39.\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nMore Information: https://land.copernicus.eu/pan-european"
  },
  {
    "objectID": "Data/ComplementaryData/CLMS.html#copernicus-land-monitoring-service-clms--local",
    "href": "Data/ComplementaryData/CLMS.html#copernicus-land-monitoring-service-clms--local",
    "title": "Copernicus Land Monitoring Service (CLMS)",
    "section": "Copernicus Land Monitoring Service (CLMS)- Local",
    "text": "Copernicus Land Monitoring Service (CLMS)- Local\n\nOverview\nThe production of local component’s datasets is coordinated by the European Environment Agency (EEA). The goal of local components products is to provide more detailed information that is complementary to the information obtained through the Pan-European component. The local component focuses on areas which are prone to specific environmental challenges and problems.\nCopernius Data Space Ecosystem provides three components of CLMS - Local: Urban Atlas, Riparian Zones and Natura 2000.\n\nOffered Data\n\n\nLocal\n\n\n\n\n\n\nProduct Type\n\n\nProducts\n\n\nSpecific Products\n\n\nSpatial\n\n\nS3 path\n\n\nProduct Detail\n\n\n\n\n\n\nSTATUS\n\n\nURBAN ATLAS\n\n\nUA_2006,\nUA_2012\n\n\nEurope\n\n\n/eodata/CLMS/Local/Urban_Atlas/Urban_Atlas_2006/,\n/eodata/CLMS/Local/Urban_Atlas/Urban_Atlas_2012/\n\n\nDetails\n\n\n\n\nRIPARIAN ZONES\n\n\nRZ_2012\n\n\nEurope\n\n\n/eodata/CLMS/Local/Riparian_Zones/Land_Cover_Land_Use/\n\n\nDetails\n\n\n\n\nNATURA 2000\n\n\nN2K_2006,\nN2K_2012\n\n\nEurope\n\n\n/eodata/CLMS/Local/Natura_2000/\n\n\nDetails\n\n\n\n\nCHANGES\n\n\nURBAN ATLAS\n\n\nUA_CHA_2006_2012\n\n\nEurope\n\n\n/eodata/CLMS/Local/Urban_Atlas/Change_2006-2012/\n\n\nDetails\n\n\n\n\nOTHER\n\n\nURBAN ATLAS\n\n\nBUILDING HEIGHT 2012 (BH_2012)\n\n\nEuropean’s capital cities\n\n\n/eodata/CLMS/Local/Urban_Atlas/Building_Height_2012/\n\n\nDetails\n\n\n\n\nRIPARIAN ZONES\n\n\nDELINEATION_RZ,\nGREEN_LINEAR_ELEMENTS\n\n\nEurope\n\n\n/eodata/CLMS/Local/Riparian_Zones/Green_Linear_Elements/,\n/eodata/CLMS/Local/Riparian_Zones/Delineation_of_Riparian_Zones/\n\n\nDetails\n\n\n\n\n\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Links\n\n\n\nMore Information: https://land.copernicus.eu/local"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html",
    "href": "Data/ComplementaryData/Additional.html",
    "title": "Additional Data",
    "section": "",
    "text": "Copernicus Data Space Ecosystem provides data that are not associated with any of the Copernicus Services or are generated by third parties. These datasets include Sentinel-1 related products such as RTC (Radiometrically Terrain Corrected), CARD-BS (Terrain-Corrected Backscatter), Orbits; Sentinel-2 based global mosaics; land cover for Europe and Poland (S2GLC) and Digital Elevation Models (COP DEM and SRTM DEM)."
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#sentinel-1-rtc",
    "href": "Data/ComplementaryData/Additional.html#sentinel-1-rtc",
    "title": "Additional Data",
    "section": "Sentinel-1 RTC",
    "text": "Sentinel-1 RTC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nSenitnel-1 RTC (Radiometrically Terrain Corrected) provides radiometrically terrain corrected product derived from the Ground Range Detected (GRD) Level-1 products.\n\nOffered Data\n\n\n\n\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\n\n\n\n\nEurope, Africa\n\n\nJan 2018 - Present"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#sentinel-1-card-bs",
    "href": "Data/ComplementaryData/Additional.html#sentinel-1-card-bs",
    "title": "Additional Data",
    "section": "Sentinel-1 CARD-BS",
    "text": "Sentinel-1 CARD-BS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe product Sentinel-1 L2 BackScatter (CARD-BS) provides terrain corrected backscatter data over Europe. Usually a data set is available shortly after the Sentinel-1 SLC data is available. IW operational mode is available within this dataset.\n\nOffered Data\n\n\n\n\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\n\n\n\n\nEurope\n\n\nOct 2014 - Present"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#sentinel-1-orbits",
    "href": "Data/ComplementaryData/Additional.html#sentinel-1-orbits",
    "title": "Additional Data",
    "section": "Sentinel-1 Orbits",
    "text": "Sentinel-1 Orbits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nTwo kind of product are provided within this dataset: POEORB and RESORB. The period of time for PREORB and RESORB products correspond to the latest RSR report.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\n\n\n\n\nPOEORB\n\n\nWorld\n\n\nApr 2014 - Oct 2022\n\n\n\n\nRESORB\n\n\nWorld\n\n\nApr 2014 - Nov 2022"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#sentinel-2-level-2a-worldcover-annual-cloudless-mosaics-rgbnir",
    "href": "Data/ComplementaryData/Additional.html#sentinel-2-level-2a-worldcover-annual-cloudless-mosaics-rgbnir",
    "title": "Additional Data",
    "section": "Sentinel-2 Level 2A WorldCover Annual Cloudless Mosaics (RGBNIR)",
    "text": "Sentinel-2 Level 2A WorldCover Annual Cloudless Mosaics (RGBNIR)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView in browser\n\n\n\n\n\nOverview\nThe Sentinel-2 L2A WorldCover Annual composites are global cloud-free analysis ready mosaics at 10m resolution. They are obtained from the yearly Sentinel-2 archives, for the years 2020 and 2021. From the yearly time-series of each band, clouds are masked and the median value is computed.\nThe RGBNIR mosaics contain the 10m bands (B04, B03, B02, B08) and are delivered as Cloud Optimized Geotiffs (COGs).\n\nOffered Data\n\n\n\n\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\n\n\n\n\nWorld\n\n\n2020 - 2021"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#sentinel-2-mosaics",
    "href": "Data/ComplementaryData/Additional.html#sentinel-2-mosaics",
    "title": "Additional Data",
    "section": "Sentinel-2 Mosaics",
    "text": "Sentinel-2 Mosaics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nSentinel-2 L2A 120m mosaic is a derived product, which contains best pixel values for 10-daily periods, modeled by removing the cloudy pixels and then performing interpolation among remaining values.\n\nOffered Data\n\n\n\n\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\n\n\n\n\nWorld\n\n\n2019 - 2020"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#sentinel-2-glc-s2glc",
    "href": "Data/ComplementaryData/Additional.html#sentinel-2-glc-s2glc",
    "title": "Additional Data",
    "section": "Sentinel-2 GLC (S2GLC)",
    "text": "Sentinel-2 GLC (S2GLC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Sentinel-2 Global Land Cover (S2GLC) provides high resolution Poland (2019-2021) and Europe (2017) land cover map based on Sentinel-2 imagery.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\nTemporal Extent\n\n\n\n\n\n\nEurope\n\n\nEurope\n\n\n2017\n\n\n\n\nPoland\n\n\nPoland\n\n\n2019 - 2021"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#copernicus-digital-elevation-model-cop-dem-global",
    "href": "Data/ComplementaryData/Additional.html#copernicus-digital-elevation-model-cop-dem-global",
    "title": "Additional Data",
    "section": "Copernicus Digital Elevation Model (COP DEM) Global",
    "text": "Copernicus Digital Elevation Model (COP DEM) Global\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nTwo kinds of product are provided GLO-30, GLO-90. GLO-30 offers global coverage at a resolution of 30 metres and GLO-90 offers global coverage at a resolution of 90 metres. Users must register to gain access and access rights depend on the user category, see Copernicus Contributing Missions User Categories for more information.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\n\n\n\n\nCOP_DEM, COP_DEM_COG\n\n\nWorld"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#copernicus-digital-elevation-model-cop-dem-europe",
    "href": "Data/ComplementaryData/Additional.html#copernicus-digital-elevation-model-cop-dem-europe",
    "title": "Additional Data",
    "section": "Copernicus Digital Elevation Model (COP DEM) Europe",
    "text": "Copernicus Digital Elevation Model (COP DEM) Europe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nEEA-10 offers coverage on Europe at a resolution of 10 metres. Users must register to gain access and access rights depend on the user category, see Copernicus Contributing Missions User Categories for more information.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\n\n\n\n\nSAR_DGE_10_9319, SAR_INS_10_52C5\n\n\nEurope"
  },
  {
    "objectID": "Data/ComplementaryData/Additional.html#shuttle-radar-topography-mission-dem-srtm-dem",
    "href": "Data/ComplementaryData/Additional.html#shuttle-radar-topography-mission-dem-srtm-dem",
    "title": "Additional Data",
    "section": "Shuttle Radar Topography Mission DEM (SRTM DEM)",
    "text": "Shuttle Radar Topography Mission DEM (SRTM DEM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\nThe Shuttle Radar Topography Mission (SRTM) aimed to obtain elevation data on a near-global scale in order to generate the complete and high-resolution digital topographic database of Earth.\n\nOffered Data\n\n\n\n\n\n\nSpecific Products\n\n\nSpatial Extext\n\n\n\n\n\n\nSRTMGL1\n\n\nWorld"
  },
  {
    "objectID": "Data/ComplementaryData/CEMS.html",
    "href": "Data/ComplementaryData/CEMS.html",
    "title": "Copernicus Emergency Management Service (CEMS)",
    "section": "",
    "text": "The Copernicus Emergency Management Service (CEMS) provides geospatial data and images for informed decision making in order to support all involved in the management of natural or manmade disasters. CEMS constantly monitors Europe and the globe for signals of an impending disaster or evidence of one happening in real time. Products of CEMS are generated using remote sensing, satellite, in-situ (non-space) and modelled data.\nOne of the products offered by CEMS is Rapid Mapping which provides geospatial information within hours or days of a service request in order to support emergency management activities in the immediate aftermath of a disaster.\nThe Rapid Mapping datasets are provided categorised by emergency type, be it floods, fires, earthquakes, epidemic, humanitarian crisis, industrial accidents, mass movements, storms, volcanic activity etc. For each emergency activation, Rapid Mapping data is available as shapefiles of the event itself (flood extent, fire scar, etc.), transportation systems, hydrography, land use, etc.\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore Information: https://emergency.copernicus.eu/mapping/list-of-activations-risk-and-recovery"
  },
  {
    "objectID": "Data/ComplementaryData/CEMS.html#copernicus-emergency-management-service---cems",
    "href": "Data/ComplementaryData/CEMS.html#copernicus-emergency-management-service---cems",
    "title": "Copernicus Emergency Management Service (CEMS)",
    "section": "",
    "text": "The Copernicus Emergency Management Service (CEMS) provides geospatial data and images for informed decision making in order to support all involved in the management of natural or manmade disasters. CEMS constantly monitors Europe and the globe for signals of an impending disaster or evidence of one happening in real time. Products of CEMS are generated using remote sensing, satellite, in-situ (non-space) and modelled data.\nOne of the products offered by CEMS is Rapid Mapping which provides geospatial information within hours or days of a service request in order to support emergency management activities in the immediate aftermath of a disaster.\nThe Rapid Mapping datasets are provided categorised by emergency type, be it floods, fires, earthquakes, epidemic, humanitarian crisis, industrial accidents, mass movements, storms, volcanic activity etc. For each emergency activation, Rapid Mapping data is available as shapefiles of the event itself (flood extent, fire scar, etc.), transportation systems, hydrography, land use, etc.\n\nFurther details about the data collection\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore Information: https://emergency.copernicus.eu/mapping/list-of-activations-risk-and-recovery"
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html",
    "href": "APIs/SentinelHub/BatchStatistical.html",
    "title": "Batch Statistical API",
    "section": "",
    "text": "The Batch Statistical API is only available for users with Copernicus Service accounts. Please refer to our FAQ on account typology change and Submit A Request to our Copernicus Data Space Ecosystem Support Team to request your Copernicus Service account accordingly.\nThe Batch Statistical API (or shortly \"Batch Stats API\") enables you to request statistics similarly as with the Statistical API but for multiple polygons at once and/or for longer aggregations. A typical use case would be calculating statistics for all parcels in a country.\nSimilarly to the Batch Processing API, this is an asynchronous REST service. This means that data will not be immediately returned in the response of the request but delivered to your object storage, which needs to be specified in the request (e.g. bucket, see bucket settings below).\nYou can find more details about the API in the API Reference or in the examples of the workflow."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html#workflow",
    "href": "APIs/SentinelHub/BatchStatistical.html#workflow",
    "title": "Batch Statistical API",
    "section": "Workflow",
    "text": "Workflow\nThe Batch statistical API workflow in many ways resembles the Batch Processing API workflow. Available actions and statuses are:\n\nuser's actions ANALYSE, START and STOP.\nrequest's statuses CREATED, ANALYSING, ANALYSIS_DONE, STOPPED, PROCESSING, DONE, and FAILED.\n\nThe Batch statistical API comes with a set of REST actions that support the execution of various steps in the workflow. The diagram below shows all possible statuses of the batch statistical request and users' actions which trigger transitions among them.\n👤 START/ANALYSE👤 START👤 STOP👤 STOP👤 STARTCREATEDANALYSINGFAILEDANALYSIS_DONEPROCESSINGSTOPPEDDONE\nThe workflow starts when a user posts a new batch statistical request. In this step the system:\n\ncreates a new batch statistical request with status CREATED,\nvalidates the user's input (not the evalscript),\nreturns the overview of the created request.\n\nThe user can then decide to either request an additional analysis of the request or start the processing. When an additional analysis is requested:\n\nthe status of the request changes to ANALYSING,\nthe evalscript is validated,\nAfter the analysis is finished the status of the request changes to ANALYSIS_DONE.\n\nIf the user chooses to directly start processing, the system still executes the analysis but when the analysis is done it automatically starts with processing. This is not explicitly shown in the diagram in order to keep it simple.\nWhen the user starts the processing:\n\nthe status of the request changes to PROCESSING (this may take a while),\nthe processing starts,\nspent processing units are billed periodically.\n\nWhen the processing finishes, the status of the request changes to DONE.\n\nStopping the request\nA request might be stopped for following reasons:\n\nit's requested by a user (user action)\nuser is out of processing units (see chapter below)\nsomething is wrong with the processing of the request\n\nA user may stop the request in following states: ANALYSING, ANALYSIS_DONE and PROCESSING. However:\n\nif the status is ANALYSING, the analysis will complete,\nif the status is PROCESSING, all features (polygons) that have been processed or are being processed at that moment are charged for,\nuser is not allowed to restart the request in the next 30 minutes.\n\nThe service itself may also stop the request when processing of a lot of features is repeatedly failing. stoppedStatusReason of such requests will be UNHEALTHY. This can happen if the service is unstable or something is wrong with the request. If former, the request should eventually be restarted by Sentinel Hub team.\n\n\nProcessing unit costs\nTo be able to create, analyse or start a request the user has to have at least 1000 processing units available in their account. If available processing units of a user drop below 1000 while request is being processed the request is automatically stopped and cannot be restarted in the next 60 minutes. Therefore it is highly recommended to start a request with a sufficient reserve.\nMore information about batch statistical costs is available here.\n\n\nAutomatic deletion of stale data\nStale (inactive) requests will be deleted after a certain period of inactivity, depending on their status:\n\nrequests with status CREATED are deleted after 7 days of inactivity\nrequests with status FAILED are deleted after 15 days of inactivity\nall other requests are deleted after 30 days of inactivity.\n\nNote that only such requests themselves will be deleted, while the requests' result (created statistics) will remain under your control in your bucket."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html#input-polygons-as-geopackage-file",
    "href": "APIs/SentinelHub/BatchStatistical.html#input-polygons-as-geopackage-file",
    "title": "Batch Statistical API",
    "section": "Input polygons as GeoPackage file",
    "text": "Input polygons as GeoPackage file\nThe Batch Statistical API accepts a GeoPackage file containing features (polygons) as an input. The GeoPackage must be stored in your object storage (e.g. AWS bucket) and Sentinel Hub must be able to read from the storage (find more details about this in the bucket settings section below). In a batch statistical request, the input GeoPackage is specified by setting the path to the .gpkg file in the input.features.s3 parameter.\nAll features (polygons) in an input GeoPackage must be in the same CRS supported by Sentinel Hub. There can be a maximum of 700,000 features in the GeoPackage.\nAn example of a GeoPackage file can be downloaded here."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html#evalscript-and-batch-statistical-api",
    "href": "APIs/SentinelHub/BatchStatistical.html#evalscript-and-batch-statistical-api",
    "title": "Batch Statistical API",
    "section": "Evalscript and Batch statistical API",
    "text": "Evalscript and Batch statistical API\nThe same specifics as described for evalscript and Statistical API apply also for Batch statistical API.\nEvalscripts smaller than 32KB in size can be provided directly in a batch statistical request under evalscript parameter. If your evalsript exceeds this limit, you can store it to your bucket and provide a reference to it in a batch statistical request under evalscriptReference parameter."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html#processing-results",
    "href": "APIs/SentinelHub/BatchStatistical.html#processing-results",
    "title": "Batch Statistical API",
    "section": "Processing results",
    "text": "Processing results\nOutputs of a Batch Statistical API request are json files stored in your object storage. Each .json file will contain requested statistics for one feature (polygon) in the provided GeoPackage. You can connect statistics in a json file with corresponding feature (polygon) in the GeoPackge based on:\n\nid of a feature from GeoPackage is used as name of json file (e.g. 1.json, 2.json) and available in the json file as id property OR\na custom column identifier of type string can be added to GeoPackage and its value will be available in json file as identifier property.\n\nThe outputs will be stored in the bucket and the folder specified by output.s3.path parameter of the batch statistical request. The outputs will be available in a sub-folder named after the ID of your request (e.g. s3://&lt;my-bucket&gt;/&lt;my-folder&gt;/db7de265-dfd4-4dc0-bc82-74866078a5ce)."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html#bucket-settings",
    "href": "APIs/SentinelHub/BatchStatistical.html#bucket-settings",
    "title": "Batch Statistical API",
    "section": "Bucket settings",
    "text": "Bucket settings\nAs noted above, the Batch Statistical API uses buckets to:\n\nread GeoPackage file with input features (polygons) from a bucket,\nread evalscript from a bucket (this is optional because an evalscript can also be provided directly in a request),\nwrite results of processing to a bucket.\n\nOne bucket or different buckets can be used for all three purposes.\nIf you do not yet have a bucket at Copernicus Data Space Ecosystem, please follow these steps to get one.\nYou will have to configure your bucket to allow full access to Sentinel Hub. To do this, update your bucket policy to include the following statement (don’t forget to replace &lt;bucket_name&gt; with your actual bucket name):\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Sentinel Hub permissions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::ddf4c98b5e6647f0a246f0624c8341d9:root\"\n            },\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\",\n                \"arn:aws:s3:::&lt;bucket_name&gt;/*\"\n            ]\n        }\n    ]\n}\nA python script to set a bucket policy can be downloaded here."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical.html#examples",
    "href": "APIs/SentinelHub/BatchStatistical.html#examples",
    "title": "Batch Statistical API",
    "section": "Examples",
    "text": "Examples\nExample of a Batch Statistical Workflow"
  },
  {
    "objectID": "APIs/SentinelHub/BatchV2/Examples.html",
    "href": "APIs/SentinelHub/BatchV2/Examples.html",
    "title": "Examples",
    "section": "",
    "text": "The requests below are written in Python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nCreate a BatchV2 processing request\n\nOption 1: GeoTiff format output\nThis request defines which data is requested and how it will be processed. In this particular example we will calculate maximum NDVI over two months period for an area in Corsica and visualize the results using a built-in visualizer. The resulting image will in a Geotiff format. To create a batch processing request replace &lt;MyBucket&gt; with the name of your S3 bucket and run:\nurl = \"https://sh.dataspace.copernicus.eu/api/v2/batch/process\"\n\nevalscript = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B04\", \"B08\"]\n            }],\n            output: [{\n                id: \"default\",\n                bands: 3\n            }],\n            mosaicking: Mosaicking.ORBIT\n        }\n    }\n\n    function calcNDVI(sample) {\n        var denom = sample.B04 + sample.B08\n        return ((denom != 0) ? (sample.B08 - sample.B04) / denom : 0.0)\n    }\n\n    const maxNDVIcolors = [\n        [-0.2, 0xbfbfbf],\n        [0, 0xebebeb],\n        [0.1, 0xc8c682],\n        [0.2, 0x91bf52],\n        [0.4, 0x4f8a2e],\n        [0.6, 0x0f540c]\n    ]\n\n    const visualizer = new ColorRampVisualizer(maxNDVIcolors);\n\n    function evaluatePixel(samples) {\n        var max = 0\n        for (var i = 0; i &lt; samples.length; i++) {\n            var ndvi = calcNDVI(samples[i])\n            max = ndvi &gt; max ? ndvi : max\n        }\n        ndvi = max\n        return visualizer.process(ndvi)\n    }\n\"\"\"\n\npayload = {\n    \"processRequest\": {\n        \"input\": {\n            \"bounds\": {\n                \"bbox\": [\n                    8.44,\n                    41.31,\n                    9.66,\n                    43.1\n                ],\n                \"properties\": {\n                    \"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"\n                }\n            },\n            \"data\": [{\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-01T00:00:00Z\",\n                        \"to\": \"2019-06-30T00:00:00Z\"\n                    },\n                \"maxCloudCoverage\": 70.0\n                },\n                \"type\": \"sentinel-2-l2a\"\n            }]\n        },\n        \"output\": {\n            \"responses\": [{\n                \"identifier\": \"default\",\n                \"format\": {\n                    \"type\": \"image/tiff\"\n                }\n            }]\n        },\n        \"evalscript\": evalscript\n    },\n    \"input\": {\n        \"type\" : \"tiling-grid\",\n        \"id\": 0,\n        \"resolution\": 60.0\n    },\n    \"output\": {\n        \"type\": \"raster\",\n        \"delivery\": {\n            \"s3\": {\n                \"url\": \"s3://&lt;your-bucket&gt;\",\n                \"accessKey\": \"&lt;your-accessKey&gt;\",\n                \"secretAccessKey\": \"&lt;your-secretAccessKey&gt;\"\n            }\n        }\n    },\n    \"description\": \"Max NDVI over Corsica\"\n}\n\nheaders = {\n    'Content-Type': 'application/json'\n}\n\nresponse = oauth.request(\"POST\", url, headers=headers, json = payload)\n\nresponse.json()\nExtracting the batch request id from the response:\nbatch_request_id = response.json()['id']\n\n\nOption 2: Zarr format output\nIn this example we will calculate maximum NDVI over two months period for an area in Corsica. Besides maximum NDVI, we will also return values of bands B04 and B08, which were used to calculate maximum NDVI. All three results will be stored as arrays of an output Zarr file. To create a batch processing request replace &lt;MyBucket&gt; with the name of your S3 bucket and run:\nurl = \"https://sh.dataspace.copernicus.eu/api/v2/batch/process\"\n\nevalscript = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B04\", \"B08\"]\n            }],\n            output: [{\n                id: \"maxNDVI\",\n                sampleType: \"FLOAT32\",\n                bands: 1\n            },\n            {\n                id: \"band04\",\n                sampleType: \"UINT16\",\n                bands: 1\n            },\n            {\n                id: \"band08\",\n                sampleType: \"UINT16\",\n                bands: 1\n            }],\n            mosaicking: Mosaicking.ORBIT\n        }\n    }\n\n    function calcNDVI(sample) {\n        var denom = sample.B04 + sample.B08\n        return ((denom != 0) ? (sample.B08 - sample.B04) / denom : 0.0)\n    }\n\n    function evaluatePixel(samples) {\n        var maxNDVI = 0\n        var band04 = 0\n        var band08 = 0\n        for (var i = 0; i &lt; samples.length; i++) {\n            var ndvi = calcNDVI(samples[i])\n            if (ndvi &gt; maxNDVI){\n                maxNDVI = ndvi\n                band04 = samples[i].B04\n                band08 = samples[i].B08\n            }\n        }\n\n        return {\n            maxNDVI: [maxNDVI],\n            band04: [band04],\n            band08: [band08]\n        }\n    }\n\"\"\"\n\npayload = {\n    \"processRequest\": {\n        \"input\": {\n            \"bounds\": {\n                \"bbox\": [\n                    8.44,\n                    41.31,\n                    9.66,\n                    43.1\n                ],\n                \"properties\": {\n                    \"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"\n                }\n            },\n            \"data\": [\n                {\n                    \"dataFilter\": {\n                        \"timeRange\": {\n                            \"from\": \"2019-04-01T00:00:00Z\",\n                            \"to\": \"2019-06-30T00:00:00Z\"\n                        },\n                        \"maxCloudCoverage\": 70\n                    },\n                    \"type\": \"sentinel-2-l2a\"\n                }\n            ]\n        },\n        \"output\": {\n            \"responses\": [\n                {\n                    \"identifier\": \"band08\",\n                    \"format\": {\n                        \"type\": \"zarr/array\"\n                    }\n                },\n                {\n                    \"identifier\": \"band04\",\n                    \"format\": {\n                        \"type\": \"zarr/array\"\n                    }\n                },\n                {\n                    \"identifier\": \"maxNDVI\",\n                    \"format\": {\n                        \"type\": \"zarr/array\"\n                    }\n                }\n            ]\n        },\n        \"evalscript\": evalscript\n    },\n    \"input\": {\n        \"type\": \"tiling-grid\",\n        \"id\": 6,\n        \"resolution\": 100.0\n    },\n    \"output\": {\n        \"type\": \"zarr\",\n        \"delivery\": {\n            \"s3\": {\n                \"url\": \"s3://&lt;your-bucket&gt;/&lt;requestId&gt;\",\n                \"accessKey\": \"&lt;your-accessKey&gt;\",\n                \"secretAccessKey\": \"&lt;your-secretAccessKey&gt;\"\n            }\n        },\n        \"group\": {\n            \"zarr_format\": 2\n        },\n        \"arrayParameters\": {\n            \"dtype\": \"&lt;u2\",\n            \"order\": \"C\",\n            \"chunks\": [\n                1,\n                1000,\n                1000\n            ],\n            \"fill_value\": 0\n        },\n        \"arrayOverrides\": {\n            \"maxNDVI\": {\n                \"dtype\": \"&lt;f4\",\n                \"fill_value\": \"NaN\"\n            }\n        }\n    },\n    \"description\": \"Max NDVI over Corsica with Zarr format output\"\n}\n\nheaders = {\n    'Content-Type': 'application/json'\n}\n\nresponse = oauth.request(\"POST\", url, headers=headers, json=payload)\n\nresponse.json()\n\n\nOption 3: GeoPackage input and GeoTiff output\nThis request defines which data is requested and how it will be processed. In this particular example we will calculate maximum NDVI over two months period for an area in Corsica and visualize the results using a built-in visualizer. The resulting image will in a Geotiff format. To create a batch processing request replace &lt;MyBucket&gt; with the name of your S3 bucket and run:\nurl = \"https://sh.dataspace.copernicus.eu/api/v2/batch/process\"\n\nevalscript = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B04\", \"B08\"]\n            }],\n            output: [{\n                id: \"default\",\n                bands: 3\n            }],\n            mosaicking: Mosaicking.ORBIT\n        }\n    }\n\n    function calcNDVI(sample) {\n        var denom = sample.B04 + sample.B08\n        return ((denom != 0) ? (sample.B08 - sample.B04) / denom : 0.0)\n    }\n\n    const maxNDVIcolors = [\n        [-0.2, 0xbfbfbf],\n        [0, 0xebebeb],\n        [0.1, 0xc8c682],\n        [0.2, 0x91bf52],\n        [0.4, 0x4f8a2e],\n        [0.6, 0x0f540c]\n    ]\n\n    const visualizer = new ColorRampVisualizer(maxNDVIcolors);\n\n    function evaluatePixel(samples) {\n        var max = 0\n        for (var i = 0; i &lt; samples.length; i++) {\n            var ndvi = calcNDVI(samples[i])\n            max = ndvi &gt; max ? ndvi : max\n        }\n        ndvi = max\n        return visualizer.process(ndvi)\n    }\n\"\"\"\n\npayload = {\n    \"processRequest\": {\n        \"input\": {\n            \"bounds\": {\n                \"bbox\": [\n                    8.44,\n                    41.31,\n                    9.66,\n                    43.1\n                ],\n                \"properties\": {\n                    \"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"\n                }\n            },\n            \"data\": [{\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-01T00:00:00Z\",\n                        \"to\": \"2019-06-30T00:00:00Z\"\n                    },\n                \"maxCloudCoverage\": 70.0\n                },\n                \"type\": \"sentinel-2-l2a\"\n            }]\n        },\n        \"output\": {\n            \"responses\": [{\n                \"identifier\": \"default\",\n                \"format\": {\n                    \"type\": \"image/tiff\"\n                }\n            }]\n        },\n        \"evalscript\": evalscript\n    },\n    \"input\": {\n        \"type\" : \"geopackage\",\n        \"features\": {\n            \"s3\": {\n                \"url\": \"s3://&lt;my-bucket&gt;/&lt;path-to-geopackage&gt;\",\n                \"accessKey\": \"&lt;your-accessKey&gt;\",\n                \"secretAccessKey\": \"&lt;your-secretAccessKey&gt;\",\n            }\n        }\n    },\n    \"output\": {\n        \"type\": \"raster\",\n        \"delivery\": {\n            \"s3\": {\n                \"url\": \"s3://&lt;your-bucket&gt;\",\n                \"accessKey\": \"&lt;your-accessKey&gt;\",\n                \"secretAccessKey\": \"&lt;your-secretAccessKey&gt;\"\n            }\n        }\n    },\n    \"description\": \"Max NDVI over Corsica\"\n}\n\nheaders = {\n    'Content-Type': 'application/json'\n}\n\nresponse = oauth.request(\"POST\", url, headers=headers, json=payload)\n\nresponse.json()\n\n\n\nGet information about all your batch processing requests\nurl = \"https://sh.dataspace.copernicus.eu/api/v2/batch/process\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\n\n\nGet information about a batch processing request\nurl = f\"https://sh.dataspace.copernicus.eu/api/v2/batch/process/{batch_request_id}\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\n\n\nRequest detailed analysis (ANALYSE)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v2/batch/process/{batch_request_id}/analyse\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code\n\n\nRequest the start of processing (START)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v2/batch/process/{batch_request_id}/start\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code\n\n\nCancel a batch processing request (STOP)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v2/batch/process/{batch_request_id}/stop\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code"
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html",
    "href": "APIs/SentinelHub/AsyncProcess.html",
    "title": "Asynchronous Processing API",
    "section": "",
    "text": "Asynchronous Processing API is currently in beta release.\nAsynchronous Processing API (or shortly \"Async API\") allows you to process more data with a single request than Processing API. This is possible because the processing results are not returned immediately but are - after some time - delivered in your object storage. We recommend using Async API when you want to process bigger images, when you do not want to deal with tiled results and when it is not crucial for you to get the processing results immediately."
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html#general-approach",
    "href": "APIs/SentinelHub/AsyncProcess.html#general-approach",
    "title": "Asynchronous Processing API",
    "section": "General approach",
    "text": "General approach\nThe Async API allows you to process the data in a similar way as Processing API; you define input data, area of interest and time range in the body of an Async API request and Sentinel Hub will process the data as defined in your evalscript. When using Async API keep in mind that:\n\nThe maximum output image size can not exceed 10000 pixels in any dimension.\nEvalscript can be either sent directly in the request or it can be stored in object storage and referenced in an async request ( see parameter evalscriptReference in Async API reference for more details). This allows you to use bigger evalscripts.\nThe processing is asynchronous, which means that you do not get results in a response of your request. Instead they are delivered in your object storage.\nA copy of your Async API request is also stored in your object storage. After the request is processed, the copy is updated and additional information about the cost of the request is added.\nOnly a limited number of async requests can run concurrently for each Sentinel Hub user. The exact limit depends on the account type.\nThe processing time depends on request size and on the current and past load of the service. Generally, the first request is the slowest while the subsequent requests run faster.\nThe cost is the same as with Process API, except that the minimal cost is 10 PU per request."
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html#bucket-access",
    "href": "APIs/SentinelHub/AsyncProcess.html#bucket-access",
    "title": "Asynchronous Processing API",
    "section": "Bucket access",
    "text": "Bucket access\nThe Async API uses buckets to:\n\nread evalscript from a bucket (this is optional because an evalscript can also be provided directly in a request),\nwrite results of processing to a bucket.\n\nOne bucket or different buckets can be used for these purposes."
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html#bucket-settings",
    "href": "APIs/SentinelHub/AsyncProcess.html#bucket-settings",
    "title": "Asynchronous Processing API",
    "section": "Bucket settings",
    "text": "Bucket settings\nIf you do not yet have a bucket at Copernicus Data Space Ecosystem, please follow these steps to get one.\nYou will have to configure your bucket to allow read access to Sentinel Hub. To do this, update your bucket policy to include the following statement (don’t forget to replace &lt;bucket_name&gt; with your actual bucket name):\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Sentinel Hub permissions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::ddf4c98b5e6647f0a246f0624c8341d9:root\"\n            },\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\",\n                \"arn:aws:s3:::&lt;bucket_name&gt;/*\"\n            ]\n        }\n    ]\n}\nA python script to set a bucket policy can be downloaded here."
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html#checking-the-status-of-the-request",
    "href": "APIs/SentinelHub/AsyncProcess.html#checking-the-status-of-the-request",
    "title": "Asynchronous Processing API",
    "section": "Checking the status of the request",
    "text": "Checking the status of the request\nWhile the request is running, you can get its status (see this example). Once the processing is finished the request is deleted from our system. If you try to check its status after it has been deleted, you will get a '404 Not Found' response even if the request was processed successfully."
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html#troubleshooting",
    "href": "APIs/SentinelHub/AsyncProcess.html#troubleshooting",
    "title": "Asynchronous Processing API",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIn case anything goes wrong when creating an Async request, we will return an error message immediately. If anything goes wrong once the Async request has been created, we will deliver an \"error.json\" file with an error message to your object storage."
  },
  {
    "objectID": "APIs/SentinelHub/AsyncProcess.html#examples",
    "href": "APIs/SentinelHub/AsyncProcess.html#examples",
    "title": "Asynchronous Processing API",
    "section": "Examples",
    "text": "Examples\nExample of a Async API request"
  },
  {
    "objectID": "APIs/SentinelHub/Statistical.html",
    "href": "APIs/SentinelHub/Statistical.html",
    "title": "Statistical API",
    "section": "",
    "text": "The Statistical API (or shortly \"Stats API\") enables you to get statistics calculated based on satellite imagery without having to download images. In your Statistical API request, you can specify your area of interest, time period, evalscript and which statistical measures should be calculated. The requested statistics are returned in the API response. Using Statistical API you can calculate the percentage of cloudy pixels for a given area of interest and time period, or calculate mean, standard deviation, and histogram of band values for a parcel in a given time period. Find more examples here.\nTo familiarise yourself with the Statistical API, we recommend checking the Requests builder, our API reference and our Statistical API webinar."
  },
  {
    "objectID": "APIs/SentinelHub/Statistical.html#general-approach",
    "href": "APIs/SentinelHub/Statistical.html#general-approach",
    "title": "Statistical API",
    "section": "General approach",
    "text": "General approach\nBased on parameters specified by users in requests (e.g. area of interest, time range, evalscript) the Statistical API processes satellite data in a similar way as Processing API. Instead of returning images, it calculates requested statistics and returns the results in a json format."
  },
  {
    "objectID": "APIs/SentinelHub/Statistical.html#statistical-api-and-evalscript",
    "href": "APIs/SentinelHub/Statistical.html#statistical-api-and-evalscript",
    "title": "Statistical API",
    "section": "Statistical API and evalscript",
    "text": "Statistical API and evalscript\nAll general rules for building evalscripts apply. However, there are some specifics when using evalscripts with the Statistical API:\n\nThe evaluatePixel() function must, in addition to other output, always return also dataMask output. This output defines which pixels are excluded from calculations. For more details and an example, see here.\nThe default value of sampleType is FLOAT32.\nThe output.bands parameter in the setup() function can be an array. This makes it possible to specify custom names for the output bands and different output dataMask for different outputs, see this example."
  },
  {
    "objectID": "APIs/SentinelHub/Statistical.html#apis-features",
    "href": "APIs/SentinelHub/Statistical.html#apis-features",
    "title": "Statistical API",
    "section": "API's features",
    "text": "API's features\n\nSplit requested timeRange into multiple time intervals\nThe Statistical API supports requesting statistics for multiple time intervals with only one request. For example, requesting the aggregationInterval and timeRange as:\n...\n\"timeRange\": {\n    \"from\": \"2020-06-01T00:00:00Z\",\n    \"to\": \"2020-07-31T00:00:00Z\"\n    },\n\"aggregationInterval\": {\n    \"of\": \"P10D\"\n}\n...\nreturns the requested statistics calculated for multiple 10-day intervals, see this example. The aggregation intervals should be at least one day long (e.g. \"P5D\", \"P30D\"). You can only use period OR time designator not both.\nIf a timeRange is not divisible by an aggregationInterval, the last (\"not full\") time interval will be dismissed by default (SKIP option). The user can instead set the lastIntervalBehavior to SHORTEN (shortens the last interval so that it ends at the end of the provided time range) or EXTEND (extends the last interval over the end of the provided time range so that all the intervals are of equal duration).\nNote that the data is mosaicked for each of the time intervals (as defined with the mosaicking parameter in an evalscript) before the statistics are calculated. To calculate statistics over time (for example, the maximum NDVI value in a month), you should set mosaicking to ORBIT or TILE and calculate the required value in an evalscript, see this example. If you use mosaicking SIMPLE, one mosaicked output for each time interval is a basis for calculating statistics.\n\n\nHistogram\nRequesting histograms is optional. A variety of histogram customisations are available. Users can specify either of the following:\n\narbitrary bins or\nwidth of bins binWidthor\nnumber of bins nBins.\n\nAlong with binWidth and nBins users can also provide values for lowEdge and/or highEdge parameters. Otherwise, their default values will be used, which correspond to min and max statistics for particular output band.\nThis example demonstrates all three options.\n\n\nPercentile calculations\nIt is possible to get values for any percentile. For example, to get values for 33%, 75%, and 90% percentile, add the \"percentiles\" parameter to your requests as:\n...\n{\n  \"percentiles\": {\n    \"k\": [33, 75, 90]\n  }\n}\n...\nSee also this example.\n\n\nExclude pixels from calculations (dataMask output)\nIt is possible to exclude certain pixels from the calculation of the statistics. The most common use cases are excluding no data and cloudy pixels.\nWith the Statistical API, this is achieved by defining a special output called \"dataMask\". This output should have value \"0\" assigned for the pixels that should be excluded from the calculations, and a value of \"1\" elsewhere. The values of the \"dataMask\" output are defined by the user in an evalscript. An illustrative example is excluding water pixels from statistics of NDVI, see this example.\nNote that the Statistical API does not automatically exclude the no data pixels from calculating the statistics. We recommend that you always exclude those unless there is a good reason not to. This is especially important when you are requesting statistics for a polygon, as it will ensure that pixels outside of the polygon (and inside of the bounding box) are excluded. To exclude no data pixels you need to pass input dataMask band to the dataMask output, e.g.:\nfunction evaluatePixel(samples) {\n    return {\n        ...,\n        dataMask: [samples.dataMask]\n        }\n}\nAll evalscripts in the examples here exclude no data pixels.\n\n\nMultiple outputs and multi bands outputs\nStatistics can be requested for multiple outputs. This is useful when we need to use different dataMasks or different sampleTypes for each output. Additionally, each output can have multiple bands. It is possible to request different statistics for each band and for each output. This example demonstrates how to do all this.\n\n\nExamples\nExamples of Statistical API\n\n\nTutorials and Other Related Materials\n\nTo get you started, we created a detailed beginner webinar on statistical API, where you can learn how to get statistics for your data, how to manipulate the evalscript to return several outputs, each with its own statistical information, how to make use of powerful aggregations, exclude pixels from the calculation, make custom histograms and visualize your statistics in Python."
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html",
    "href": "APIs/SentinelHub/Batch.html",
    "title": "Batch Processing API",
    "section": "",
    "text": "Warning: This API is deprecated and will be removed in future versions. Please migrate to the Batch Processing V2 API as soon as possible.\nThe Batch Processing API is only available for users with Copernicus Service accounts. Please refer to our FAQ on account typology change and Submit A Request to our Copernicus Data Space Ecosystem Support Team to request your Copernicus Service account accordingly.\nBatch Processing API (or shortly \"Batch API\") enables you to request data for large areas and/or longer time periods for any Sentinel Hub supported collection, including BYOC (bring your own data).\nIt is an asynchronous REST service. This means that data will not be returned immediately in a request response but will be delivered to your object storage, which needs to be specified in the request (e.g. bucket, see bucket settings below). The processing results will be divided in tiles as described below."
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html#workflow",
    "href": "APIs/SentinelHub/Batch.html#workflow",
    "title": "Batch Processing API",
    "section": "Workflow",
    "text": "Workflow\nThe batch processing API comes with the set of REST APIs which support the execution of various workflows. The diagram below shows all possible statuses of a batch processing request (CREATED, ANALYSING, ANALYSIS_DONE, PROCESSING, DONE, FAILED, CANCELED, PARTIAL) and user's actions (ANALYSE, START, RESTART, CANCEL) which trigger transitions among them.\n👤 START👤 ANALYSE👤 START👤 CANCEL👤 CANCEL👤 CANCEL👤 RESTARTCREATEDPROCESSINGANALYSINGANALYSIS_DONECANCELEDDONEPARTIALFAILED\nThe workflow starts when a user posts a new batch processing request. In this step the system:\n\ncreates new batch processing request with status CREATED,\nvalidates the user's inputs, and\nreturns an estimated number of output tiles that will be processed.\n\nUser can then decide to either request an additional analysis of the request, start the processing or cancel the request. When additional analysis is requested:\n\nthe status of the request changes to ANALYSING,\nthe evalscript is validated,\na list of required tiles is created, and\nthe request's cost is estimated, i.e. the estimated number of processing units (PU) needed for the requested processing. Note that in case of ORBIT or TILE mosaicking the cost estimate can be significantly inaccurate, as described below.\nAfter the analysis is finished the status of the request changes to ANALYSIS_DONE.\n\nIf the user chooses to directly start processing, the system still executes the analysis but when the analysis is done it automatically starts with processing. This is not explicitly shown in the diagram in order to keep it simple.\nThe user can now request a list of tiles for their request, start the processing, or cancel the request. When the user starts the processing:\n\nthe estimated number of PU is reserved,\nthe status of the request changes to PROCESSING (this may take a while),\nthe processing starts.\n\nWhen the processing finishes, the status of the request changes to:\n\nFAILED when all tiles failed processing,\nPARTIAL when some tiles were processed and some failed,\nDONE when all tiles were processed.\n\nAlthough the process has built-in fault tolerance, occasionally, tile processing may fail. In this case, the batch request ends up in status PARTIAL and user can request restart its processing as shown in this example. This will restart processing of all FAILED tiles.\n\nEvalscript validation\nDuring analysis your evalscript will be validated against a portion of your selected area of interest. With SIMPLE mosaicking, your evalscript will only be evaluated against one scene. With ORBIT or TILE mosaicking, your evalscript will be evaluated against any number of possible scenes. That also includes a case with zero scenes (when no orbit or tile is available). Your evalscript must therefore be able to handle all such cases, even if your specific request does not contain such a case.\nExample of a request that handles such cases:\nfunction evaluatePixel(S2L2A) {\n  let sum = 0\n\n  for (var i = 0; i &lt; S2L2A.length; i++){\n    sum += S2L2A[i].B02\n  }\n\n  return sum;\n}\nAs can be seen in the case above. The array represents the S2L2A scenes that are available. In the case of zero scenes, the for loop is not entered, ensuring the script also works with zero scenes.\n\nCanceling the request\nUser may cancel the request at any time. However:\n\nif the status is ANALYSING, the analysis will complete,\nif the status is PROCESSING, all tiles that have been processed or are being processed at that moment are charged for. The remaining PUs are returned to the user.\n\n\n\nAutomatic deletion of stale data\nStale requests will be deleted after some time. Specifically, the following requests will be deleted:\n\nfailed requests (request status FAILED),\nrequests that were created but never started (request statuses CREATED, ANALYSIS_DONE),\nsuccessful requests (request statuses DONE and PARTIAL) for which it was not requested to add the results to your collections. Note that only such requests themselves will be deleted, while the requests' result (created imagery) will remain under your control in your bucket.\n\n\n\nCost estimate\nThe cost estimate, provided in the analysis stage, is based on the rules for calculating processing units. It takes the number of output pixels, the number of input bands, and the output format into account. However, for mosaicking ORBIT or TILE the number of data samples (i.e. the no. of observations available in the requested time range) can not be calculated accurately during the analysis. Our cost estimate is thus based on the assumption that one data sample is available every three days within the requested time range. For example, we assume 10 available data samples between 1.1.2021 and 31.1.2021. If you request batch processing of more/fewer data samples, the actual cost will be proportionally higher/lower.\nThe actual costs can be significantly different from the estimate if:\n\nthe number of data samples is reduced in your evalscript by preProcessScenes function or by filters such as maxCloudCoverage. The actual cost will be lower than the estimate.\nyour AOI (area of interest) includes large areas with no data, e.g. when requesting Sentinel-2 data over oceans. The actual cost will be lower than the estimate.\nyou request processing of data collections with revisit period shorter/longer than three days (e.g. your BYOC collection). The actual cost will be proportionally higher/lower than the estimate. Revisit period depends also on selected AOI, e.g. the actual costs of processing Sentinel-2 data close to the equator/at high latitudes will be lower/higher than the estimate.\n\nIf you know how many data samples per pixel will be processed, you can adjust the estimate yourself. For example, if you request processing for data that is available daily, the cost will be 3 times higher than our estimate.\nNote that the cost estimate does not take the multiplication factor of 1/3 for batch processing into account. The actual costs will be 3 times lower than the estimate.\n\n\nTile status\nUsers can follow the progress of tile processing by checking for their current status. This can be done directly in Dashboard, or via the API. The statuses are as follows:\n\nIn the analysis phase, tiles are created with status PENDING.\nWhen tiles move into scheduling queue, their status changes to SCHEDULED.\nWhen a tile is pulled from the queue and processing starts, it becomes PROCESSING.\nWhen tile processing succeeds/fails, it's DONE/FAILED.\nIf a tile gets stuck, it goes back into PENDING up to twice. If it gets stuck the third time, it becomes a FAILED tile.\nWhen a batch request with status PARTIAL is restarted, all its FAILed tiles go back into PENDING.\n\nAnalyzer creates tilesMove to scheduling queueStart processingFinishedErrorStuck (up to 2x)PENDINGSCHEDULEDPROCESSINGDONEFAILED"
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html#tiling-grids",
    "href": "APIs/SentinelHub/Batch.html#tiling-grids",
    "title": "Batch Processing API",
    "section": "Tiling grids",
    "text": "Tiling grids\nFor more effective processing we divide the area of interest into tiles and process each tile separately. While process API uses grids which come together with each datasource for processing of the data, the batch API uses one of the predefined tiling grids. The predefined tiling grids 0-2 are based on the Sentinel-2 tiling in WGS84/UTM projection with some adjustments:\n\nThe width and height of tiles in the original Sentinel 2 grid is 100 km while the width and height of tiles in our grids are given in the table below.\nAll redundant tiles (i.e. fully overlapped tiles) are removed.\n\nAll available tiling grids can be requested with (NOTE: To run this example you need to first create an OAuth client as is explained here):\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/batch/tilinggrids/\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\nThis will return the list of available grids and information about tile size and available resolutions for each grid. Currently, available grids are:\n\n\n\nname\nid\ntile size\nresolutions\ncoverage\noutput CRS\ndownload the grid [zip with shp file] **\n\n\n\n\nUTM 20km grid\n0\n20040 m\n10 m, 20 m, 30m*, 60 m\nWorld, latitudes from -80.7° to 80.7°\nUTM\nUTM 20km grid\n\n\nUTM 10km grid\n1\n10000 m\n10 m, 20 m\nWorld, latitudes from -80.6° to 80.6°\nUTM\nUTM 10km grid\n\n\nUTM 100km grid\n2\n100080 m\n30m*, 60 m, 120 m, 240 m, 360 m\nWorld, latitudes from -81° to 81°\nUTM\nUTM 100km grid\n\n\nWGS84 1 degree grid\n3\n1 °\n0.0001°, 0.0002°\nWorld, all latitudes\nWGS84\nWGS84 1 degree grid\n\n\nLAEA 100km grid\n6\n100000 m\n40 m, 50 m, 100 m\nEurope, including Turkey, Iceland, Svalbald, Azores, and Canary Islands\nEPSG:3035\nLAEA 100km grid\n\n\nLAEA 20km grid\n7\n20000 m\n10 m, 20 m\nEurope, including Turkey, Iceland, Svalbald, Azores, and Canary Islands\nEPSG:3035\nLAEA 20km grid\n\n\n\n** The geometries of the tiles are reprojected to WGS84 for download. Because of this and other reasons the geometries of the output rasters may differ from the tile geometries provided here.\nTo use 20km grid with 60 m resolution, for example, specify id and resolution parameters of the tilingGrid object when creating a new batch request (see an example of full request) as:\n{\n  ...\n  \"tilingGrid\": {\n    \"id\": 0,\n    \"resolution\": 60.0\n  },\n  ...\n}\nContact us if you would like to use any other grid for processing."
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html#batch-collection",
    "href": "APIs/SentinelHub/Batch.html#batch-collection",
    "title": "Batch Processing API",
    "section": "Batch collection",
    "text": "Batch collection\nBatch processing results can also be uploaded into a BYOC-like collection, which makes it possible to:\n\nAccess data with Processing API, by using the collection ID\nCreate a configuration with custom layers\nMake OGC requests to a configuration\nView data in EO Browser\n\nThe users can either upload data to an existing batch collection by specyfing the collectionId, or create a new one by using the createCollection parameter. Read about both options in BATCH API reference.\nWhen creating a new batch collection, one has to be careful to:\n\nMake sure that \"cogOutput\"=true\nMake sure the evalscript returns only single-band outputs\nKeep sampleType in mind, as the values the evalscript returns when creating a collection will be the values available when making a request to access it\n\nRegardless of whether the user specifies an existing collection or requests a new one, processed data will still upload to the users bucket, where they will be available for download and analysis."
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html#processing-results",
    "href": "APIs/SentinelHub/Batch.html#processing-results",
    "title": "Batch Processing API",
    "section": "Processing results",
    "text": "Processing results\nThe outputs of a batch processing will be stored to your object storage in either GeoTIFF (and JSON for metadata) or in Zarr format.\n\nGeoTIFF output format\nGeoTIFF format will be used if your request contains the output field. An example of a batch request with GeoTIFF output is available here.\nBy default, the results will be organized in sub-folders where one sub-folder will be created for each tile. Each sub-folder might contain one or more images depending on how many outputs were defined in the evalscript of the request.\nYou can also customize the sub-folder structure and file naming as described in the defaultTilePath parameter under output in BATCH API reference .\nYou can choose to return your GeoTIFF files as Cloud Optimized GeoTIFF (COG), by setting the cogOutput parameter under output in your request as true. Several advanced COG options can be selected as well - read about the parameter in BATCH API reference.\nThe results of batch processing will be in the projection of the selected tiling grid. For UTM-based grids, each part of the AOI (area of interest) is delivered in the UTM zone with which it intersects. In other words, in case your AOI intersects with more UTM zones, the results will be delivered as tiles in different UTM zones (and thus different CRSs).\n\n\nZarr output format\nZarr format will be used if your request contains the zarrOutput field. An example of a batch request with Zarr output is available here. Your request must only have one band per output and the application/json format in responses is not supported.\nThe outputs of batch processing will be stored as a single Zarr group containing one data array for each evalscript output and multiple coordinate arrays. By default, the Zarr will be stored in the folder you pass to the batch processing api in the path parameter under zarrOutput (see BATCH API reference). The folder must not contain any existing Zarr files. We recommend using a placeholder &lt;requestId&gt; as explained in the API reference to keep the results of your processing better organized.\nThe results of batch processing will be in the projection of the selected tiling grid. The tiling grids where output CRS is UTM zone are not supported for Zarr format output."
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html#bucket-settings",
    "href": "APIs/SentinelHub/Batch.html#bucket-settings",
    "title": "Batch Processing API",
    "section": "Bucket settings",
    "text": "Bucket settings\nThe results will be delivered in your own bucket hosted at Copernicus Data Space Ecosystem.\nIf you do not yet have a bucket at Copernicus Data Space Ecosystem, please follow these steps to get one.\nYou will have to configure your bucket to allow full access to Sentinel Hub. To do this, update your bucket policy to include the following statement (don’t forget to replace &lt;bucket_name&gt; with your actual bucket name):\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Sentinel Hub permissions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::ddf4c98b5e6647f0a246f0624c8341d9:root\"\n            },\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\",\n                \"arn:aws:s3:::&lt;bucket_name&gt;/*\"\n            ]\n        }\n    ]\n}\nA python script to set a bucket policy can be downloaded here."
  },
  {
    "objectID": "APIs/SentinelHub/Batch.html#examples",
    "href": "APIs/SentinelHub/Batch.html#examples",
    "title": "Batch Processing API",
    "section": "Examples",
    "text": "Examples\nExample of Batch Processing Workflow"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3SLSTR.html",
    "href": "APIs/SentinelHub/Data/S3SLSTR.html",
    "title": "Sentinel-3 SLSTR L1B",
    "section": "",
    "text": "General information about Sentinel-3 mission can be found here. Sentinel Hub offers Sentinel-3 SLSTR Level 1B products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3SLSTR.html#about-sentinel-3-slstr-l1b-data",
    "href": "APIs/SentinelHub/Data/S3SLSTR.html#about-sentinel-3-slstr-l1b-data",
    "title": "Sentinel-3 SLSTR L1B",
    "section": "",
    "text": "General information about Sentinel-3 mission can be found here. Sentinel Hub offers Sentinel-3 SLSTR Level 1B products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3SLSTR.html#accessing-sentinel-3-slstr-data",
    "href": "APIs/SentinelHub/Data/S3SLSTR.html#accessing-sentinel-3-slstr-data",
    "title": "Sentinel-3 SLSTR L1B",
    "section": "Accessing Sentinel-3 SLSTR Data",
    "text": "Accessing Sentinel-3 SLSTR Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the datasource you are querying. This chapter will help you understand the parameters for S3SLSTR data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: sentinel-3-slstr\nUse sentinel-3-slstr (previously S3SLSTR) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-3 SLSTR L1B data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S3SLSTR process API.\n\nmosaickingOrder\nSets the order of overlapping tiles from which the output result is mosaicked. The tiling is based on ESA's Product Dissemination Units for easier distribution.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\nthe pixel will be selected from the most recently acquired tile\nIf there are multiple products with the same timestamp then NTC will be used over NRT. Default value.\n\n\nleastRecent\nthe pixel will be selected from the oldest acquired tile\nIf there are multiple products with the same timestamp then NTC will be used over NRT.\n\n\nleastCC\npixel is selected from tile with the least cloud coverage metadata\nNote that \"per tile\" information is used here.\n\n\n\n\n\norbitDirection\nFilters the acquisition orbit direction. For ascending orbits optical bands will typically return a black image as there is no sunlight to illuminate the earth, though you may get some usable data in regions of midnight sun. Thermal bands will return data normally as they do not depend on sunlight.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nASCENDING\nData acquired when the satellite was traveling approximately towards the Earth's North pole\nNight (not for optical bands)\n\n\nDESCENDING\nData acquired when the satellite was traveling approximately towards the Earth's South pole\nDay (for optical bands)\n\n\n\n\n\nview\nFilters the acquisition by view.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nNADIR\nthe image acquired by the nadir viewing scanner will be selected\nDefault value\n\n\nOBLIQUE\nthe image acquired by the oblique (rear) viewing scanner will be selected\n\n\n\n\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S3SLSTR process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing, regardless of the resolution\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nNot used, use upsampling instead\nN/A\nIgnored\n\n\n\n\n\nAvailable Bands and Data\nThis chapter will explain the bands and data which can be set in the evalscript input object: Any string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nName\nDescription\nWavelength centre (nm)\nResolution (m/px)\n\n\n\n\nS1\nCloud screening, vegetation monitoring, aerosol\n554.27\n500\n\n\nS2\nNDVI, vegetation monitoring, aerosol\n659.47\n500\n\n\nS3\nNDVI, cloud flagging, pixel co-registration\n868\n500\n\n\nS4\nCirrus detection over land\n1374.80\n500\n\n\nS5\nCloud clearing, ice, snow, vegetation monitoring\n1613.40\n500\n\n\nS6\nVegetation state and cloud clearing\n2255.70\n500\n\n\nS7\nSST, LST, Active fire\n3742\n1000\n\n\nS8\nSST, LST, Active fire\n10854\n1000\n\n\nS9\nSST, LST\n12022.50\n1000\n\n\nF1\nActive fire\n3742\n1000\n\n\nF2\nActive fire\n10854\n1000\n\n\ndataMask\nThe mask of data/no data pixels (more).\nN/A*\nN/A**\n\n\n\n*dataMask has no wavelength information, as it carries only boolean information on whether a pixel has data or not. See the chapter on Units for more.  **dataMask has no source resolution as it is calculated for each output pixel.\nFor more about Sentinel-3 SLSTR bands, visit this ESA website.\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data. Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\nNotes\n\n\n\n\nOptical bands  S1 - S6\nReflectance (unitless)\nREFLECTANCE (default)\nUINT16\n0 - 0.4\nHigher values in infrared bands. Reflectance values can easily be above 1.\n\n\nOptical bands  S1 - S6\nRadiance (mW/m2/sr/nm)\nRADIANCE\nUINT16\n0 - 300\n\n\n\nThermal infrared bands  S7 - F2\nBrightness temperature (kelvin)\nBRIGHTNESS_TEMPERATURE\nUINT16\n250 - 320\nRoughly -20 to +50 C. Can reach outside this range in extreme environments.\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\n\nMosaicking\nAll mosaicking types are supported.\n\n\nScenes Object\nscenes object stores metadata. An example of metadata available in scenes object for Sentinel-3 SLSTR when mosaicking is ORBIT:\n\n\n\nProperty name\nValue\n\n\n\n\ntiles[i].date\n'2020-04-04T20:51:52.402Z'\n\n\ntiles[i].shId\n1873419\n\n\ntiles[i].cloudCoverage\n30.95142364501953\n\n\ntiles[i].dataPath\n'http://data.cloudferro.com/EODATA/Sentinel-3/SLSTR/SL_1_RBT/2020/04/04/S3A_SL_1_RBT____20200404T205152_20200404T205452_20200406T015620_0179_056_371_0540_LN2_O_NT_004.SEN3'\n\n\n\nProperties of a scenes object can differ depending on the selected mosaicking and in which evalscript function the object is accessed. Working with metadata in evalscript user guide explains all details and provides examples."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3SLSTR.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S3SLSTR.html#catalog-api-capabilities",
    "title": "Sentinel-3 SLSTR L1B",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel 3 SLSTR product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request.\n\nCollection identifier: sentinel-3-slstr\n\n\nFilter extension\n\neo:cloud_cover cloud cover percentage\nsat:orbit_state (possible values)\n\n\n\nDistinct extension\n\ndate\nsat:orbit_state"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S3SLSTR.html#examples",
    "href": "APIs/SentinelHub/Data/S3SLSTR.html#examples",
    "title": "Sentinel-3 SLSTR L1B",
    "section": "Examples",
    "text": "Examples\nS3SLSTR examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S5PL2.html",
    "href": "APIs/SentinelHub/Data/S5PL2.html",
    "title": "Sentinel-5P L2",
    "section": "",
    "text": "General information about Sentinel-5p mission can be found here. Sentinel Hub offers Sentinel-5p Level 2 products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S5PL2.html#about-sentinel-5p-data",
    "href": "APIs/SentinelHub/Data/S5PL2.html#about-sentinel-5p-data",
    "title": "Sentinel-5P L2",
    "section": "",
    "text": "General information about Sentinel-5p mission can be found here. Sentinel Hub offers Sentinel-5p Level 2 products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S5PL2.html#accessing-sentinel-5p-data",
    "href": "APIs/SentinelHub/Data/S5PL2.html#accessing-sentinel-5p-data",
    "title": "Sentinel-5P L2",
    "section": "Accessing Sentinel-5P Data",
    "text": "Accessing Sentinel-5P Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the datasource you are querying. This chapter will help you understand the parameters for S5PL2 data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: sentinel-5p-l2\nUse sentinel-5p-l2 (previously S5PL2) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-5P L2 data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S5PL2 process API.\n\ntimeRange\nFor simple mosaicking, the time range which is requested is clipped to start at most 24 hours before the to date-time. The reason for this is that Sentinel-5P covers the globe in one day therefore longer time ranges are not neccessary. The limitation also improves the responsiveness of Sentinel Hub.\n\n\nmosaickingOrder\nSets the order of sources from which the output result is mosaicked. If there are multiple sources available for the same time, unless explicitly set otherwise, Sentinel Hub will take the source with the slowest timeliness (i.e. RPRO prefered to OFFL prefered to NRTI).\n\n\n\nValue\nDescription\n\n\n\n\nmostRecent\nthe pixel will be selected from the tile, which was acquired most recently\n\n\nleastRecent\nsimilar to mostRecent but in reverse order\n\n\n\n\n\ntimeliness\nYou can force the timeliness of the requested data. If not set and there are multiple sources available for the same time, Sentinel Hub will take the source with the slowest timeliness (RPRO prefered to OFFL prefered to NRTI). To explicitly set, the options are:\n\nNRTI for near realtime,\nOFFL for offline,\nRPRO for reprocessing.\n\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S5PL2 process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing, regardless of the resolution\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nNot used, use upsampling instead\nN/A\nIgnored\n\n\nminQa\nSentinel-5P data is flagged with quality values (“qa_value”). minQa is the minimum (inclusive) pixel quality to be displayed in percent. For example, setting minQa = 75 will only display pixels with qa_value &gt;= 75%\nValues between 0 and 100.\n75 For NO250 for other products\n\n\n\n\n\nAvailable Bands and Data\nInformation in this chapter is useful when defining input object in evalscript: any string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nName\nDescription\n\n\n\n\nCO\nCarbon monoxide, more.\n\n\nHCHO\nFormaldehyde, more.\n\n\nNO2\nNitrogen oxide, more.\n\n\nO3\nOzone, more.\n\n\nSO2\nSulphur dioxide, more.\n\n\nCH4\nMethane, more.\n\n\nAER_AI_340_380\nUV (Ultraviolet) Aerosol Index calculated based on wavelengths of 340 nm and 380 nm. More.\n\n\nAER_AI_354_388\nUV (Ultraviolet) Aerosol Index calculated based on wavelengths of 354 nm and 388 nm. More.\n\n\nCLOUD_BASE_PRESSURE\nCloud base pressure, more.\n\n\nCLOUD_TOP_PRESSURE\nCloud top pressure, more.\n\n\nCLOUD_BASE_HEIGHT\nCloud base height, more.\n\n\nCLOUD_TOP_HEIGHT\nCloud top height, more.\n\n\nCLOUD_OPTICAL_THICKNESS\nCloud optical thickness, more.\n\n\nCLOUD_FRACTION\nEffective radiometric cloud fraction, more.\n\n\ndataMask\nThe mask of data/no data pixels, more.\n\n\n\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data. Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\n\n\n\nProduct/Band\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\nNotes\n\n\n\n\nCO\nCarbon monoxide total column (mol/m^2)\nMOL_M2\nFLOAT32\n0 - 0.1\nCertain events (wildfires) may cause these limits to be exceeded.\n\n\nHCHO\nFormaldehyde troposheric vertical column (mol/m^2)\nMOL_M2\nFLOAT32\n0 - 0.001\nCertain events (wildfires) may cause these limits to be exceeded.\n\n\nNO2\nNitrogen dioxide tropospheric column (mol/m^2)\nMOL_M2\nFLOAT32\n0 - 0.0003\nPeak values for polluted cities may reach two or three times the upper value.\n\n\nO3\nOzone total column (mol/m^2)\nMOL_M2\nFLOAT32\n0 - 0.36\n\n\n\nSO2\nSulfur dioxide total column (mol/m^2)\nMOL_M2\nFLOAT32\n0 - 0.01\nExplosive volcanic eruptions can exceed 0.35 mol/m^2 and instrumental noise can produce negative values.\n\n\nCH4\nColumn averaged dry air mixing ratio of methane (parts per billion)\nPPB\nFLOAT32\n1,600 - 2,000\n\n\n\nAER_AI_340_380\nUV aerosol index from 380 and 340 nm (unitless)\nINDEX\nFLOAT32\n-1 - 5\n\n\n\nAER_AI_354_388\nUV aerosol index from 388 and 354 nm (unitless)\nINDEX\nFLOAT32\n-1 - 5\n\n\n\nCLOUD_BASE_PRESSURE\nCloud base pressure (pascals)\nPASCALS\nFLOAT32\n1,000 - 110,000\n\n\n\nCLOUD_TOP_PRESSURE\nCloud top pressure (pascals)\nPASCALS\nFLOAT32\n1,000 - 110,000\n\n\n\nCLOUD_BASE_HEIGHT\nCloud base height (meters)\nMETERS\nFLOAT32\n0 - 20,000\n\n\n\nCLOUD_TOP_HEIGHT\nCloud top height (meters)\nMETERS\nFLOAT32\n0 - 20,000\n\n\n\nCLOUD_OPTICAL_THICKNESS\nCloud optical thickness (unitless)\nOPTICAL_DEPTH\nFLOAT32\n0 - 250\n\n\n\nCLOUD_FRACTION\nEffective radiometric cloud fraction (unitless)\nFRACTION\nFLOAT32\n0 - 1\n\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\n\nMosaicking\nSIMPLE and ORBIT mosaicking types are supported.\n\n\nScenes Object\nscenes object stores metadata. An example of metadata available in scenes object for Sentinel-5p L2 when mosaicking is ORBIT:\n\n\n\nProperty name\nValue\n\n\n\n\ntiles[i].date\n'2018-12-30T10:43:00Z'\n\n\ntiles[i].shId\n1900340\n\n\ntiles[i].dataPath\n'http://data.cloudferro.com/EODATA/Sentinel-5P/TROPOMI/L2__CO____/2018/12/30/S5P_OFFL_L2__CO_____20181230T104300_20181230T122430_06286_01_010202_20190105T100707/S5P_OFFL_L2__CO_____20181230T104300_20181230T122430_06286_01_010202_20190105T100707.nc'\n\n\n\nProperties of a scenes object can differ depending on the selected mosaicking and in which evalscript function the object is accessed. Working with metadata in evalscript user guide explains all details and provide examples.\n\n\nCollection Specific Constraints\nThe raw data is encoded as 32-bit float samples. For scientific usage it is best to set tiff as an output format and sampleType: SampleType.FLOAT32.\nSentinel-5P data can potentially contain many no data pixels which is a consequence of the way it is measured. We therefore suggest using the dataMask band to differentiate between actual zero values and no data."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S5PL2.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S5PL2.html#catalog-api-capabilities",
    "title": "Sentinel-5P L2",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel 5P L2 product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request.\n\nCollection identifier: sentinel-5p-l2\n\n\nFilter extension\n\nsat:absolute_orbit\ns5p:timeliness (possible values)\ns5p:type (possible values: O3, O3_TCL, O3_PR, O3_TPR, NO2, SO2, CO, CH4, HCHO, CLOUD, AER_AI, AER_LH, FRESCO, BD3, BD6, BD7)\n\n\n\nDistinct extension\n\ndate\ns5p:type\n\n\n\nExamples\nS5PL2 Examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html",
    "href": "APIs/SentinelHub/Data/S1GRD.html",
    "title": "Sentinel-1 GRD",
    "section": "",
    "text": "General information about Sentinel-1 mission can be found here. Sentinel Hub offers Sentinel-1 Level 1 Ground Range Detected products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html#about-sentinel-1-grd-data",
    "href": "APIs/SentinelHub/Data/S1GRD.html#about-sentinel-1-grd-data",
    "title": "Sentinel-1 GRD",
    "section": "",
    "text": "General information about Sentinel-1 mission can be found here. Sentinel Hub offers Sentinel-1 Level 1 Ground Range Detected products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html#processing-chain",
    "href": "APIs/SentinelHub/Data/S1GRD.html#processing-chain",
    "title": "Sentinel-1 GRD",
    "section": "Processing Chain",
    "text": "Processing Chain\nThe following describes the way Sentinel-1 GRD data is processed in Sentinel Hub. For information on how to set processing parameters, see Processing Options.\n\nOriginal or multilooked source chosen (depending on the resolution level; multilooking is done in ground range. Also see.)\nCalibration to the chosen backscatter coefficient and thermal noise removal applied.\n(Optional) Speckle filtering on the source data (see the example).\n(Optional) Radiometric terrain correction using area integration is performed. The Mapzen or Copernicus DEM is used (see the example).\n(Optional) Orthorectification using Range-Doppler terrain correction using the Mapzen or Copernicus DEM.\n\nNotes:\n\nThe orbit files used are the ones bundled in the products themselves. We find these more than sufficient for GRD use. Non-realtime products typically contain restituted orbit information.\nThe areas of border noise are not displayed by Sentinel Hub.\nRadiometric terrain correction can only be performed if orthorectification is enabled. The DEM oversampling parameter is by default set to 2 and can be user adjusted. For low resolution requests (many meters per pixel) increasing it to 3, for example, can be worthwhile to reduce artifacts. Integer values are recommended. See also the S1GRD API Reference.\nThe DEM, if used, is resampled using bilinear interpolation, and the same DEM is used for both radiometric terrain correction and orthorectification, provided both are enabled."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html#accessing-sentinel-1-grd-data",
    "href": "APIs/SentinelHub/Data/S1GRD.html#accessing-sentinel-1-grd-data",
    "title": "Sentinel-1 GRD",
    "section": "Accessing Sentinel-1 GRD Data",
    "text": "Accessing Sentinel-1 GRD Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the data collection you are querying. This chapter will help you understand the parameters for S1GRD data. To see examples of such requests go here, and for an overview of all API parameters see the S1GRD API Reference.\n\nData type identifier: sentinel-1-grd\nUse sentinel-1-grd (previously S1GRD) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-1 GRD data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S1GRD process API.\n\nmosaickingOrder\nThe same as for S2L1C.\n\n\nresolution (pixel spacing)\n\n\n\nValue\nDescription\n\n\n\n\nHIGH\n10m/px for IW/SM and 25m/px for EW\n\n\nMEDIUM\n40m/px for IW/SM and EW\n\n\n\n\n\nacquisitionMode\nSentinel-1 operates in four different acquisition modes (more).\n\n\n\nValue\nDescription\nPolarization options\n\n\n\n\nSM\nStripmap mode (more).\nHH+HV, VV+VH, HH, VV\n\n\nIW\nInterferometric Wide (IW) swath mode (more).\nHH+HV, VV+VH, HH, VV\n\n\nEW\nExtra Wide (EW) swath mode (more).\nHH+HV, VV+VH, HH, VV\n\n\nWV\nWave mode (more).\nHH, VV\n\n\n\n\n\npolarization\nThis table contains information about the polarization two letter code used by ESA and the product's contained polarizations.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nSH\nHH\n\n\n\nSV\nVV\n\n\n\nDH\nHH+HV\nTypical for EW acquisitions\n\n\nDV\nVV+VH\nTypical for IW acquisitions\n\n\nHH\nPartial Dual, HH only\nHH+HV was acquired, only HH is available in this product\n\n\nHV\nPartial Dual, HV only\nHH+HV was acquired, only HV is available in this product\n\n\nVV\nPartial Dual, VV only\nVV+VH was acquired, only VV is available in this product\n\n\nVH\nPartial Dual, VH only\nVV+VH was acquired, only VH is available in this product\n\n\n\n\n\norbitDirection\n\n\n\nValue\nDescription\n\n\n\n\nASCENDING\nData acquired when the satellite was traveling approx. towards the Earth's North pole.\n\n\nDESCENDING\nData acquired when the satellite was traveling approx. towards the Earth's South pole.\n\n\n\n\n\ntimeliness\n\n\n\nValue\n\n\n\n\nNRT10m\n\n\nNRT1h\n\n\nNRT3h\n\n\nFast24h\n\n\nOffline\n\n\nReprocessing\n\n\nArchNormal\n\n\n\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S1GRD process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing, regardless of the resolution\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nNot used, use upsampling instead\nN/A\nIgnored\n\n\nbackCoeff [1]\nBackscatter coefficient\nBETA0SIGMA0_ELLIPSOID GAMMA0_ELLIPSOID GAMMA0_TERRAIN\nGAMMA0_ELLIPSOID\n\n\northorectify [2]\nEnables/disables orthorectification\nTRUE - OrthorectifiedFALSE - non-Orthorectified\nFALSE\n\n\ndemInstance\nThe DEM used for orthorectification\nCOPERNICUS - Copernicus DEM 10m and 30m [4][5]  COPERNICUS_30 - Copernicus DEM 30m [5]  COPERNICUS_90 - Copernicus DEM 90m\nCOPERNICUS\n\n\nradiometricTerrainOversampling\nSets the DEM oversampling parameter for radiometric terrain correction. Integer values recommended.\n1 to 4\n2\n\n\nspeckleFilter\nDefines the speckle filtering method and parameters to use.\nSee Speckle Filtering\nNONE\n\n\n\n[1]: gamma0_ellipsoid and sigma0_ellipsoid use an ellipsoid earth model. Radiometric terrain correction can be enabled by setting the backscatter coefficient to gamma0_terrain; orthorectification must be enabled in this case.\n[2]: For orthorectification, we use the DEM instance specified in the demInstance field or the default DEM instance if this is not set. The Copernicus DEM is generally of higher quality and recommended in most cases. The non-orthorectified products use a simple earth model as provided in the products themselves. This may be sufficient for very flat target areas and is faster to process.\n[4]: It has 10m resolution inside 39 European states including islands and 30m elsewhere. The 30m DEM is used exclusively if the request resolution is lower (more zoomed out) than 120m/px.\n[5]: The Copernicus 30m DEM has global coverage if used for the processing of Sentinel-1 data.\n\nSpeckle Filtering\nSpeckle filtering is applied right after calibration and noise removal and done on source data. To enable speckle filtering, add the speckle filter object with the correct type and parameters to your processing options, as shown in this example).\nAvailable filters:\n\nThe NONE filter, which as the name implies, does nothing, and is equivalent to not having the filter defined at all.\n\n\"speckleFilter\": {\n   \"type\": \"NONE\"\n}\n\nThe LEE speckle filter. Window sizes from 1 to 7 are supported in each dimension. Odd valued window sizes are recommended. Processing time rapidly increases as window size increases. Note also that the effect of the filter depends on the resolution/zoom level; it is most pronounced at native resolution and gets reduced as you zoom out. We therefore suggest you use it at or near native resolution and switch it off at low resolution to save processing time. The effect of the filter is negligible if greatly zoomed out. An example with a 5x5 window:\n\n\"speckleFilter\": {\n   \"type\": \"LEE\",\n   \"windowSizeX\": 5,\n   \"windowSizeY\": 5\n}\n\n\n\n\n\n\nNote\n\n\n\nAs an alternative or in addition to this, you can also perform multitemporal averaging to reduce speckle.\n\n\n\n\n\nAvailable Bands and Data\nInformation in this chapter is useful when defining input object in evalscript. Any string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nName\nDescription\n\n\n\n\nVV\nPresent when the product polarization type is one of SV, DV or VV.\n\n\nVH\nPresent when the product polarization type is VH or DV.\n\n\nHV\nPresent when the product polarization type is HV or DH.\n\n\nHH\nPresent when the product polarization type is one of SH, DH or HH.\n\n\nlocalIncidenceAngle\nThe local incidence angle for each output pixel. Only available if orthorectification is enabled.\n\n\nscatteringArea\nThe normalized scattering area for each output pixel. Used for conversion of beta0 to terrain corrected gamma0. Only available if radiometric terrain correction is performed.\n\n\nshadowMask\nFlags output pixels which are in or near radar shadow. Is true if the nearest GRD source pixel is at most one GRD pixel away from a GRD pixel with a scatteringArea of less than 0.05. Only available if radiometric terrain correction is performed.\n\n\ndataMask\nThe mask of data/nodata pixels (more).\n\n\n\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data. Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\nFor Sentinel-1, data values are linear power in the chosen backscatter coefficient. To specify the backscatter coefficient, set BETA0, SIGMA0_ELLIPSOID, GAMMA0_ELLIPSOID (default) or GAMMA0_TERRAIN as the value of input.data.processing.backCoeff in your request. The default is GAMMA0_ELLIPSOID.\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\nNotes\n\n\n\n\nPolarization VV, HH, VH, HV\nLinear power in the chosen backscatter coefficient (unitless)\nLINEAR_POWER\nUINT16\n0 - 0.5\nCan reach very high values (such as 1000); for visualizing a large dynamic range consider converting to decibels: decibel = 10 * log10 (linear).\n\n\nlocalIncidenceAngle\nAngle (degrees)\nDN\nN/A\n0 - 180\nComputed for each output pixel. Requires orthorectification.\n\n\nscatteringArea\nNormalized area (unitless)\nDN\nN/A\n0 - 2\nCan reach high values on foreslopes. Requires radiometric terrain correction.\n\n\nshadowMask\nN/A\nDN\nN/A\n0 - likely not radar shadow  1 - likely in/near radar shadow\nRequires radiometric terrain correction.\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\n\nScenes Object\nscenes object stores metadata. An example of metadata available in scenes object for Sentinel-1 GRD when mosaicking is ORBIT:\n\n\n\nProperty name\nValue\n\n\n\n\ndateFrom\n'2019-04-02T00:00:00Z'\n\n\ndateTo\n'2019-04-02T23:59:59Z'\n\n\ntiles[i].date\n'2019-04-02T17:05:39Z'\n\n\ntiles[i].shId\n881338\n\n\ntiles[i].dataPath\n's3://sentinel-s1-l1c/GRD/2019/4/2/IW/DV/S1A_IW_GRDH_1SDV_20190402T170539_20190402T170604_026614_02FC31_7D8E'\n\n\n\nProperties of a scenes object can differ depending on the selected mosaicking and in which evalscript function the object is accessed. Working with metadata in evalscript user guide explains all details and provide examples.\n\n\nMosaicking\nSIMPLE and ORBIT mosaicking types are supported."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html#collection-specific-constraints",
    "href": "APIs/SentinelHub/Data/S1GRD.html#collection-specific-constraints",
    "title": "Sentinel-1 GRD",
    "section": "Collection specific constraints",
    "text": "Collection specific constraints\n\nNoise: All products on both services have thermal noise reduction applied.\nDecibel units: For decibel outputs, a conversion is necessary within your evalscript, see an example here. We offer pre-defined evalscripts, which return S1GRD values in decibel units, as products in the Configuration Utility for your convenience.\nOrbit state vectors: We currently use the orbit state vectors provided in the products themselves as we find these sufficient for GRD use."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S1GRD.html#catalog-api-capabilities",
    "title": "Sentinel-1 GRD",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel-1 GRD product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request. This chapter will help with understanding Sentinel 1 GRD specific parameters for search request.\n\nCollection identifier: sentinel-1-grd\n\n\nFilter extension\n\nsar:instrument_mode (possible values)\nsat:orbit_state (possible values)\ns1:polarization (possible values)\ns1:resolution (possible values)\ns1:timeliness (possible values)\n\n\n\nDistinct extension\n\ndate\nsar:instrument_mode\nsat:orbit_state\ns1:polarization"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S1GRD.html#examples",
    "href": "APIs/SentinelHub/Data/S1GRD.html#examples",
    "title": "Sentinel-1 GRD",
    "section": "Examples",
    "text": "Examples\nS1 GRD examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/DataFusion.html",
    "href": "APIs/SentinelHub/Data/DataFusion.html",
    "title": "Data Fusion",
    "section": "",
    "text": "Sentinel Hub (SH) allows you to combine the data from various data sources in the same request. To use this functionality you need to prepare a request with several data sources as explained below. Data fusion can be used for any data available in Sentinel Hub including PlanetScope (TPDI) and your own data (BYOC) and with all SH data-processing APIs (Process, Statistical, Batch, etc).\nAll SH endpoint locations support data fusion of collections hosted at that endpoint. However, only the Processing API endpoint sh.dataspace.copernicus.eu/api/v1/process also allows combining collections hosted at different SH endpoints, such as this example.\nWe invite you to read our Data Fusion blog post, where you will find 6 interesting use cases and a short guide on how to use data fusion in Sentinel Hub."
  },
  {
    "objectID": "APIs/SentinelHub/Data/DataFusion.html#preparing-a-data-fusion-request",
    "href": "APIs/SentinelHub/Data/DataFusion.html#preparing-a-data-fusion-request",
    "title": "Data Fusion",
    "section": "Preparing a Data Fusion Request",
    "text": "Preparing a Data Fusion Request\nPreparing a data fusion request is very similar to preparing any process API request that uses a single data source. Thus, only the parts which differ when performing data fusion requests are described below.\n\nRequest Body\nIn the body of the request, more specifically in the input.data array, you need to add more than one data object. For each of these objects you can optionally specify the property:\n\nid (optional) - a string of your choosing. It is used as an identifier for this input so that it can be referred to in the evalscript. It is not mandatory to define it but we recommend to do so, see examples below. Note that the type (e.g., \"sentinel-2-l1c\") is insufficient as an identifier because you may use multiple inputs from the same data collection.\n\nAn example of the input.data array with two elements:\n{\n  \"input\": {\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l1c\",\n        \"id\": \"l1c\",\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2018-10-01T00:00:00Z\",\n            \"to\": \"2018-11-01T00:00:00Z\"\n          },\n          \"mosaickingOrder\": \"leastRecent\"\n        }\n      },\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"id\": \"l2a\",\n        \"dataFilter\": {\n          \"timeRange\": {\n            \"from\": \"2018-10-01T00:00:00Z\",\n            \"to\": \"2018-11-01T00:00:00Z\"\n          },\n          \"mosaickingOrder\": \"leastRecent\"\n        }\n      }\n    ]\n  }\n}\nAs you can see in the example above all data collection specific options are still available.\n\n\nEvalscript\n\nSetup\nIn the evalscript, under the input array in the setup function, specify all input collections. Optionally, match the id from the input.data object of the request body to the datasource parameter of each input collection. This ensures that the correct collection will be used for input bands. For example:\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n        {datasource: \"l2a\", bands: [\"SCL\"], units: \"DN\"},\n        {datasource: \"l1c\", bands: [\"B02\", \"B03\", \"B04\"], units: \"REFLECTANCE\"}\n    ],\n    output: [\n      {bands: 3, sampleType: SampleType.AUTO}\n    ]\n  }\n}\nSince the datasource parameter is optional, in case you choose to omit it, the order of the input objects becomes relevant and must be the same as in the request body.\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {bands: [\"B02\", \"B03\", \"B04\"], units: \"REFLECTANCE\"}, // sentinel-2-l1c\n      {bands: [\"SCL\"], units: \"DN\"} // sentinel-2-l2a\n    ],\n    output: [\n      {bands: 3, sampleType: SampleType.AUTO}\n    ]\n  }\n}\nSpecifying mosaicking for each input is also possible. Simply add the mosaicking parameter to each input object. This overrides the global mosaicking parameter which you typically use outside the input object. Combinations of both are therefore possible, and when present, the value within the input object is used. The default remains SIMPLE. In the example below, the global default SIMPLE mosaicking value is never used as ORBIT is specified for all inputs.\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {bands: [\"B02\", \"B03\", \"B04\"], units: \"REFLECTANCE\", mosaicking: \"ORBIT\"}, // sentinel-2-l1c\n      {bands: [\"SCL\"], units: \"DN\", mosaicking: \"ORBIT\"} // sentinel-2-l2a\n    ],\n    output: [\n      {bands: 3, sampleType: SampleType.AUTO}\n    ]\n  }\n}\n\n\nData Access\nData from each collection can be accessed inside the evaluatePixel function, however as now multiple inputs are accessible the syntax is slightly different to the single data collection case. Assuming the evaluatePixel parameter is called samples, this object is always a key-value pair (dictionary). Obtain data for each collection in one of two ways.\nIf you are using the datasource object in your setup function, simply use its values as keys to the samples object. This gets an array of mosaicked values. Each array behaves exactly the same way as a non-datafusion non-SIMPLE mosaicking samples object. Note that this is an array even for datafusion with SIMPLE mosaicking, unlike non-datafusion requests; of course in this case the array either contains exactly one object or is empty.\nIf datasource was specified in setup:\nfunction evaluatePixel(samples) {\n  // \"l1c\" and \"l2a\" match the datasource values specified in the setup function\n  var l1cMosaics = samples.l1c;  // gets the array of sentinel-2-l1c mosaics\n  var l1cSample = l1cMosaics[0]; // gets the first mosaic. access bands from this object\n  var scl = samples.l2a[0].SCL;  // gets the SCL band of the first sentinel-2-l2a mosaic\n  if (2 &lt;= scl && scl &lt;= 7) {\n    return [l1cSample.B04, l1cSample.B03, l1cSample.B02];\n  }\n  return [0,0,0];\n}\nIf datasource wasn't specified in setup, you can access the data with keys being ordinal numbers starting with 0. Again, the order is as specified in the request body.\nfunction evaluatePixel(samples) {\n  // '0' is the identifier of the first input in the input.data array (sentinel-2-l1c)\n  // '1' is the identifier of the second input in the input.data array (sentinel-2-l2a)\n  var l1cMosaics = samples['0']; // gets the array of mosaics of the first input (sentinel-2-l1c)\n  var l1cSample = l1cMosaics[0]; // gets the first mosaic. access bands from this object\n  var scl = samples['1'][0].SCL; // gets the SCL band of the first mosaic of the second input (sentinel-2-l2a)\n  if (2 &lt;= scl && scl &lt;= 7) {\n    return [l1cSample.B04, l1cSample.B03, l1cSample.B02];\n  }\n  return [0,0,0];\n}\n\n\n\n\n\n\nNote\n\n\n\nWhile the above example works for all mosaicking types, it makes most sense for SIMPLE mosaicking. This is because only one mosaic is accessed for each input in the script, any additional mosaics, which would get generated if ORBIT or TILE mosaicking was used, are ignored."
  },
  {
    "objectID": "APIs/SentinelHub/Data/DataFusion.html#examples",
    "href": "APIs/SentinelHub/Data/DataFusion.html#examples",
    "title": "Data Fusion",
    "section": "Examples",
    "text": "Examples\n\nData Fusion Examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/DEM.html#mission-information",
    "href": "APIs/SentinelHub/Data/DEM.html#mission-information",
    "title": "Digital Elevation Model (DEM) Data",
    "section": "Mission information",
    "text": "Mission information\nA DEM is a digital model or 3D1 representation of a terrain's surface. With a DEM you are able to obtain and analayse heights within your area of interest, and integrate the data in 3D applications. The data can also be used for the orthorectification of satellite imagery (e.g., Sentinel 1).\nCopernicus DEM is based on WorldDEM that is infilled on a local basis with the following DEMs: ASTER, SRTM90, SRTM30, SRTM30plus, GMTED2010, TerraSAR-X Radargrammetric DEM, ALOS World 3D-30m. We provide two instances named COPERNICUS_30 and COPERNICUS_90, with worldwide coverage. COPERNICUS_90 uses COP-DEM GLO-90, which has 90 meters resolution. COPERNICUS_30 uses COP-DEM GLO-30 Public, which has 30 meters resolution, where it's available, and for the rest is uses GLO-90. Tiles that are missing from GLO-30 Public are not yet released to the public by Copernicus Programme. Both instances are static and do not depend on the date. We return a homogeneous DEM with zeros in regions where there are no source tiles (e.g. in ocean areas). More information here."
  },
  {
    "objectID": "APIs/SentinelHub/Data/DEM.html#attribution-and-use",
    "href": "APIs/SentinelHub/Data/DEM.html#attribution-and-use",
    "title": "Digital Elevation Model (DEM) Data",
    "section": "Attribution and use",
    "text": "Attribution and use\nFor Copernicus DEM GLO-90, check the terms on page 15, where it says \"Licence for COP-DEM-GLO-90-F Global 90m Full, Free & Open. Licence for the use of the Copernicus WorldDEM™-90\".\nFor Copernicus DEM GLO-30 Public, check the license and terms here."
  },
  {
    "objectID": "APIs/SentinelHub/Data/DEM.html#accessing-dem-data",
    "href": "APIs/SentinelHub/Data/DEM.html#accessing-dem-data",
    "title": "Digital Elevation Model (DEM) Data",
    "section": "Accessing DEM Data",
    "text": "Accessing DEM Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the collection you are querying. This chapter will help you understand the parameters for DEM data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: dem\nUse dem (previously DEM) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get DEM data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the DEM process API.\n\ndemInstance\nSets the DEM to use. Will return the default DEM (COPERNICUS_30) if no parameter is set.\n\n\n\nIdentifier\nDEM\nNotes\n\n\n\n\nCOPERNICUS_30\nCopernicus DEM GLO-30 Public and GLO-90\n30m is infilled with 90m where 30m tiles are not released\n\n\nCOPERNICUS_90\nCopernicus DEM GLO-90\nGlobal\n\n\n\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the DEM process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing when the pixel resolution is greater than the source resolution (e.g. 5m/px with a 10m/px source).\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nAs above except when the resolution is lower.\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\negm\nAn option to add geoid heights from an earth gravitational model to the orthometric heights in which case the returned values represent ellipsoidal heights relative to the WGS84 ellipsoid.  For Copernicus DEMs, we use EGM2008.\nTRUE - returned values are ellipsoid heights FALSE - returned values are orthometric heights\nFALSE\n\n\n\n\n\nAvailable Bands and Data\nInformation in this chapter is useful when defining input object in evalscript. A string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nProperty name\nDescription\nResolution\n\n\n\n\nDEM\nHeights in meters\nVarious, depending on the datasource used for the generation of the DEM, see.\n\n\ndataMask\nThe mask of data/no data pixels (more).\nN/A dataMask has no source resolution as it is calculated for each output pixel.\n\n\n\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data. Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\nFor DEM, DN (digital numbers) are the default and only unit. DN values equal elevation.\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\n\n\n\n\nDEM\nHeight (meters)\nMETERS\nFLOAT32\ntypically positive\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n\n\nScene Object\nThe evalscript evaluatePixel scene object is not returned/is null when requesting DEM.\n\n\nCollection specific constraints\nDEM values are in meters and can be negative for areas which lie below sea level (e.g. ocean areas or much of the Netherlands). When requesting a DEM a simple way to mitigate this is to set the output format in your evalscript to sampleType: SampleType.FLOAT32 or sampleType: SampleType.INT16 if this provides sufficient precision for your use. More about output formats can be found here.\nFor output formats sampleType: SampleType.UINT8 and sampleType: SampleType.UINT16 be careful as negative values can be misinterpreted due to signedness issues as well as potential problems due to integer overflow. For example:\n\nsampleType: SampleType.UINT8 - a height of 256 meters will be encoded as 0 in such a file due to overflow. A height of -1 meter will be encoded as 255.\nsampleType: SampleType.UINT16- a height of -15 meters for instance will be encoded as 65520.\n\nOne way to handle this adjustment is to ensure there are no negative values in the output by adding a constant to DEM values, e.g. 12000 (the minimum value in DEM is not smaller than -11000 m). If you need actual DEM values (i.e. heights), the constant 12000 must be subtracted from the output values outside of Sentinel Hub.\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output:{\n        id: \"default\",\n        bands: 1,\n        sampleType: SampleType.UINT16\n    }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [sample.DEM + 12000]\n}"
  },
  {
    "objectID": "APIs/SentinelHub/Data/DEM.html#examples",
    "href": "APIs/SentinelHub/Data/DEM.html#examples",
    "title": "Digital Elevation Model (DEM) Data",
    "section": "Examples",
    "text": "Examples\nDEM examples"
  },
  {
    "objectID": "APIs/SentinelHub/Data/DEM.html#footnotes",
    "href": "APIs/SentinelHub/Data/DEM.html#footnotes",
    "title": "Digital Elevation Model (DEM) Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually, we should say \"2.5D\" to be more precise. The terrain surface embedded in 3D space is modeled in a way that precisely one height is assigned to each pixel. This brings limitations as not all 3D shapes (e.g., overhangs, vertical walls, caves) can be fully modeled.↩︎"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L1C.html",
    "href": "APIs/SentinelHub/Data/S2L1C.html",
    "title": "Sentinel-2 L1C",
    "section": "",
    "text": "General information about Sentinel-2 mission can be found here. Sentinel Hub offers Sentinel-2 Level 1C products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L1C.html#about-sentinel-2-l1c-data",
    "href": "APIs/SentinelHub/Data/S2L1C.html#about-sentinel-2-l1c-data",
    "title": "Sentinel-2 L1C",
    "section": "",
    "text": "General information about Sentinel-2 mission can be found here. Sentinel Hub offers Sentinel-2 Level 1C products."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L1C.html#accessing-sentinel-2-l1c-data",
    "href": "APIs/SentinelHub/Data/S2L1C.html#accessing-sentinel-2-l1c-data",
    "title": "Sentinel-2 L1C",
    "section": "Accessing Sentinel-2 L1C Data",
    "text": "Accessing Sentinel-2 L1C Data\nTo access data you need to send a POST request to our process API. The requested data will be returned as the response to your request. Each POST request can be tailored to get you exactly the data you require. To do this requires setting various parameters which depend on the datasource you are querying. This chapter will help you understand the parameters for S2L1C data. To see examples of such requests go here, and for an overview of all API parameters see the API Reference.\n\nData type identifier: sentinel-2-l1c\nUse sentinel-2-l1c (previously S2L1C) as the value of the input.data.type parameter in your API requests. This is mandatory and will ensure you get Sentinel-2 L1C data.\n\n\nFiltering Options\nThis chapter will explain the input.data.dataFilter object of the S2L1C process API.\n\nmosaickingOrder\nSets the order of overlapping tiles from which the output result is mosaicked. Note that tiles will in most cases come from the same orbit/acquisition. The tiling is done by ESA for easier distribution.\n\n\n\nValue\nDescription\nNotes\n\n\n\n\nmostRecent\nselected by default. The pixel will be selected from the tile, which was acquired most recently\nin case there are more tiles available with the same timestamp (some tiles are processed by many ground stations, some are reprocessed, etc.), the one, which was downloaded from SciHub later will be used.\n\n\nleastRecent\nsimilar to mostRecent but in reverse order\n\n\n\nleastCC\npixel is selected from tile with the least cloud coverage metadata\nnote that \"per tile\" information is used here, each covering about a 12,000 sq. km area, so this information is only an estimate .\n\n\n\n\n\nmaxCloudCoverage\nSets the upper limit for cloud coverage in percent based on the precomputed cloud coverage estimate for each Sentinel-2 tile as present in the tile metadata. Satellite data will therefore not be retrieved for tiles with a higher cloud coverage estimate. For example, by setting the value to 20, only tiles with at most 20% cloud coverage will be used. Note that this parameter is set per tile and might not be directly applicable to the chosen area of interest.\n\n\n\nProcessing Options\nThis chapter will explain the input.data.processing object of the S2L1C process API.\n\n\n\nParameter\nDescription\nValues\nDefault\n\n\n\n\nupsampling\nDefines the interpolation used for processing when the pixel resolution is greater than the source resolution (e.g. 5m/px with a 10m/px source)\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\ndownsampling\nAs above except when the resolution is lower.\nNEAREST - nearest neighbour interpolation  BILINEAR - bilinear interpolation  BICUBIC - bicubic interpolation\nNEAREST\n\n\n\n\n\nAvailable Bands and Data\nThis chapter will explain the bands and data which can be set in the evalscript input object: Any string listed in the column Name can be an element of the input.bands array in your evalscript.\n\n\n\nName\nDescription\nResolution\nNotes\n\n\n\n\nB01\nCoastal aerosol, 442.7 nm (S2A), 442.3 nm (S2B)\n60m\n\n\n\nB02\nBlue, 492.4 nm (S2A), 492.1 nm (S2B)\n10m\n\n\n\nB03\nGreen, 559.8 nm (S2A), 559.0 nm (S2B)\n10m\n\n\n\nB04\nRed, 664.6 nm (S2A), 665.0 nm (S2B)\n10m\n\n\n\nB05\nVegetation red edge, 704.1 nm (S2A), 703.8 nm (S2B)\n20m\n\n\n\nB06\nVegetation red edge, 740.5 nm (S2A), 739.1 nm (S2B)\n20m\n\n\n\nB07\nVegetation red edge, 782.8 nm (S2A), 779.7 nm (S2B)\n20m\n\n\n\nB08\nNIR, 832.8 nm (S2A), 833.0 nm (S2B)\n10m\n\n\n\nB8A\nNarrow NIR, 864.7 nm (S2A), 864.0 nm (S2B)\n20m\n\n\n\nB09\nWater vapour, 945.1 nm (S2A), 943.2 nm (S2B)\n60m\n\n\n\nB10\nSWIR – Cirrus, 1373.5 nm (S2A), 1376.9 nm (S2B)\n60m\n\n\n\nB11\nSWIR, 1613.7 nm (S2A), 1610.4 nm (S2B)\n20m\n\n\n\nB12\nSWIR, 2202.4 nm (S2A), 2185.7 nm (S2B)\n20m\n\n\n\nsunAzimuthAngles\nSun azimuth angle\n5000m\n[1]\n\n\nsunZenithAngles\nSun zenith angle\n5000m\n[1]\n\n\nviewAzimuthMean\nViewing azimuth angle\n5000m\n[1]\n\n\nviewZenithMean\nViewing zenith angle\n5000m\n[1]\n\n\ndataMask\nThe mask of data/no data pixels (more).\nN/A\n[2]\n\n\n\n[1]: The data of this band is always resampled using nearest neighbor interpolation, regardless of the requested interpolation type.\n[2]: dataMask has no source resolution as it is calculated for each output pixel.\n\n\nUnits\nThe data values for each band in your custom script are presented in the units as specified here. In case more than one unit is available for a given band, you may optionally set the value of input.units in your evalscript setup function to one of the values in the Sentinel Hub Units column. Doing so will present data in that unit. The Sentinel Hub units parameter combines the physical quantity and corresponding units of measurement values. As such, some names more closely resemble physical quantities, others resemble units of measurement.\nThe Source Format specifies how and with what precision the digital numbers (DN) from which the unit is derived are encoded. Bands requested in DN units contain exactly the pixel values of the source data (see also Harmonize Values). Note that resampling may produce interpolated values. DN is also used whenever a band is derived computationally (like dataMask); such bands can be identified by having DN units and N/A source format. DN values are typically not offered if they do not simply represent any physical quantity, in particular, when DN values require source-specific (i.e. non-global) conversion to physical quantities.\nValues in non-DN units are computed from the source (DN) values with at least float32 precision. Note that the conversion might be nonlinear, therefore the full value range and quantization step size of such a band can be hard to predict. Band values in evalscripts always behave as floating point numbers, regardless of the actual precision.\nThe Typical Range indicates what values are common for a given band and unit, however outliers can be expected.\nFor Sentinel-2 optical data, the relation between DN and REFLECTANCE (default unit) is: DN = 10000 * REFLECTANCE. See also Harmonize Values.\n\n\n\nBand\nPhysical Quantity (units)\nSentinel Hub Units\nSource Format\nTypical Range\n\n\n\n\nOptical bands\nReflectance (unitless)\nREFLECTANCE (default)\nUINT15\n0 - 0.4*\n\n\nOptical bands\nDigital numbers (unitless)\nDN\nUINT15\n0 - 4000*\n\n\nsunAzimuthAngles\nAngle (degrees)\nDEGREES\nFLOAT32\n30 - 200\n\n\nviewAzimuthMean\nAngle (degrees)\nDEGREES\nFLOAT32\n90 - 300\n\n\nsunZenithAngles\nAngle (degrees)\nDEGREES\nFLOAT32\n15 - 80\n\n\nviewZenithMean\nAngle (degrees)\nDEGREES\nFLOAT32\n0 - 12\n\n\ndataMask\nN/A\nDN\nN/A\n0 - no data 1 - data\n\n\n\n*Higher values are expected in infrared bands. Reflectance values can easily be above 1.\n\nHarmonize Values\nESA updated the Sentinel-2 processing baseline to version 04.00 in January, 2022, which introduced breaking changes to the interpretation of digital numbers (DN). The optional harmonizeValues parameter gives you extra control over the values which enter your evalscript.\nharmonizeValues can be true (default) or false, and it's behavior depends on the units chosen:\n\nREFLECTANCE:\n\nharmonizeValues = true: negative reflectance values are clamped to zero. In other words, pixels with negative reflectance return zero reflectance instead.\nharmonizeValues = false: negative reflectance values can be returned.\n\nDN:\n\nharmonizeValues = true: DN values are harmonized so they are comparable with data from previous baselines. Therefore it still holds that DN = 10000 * REFLECTANCE. In addition, negative values are clamped to zero.\nharmonizeValues = false: DN values are exactly as provided in the source files themselves. The \"true\" DN value, you could say. Don't forget that values have different definitions with different processing baselines, careful with mosaicking!\n\n\n\n\n\nMosaicking\nAll mosaicking types are supported.\n\n\nScenes Object\nscenes object stores metadata. An example of metadata available in scenes object for Sentinel-2 L1C when mosaicking is ORBIT:\n\n\n\nProperty name\nValue\n\n\n\n\ndateFrom\n'2020-09-15T00:00:00Z'\n\n\ndateTo\n'2020-09-15T00:00:00Z'\n\n\ntiles[i].date\n'2020-09-15T10:17:52Z'\n\n\ntiles[i].shId\n11583048\n\n\ntiles[i].cloudCoverage\n2.09\n\n\ntiles[i].dataPath\n's3://sentinel-s2-l2a/tiles/33/T/VM/2020/9/15/0'\n\n\n\nProperties of a scenes object can differ depending on the selected mosaicking and in which evalscript function the object is accessed. Working with metadata in evalscript user guide explains all details and provide examples."
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L1C.html#catalog-api-capabilities",
    "href": "APIs/SentinelHub/Data/S2L1C.html#catalog-api-capabilities",
    "title": "Sentinel-2 L1C",
    "section": "Catalog API Capabilities",
    "text": "Catalog API Capabilities\nTo access Sentinel 2 L1C product metadata you need to send search request to our Catalog API. The requested metadata will be returned as JSON formatted response to your request. This chapter will help with understanding Sentinel 2 L1C specific parameters for search request.\n\nCollection identifier: sentinel-2-l1c\n\n\nFilter extension\n\neo:cloud_cover cloud cover percentage\n\n\n\nDistinct extension\n\ndate"
  },
  {
    "objectID": "APIs/SentinelHub/Data/S2L1C.html#examples",
    "href": "APIs/SentinelHub/Data/S2L1C.html#examples",
    "title": "Sentinel-2 L1C",
    "section": "Examples",
    "text": "Examples\nS2L1C examples"
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WCS.html",
    "href": "APIs/SentinelHub/OGC/WCS.html",
    "title": "Web Coverage Service",
    "section": "",
    "text": "The Sentinel Hub WCS (Web Coverage Service) service conforms to the WCS standard. Provides access to the same bands product and additional informational layers as the WMS service except only one layer can be specified at once, even when only raw Sentinel-2 bands are used. In addition to raster products, the WCS service can also return the vector features of the Sentinel-2 tiles' metadata. As with the WMS service, WCS is also only available via a user-preconfigured custom server instance URL.\nSee our OGC API Webinar, which will guide you through different OGC services, including WCS, help you understand the structure, show you how to run the requests in different environments and how they can be integrated with QGIS, ArcGIS and web applications.\nThe base URL for the WCS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wcs/&lt;INSTANCE_ID&gt;\nThe service supports the same output formats as the WMS request (with addition of vector output formats, when \"TILE\" is selected as the COVERAGE) and supports the standard WCS requests: GetCoverage, DescribeCoverage and GetCapabilities. It supports WCS versions 1.0.0 and 1.1.2."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WCS.html#wcs-request",
    "href": "APIs/SentinelHub/OGC/WCS.html#wcs-request",
    "title": "Web Coverage Service",
    "section": "",
    "text": "The Sentinel Hub WCS (Web Coverage Service) service conforms to the WCS standard. Provides access to the same bands product and additional informational layers as the WMS service except only one layer can be specified at once, even when only raw Sentinel-2 bands are used. In addition to raster products, the WCS service can also return the vector features of the Sentinel-2 tiles' metadata. As with the WMS service, WCS is also only available via a user-preconfigured custom server instance URL.\nSee our OGC API Webinar, which will guide you through different OGC services, including WCS, help you understand the structure, show you how to run the requests in different environments and how they can be integrated with QGIS, ArcGIS and web applications.\nThe base URL for the WCS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wcs/&lt;INSTANCE_ID&gt;\nThe service supports the same output formats as the WMS request (with addition of vector output formats, when \"TILE\" is selected as the COVERAGE) and supports the standard WCS requests: GetCoverage, DescribeCoverage and GetCapabilities. It supports WCS versions 1.0.0 and 1.1.2."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WCS.html#wcs-url-parameters",
    "href": "APIs/SentinelHub/OGC/WCS.html#wcs-url-parameters",
    "title": "Web Coverage Service",
    "section": "WCS URL Parameters",
    "text": "WCS URL Parameters\nStandard common WCS URL parameters (parameter names are case insensitive):\n\n\n\nWCS parameter\nInfo\n\n\n\n\nSERVICE\nRequired, must be \"WCS\".\n\n\nVERSION\nWCS version standard. Optional, default: \"1.1.2\". Supported values: \"1.0.0\" and \"1.1.2\".\n\n\nREQUEST\nWhat is requested, valid values: GetCoverage, DescribeCoverage or GetCapabilities. Required.\n\n\nTIME\n(when REQUEST = GetTile) The time range for which to return the results. The result is based on all scenes between the specified times conforming to the cloud coverage criteria and stacked based on priority setting - e.g. most recent on top. It is written as two time values in ISO8601 format separated by a slash, for example: TIME=2016-01-01T09:02:44Z/2016-02-01T11:00:00Z. Reduced accuracy times, where parts of the time string are omitted, are also supported. For example, TIME=2016-07-15/2016-07-15 will be interpreted as \"TIME=2016-07-15T00:00:00Z/2016-07-15T23:59:59Z\" and TIME=2016-07/2016-08 will be interpreted as \"TIME=2016-07-01T00:00:00Z/2016-08-31T23:59:59Z\"  Optional, default: none (the last valid image is returned).  Note: Requesting a single value for TIME parameter is deprecated. Sentinel Hub interpreted it as a time interval [given time - 6 months, given time]. For vast majority of cases this resulted in unnecessary long processing time thus we strongly encourage you to always use the smallest possible time range instead.\n\n\n\nIn addition to the standard WCS URL parameters, the WCS service also supports many custom URL parameters. See Custom service URL parameters for details.\nStandard GetCoverage request URL parameters:\n\n\n\nWCS parameter\nInfo\n\n\n\n\nCOVERAGE\nThe preconfigured (in the instance) layer for which to generate the output image, or \"TILE\" to return the vector format features.\n\n\nFORMAT\nThe returned image format. Optional, default: \"image/png\", other options: \"image/jpeg\", \"image/tiff\". Detailed information about supported values.\n\n\n\nStandard DescribeCoverage request URL parameters:\n\n\n\nWCS parameter\nInfo\n\n\n\n\nComing soon..."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/AdditionalRequestParameters.html",
    "href": "APIs/SentinelHub/OGC/AdditionalRequestParameters.html",
    "title": "Additional Request Parameters",
    "section": "",
    "text": "WMS/WMTS/WFS/WCS services support many custom parameters which affect the generation of the service responses. In the following table, all the available custom parameters, such as preview modes, are listed. All these parameters are optional.\nFor the examples on how to use them, see this documentation.\nNote that atmospheric correction is not a parameter anymore, as we now only support L2A atmospheric correction. Read more about it here.\n\n\n\nCustom parameter\nInfo\nDefault value\nValid value range\nAvailable for\n\n\n\n\nMAXCC\nThe maximum allowable cloud coverage in percent. Cloud coverage is a product average and not viewport accurate hence images may have more cloud cover than specified here.\n100.0\n0.0 - 100.0\nWMS/WMTS/WFS/WCS (when REQUEST = \"GetMap\", \"GetTile\", \"GetCoverage\", \"GetFeature\", \"GetFeatureInfo\" or \"GetIndex\")\n\n\nPRIORITY\nThe priority by which to select and sort the overlapping valid tiles from which the output result is made. For example, using mostRecent will place newer tiles over older ones therefore showing the latest image possible. Using leastCC will place the least cloudy tiles available on top.\nmostRecent\nmostRecent, leastRecent, leastCC, leastTimeDifference, maximumViewingElevation\nWMS/WMTS/WFS/WCS (when REQUEST = \"GetMap\", \"GetTile\", \"GetCoverage\", \"GetFeature\", \"GetFeatureInfo\" or \"GetIndex\")\n\n\nEVALSCRIPT\nThis parameter allows for a custom evaluation script or formula specifying how the output image will be generated from the input bands. See Custom evaluation script usage for details.   EVALSCRIPT parameter has to be BASE64 encoded before it is passed to the service.\n\n\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nEVALSCRIPTURL\nThis parameter allows for a custom evaluation script or formula to be passed as an URL parameter, where the script itself is located (it should be on HTTPS).\n\n\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nPREVIEW\nSee Preview modes for details.\n0\n0, 1, 2\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nGEOMETRY\nOutputs imagery only within the given geometry and cropped to the geometry's minimum bounding box.\n\none WKT string, one WKB hex string, or a list of coordinate pairs representing a polygon (pairs separated by semicolons, components by comma, i.e. 1 1, 2 2;...) Coordinates should be specified using the CRS of the request (i.e. same CRS as BBOX).\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nQUALITY\nUsed only when requesting JPEGs.\n90\n0 - 100; where 0 is the lowest and 100 the highest quality\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nUPSAMPLING\nSets the image upsampling method. Used when the requested resolution is higher than the source resolution.\nNEAREST\nNEAREST, BILINEAR, BICUBIC\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nDOWNSAMPLING\nSets the image downsampling method. Used when the requested resolution is lower than the source resolution.\nNEAREST\nNEAREST, BILINEAR, BICUBIC, BOX\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nWARNINGS\nEnables or disables the display of in-image warnings, like \"No data available for the specified area\".\nYES\nYES, NO\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\n\n\n\nPreview modes make it possible to receive data from across all zoom levels. Sentinel Hub is optimised for full resolution data access as this is what most users need. There are, however, some cases when lower resolution previews of the data make sense as well. This is done by adding the URL parameter PREVIEW. Optional, default=\"0\". Supported values:\n\n\n\nValues\nInfo\n\n\n\n\nPREVIEW=0\nOnly high resolution data from Sentinel-2 is used. This corresponds to real world distances up to 200m/pixel. This is the default.\n\n\nPREVIEW=1\nAllows zooming further out, up to a point. Up to 200m/pixel it displays the same data as PREVIEW=0. In addition to this it uses lower resolution data for real world distances up to 1500m/pixel.   With resolutions between 200m/pixel and 1500m/pixel cloud filtering is no longer applied.\n\n\nPREVIEW=2\nAllows any zoom level but is limited to a maximum of one month of data when most zoomed out. Up to 1500m/pixel it displays the same data as PREVIEW=1. With resolutions lower than 1500m/pixel (more zoomed out) it limits the data to one month prior to the \"TO\" date.   With resolutions less than 200m/pixel (more zoomed out) cloud filtering is no longer applied.\n\n\n\n\n\n\nSatellite images sometimes seem washed out or foggy, as atmosphere absorbs and scatters light on its way to the ground. We can correct for this to get clearer images using atmospheric correction. ESA provides a Sen2Cor processor, that applies atmospheric correction to the input Sentinel-2 L1C data with global coverage. The resulting product is called S2L2A data. To use Atmospheric correction, use the Sentinel-2 L2A (S2L2A) data collection.\nBelow, you can see the difference atmospheric correction makes. The first image of Marseille was made in EO Browser using S2L1C data, and the lower image was made using S2L2A atmospheric correction."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/AdditionalRequestParameters.html#additional-request-parameters",
    "href": "APIs/SentinelHub/OGC/AdditionalRequestParameters.html#additional-request-parameters",
    "title": "Additional Request Parameters",
    "section": "",
    "text": "WMS/WMTS/WFS/WCS services support many custom parameters which affect the generation of the service responses. In the following table, all the available custom parameters, such as preview modes, are listed. All these parameters are optional.\nFor the examples on how to use them, see this documentation.\nNote that atmospheric correction is not a parameter anymore, as we now only support L2A atmospheric correction. Read more about it here.\n\n\n\nCustom parameter\nInfo\nDefault value\nValid value range\nAvailable for\n\n\n\n\nMAXCC\nThe maximum allowable cloud coverage in percent. Cloud coverage is a product average and not viewport accurate hence images may have more cloud cover than specified here.\n100.0\n0.0 - 100.0\nWMS/WMTS/WFS/WCS (when REQUEST = \"GetMap\", \"GetTile\", \"GetCoverage\", \"GetFeature\", \"GetFeatureInfo\" or \"GetIndex\")\n\n\nPRIORITY\nThe priority by which to select and sort the overlapping valid tiles from which the output result is made. For example, using mostRecent will place newer tiles over older ones therefore showing the latest image possible. Using leastCC will place the least cloudy tiles available on top.\nmostRecent\nmostRecent, leastRecent, leastCC, leastTimeDifference, maximumViewingElevation\nWMS/WMTS/WFS/WCS (when REQUEST = \"GetMap\", \"GetTile\", \"GetCoverage\", \"GetFeature\", \"GetFeatureInfo\" or \"GetIndex\")\n\n\nEVALSCRIPT\nThis parameter allows for a custom evaluation script or formula specifying how the output image will be generated from the input bands. See Custom evaluation script usage for details.   EVALSCRIPT parameter has to be BASE64 encoded before it is passed to the service.\n\n\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nEVALSCRIPTURL\nThis parameter allows for a custom evaluation script or formula to be passed as an URL parameter, where the script itself is located (it should be on HTTPS).\n\n\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nPREVIEW\nSee Preview modes for details.\n0\n0, 1, 2\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nGEOMETRY\nOutputs imagery only within the given geometry and cropped to the geometry's minimum bounding box.\n\none WKT string, one WKB hex string, or a list of coordinate pairs representing a polygon (pairs separated by semicolons, components by comma, i.e. 1 1, 2 2;...) Coordinates should be specified using the CRS of the request (i.e. same CRS as BBOX).\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nQUALITY\nUsed only when requesting JPEGs.\n90\n0 - 100; where 0 is the lowest and 100 the highest quality\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nUPSAMPLING\nSets the image upsampling method. Used when the requested resolution is higher than the source resolution.\nNEAREST\nNEAREST, BILINEAR, BICUBIC\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nDOWNSAMPLING\nSets the image downsampling method. Used when the requested resolution is lower than the source resolution.\nNEAREST\nNEAREST, BILINEAR, BICUBIC, BOX\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\nWARNINGS\nEnables or disables the display of in-image warnings, like \"No data available for the specified area\".\nYES\nYES, NO\nWMS/WMTS/WCS (when REQUEST = \"GetMap\", \"GetTile\" or \"GetCoverage\")\n\n\n\n\n\nPreview modes make it possible to receive data from across all zoom levels. Sentinel Hub is optimised for full resolution data access as this is what most users need. There are, however, some cases when lower resolution previews of the data make sense as well. This is done by adding the URL parameter PREVIEW. Optional, default=\"0\". Supported values:\n\n\n\nValues\nInfo\n\n\n\n\nPREVIEW=0\nOnly high resolution data from Sentinel-2 is used. This corresponds to real world distances up to 200m/pixel. This is the default.\n\n\nPREVIEW=1\nAllows zooming further out, up to a point. Up to 200m/pixel it displays the same data as PREVIEW=0. In addition to this it uses lower resolution data for real world distances up to 1500m/pixel.   With resolutions between 200m/pixel and 1500m/pixel cloud filtering is no longer applied.\n\n\nPREVIEW=2\nAllows any zoom level but is limited to a maximum of one month of data when most zoomed out. Up to 1500m/pixel it displays the same data as PREVIEW=1. With resolutions lower than 1500m/pixel (more zoomed out) it limits the data to one month prior to the \"TO\" date.   With resolutions less than 200m/pixel (more zoomed out) cloud filtering is no longer applied.\n\n\n\n\n\n\nSatellite images sometimes seem washed out or foggy, as atmosphere absorbs and scatters light on its way to the ground. We can correct for this to get clearer images using atmospheric correction. ESA provides a Sen2Cor processor, that applies atmospheric correction to the input Sentinel-2 L1C data with global coverage. The resulting product is called S2L2A data. To use Atmospheric correction, use the Sentinel-2 L2A (S2L2A) data collection.\nBelow, you can see the difference atmospheric correction makes. The first image of Marseille was made in EO Browser using S2L1C data, and the lower image was made using S2L2A atmospheric correction."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WFS.html",
    "href": "APIs/SentinelHub/OGC/WFS.html",
    "title": "Web Feature Service",
    "section": "",
    "text": "The Sentinel Hub WFS (Web Feature Service) service conforms to the WFS standard. It provides access to the geometric (vector) metadata about the available data collection tiles. As with the WMS service, WFS is also only available via a user-preconfigured custom server instance URL.\nSee our OGC API Webinar, which will guide you through different OGC services, including WFS, help you understand the structure, show you how to run the requests in different environments and how they can be integrated with QGIS, ArcGIS and web applications.\nThe base URL for the WFS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wfs/&lt;INSTANCE_ID&gt;\nThe service supports many vector formats, including GML, XML, JSON and also raw HTML and plain text. Check GetCapabilities for a list of all supported formats. It supports WFS version 2.0.0."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WFS.html#wfs-request",
    "href": "APIs/SentinelHub/OGC/WFS.html#wfs-request",
    "title": "Web Feature Service",
    "section": "",
    "text": "The Sentinel Hub WFS (Web Feature Service) service conforms to the WFS standard. It provides access to the geometric (vector) metadata about the available data collection tiles. As with the WMS service, WFS is also only available via a user-preconfigured custom server instance URL.\nSee our OGC API Webinar, which will guide you through different OGC services, including WFS, help you understand the structure, show you how to run the requests in different environments and how they can be integrated with QGIS, ArcGIS and web applications.\nThe base URL for the WFS service:\nhttps://sh.dataspace.copernicus.eu/ogc/wfs/&lt;INSTANCE_ID&gt;\nThe service supports many vector formats, including GML, XML, JSON and also raw HTML and plain text. Check GetCapabilities for a list of all supported formats. It supports WFS version 2.0.0."
  },
  {
    "objectID": "APIs/SentinelHub/OGC/WFS.html#wfs-url-parameters",
    "href": "APIs/SentinelHub/OGC/WFS.html#wfs-url-parameters",
    "title": "Web Feature Service",
    "section": "WFS URL Parameters",
    "text": "WFS URL Parameters\nStandard common WFS URL parameters (parameter names are case insensitive):\n\n\n\nWFS parameter\nInfo\n\n\n\n\nSERVICE\nRequired, must be \"WFS\".\n\n\nVERSION\nWFS version standard. Optional, default: \"2.0.0\". Supported values: \"2.0.0\".\n\n\nREQUEST\nWhat is requested, valid values: DescribeFeatureType, GetFeature or GetCapabilities. Required.\n\n\nTIME\n(when REQUEST = GetTile) The time range for which to return the results. The result is based on all scenes between the specified times conforming to the cloud coverage criteria and stacked based on priority setting - e.g. most recent on top. It is written as two time values in ISO8601 format separated by a slash, for example: TIME=2016-01-01T09:02:44Z/2016-02-01T11:00:00Z. Reduced accuracy times, where parts of the time string are omitted, are also supported. For example, TIME=2016-07-15/2016-07-15 will be interpreted as \"TIME=2016-07-15T00:00:00Z/2016-07-15T23:59:59Z\" and TIME=2016-07/2016-08 will be interpreted as \"TIME=2016-07-01T00:00:00Z/2016-08-31T23:59:59Z\"  Optional, default: none (the last valid image is returned).  Note: Requesting a single value for TIME parameter is deprecated. Sentinel Hub interpreted it as a time interval [given time - 6 months, given time]. For vast majority of cases this resulted in unnecessary long processing time thus we strongly encourage you to always use the smallest possible time range instead.\n\n\n\nIn addition to the standard WFS URL parameters, the WFS service also supports many custom URL parameters. See Custom service URL parameters for details.\nStandard GetFeature request URL parameters:\n\n\n\nWFS parameter\nInfo\n\n\n\n\nTYPENAMES\nMore information found below.\n\n\nMAXFEATURES\nThe maximum number of features to be returned by a single request. Default value: 100. Valid range: 0..100.\n\n\nBBOX\nThe bounding box area for which to return the features.\n\n\nSRSNAME\nThe CRS in which the BBOX is specified.\n\n\nFEATURE_OFFSET\nOffset controls the starting point within the returned features.\n\n\nOUTPUTFORMAT\nThe MIME format of the returned features.\n\n\n\nStandard DescribeFeatureType request URL parameters:\n\n\n\nWFS parameter\nInfo\n\n\n\n\nTYPENAMES\nMore information found below.\n\n\nOUTPUTFORMAT\nThe MIME format of the returned features.\n\n\n\n\nTypenames\n\n\n\nData collection\nTYPENAMES for services\n\n\n\n\nSENTINEL-2 L1C\nDSS1\n\n\nSENTINEL-2 L2A\nDSS2\n\n\nSENTINEL-1 IW\nDSS3\n\n\nSENTINEL-1 EW\nDSS3\n\n\nSENTINEL-1 EW SH\nDSS3\n\n\nSENTINEL 3 OLCI\nDSS8\n\n\nSENTINEL 3 L2\nDSS22\n\n\nSENTINEL 3 SLSTR\nDSS9\n\n\nSENTINEL 5P\nDSS7\n\n\nBYOC\nbyoc-&lt;collectionId&gt;"
  },
  {
    "objectID": "APIs/SentinelHub/Data.html",
    "href": "APIs/SentinelHub/Data.html",
    "title": "Data",
    "section": "",
    "text": "Some of the data collections available in the Copernicus Data Space Ecosystem are indexed (or we could say “imported”) in Sentinel Hub, which means you can use Sentinel Hub APIs to work with these data collections. Each of the data collections available in Sentinel Hub has its own subchapter below, which describes how the data is pre-processed and how you can access it with Sentinel Hub. For a general description of these data collections and related satellite missions, please refer to the general Copernicus Data Space Ecosystem Data chapter.\n\n\n\n\n\n\n\nSentinel-1 GRD\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2 L1C\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2 L2A\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-3 OLCI L1B\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-3 OLCI L2\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-3 SLSTR L1B\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-5P L2\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigital Elevation Model (DEM) Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nBring Your Own COG / Batch\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Fusion\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Crs.html",
    "href": "APIs/SentinelHub/Process/Crs.html",
    "title": "CRS support",
    "section": "",
    "text": "The list of coordinate reference systems supported by Sentinel Hub API is provided below. The coordinate reference system must be set with an URL starting with http://www.opengis.net/def/crs/ and it must be set under the field input.bounds.properties.crs, e.g. request in WGS 84 reference system, defined with the URL http://www.opengis.net/def/crs/EPSG/0/4326:"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Crs.html#wgs-84",
    "href": "APIs/SentinelHub/Process/Crs.html#wgs-84",
    "title": "CRS support",
    "section": "WGS 84:",
    "text": "WGS 84:\n\nhttp://www.opengis.net/def/crs/OGC/1.3/CRS84\nhttp://www.opengis.net/def/crs/EPSG/0/4326"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Crs.html#wgs-84--pseudo-mercator",
    "href": "APIs/SentinelHub/Process/Crs.html#wgs-84--pseudo-mercator",
    "title": "CRS support",
    "section": "WGS 84 / Pseudo-Mercator:",
    "text": "WGS 84 / Pseudo-Mercator:\n\nhttp://www.opengis.net/def/crs/EPSG/0/3857"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Crs.html#utm-northern-hemisphere",
    "href": "APIs/SentinelHub/Process/Crs.html#utm-northern-hemisphere",
    "title": "CRS support",
    "section": "UTM northern hemisphere:",
    "text": "UTM northern hemisphere:\n\nhttp://www.opengis.net/def/crs/EPSG/0/32601\nhttp://www.opengis.net/def/crs/EPSG/0/32602\n...\nhttp://www.opengis.net/def/crs/EPSG/0/32660\n\nThe last two digits of EPSG codes above represent the number of corresponding UTM zone in northern hemisphere, e.g. use http://www.opengis.net/def/crs/EPSG/0/32612 for UTM zone 12N."
  },
  {
    "objectID": "APIs/SentinelHub/Process/Crs.html#utm-southern-hemisphere",
    "href": "APIs/SentinelHub/Process/Crs.html#utm-southern-hemisphere",
    "title": "CRS support",
    "section": "UTM southern hemisphere:",
    "text": "UTM southern hemisphere:\n\nhttp://www.opengis.net/def/crs/EPSG/0/32701\nhttp://www.opengis.net/def/crs/EPSG/0/32702\n...\nhttp://www.opengis.net/def/crs/EPSG/0/32760\n\nThe last two digits of EPSG codes above represent the number of corresponding UTM zone in southern hemisphere, e.g. use http://www.opengis.net/def/crs/EPSG/0/32712 for UTM zone 12S."
  },
  {
    "objectID": "APIs/SentinelHub/Process/Crs.html#others",
    "href": "APIs/SentinelHub/Process/Crs.html#others",
    "title": "CRS support",
    "section": "Others:",
    "text": "Others:\n\nhttp://www.opengis.net/def/crs/EPSG/0/2154 (RGF93 / Lambert-93)\nhttp://www.opengis.net/def/crs/EPSG/0/2180 (ETRS89 / Poland CS92)\nhttp://www.opengis.net/def/crs/EPSG/0/2193 (NZGD2000 / New Zealand Transverse Mercator 2000)\nhttp://www.opengis.net/def/crs/EPSG/0/3003 (Monte Mario / Italy zone 1)\nhttp://www.opengis.net/def/crs/EPSG/0/3004 (Monte Mario / Italy zone 2)\nhttp://www.opengis.net/def/crs/EPSG/0/3005 (NAD83 / BC Albers)\nhttp://www.opengis.net/def/crs/EPSG/0/3006 (SWEREF99 TM)\nhttp://www.opengis.net/def/crs/EPSG/0/3031 (WGS84 / Antarctic Polar Stereographic)\nhttp://www.opengis.net/def/crs/EPSG/0/3035 (ETRS89 / LAEA Europe)\nhttp://www.opengis.net/def/crs/EPSG/0/3161 (NAD83 / Ontario MNR Lambert)\nhttp://www.opengis.net/def/crs/EPSG/0/3346 (LKS94 / Lithuania TM)\nhttp://www.opengis.net/def/crs/EPSG/0/3413 (NSIDC Sea Ice Polar Stereographic North)\nhttp://www.opengis.net/def/crs/EPSG/0/3416 (ETRS89 / Austria Lambert)\nhttp://www.opengis.net/def/crs/EPSG/0/3578 (NAD83 / Yukon Albers)\nhttp://www.opengis.net/def/crs/EPSG/0/3580 (NAD83 / NWT Lambert)\nhttp://www.opengis.net/def/crs/EPSG/0/3765 (HTRS 96 / TM)\nhttp://www.opengis.net/def/crs/EPSG/0/3794 (D96 / TM)\nhttp://www.opengis.net/def/crs/EPSG/0/3844 (Pulkovo 1942(58) / Stereo70)\nhttp://www.opengis.net/def/crs/EPSG/0/3912 (D48 / GK)\nhttp://www.opengis.net/def/crs/EPSG/0/3995 (WGS 84 / Arctic Polar Stereographic)\nhttp://www.opengis.net/def/crs/EPSG/0/4026 (MOLDREF99 / Moldova TM)\nhttp://www.opengis.net/def/crs/EPSG/0/5514 (S-JTSK / Krovak East North)\nhttp://www.opengis.net/def/crs/EPSG/0/28992 (Amersfoort / RD New)\nhttp://www.opengis.net/def/crs/EPSG/0/32184 (NAD83 / MTM zone 4)"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/S5PL2.html",
    "href": "APIs/SentinelHub/Process/Examples/S5PL2.html",
    "title": "Examples for S5PL2",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nCarbon Monoxide, CO (RGB visualization and transparency with dataMask)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CO\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 0.0\nconst maxVal = 0.1\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CO)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNitrogen Dioxide, NO2 (NRTI timeliness, RGB visualization and transparency with dataMask)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"NO2\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 0.0\nconst maxVal = 0.0001\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.NO2)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-30T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    },\n                    \"timeliness\": \"NRTI\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nFormaldehyde, HCHO (float32 format, specific value for no data, GeoTIFF)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"HCHO\", \"dataMask\"],\n    output: { bands: 1, sampleType: \"FLOAT32\" },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  if (sample.dataMask == 1) {\n    return [sample.HCHO]\n  } else {\n    return [-9999]\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-30T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"image/tiff\"})\n\n\nOzone, O3 (RPRO timeliness, streched values and dataMask)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"O3\", \"dataMask\"],\n    output: { bands: 2 },\n  }\n}\n\nfunction evaluatePixel(sample, scene) {\n  var maxVal = 0.36\n  return [sample.O3 / maxVal, sample.dataMask]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-22T00:00:00Z\",\n                        \"to\": \"2019-04-23T00:00:00Z\",\n                    },\n                    \"timeliness\": \"RPRO\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nSulfur Dioxide, SO2 (minQa=20 applied, streched values)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"SO2\", \"dataMask\"],\n    output: { bands: 2 },\n  }\n}\n\nfunction evaluatePixel(sample, scene) {\n  var maxVal = 0.01\n  return [sample.SO2 / maxVal, sample.dataMask]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-30T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"minQa\": 20},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nMethane, CH4\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CH4\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 1600.0\nconst maxVal = 2000.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CH4)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                10,\n                20,\n                15,\n                25,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nAER AI 340 and 380\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"AER_AI_340_380\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = -1.0\nconst maxVal = 5.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.AER_AI_340_380)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nAER AI 354 and 388\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"AER_AI_354_388\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = -1.0\nconst maxVal = 5.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.AER_AI_354_388)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCloud base height\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CLOUD_BASE_HEIGHT\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 0\nconst maxVal = 20000.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CLOUD_BASE_HEIGHT)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCloud base pressure\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CLOUD_BASE_PRESSURE\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 10000.0\nconst maxVal = 110000.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CLOUD_BASE_PRESSURE)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nEffective radiometric cloud fraction\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CLOUD_FRACTION\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 0.0\nconst maxVal = 1.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CLOUD_FRACTION)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCloud optical thickness\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CLOUD_OPTICAL_THICKNESS\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 0.0\nconst maxVal = 250.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CLOUD_OPTICAL_THICKNESS)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCloud top height\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CLOUD_TOP_HEIGHT\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 0.0\nconst maxVal = 20000.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CLOUD_TOP_HEIGHT)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCloud top pressure\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"CLOUD_TOP_PRESSURE\", \"dataMask\"],\n    output: { bands: 4 },\n  }\n}\n\nconst minVal = 10000.0\nconst maxVal = 110000.0\nconst diff = maxVal - minVal\n\nconst rainbowColors = [\n  [minVal, [0, 0, 0.5]],\n  [minVal + 0.125 * diff, [0, 0, 1]],\n  [minVal + 0.375 * diff, [0, 1, 1]],\n  [minVal + 0.625 * diff, [1, 1, 0]],\n  [minVal + 0.875 * diff, [1, 0, 0]],\n  [maxVal, [0.5, 0, 0]],\n]\n\nconst viz = new ColorRampVisualizer(rainbowColors)\n\nfunction evaluatePixel(sample) {\n  var rgba = viz.process(sample.CLOUD_TOP_PRESSURE)\n  rgba.push(sample.dataMask)\n  return rgba\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13,\n                45,\n                15,\n                47,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-5p-l2\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-28T00:00:00Z\",\n                        \"to\": \"2018-12-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/S1GRD.html",
    "href": "APIs/SentinelHub/Process/Examples/S1GRD.html",
    "title": "Examples for S1GRD",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nS1GRD orthorectified linear gamma0 VV between 0 and 0.5 (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1360000,\n                5121900,\n                1370000,\n                5131900,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\"orthorectify\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD orthorectified linear gamma0 VV between 0 and 0.5 in approximate real-world 10 m resolution (IW) (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                268574.43,\n                4624494.84,\n                276045.41,\n                4631696.16,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"},\n        },\n        \"data\": [\n            {\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    },\n                    \"resolution\": \"HIGH\",\n                    \"acquisitionMode\": \"IW\",\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"demInstance\": \"COPERNICUS_30\",\n                },\n                \"type\": \"sentinel-1-grd\",\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 10,\n        \"resy\": 10,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD orthorectified with Copernicus DEM 30 (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1360000,\n                5121900,\n                1370000,\n                5131900,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"demInstance\": \"COPERNICUS_30\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD orthorectified linear gamma0 VV, ascending orbit direction, GeoTIFF in EPSG:32648 (UTM zone 48N)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1, sampleType: SampleType.FLOAT32 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                699800,\n                1190220,\n                709800,\n                1200220,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32648\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2017-11-15T00:00:00Z\",\n                        \"to\": \"2017-11-15T23:00:00Z\",\n                    },\n                    \"acquisitionMode\": \"IW\",\n                    \"polarization\": \"DV\",\n                    \"orbitDirection \": \"ASCENDING\",\n                },\n                \"processing\": {\n                    \"backCoeff\": \"GAMMA0_ELLIPSOID\",\n                    \"orthorectify\": \"true\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 1000,\n        \"height\": 1000,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"image/tiff\"})\n\n\nS1GRD orthorectified decibel gamma0 VH between -20 dB and 0 dB (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VH\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [toDb(samples.VH)]\n}\n\n// visualizes decibels from -20 to 0\n\nfunction toDb(linear) {\n  // the following commented out lines are simplified below\n  // var log = 10 * Math.log(linear) / Math.LN10\n  // var val = Math.max(0, (log + 20) / 20)\n  return Math.max(0, Math.log(linear) * 0.21714724095 + 1)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1360000,\n                5121900,\n                1370000,\n                5131900,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\"orthorectify\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD orthorectified decibel gamma0 RGB composite of VV, VH, VV/VH/10 between -20 dB and 0 dB (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\", \"VH\"],\n    output: { id: \"default\", bands: 3 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  var vvdB = toDb(samples.VV)\n  var vhdB = toDb(samples.VH)\n  return [vvdB, vhdB, vvdB / vhdB / 10]\n}\n\n// displays VV in decibels from -20 to 0\n\nfunction toDb(linear) {\n  // the following commented out lines are simplified below\n  // var log = 10 * Math.log(linear) / Math.LN10\n  // var val = Math.max(0, (log + 20) / 20)\n  return Math.max(0, Math.log(linear) * 0.21714724095 + 1)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1360000,\n                5121900,\n                1370000,\n                5131900,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\"orthorectify\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD non-orthorectified linear sigma0 VH between 0 and 0.5 (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VH\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VH]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1360000,\n                5121900,\n                1370000,\n                5131900,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"false\",\n                    \"backCoeff\": \"SIGMA0_ELLIPSOID\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD non-orthorectified Lee speckle filtered decibel gamma0 HH between -20 dB and +10 dB (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"HH\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [toDb(samples.HH)]\n}\n\n// visualizes decibels from -20 to +10\n\nfunction toDb(linear) {\n  var log = (10 * Math.log(linear)) / Math.LN10\n  return Math.max(0, (log + 20) / 30)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                18400000,\n                -11330000,\n                18500000,\n                -11430000,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"acquisitionMode\": \"EW\",\n                    \"timeRange\": {\n                        \"from\": \"2020-09-29T00:00:00Z\",\n                        \"to\": \"2020-09-29T23:59:59Z\",\n                    },\n                },\n                \"processing\": {\n                    \"orthorectify\": \"false\",\n                    \"backCoeff\": \"GAMMA0_ELLIPSOID\",\n                    \"speckleFilter\": {\n                        \"type\": \"LEE\",\n                        \"windowSizeX\": 5,\n                        \"windowSizeY\": 5,\n                    },\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 1000,\n        \"height\": 1000,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD orthorectified gamma0 two month temporal averaged decibel VV between -20 dB and 0 dB (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\", \"dataMask\"],\n    output: { id: \"default\", bands: 1 },\n    mosaicking: Mosaicking.ORBIT,\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [calculateAverage(samples)]\n}\n\nfunction calculateAverage(samples) {\n  var sum = 0\n  var nValid = 0\n  for (let sample of samples) {\n    if (sample.dataMask != 0) {\n      nValid++\n      sum += toDb(sample.VV)\n    }\n  }\n  return sum / nValid\n}\n\n// visualizes decibels from -20 to 0\n\nfunction toDb(linear) {\n  // the following commented out lines are simplified below\n  // var log = 10 * Math.log(linear) / Math.LN10\n  // var val = Math.max(0, (log + 20) / 20)\n  return Math.max(0, Math.log(linear) * 0.21714724095 + 1)\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1360000,\n                5121900,\n                1370000,\n                5131900,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-01T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    },\n                    \"orbitDirection\": \"ASCENDING\",\n                },\n                \"processing\": {\"orthorectify\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD radiometrically terrain corrected linear gamma0 VV between 0 and 0.5 (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1095431,\n                5714610,\n                1146158,\n                5754129,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"GAMMA0_TERRAIN\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD radiometrically terrain corrected using Copernicus DEM 30 (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1095431,\n                5714610,\n                1146158,\n                5754129,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"GAMMA0_TERRAIN\",\n                    \"demInstance\": \"COPERNICUS_30\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD radiometrically terrain corrected with custom DEM oversampling of 3 (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\"],\n    output: { id: \"default\", bands: 1 },\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return [2 * samples.VV]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                1095431,\n                5714610,\n                1146158,\n                5754129,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/3857\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"GAMMA0_TERRAIN\",\n                    \"radiometricTerrainOversampling\": 3,\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nS1GRD radiometrically terrain corrected gamma0 VV and auxiliary data: local incidence angle, scattering area, and shadow mask\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"VV\", \"localIncidenceAngle\", \"scatteringArea\", \"shadowMask\"],\n    output: [\n      { id: \"s1_rtc_VV_area\", bands: 2, sampleType: \"FLOAT32\" },\n      { id: \"s1_rtc_angle_mask\", bands: 2, sampleType: \"UINT8\" },\n    ],\n  }\n}\n\nfunction evaluatePixel(samples) {\n  return {\n    s1_rtc_VV_area: [samples.VV, samples.scatteringArea],\n    s1_rtc_angle_mask: [samples.localIncidenceAngle, samples.shadowMask],\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                565556.94,\n                5048644.47,\n                600656.56,\n                5076658.33,\n            ],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32632\"},\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-1-grd\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-02-02T00:00:00Z\",\n                        \"to\": \"2019-04-02T23:59:59Z\",\n                    }\n                },\n                \"processing\": {\n                    \"orthorectify\": \"true\",\n                    \"backCoeff\": \"GAMMA0_TERRAIN\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 1024,\n        \"height\": 796,\n        \"responses\": [\n            {\n                \"identifier\": \"s1_rtc_VV_area\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"s1_rtc_angle_mask\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/DEM.html",
    "href": "APIs/SentinelHub/Process/Examples/DEM.html",
    "title": "Examples for DEM",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nCopernicus DEM 30 image (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output: { bands: 1 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [sample.DEM / 1000]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"dem\",\n                \"dataFilter\": {\"demInstance\": \"COPERNICUS_30\"},\n                \"processing\": {\n                    \"upsampling\": \"BILINEAR\",\n                    \"downsampling\": \"BILINEAR\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCopernicus DEM 30, 0.0003° (~33m) resolution (tiff)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output: { bands: 1 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [sample.DEM / 1000]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"dem\",\n                \"dataFilter\": {\"demInstance\": \"COPERNICUS_30\"},\n                \"processing\": {\n                    \"upsampling\": \"BILINEAR\",\n                    \"downsampling\": \"BILINEAR\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 0.0003,\n        \"resy\": 0.0003,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCopernicus DEM 90 values, orthometric heights (tif)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.FLOAT32,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [sample.DEM]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"dem\",\n                \"dataFilter\": {\"demInstance\": \"COPERNICUS_90\"},\n                \"processing\": {\n                    \"upsampling\": \"BILINEAR\",\n                    \"downsampling\": \"BILINEAR\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCopernicus DEM 90 values, ellipsoidal heights (tif)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.FLOAT32,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [sample.DEM]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"dem\",\n                \"dataFilter\": {\"demInstance\": \"COPERNICUS_90\"},\n                \"processing\": {\n                    \"egm\": True,\n                    \"upsampling\": \"BILINEAR\",\n                    \"downsampling\": \"BILINEAR\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nCopernicus DEM 90 image at sea level (png)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  if (sample.DEM &gt; 0) {\n    return [0, sample.DEM / 1000, 0]\n  } else {\n    return [0, 0, -sample.DEM / 100]\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                10.082016,\n                42.625876,\n                10.496063,\n                42.927268,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"dem\",\n                \"dataFilter\": {\"demInstance\": \"COPERNICUS_90\"},\n                \"processing\": {\n                    \"upsampling\": \"BILINEAR\",\n                    \"downsampling\": \"BILINEAR\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "APIs/SentinelHub/Process/Examples/S2L1C.html",
    "href": "APIs/SentinelHub/Process/Examples/S2L1C.html",
    "title": "Examples for S2L1C",
    "section": "",
    "text": "The requests below are written in python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nTrue Color\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: {\n      bands: 3,\n      sampleType: \"AUTO\", // default value - scales the output values from [0,1] to [0,255].\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color (EPSG 32633)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"},\n            \"bbox\": [\n                408553.58,\n                5078145.48,\n                466081.02,\n                5126576.61,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color, resolution (EPSG 32633)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"},\n            \"bbox\": [\n                408553.58,\n                5078145.48,\n                466081.02,\n                5126576.61,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"resx\": 100,\n        \"resy\": 100,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue Color, multi-band GeoTIff\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"image/tiff\"})\n\n\nTrue Color, mosaicking with leastRecent\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { bands: 3 },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-10-11T00:00:00Z\",\n                        \"to\": \"2018-11-18T00:00:00Z\",\n                    },\n                    \"mosaickingOrder\": \"leastRecent\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/png\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nTrue color and metadata (multi-part response GeoTIFF and json)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    mosaicking: Mosaicking.ORBIT,\n    output: { id: \"default\", bands: 3 },\n  }\n}\n\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n  outputMetadata.userData = { scenes: scenes.orbits }\n}\n\nfunction evaluatePixel(samples) {\n  return [2.5 * samples[0].B04, 2.5 * samples[0].B03, 2.5 * samples[0].B02]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [\n                13.822174072265625,\n                45.85080395917834,\n                14.55963134765625,\n                46.29191774991382,\n            ]\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-12-27T00:00:00Z\",\n                        \"to\": \"2018-12-27T23:59:59Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"userdata\",\n                \"format\": {\"type\": \"application/json\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nTrue color multi-part-reponse (different formats and SampleType)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B03\", \"B02\"],\n        units: \"REFLECTANCE\", // default units\n      },\n    ],\n    output: [\n      {\n        id: \"default\",\n        bands: 3,\n        sampleType: \"AUTO\", // default  - scales the output values from input values [0,1] to [0,255].\n      },\n      {\n        id: \"true_color_8bit\",\n        bands: 3,\n        sampleType: \"UINT8\", //floating point values are automatically rounded to the nearest integer by the service.\n      },\n      {\n        id: \"true_color_16bit\",\n        bands: 3,\n        sampleType: \"UINT16\", //floating point values are automatically rounded to the nearest integer by the service.\n      },\n      {\n        id: \"true_color_32float\",\n        bands: 3,\n        sampleType: \"FLOAT32\",\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return {\n    // output band values are scaled from [0,1] to [0,255]. Multiply by 2.5 to increase brightness\n    default: [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02],\n\n    // Multiply input reflectance values by 2.5 to increase brighness and by 255 to return the band values clamped to [0, 255] unsigned 8 bit range.\n    true_color_8bit: [\n      2.5 * sample.B04 * 255,\n      2.5 * sample.B03 * 255,\n      2.5 * sample.B02 * 255,\n    ],\n\n    // Multiply input reflectance values by 2.5 to increase brightness and by 65535 to return the band values clamped to [0, 65535] unsigned 16 bit range.\n    true_color_16bit: [\n      2.5 * sample.B04 * 65535,\n      2.5 * sample.B03 * 65535,\n      2.5 * sample.B02 * 65535,\n    ],\n\n    // Returns band reflectance.\n    true_color_32float: [sample.B04, sample.B03, sample.B02],\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"bbox\": [\n                12.206251,\n                41.627351,\n                12.594042,\n                41.856879,\n            ],\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2018-06-01T00:00:00Z\",\n                        \"to\": \"2018-08-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/jpeg\"},\n            },\n            {\n                \"identifier\": \"true_color_8bit\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"true_color_16bit\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n            {\n                \"identifier\": \"true_color_32float\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nNDVI as jpeg image with bounds given as polygon\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 3,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n\n  if (ndvi &lt; -0.5) return [0.05, 0.05, 0.05]\n  else if (ndvi &lt; -0.2) return [0.75, 0.75, 0.75]\n  else if (ndvi &lt; -0.1) return [0.86, 0.86, 0.86]\n  else if (ndvi &lt; 0) return [0.92, 0.92, 0.92]\n  else if (ndvi &lt; 0.025) return [1, 0.98, 0.8]\n  else if (ndvi &lt; 0.05) return [0.93, 0.91, 0.71]\n  else if (ndvi &lt; 0.075) return [0.87, 0.85, 0.61]\n  else if (ndvi &lt; 0.1) return [0.8, 0.78, 0.51]\n  else if (ndvi &lt; 0.125) return [0.74, 0.72, 0.42]\n  else if (ndvi &lt; 0.15) return [0.69, 0.76, 0.38]\n  else if (ndvi &lt; 0.175) return [0.64, 0.8, 0.35]\n  else if (ndvi &lt; 0.2) return [0.57, 0.75, 0.32]\n  else if (ndvi &lt; 0.25) return [0.5, 0.7, 0.28]\n  else if (ndvi &lt; 0.3) return [0.44, 0.64, 0.25]\n  else if (ndvi &lt; 0.35) return [0.38, 0.59, 0.21]\n  else if (ndvi &lt; 0.4) return [0.31, 0.54, 0.18]\n  else if (ndvi &lt; 0.45) return [0.25, 0.49, 0.14]\n  else if (ndvi &lt; 0.5) return [0.19, 0.43, 0.11]\n  else if (ndvi &lt; 0.55) return [0.13, 0.38, 0.07]\n  else if (ndvi &lt; 0.6) return [0.06, 0.33, 0.04]\n  else return [0, 0.27, 0]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\n                    \"type\": \"image/jpeg\",\n                    \"quality\": 80,\n                },\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nExact NDVI values using a floating point GeoTIFF\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.FLOAT32,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n  return [ndvi]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"harmonizeValues\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNDVI values as INT16 raster\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n        units: \"REFLECTANCE\",\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.INT16, //floating point values are automatically rounded to the nearest integer by the service.\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n  // Return NDVI multiplied by 10000 as integers to save processing units. To obtain NDVI values, simply divide the resulting pixel values by 10000.\n  return [ndvi * 10000]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"harmonizeValues\": \"true\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)\n\n\nNDVI image and value (multi-part response png and GeoTIFF)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\"B04\", \"B08\"],\n      },\n    ],\n    output: [\n      {\n        id: \"default\",\n        bands: 1,\n        sampleType: SampleType.FLOAT32,\n      },\n      {\n        id: \"ndvi_image\",\n        bands: 3,\n        sampleType: SampleType.AUTO,\n      },\n    ],\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n\n  if (ndvi &lt; -0.5) image = [0.05, 0.05, 0.05]\n  else if (ndvi &lt; -0.2) image = [0.75, 0.75, 0.75]\n  else if (ndvi &lt; -0.1) image = [0.86, 0.86, 0.86]\n  else if (ndvi &lt; 0) image = [0.92, 0.92, 0.92]\n  else if (ndvi &lt; 0.025) image = [1, 0.98, 0.8]\n  else if (ndvi &lt; 0.05) image = [0.93, 0.91, 0.71]\n  else if (ndvi &lt; 0.075) image = [0.87, 0.85, 0.61]\n  else if (ndvi &lt; 0.1) image = [0.8, 0.78, 0.51]\n  else if (ndvi &lt; 0.125) image = [0.74, 0.72, 0.42]\n  else if (ndvi &lt; 0.15) image = [0.69, 0.76, 0.38]\n  else if (ndvi &lt; 0.175) image = [0.64, 0.8, 0.35]\n  else if (ndvi &lt; 0.2) image = [0.57, 0.75, 0.32]\n  else if (ndvi &lt; 0.25) image = [0.5, 0.7, 0.28]\n  else if (ndvi &lt; 0.3) image = [0.44, 0.64, 0.25]\n  else if (ndvi &lt; 0.35) image = [0.38, 0.59, 0.21]\n  else if (ndvi &lt; 0.4) image = [0.31, 0.54, 0.18]\n  else if (ndvi &lt; 0.45) image = [0.25, 0.49, 0.14]\n  else if (ndvi &lt; 0.5) image = [0.19, 0.43, 0.11]\n  else if (ndvi &lt; 0.55) image = [0.13, 0.38, 0.07]\n  else if (ndvi &lt; 0.6) image = [0.06, 0.33, 0.04]\n  else image = [0, 0.27, 0]\n\n  return {\n    default: [ndvi],\n    ndvi_image: image,\n  }\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"ndvi_image\",\n                \"format\": {\"type\": \"image/png\"},\n            },\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            },\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request, headers={\"Accept\": \"application/tar\"})\n\n\nAll S2L1C raw bands, original data (no harmonization)\nLearn about harmonization here.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        bands: [\n          \"B01\",\n          \"B02\",\n          \"B03\",\n          \"B04\",\n          \"B05\",\n          \"B06\",\n          \"B07\",\n          \"B08\",\n          \"B8A\",\n          \"B09\",\n          \"B10\",\n          \"B11\",\n          \"B12\",\n        ],\n        units: \"DN\",\n      },\n    ],\n    output: {\n      id: \"default\",\n      bands: 13,\n      sampleType: SampleType.UINT16,\n    },\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [\n    sample.B01,\n    sample.B02,\n    sample.B03,\n    sample.B04,\n    sample.B05,\n    sample.B06,\n    sample.B07,\n    sample.B08,\n    sample.B8A,\n    sample.B09,\n    sample.B10,\n    sample.B11,\n    sample.B12,\n  ]\n}\n\"\"\"\n\nrequest = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"},\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04803276062012,\n                            41.805773608962866,\n                        ],\n                        [\n                            -94.06738758087158,\n                            41.805901566741305,\n                        ],\n                        [\n                            -94.06734466552734,\n                            41.7967199475024,\n                        ],\n                        [\n                            -94.06223773956299,\n                            41.79144072064381,\n                        ],\n                        [\n                            -94.0504789352417,\n                            41.791376727347966,\n                        ],\n                        [\n                            -94.05039310455322,\n                            41.7930725281021,\n                        ],\n                        [\n                            -94.04798984527588,\n                            41.7930725281021,\n                        ],\n                    ]\n                ],\n            },\n        },\n        \"data\": [\n            {\n                \"type\": \"sentinel-2-l1c\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2022-10-01T00:00:00Z\",\n                        \"to\": \"2022-10-31T00:00:00Z\",\n                    }\n                },\n                \"processing\": {\"harmonizeValues\": \"false\"},\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": 512,\n        \"height\": 512,\n        \"responses\": [\n            {\n                \"identifier\": \"default\",\n                \"format\": {\"type\": \"image/tiff\"},\n            }\n        ],\n    },\n    \"evalscript\": evalscript,\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/process\"\nresponse = oauth.post(url, json=request)"
  },
  {
    "objectID": "APIs/SentinelHub/Byoc/Examples.html",
    "href": "APIs/SentinelHub/Byoc/Examples.html",
    "title": "BYOC API examples",
    "section": "",
    "text": "The following API requests are written in Python. To execute them, you need to create an OAuth client as is explained here. The client is named oauth in these examples. The examples are structured in a way to be as separable as possible, however in many cases doing all the steps in each chapter makes sense.\n\nCreating a collection\nTo create a collection with the name &lt;MyCollection&gt; and S3 bucket &lt;MyBucket&gt;:\ncollection = {\n  'name': '&lt;MyCollection&gt;',\n  's3Bucket': '&lt;MyBucket&gt;'\n}\n\nresponse = oauth.post('https://sh.dataspace.copernicus.eu/api/v1/byoc/collections', json=collection)\nresponse.raise_for_status()\nExtracting the collection id from the response:\nimport json\n\ncollection = json.loads(response.text)['data']\ncollection_id = collection['id']\n\n\nCreating a tile\nTo create a tile with the path &lt;MyTile&gt;:\ntile = {\n  'path': '&lt;MyTile&gt;',\n}\n\nresponse = oauth.post(f'https://sh.dataspace.copernicus.eu/api/v1/byoc/collections/{collection_id}/tiles', json=tile)\nresponse.raise_for_status()\nIf your tile has a known sensing time, e.g. October 21, 2019 at 14:51 by UTC time, add this information by using the following payload:\ntile = {\n  'path': '&lt;MyTile&gt;',\n  'sensingTime': '2019-10-21T14:51:00Z'\n}\nIf you want to provide a cover geometry, set it as the value of the coverGeometry field:\ntile = {\n  'path': '&lt;MyTile&gt;',\n  'coverGeometry': &lt;MyCoverGeometry&gt;\n}\nFor information on how to prepare a cover geometry, see Preparing a cover geometry.\nTo extract the tile id from the response:\nimport json\n\ntile = json.loads(response.text)['data']\ntile_id = tile['id']\n\n\nPreparing a cover geometry\nTo obtain a cover geometry automatically, you can use the gdal_trace_outline script which gives you a cover geometry in the WKT format:\nimport subprocess\n\ncommand = f'gdal_trace_outline &lt;MyCOG&gt; -out-cs en -wkt-out wkt.txt'\nsubprocess.run(command, shell=True, check=True)\nOnce complete, transform the geometry into the GeoJSON format:\nfrom osgeo import ogr\nimport json\n\nf = open('wkt.txt')\ngeom = ogr.CreateGeometryFromWkt(f.read())\ncover_geometry = json.loads(geom.ExportToJson())\nIf the CRS is something other than WGS84, make sure to set its URN &lt;CrsUrn&gt; under crs.properties.name. For example urn:ogc:def:crs:EPSG::32633 for EPSG:32633.\ncover_geometry['crs'] = {\n  'properties': {\n    'name': '&lt;CrsUrn&gt;'\n  }\n}\nTo obtain the URN automatically from a raster file you can use the following Python scriptget_crn_urn.py.\n\n\nChecking the tile ingestion status\nTo check the ingestion status of the tile, first get the tile:\nresponse = oauth.get(f'https://sh.dataspace.copernicus.eu/api/v1/byoc/collections/{collection_id}/tiles/{tile_id}')\nresponse.raise_for_status()\nThen extract its status from the response:\nimport json\n\ntile = json.loads(response.text)['data']\nstatus = tile['status']\n\nif status == 'INGESTED':\n  print('Tile ingested')\nelif status == 'FAILED':\n  print('Tile failed to ingest')\nelse:\n  print(status)\nTo check why a tile failed to ingest:\nprint(tile['additionalData']['failedIngestionCause'])\n\n\nListing tiles\nTiles are paginated and to traverse all pages use the link from response that points to the next page, which is located at links.next. By default, you get back 100 tiles per page, but you can change this using the query parameter count, however it cannot be more than 100.\nimport time\n\nurl = f'https://sh.dataspace.copernicus.eu/api/v1/byoc/collections/{collection_id}/tiles'\n\nwhile url is not None:\n  response = oauth.get(url)\n  response.raise_for_status()\n\n  output = response.json()\n  tiles = output['data']\n  links = output['links']\n\n  for tile in tiles:\n    print(tile['path'])\n\n  # sets url to None if there's no link to the next set of tiles\n  url = links.get('next', None)\n\n  # waits a bit before fetching the next set\n  time.sleep(0.1)"
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical/Examples.html",
    "href": "APIs/SentinelHub/BatchStatistical/Examples.html",
    "title": "Examples of Batch Statistical Workflow",
    "section": "",
    "text": "The requests below are written in Python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples."
  },
  {
    "objectID": "APIs/SentinelHub/BatchStatistical/Examples.html#create-a-batch-statistical-request",
    "href": "APIs/SentinelHub/BatchStatistical/Examples.html#create-a-batch-statistical-request",
    "title": "Examples of Batch Statistical Workflow",
    "section": "Create a batch statistical request",
    "text": "Create a batch statistical request\nThis request defines which data is requested and how it will be processed. In this example we'll get the statistics for a single band on a given day. To create a batch statistical request replace the input.features.s3.url field for the actual path to the GeoPackage features and the output.s3.url field for the desired path where the output data will be processed.\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"output_B04\",\n        bands: 1,\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\nfunction evaluatePixel(samples) {\n    return {\n        output_B04: [samples.B04],\n        dataMask: [samples.dataMask]\n        }\n}\n\"\"\"\n\nrequest_payload = {\n  \"input\": {\n  \"features\":{\n      \"s3\": {\n          \"url\": \"s3://&lt;my-bucket&gt;/&lt;path-to-geopackage&gt;\",\n          \"accessKey\": \"&lt;my-s3-access-key&gt;,\n          \"secretAccessKey\": \"&lt;my-secret-access-key&gt;\n      }\n  },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastCC\"\n        }\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-06-01T00:00:00Z\",\n            \"to\": \"2020-07-31T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P30D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  },\n  \"output\": {\n      \"s3\": {\n          \"url\": \"s3://&lt;my-bucket&gt;/&lt;path&gt;\",\n          \"accessKey\": \"&lt;my-s3-access-key&gt;,\n          \"secretAccessKey\": \"&lt;my-secret-access-key&gt;\n      }\n  }\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n   'Accept': 'application/json'\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics/batch\"\n\nresponse = oauth.request(\"POST\", url=url, headers=headers, json=request_payload)\n\nrequest_id = response.json()['id']\nNote that in the above example we're specifying an accessKey and secretAccessKey, so Sentinel Hub can read and write to the user's bucket. You can find more details about this under the AWS access section.\nYou can download an example of a valid .gpkg (GeoPackage) file by clicking here.\n\nGet information about a batch statistical request\nresponse = oauth.request(\"GET\", f\"https://sh.dataspace.copernicus.eu/api/v1/statistics/batch/{request_id}\")\n\nresponse.json()\n\n\nGet status information about a batch statistical request\nresponse = oauth.request(\"GET\", f\"https://sh.dataspace.copernicus.eu/api/v1/statistics/batch/{request_id}/status\")\n\nresponse.json()\n\n\nRequest analysis of a batch statistical request (ANALYSIS)\nresponse = oauth.request(\"POST\", f\"https://sh.dataspace.copernicus.eu/api/v1/statistics/batch/{request_id}/analyse\")\n\nresponse.status_code\n\n\nRequest the start of a batch statistical request (START)\nresponse = oauth.request(\"POST\", f\"https://sh.dataspace.copernicus.eu/api/v1/statistics/batch/{request_id}/start\")\n\nresponse.status_code\n\n\nStop a batch statistical request (STOP)\nresponse = oauth.request(\"POST\", f\"https://sh.dataspace.copernicus.eu/api/v1/statistics/batch/{request_id}/stop\")\n\nresponse.status_code"
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Datamask.html",
    "href": "APIs/SentinelHub/UserGuides/Datamask.html",
    "title": "Data Mask",
    "section": "",
    "text": "With evalscript v3 we are now providing full control to you over what is to be returned for image parts (pixels) where there is “no data”. In the setup function, you can request dataMask as an element of the input array and then use it in the evaluatePixel function in the same manner as any other input band.\n\n\ndataMask has value 0 for “no data” pixels and 1 elsewhere.\nBy “no data” pixels we mean:\n\nAll pixels which lay outside of the requested polygon (if specified).\nAll pixels for which no source data was found.\nAll pixels for which source data was found and is explicitly “no data”.\n\nThings to note:\n\nAll “no data” pixels as defined above have a dataMask value of 0. All band values for these pixels are also 0, except for Landsat data collections, where band values for no data pixels are NaN.\n\"No data\" pixels are treated like any other in the evalscript. Their value, namely zero (or NaN in case of Landsat data collections), is applied to your evalscript just like any other other pixel. E.g. return [sample.B04*sample.B03] will return 0 for “no data” pixels, while return [sample.B04/sample.B03] would return \"Infinity\" (if requested sampleType is FLOAT32) due to division by zero (or \"NaN\" for Landsat data collection where the division would be by \"NaN\"). To treat \"no data\" pixels differently, explicitly handle them in your evalscript. See the examples below.\n\n\n\n\n\n\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"dataMask\"],\n    output: { bands: 3 }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  if (sample.dataMask == 1)  {\n    return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n  } else {\n    return [99, 99, 99]\n  }\n}\n\n\n\n//VERSION=3\nif (dataMask == 1)  {\n  return [2.5 * B04, 2.5 * B03, 2.5 * B02]\n} else {\n  return [99/255, 99/255, 99/255] //normalized with 255 for visualization in EO Browser\n}\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you'd like to use this example, you must set the output.responses.format.type parameter of your process API request to image/png or image/tiff. The png format will automatically interpret the fourth band as transparency.\n\n\n\n\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"dataMask\"],\n    output: { bands: 4 }\n  }\n}\n\nfunction evaluatePixel(sample) {\n    return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02, sample.dataMask]\n}\n\n\n\n//VERSION=3\nreturn [2.5 * B04, 2.5 * B03, 2.5 * B02, dataMask]"
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/Datamask.html#datamask---handling-of-pixels-with-no-data",
    "href": "APIs/SentinelHub/UserGuides/Datamask.html#datamask---handling-of-pixels-with-no-data",
    "title": "Data Mask",
    "section": "",
    "text": "With evalscript v3 we are now providing full control to you over what is to be returned for image parts (pixels) where there is “no data”. In the setup function, you can request dataMask as an element of the input array and then use it in the evaluatePixel function in the same manner as any other input band.\n\n\ndataMask has value 0 for “no data” pixels and 1 elsewhere.\nBy “no data” pixels we mean:\n\nAll pixels which lay outside of the requested polygon (if specified).\nAll pixels for which no source data was found.\nAll pixels for which source data was found and is explicitly “no data”.\n\nThings to note:\n\nAll “no data” pixels as defined above have a dataMask value of 0. All band values for these pixels are also 0, except for Landsat data collections, where band values for no data pixels are NaN.\n\"No data\" pixels are treated like any other in the evalscript. Their value, namely zero (or NaN in case of Landsat data collections), is applied to your evalscript just like any other other pixel. E.g. return [sample.B04*sample.B03] will return 0 for “no data” pixels, while return [sample.B04/sample.B03] would return \"Infinity\" (if requested sampleType is FLOAT32) due to division by zero (or \"NaN\" for Landsat data collection where the division would be by \"NaN\"). To treat \"no data\" pixels differently, explicitly handle them in your evalscript. See the examples below.\n\n\n\n\n\n\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"dataMask\"],\n    output: { bands: 3 }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  if (sample.dataMask == 1)  {\n    return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n  } else {\n    return [99, 99, 99]\n  }\n}\n\n\n\n//VERSION=3\nif (dataMask == 1)  {\n  return [2.5 * B04, 2.5 * B03, 2.5 * B02]\n} else {\n  return [99/255, 99/255, 99/255] //normalized with 255 for visualization in EO Browser\n}\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you'd like to use this example, you must set the output.responses.format.type parameter of your process API request to image/png or image/tiff. The png format will automatically interpret the fourth band as transparency.\n\n\n\n\n//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\", \"dataMask\"],\n    output: { bands: 4 }\n  }\n}\n\nfunction evaluatePixel(sample) {\n    return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02, sample.dataMask]\n}\n\n\n\n//VERSION=3\nreturn [2.5 * B04, 2.5 * B03, 2.5 * B02, dataMask]"
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/TimeSeries.html",
    "href": "APIs/SentinelHub/UserGuides/TimeSeries.html",
    "title": "Time series",
    "section": "",
    "text": "Processing API uses a timeRange parameter, where a user can select from-to dates. When such a request is ran, only one image is returned. timeRange is used to specify the scenes that are going to be considered for mosaicking (for example all the scenes from April 1 to June 1). Which one will be chosen for the output depends on the mosaicking type and order specified. If the user specified SIMPLE mosaicking order to be mostRecent, the first image considered for mosaicking would be the most recent image available from April 1 to June 1. If a user selected a mosaicking type TILE, and requested second samples (sample[1]), the samples of the second available scene in the specified time range would be returned. Learn more about mosaicking here and mosaicking orders here.\nIf you would like to return all the scenes in a given time range, the recommended approach is to first search for all the available scenes using our Catalog service API, which you can use to view detailed geospatial information, such as the acquisition date and time, for each of the available scenes of your specified BBOX, collection and time range. You can control your Catalog search by specifying fields, limits and other properties. See the Catalog API examples to learn how to do so. It's also possible to search the available scenes using the OGC WFS request, which might be a bit easier to use, but gives you much less search control. See a WFS request example here.\nWhen you have a list of the available scenes, you can then request each by using a separate Processing API call. To do so, limit time-range to only allow for the desired time frame, which matches the acquisition time of your scene."
  },
  {
    "objectID": "APIs/SentinelHub/UserGuides/TimeSeries.html#working-with-time-series",
    "href": "APIs/SentinelHub/UserGuides/TimeSeries.html#working-with-time-series",
    "title": "Time series",
    "section": "",
    "text": "Processing API uses a timeRange parameter, where a user can select from-to dates. When such a request is ran, only one image is returned. timeRange is used to specify the scenes that are going to be considered for mosaicking (for example all the scenes from April 1 to June 1). Which one will be chosen for the output depends on the mosaicking type and order specified. If the user specified SIMPLE mosaicking order to be mostRecent, the first image considered for mosaicking would be the most recent image available from April 1 to June 1. If a user selected a mosaicking type TILE, and requested second samples (sample[1]), the samples of the second available scene in the specified time range would be returned. Learn more about mosaicking here and mosaicking orders here.\nIf you would like to return all the scenes in a given time range, the recommended approach is to first search for all the available scenes using our Catalog service API, which you can use to view detailed geospatial information, such as the acquisition date and time, for each of the available scenes of your specified BBOX, collection and time range. You can control your Catalog search by specifying fields, limits and other properties. See the Catalog API examples to learn how to do so. It's also possible to search the available scenes using the OGC WFS request, which might be a bit easier to use, but gives you much less search control. See a WFS request example here.\nWhen you have a list of the available scenes, you can then request each by using a separate Processing API call. To do so, limit time-range to only allow for the desired time frame, which matches the acquisition time of your scene."
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html",
    "href": "APIs/SentinelHub/Byoc.html",
    "title": "Bring Your Own COG API",
    "section": "",
    "text": "Bring Your Own COG API is only available for users with Copernicus Service accounts. Please refer to our FAQ on account typology change and Submit A Request to our Copernicus Data Space Ecosystem Support Team to request your Copernicus Service account accordingly."
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#overview",
    "href": "APIs/SentinelHub/Byoc.html#overview",
    "title": "Bring Your Own COG API",
    "section": "Overview",
    "text": "Overview\nBring Your Own COG API (or shortly \"BYOC\") enables you to import your own data in Sentinel Hub and access it just like any other data you are used to. To be able to do so, the following conditions should be met:\n\nStore your raster data in the cloud optimized geotiff (COG) format on your own S3 bucket in the supported region.\nConfigure the bucket's permissions so that Sentinel Hub can read them.\nImport tiles using the Dashboard or API.\n\nYour data needs to be organized into collections of tiles. Each tile needs to contain a set of bands and (optionally) an acquisition date and time. Tiles with the same bands can be grouped into collections. Think of the Sentinel-2 data as a collection of Sentinel-2 tiles.\n\nA note about COG overviews used for processing\nWhen processing data, we select the nearest overview level which has higher resolution than your request, or the full resolution image.\n\n\nSentinel Hub BYOC Tool\nThe Sentinel Hub BYOC Tool is software which can be used to prepare your data for use in Sentinel Hub. It can be run either in Docker or as a Java JAR. It takes care of the entire process; it is simple to use for simple cases but is also highly configurable allowing for more complex requirements. The same steps can be done manually and are detailed below, should you prefer or require more control over the process. Get the tool here."
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#converting-to-cog",
    "href": "APIs/SentinelHub/Byoc.html#converting-to-cog",
    "title": "Bring Your Own COG API",
    "section": "Converting to COG",
    "text": "Converting to COG\n\nConstraints and settings\nCOGs can contain either a single band or multiple bands. For multi-band COGs we support both planar configurations formats - chunky and planar format.\nThere are a few additional constraints in addition to having COG files. These are:\n\nThe COG header size must not exceed one megabyte.\nThe internal tile size must be between 256 x 256 and 2048 x 2048.\nThe projection needs to be one of: WGS84 (EPSG:4326), WebMercator (EPGS:3857), any UTM zone (EPSG:32601-32660, 32701-32760), or Europe LAEA (EPSG:3035).\nPhotometric interpretation that is not 1 (0 is imaged as black) or 2 (RGB) is only supported with no compression and deflate or zstd compression.\nThe COG must not cross any of the two poles.\nThe band name should be a valid JavaScript identifier so it can be safely used in evalscripts; valid identifiers are case-sensitive, can contain Unicode letters, $, _, and digits (0-9), but may not start with a digit, and should not be one of the reserved JavaScript keywords.\nThere can be at most 100 bands.\nMulti-band COGs in chunky format can have at most 10 bands.\nThe file names need to be consistent for all tiles in a collection. For example, if you have B1.tiff in one tile then you also need B1.tiff in all the other tiles in your collection.\nAll files of each tile needs to have consistent extension (so a tile cannot contain both B1.tiff and B2.TIF).\nThe maximum allowable difference between the intersection of all file bounding boxes and each individual file is one pixel of that file [1].\nAll files of each band need to have the same bit depth.\nFiles can be compressed with DEFLATE, ZLIB, ZSTD, PIXTIFF_ZIP, PACKBITS or LZW compression method. JPEG compression is not supported.\nSupported sample types and bit depths are the same as those supported for outputs, as well as reading unsigned integer 1, 2, 4 bit files. See also sampleType.\n\nBands can have different resolutions.\nFor best performance we recommend the following setting for COGs: deflate compressed with 1024x1024 pixel internal tiling.\n\n[1]: Here's one example of files with slightly different bounding boxes. One file has the bounding box [0, 0, 10, 10] and resolution of one meter per pixel, and the other file has the bounding box [0.5, 0.5, 10.5, 10.5] and 0.5 meter resolution. The intersection [0.5, 0.5, 10, 10] is not more than one pixel away from each individual file, therefore such files are valid for BYOC.\n\n\n\nGDAL example command\nCOGs can be generated in a single step with GDAL 3.1 or newer using the COG raster driver. For older GDAL versions or if you want planar multi-band COGs, see below. Even though you can use any GDAL version, we highly recommend you use v3.1 or newer, as older versions have issues with average downsampling (see https://gdal.org/programs/gdaladdo.html).\nThe input file must conform to the constraints regarding the projection, units per pixel, and pixel formats. To generate a COG from an input file:\ngdal_translate -of COG -co COMPRESS=DEFLATE -co BLOCKSIZE=1024 -co RESAMPLING=AVERAGE -co OVERVIEWS=IGNORE_EXISTING input.extension output.tiff\nAdditional parameters may be needed:\n\nif the input file contains multiple bands, but you only need one or only some of them, add -b &lt;bandA&gt; -b &lt;bandB&gt; ..., where &lt;bandX&gt; is the band number, starting from 1.\nif your input data has nodata values, add them to this command using: -a_nodata NO_DATA_VALUE, e.g. for zero: -a_nodata 0.\nfor many types of data adding a predictor can further reduce the file size. It is best to test this on your own data, to enable the predictor add -co PREDICTOR=YES.\n\nMulti-band COGs generated this way, are encoded in chunky format and you cannot change it to planar format. To get a COG in planar format, follow the next chapter.\n\nOlder GDAL versions or planar multi-band COGs\nFor GDAL older than 3.1 or if you want planar multi-band COGs, multiple commands are needed. To extract individual bands, add -b &lt;band&gt;, where &lt;band&gt; is the band number, starting from 1, to the first command.\ngdal_translate -of GTIFF input.extension intermediate.tiff\n\n\n\n\n\n\nNote\n\n\n\nIf your input data has nodata values, add them to this command using: -a_nodata NO_DATA_VALUE, e.g. for zero: -a_nodata 0.\n\n\ngdaladdo -r average --config GDAL_TIFF_OVR_BLOCKSIZE 1024 intermediate.tiff 2 4 8 16 32\n(The number of overview levels you need depends on your source data. A good rule of thumb is to have as many overview levels as necessary for the entire source image to fit on one 1024x1024 tile).\ngdal_translate -co TILED=YES -co COPY_SRC_OVERVIEWS=YES --config GDAL_TIFF_OVR_BLOCKSIZE 1024 -co BLOCKXSIZE=1024 -co BLOCKYSIZE=1024 -co COMPRESS=DEFLATE intermediate.tiff output.tiff\nTo generate a planar multi-band COG, add -co INTERLEAVE=BAND. For chunky format, you don't need to pass anything, as this is the default format.\n\n\n\n\n\n\nNote\n\n\n\nfor many types of data adding a predictor can further reduce the file size. It is best you test this on your own data. To enable the predictor, add to the above command -co PREDICTOR=2 for integers, and -co PREDICTOR=3 for floating points.\n\n\nOnce the commands finish, you can delete the intermediate.tiff file.\nFor more information about each command see the GDAL documentation:\n\ngdal_translate\ngdaladdo"
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#bucket-settings",
    "href": "APIs/SentinelHub/Byoc.html#bucket-settings",
    "title": "Bring Your Own COG API",
    "section": "Bucket settings",
    "text": "Bucket settings\nIf you do not yet have a bucket at Copernicus Data Space Ecosystem, please follow these steps to get one.\nYou will have to configure your bucket to allow read access to Sentinel Hub. To do this, update your bucket policy to include the following statement (don’t forget to replace &lt;bucket_name&gt; with your actual bucket name):\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Sentinel Hub permissions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::ddf4c98b5e6647f0a246f0624c8341d9:root\"\n            },\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\",\n                \"arn:aws:s3:::&lt;bucket_name&gt;/*\"\n            ]\n        }\n    ]\n}\nA python script to set a bucket policy can be downloaded here."
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#configuring-collections",
    "href": "APIs/SentinelHub/Byoc.html#configuring-collections",
    "title": "Bring Your Own COG API",
    "section": "Configuring collections",
    "text": "Configuring collections\nWhen creating a collection:\n\nyou need to provide the S3 bucket where you data is; if you have data in CreoDIAS, prefix the bucket name by your CreoDIAS Project ID as follows: PROJECT_ID:bucket_name ,\nyou can define bands, but only using BYOC API,\nyou can provide the no data value using Dashboard or BYOC API.\n\nThe no data value cannot be configured to NaN (not a number). However, there is no need to do this, as NaNs are by default treated as no data value.\n\nAutomatic configuration\nIf bands are not configured, BYOC service automatically configures them based on the files of the first ingested tile. In this case the bands are named after the files, while for multi-band files the band index in 1-based numbering is also added at the end. For example, the bands in a multi-band file named RGB.tiff would be named RGB_1, RGB_2, etc. You can rename any band later.\nIn this process, the service also configures the \"no data\" value, if it's not set by the user. The service automatically extracts \"no data\" values from the TIFF tag GDAL_NODATA (TIFF entry ID = 42113) of the files of the first ingested tile, and sets the value as the collection \"no data\" value, if all files have the exactly same value and if the value is a number. Otherwise, it sets values per band.\n\n\nManual band configuration\nThe below example shows how to configure manually instead of relying on the automatic configuration described above. Suppose your tiles are composed of two files - \"RGB.tiff\" with three 16-bit bands and \"CLOUD_MASK.tiff\" with a single 8-bit band. You would provide such configuration in additionalData.bands field of a new collection:\n{\n  \"Red\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 16\n  },\n  \"Green\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 2,\n    \"bitDepth\": 16\n  },\n  \"Blue\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 3,\n    \"bitDepth\": 16\n  },\n  \"CloudMask\": {\n    \"source\": \"CLOUD_MASK\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 8\n  }\n}\nThe keys \"Red\", \"Green\", \"Blue\", and \"CloudMask\" are the names of the bands that you are going to use in evalscripts. These names can be changed at any time. Inside each band specification you specify where the band is stored using the fields source and bandIndex. The source, together with tile path, defines the file (see below for details), while bandIndex is the band index in 1-based numbering.\n\n\nBand renaming\nBands can be easily renamed in Dashboard. To do this using API, you need to provide the same band specs, but with new names. To obtain the current band specs, use this endpoint. For example, let's say your bands are defined like this, and you would like to rename bands \"RGB_1\", \"RGB_2\", \"RGB_3\" to \"Red\", \"Green\", and \"Blue\", respectively:\n{\n  \"RGB_1\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 16\n  },\n  \"RGB_2\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 2,\n    \"bitDepth\": 16\n  },\n  \"RGB_3\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 3,\n    \"bitDepth\": 16\n  },\n  \"CLOUD_MASK\": {\n    \"source\": \"CLOUD_MASK\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 8\n  }\n}\nTo achieve this, you need to use this endpoint. You need to provide the new names at the top level, but leave the band properties (\"source\", \"bandIndex\", etc) and values the same. So the content of additionalData.bands would be:\n{\n  \"Red\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 16\n  },\n  \"Green\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 2,\n    \"bitDepth\": 16\n  },\n  \"Blue\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 3,\n    \"bitDepth\": 16\n  },\n  \"CLOUD_MASK\": {\n    \"source\": \"CLOUD_MASK\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 8\n  }\n}\nKeep in mind:\n\nthat the bucket cannot be changed after the collection is created,\nthat once bands have been configured you can only change band names or remove bands,\nand that the no data value can be changed at anytime using Dashboard or BYOC API.\n\n\n\nConfiguring band sample format\nThe sample format is TIFF info that defines the band data type. It can be set to signed integers, unsigned integers, or floating points. Learn more about sample format here. These values are in BYOC defined as INT, UINT and FLOAT, respectively.\nYou can configure format manually in BYOC using API or Dashboard. If not set, it will get set to the value of the first ingested tile.\nTo configure it manually, set sampleFormat field for each band like this:\n{\n  \"Red\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 16,\n    \"sampleFormat\": \"INT\"\n  },\n  \"Green\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 2,\n    \"bitDepth\": 16,\n    \"sampleFormat\": \"INT\"\n  },\n  \"Blue\": {\n    \"source\": \"RGB\",\n    \"bandIndex\": 3,\n    \"bitDepth\": 16,\n    \"sampleFormat\": \"INT\"\n  },\n  \"CLOUD_MASK\": {\n    \"source\": \"CLOUD_MASK\",\n    \"bandIndex\": 1,\n    \"bitDepth\": 8,\n    \"sampleFormat\": \"UINT\"\n  }\n}\nAfter formats are set, the formats of all new files must match the formats defined in BYOC. If they do not match, files do not get ingested."
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#ingesting-the-tiles",
    "href": "APIs/SentinelHub/Byoc.html#ingesting-the-tiles",
    "title": "Bring Your Own COG API",
    "section": "Ingesting the tiles",
    "text": "Ingesting the tiles\nThere are two ways of doing this. The easier version is using the dashboard.\nTo create a new collection click the New collection button. The name can be anything and is there for your own reference. The S3 bucket name is the bucket name containing your data.\nOnce the collection is created you can add tiles. Note that only a single tile can be added in one step.\nTo add a tile, click the Add tile button. Provide a path to the COG files inside the s3 bucket. For example, if your files are stored in s3://bucket-name/folder/, simply set folder as the tile path. Optionally, set the sensing time of the tile here as well.\nWhen the tile is ingested its path will be automatically changed to folder/(BAND).tiff or similar, depending on the extension of the files in folder. Note that (BAND) is a placeholder that is replaced by the source of a band to obtain the actual file where the band is stored. In the example above your collection uses sources \"RGB\" and \"CLOUD_MASK\", thus the two files of the tile will be folder/RGB.tiff and folder/CLOUD_MASK.tiff.\nFor more complicated cases you must provide the path with the (BAND) placeholder and extension. For example, suppose your folder contains the files for multiple tiles:\n\ns3://bucket-name/folder/tile_1_B1_2019.tif,\ns3://bucket-name/folder/tile_1_B2_2019.tif,\ns3://bucket-name/folder/tile_2_B1_2019.tif,\ns3://bucket-name/folder/tile_2_B2_2019.tif.\n\nCreate the first tile with the path folder/tile_1_(BAND)_2019.tif to use the first two files and the second tile with the path folder/tile_2_(BAND)_2019.tif to use the next two files.\nDo not forget that all tiles must contain the same set of files (with different data of course); that is, if a tile is missing one or more files it will fail to ingest.\nTo ingest tiles via API requests instead of the dashboard, see BYOC API reference or Python examples.\n\nA note about changing files\nWhile you may freely modify the data in your buckets, for it to continue to work reliably through Sentinel Hub you need to reingest tiles with changed data. You can do this in Dashboard, by clicking the \"Refresh\" button next to the tile, or using the API, by calling the reingest endpoint with the collection and tile id. This is needed as it will update metadata required for processing and failing to do so can result in odd behavior.\n\n\nA note about cover geometries\nEach tile ingested also requires a cover geometry. A cover geometry is a geometry which outlines the valid data part of the tile. Nodata therefore should not be contained in the cover geometry. In the simplest case, the cover geometry will equal the bounding box of the file being ingested.\nThe cover geometry is important because it tells the system where it can expect to find data. As a consequence, this determines how data is rendered where tiles overlap. If you have tiles with overlapping cover geometries and you for example request mosaicking SIMPLE, only the data from one tile will be rendered where two (or more) cover geometries intersect. This is true even if this data is nodata or if it lies outside the tile bounding box. Sentinel Hub will render such areas as nodata even if other tiles in the data collection contain valid data for this area. Having quality cover geometries is therefore important for collections where many tiles containing nodata overlap. Not all cases need precise cover geometries, however. A single tile or a regularly gridded collection with a single date and coordinate reference system can get away with cover geometries equalling the bounding box.\n\n\n\nOverlapping tiles\n\n\nIf the cover geometry is not specified during ingestion it will automatically be set to the tile bounding box. Sentinel Hub will not attempt to generate a more precise geometry as it is impossible to prepare such a process which will work well for all users. It is therefore your responsibility to provide quality cover geometries and in doing so allow you to extract the most out of your data. If ingesting tiles using the API, set the cover geometry using the coverGeometry field in the API request. It must be in the GeoJSON format and in a projected or geodetic coordinate reference system which is supported by Sentinel Hub. Cover geometries in practice mean one polygon or multipolygon. They must also contain no more than 100 points.\n\nGenerating cover geometries\n\nGDAL\nOne way of getting a cover geometry is using the GDAL utility script gdal_trace_outline which takes a raster and returns a cover geometry in the WKT format. This then needs to be converted to GeoJSON. In this example a single band file is traced:\ngdal_trace_outline band.tif -out-cs en -wkt-out wkt.txt\nThe process might take a while if you have a large file. To speed up the process you can pass a subsampled file which you can get with gdal_translate. To get a file that is 1% of the original size:\ngdal_translate band.tif subsampled.tif -outsize 1% 1%\nor if it's stored on AWS S3:\ngdal_translate /vsis3/bucket-name/folder/band.tif subsampled.tif -outsize 1% 1%\nNote that calculating the cover geometry on subsampled rasters may not be sufficiently accurate for touching but not intersecting tiles as the imprecision caused by downsampling may leave gaps.\nFinally, you need to convert the WKT file to GeoJSON and specify the CRS under crs.properties.name (except when WGS84 when it can be omitted). CRSs with the EPSG code &lt;EpsgCode&gt; should be specified as urn:ogc:def:crs:EPSG::&lt;EpsgCode&gt;. Here is a GeoJSON example in ESPG:32633.\n{\n    \"type\": \"MultiPolygon\",\n    \"crs\": {\n        \"type\": \"name\",\n        \"properties\": {\n            \"name\": \"urn:ogc:def:crs:EPSG::32633\"\n        }\n    },\n    \"coordinates\": [\n        [\n            [\n                [\n                    370270.52147506207,\n                    5085707.891369364\n                ],\n                ...\n            ]\n        ]\n    ]\n}\n\n\nSentinel Hub BYOC Tool\nThe Sentinel Hub BYOC Tool can also help you update the cover geometry of existing tiles on Sentinel Hub. Use the set-coverage command. On Docker, get help and parameters by running: docker run sentinelhub/byoc-tool set-coverage --help\n\n\n\nWorkarounds\nIn case your input data is complex and cannot be adequately simply outlined it is nevertheless possible to obtain pixel-precise rendering. In this case, set the cover geometry to any which covers all the valid input pixels. The file bounding box as the default is such an example. What follows is doing the mosaicking in the custom script with the help of dataMask.\nFirst, set the mosaicking parameter within setup to TILE (mosaicking: Mosaicking.TILE) and add the dataMask to the array of input bands.\nThen use something like the following as your evalscript. Since dataMask precisely determines which pixels are valid and which ones are not, the moment a valid pixel is found this can be returned, alternatively the next scene should be checked.\nfunction evaluatePixel(samples, scenes) {\n  for (let i = 0; i &lt; samples.length; i++) {\n    let sample = samples[i];\n    if (sample.dataMask == 1) {\n      return someCombination(sample);\n    }\n  }\n  return someNodataValueArray;\n}\nNote that getting data in such a manner will use more processing units than SIMPLE mosaicking with precise cover geometries.\nOptionally, you may additionally use the preProcessScenes function to potentially reduce the number of tiles which will be processed. This is useful to set an upper limit for the number of processing units which will be used. The following limits the maximum number of tiles to 5, for example.\nfunction preProcessScenes (collections) {\n  collections.scenes.tiles = collections.scenes.tiles.splice(5);\n  return collections;\n}"
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#collection-metadata",
    "href": "APIs/SentinelHub/Byoc.html#collection-metadata",
    "title": "Bring Your Own COG API",
    "section": "Collection metadata",
    "text": "Collection metadata\nCollections have the following metadata available under additionalData:\n\nextent: the collection extent in WGS84\nhasSensingTimes: information if tiles have sensing time\nfromSensingTime the sensing time in ISO 8601 of the least recent tile\ntoSensingTime: the sensing time in ISO 8601 of the most recent tile\n\nThe metadata is updated in a few minutes after a tile is added or removed. To find out if your collection requires metadata updates, check out the flag requiresMetadataUpdate."
  },
  {
    "objectID": "APIs/SentinelHub/Byoc.html#examples",
    "href": "APIs/SentinelHub/Byoc.html#examples",
    "title": "Bring Your Own COG API",
    "section": "Examples",
    "text": "Examples\nBYOC API Examples"
  },
  {
    "objectID": "APIs/SentinelHub/Batch/Examples.html",
    "href": "APIs/SentinelHub/Batch/Examples.html",
    "title": "Examples of Batch Processing Workflow",
    "section": "",
    "text": "The requests below are written in Python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nCreate a batch processing request\n\nOption 1: GeoTiff format output\nThis request defines which data is requested and how it will be processed. In this particular example we will calculate maximum NDVI over two months period for an area in Corsica and visualize the results using a built-in visualizer. The resulting image will in a Geotiff format. To create a batch processing request replace &lt;MyBucket&gt; with the name of your S3 bucket and run:\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/batch/process\"\n\nevalscript = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B04\", \"B08\"]\n            }],\n            output: [{\n                id: \"default\",\n                bands: 3\n            }],\n            mosaicking: Mosaicking.ORBIT\n        }\n    }\n\n    function calcNDVI(sample) {\n        var denom = sample.B04 + sample.B08\n        return ((denom != 0) ? (sample.B08 - sample.B04) / denom : 0.0)\n    }\n\n    const maxNDVIcolors = [\n        [-0.2, 0xbfbfbf],\n        [0, 0xebebeb],\n        [0.1, 0xc8c682],\n        [0.2, 0x91bf52],\n        [0.4, 0x4f8a2e],\n        [0.6, 0x0f540c]\n    ]\n\n    const visualizer = new ColorRampVisualizer(maxNDVIcolors);\n\n    function evaluatePixel(samples) {\n        var max = 0\n        for (var i = 0; i &lt; samples.length; i++) {\n            var ndvi = calcNDVI(samples[i])\n            max = ndvi &gt; max ? ndvi : max\n        }\n        ndvi = max\n        return visualizer.process(ndvi)\n    }\n\"\"\"\n\npayload = {\n    \"processRequest\": {\n        \"input\": {\n            \"bounds\": {\n                \"bbox\": [\n                    8.44,\n                    41.31,\n                    9.66,\n                    43.1\n                ],\n                \"properties\": {\n                    \"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"\n                }\n            },\n            \"data\": [{\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-01T00:00:00Z\",\n                        \"to\": \"2019-06-30T00:00:00Z\"\n                    },\n                \"maxCloudCoverage\": 70.0\n                },\n                \"type\": \"sentinel-2-l2a\"\n            }]\n        },\n        \"output\": {\n            \"responses\": [{\n                \"identifier\": \"default\",\n                \"format\": {\n                    \"type\": \"image/tiff\"\n                }\n            }]\n        },\n        \"evalscript\": evalscript\n    },\n    \"tilingGrid\": {\n        \"id\": 0,\n        \"resolution\": 60.0\n    },\n    \"bucketName\": \"&lt;MyBucket&gt;\",\n\n    \"description\": \"Max NDVI over Corsica\"\n}\n\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = oauth.request(\"POST\", url, headers=headers, json = payload)\n\nresponse.json()\nExtracting the batch request id from the response:\nbatch_request_id = response.json()['id']\n\n\nOption 2: Zarr format output\nIn this example we will calculate maximum NDVI over two months period for an area in Corsica. Besides maximum NDVI, we will also return values of bands B04 and B08, which were used to calculate maximum NDVI. All three results will be stored as arrays of an output Zarr file. To create a batch processing request replace &lt;MyBucket&gt; with the name of your S3 bucket and run:\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/batch/process\"\n\nevalscript = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B04\", \"B08\"]\n            }],\n            output: [{\n                id: \"maxNDVI\",\n                sampleType: \"FLOAT32\",\n                bands: 1\n            },\n            {\n                id: \"band04\",\n                sampleType: \"UINT16\",\n                bands: 1\n            },\n            {\n                id: \"band08\",\n                sampleType: \"UINT16\",\n                bands: 1\n            }],\n            mosaicking: Mosaicking.ORBIT\n        }\n    }\n\n    function calcNDVI(sample) {\n        var denom = sample.B04 + sample.B08\n        return ((denom != 0) ? (sample.B08 - sample.B04) / denom : 0.0)\n    }\n\n    function evaluatePixel(samples) {\n        var maxNDVI = 0\n        var band04 = 0\n        var band08 = 0\n        for (var i = 0; i &lt; samples.length; i++) {\n            var ndvi = calcNDVI(samples[i])\n            if (ndvi &gt; maxNDVI){\n                maxNDVI = ndvi\n                band04 = samples[i].B04\n                band08 = samples[i].B08\n            }\n        }\n\n        return {\n            maxNDVI: [maxNDVI],\n            band04: [band04],\n            band08: [band08]\n        }\n    }\n\"\"\"\n\npayload = {\n    \"processRequest\": {\n        \"input\": {\n            \"bounds\": {\n                \"bbox\": [\n                    8.44,\n                    41.31,\n                    9.66,\n                    43.1\n                ],\n                \"properties\": {\n                    \"crs\": \"http://www.opengis.net/def/crs/OGC/1.3/CRS84\"\n                }\n            },\n            \"data\": [{\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2019-04-01T00:00:00Z\",\n                        \"to\": \"2019-06-30T00:00:00Z\"\n                    },\n                \"maxCloudCoverage\": 70.0\n                },\n                \"type\": \"sentinel-2-l2a\"\n            }]\n        },\n        \"output\": {\n            \"responses\": [{\n                \"identifier\": \"band08\",\n                \"format\": {\n                    \"type\": \"zarr/array\"\n                }\n            },\n                {\n                \"identifier\": \"band04\",\n                \"format\": {\n                    \"type\": \"zarr/array\"\n                }\n            },\n                {\n                \"identifier\": \"maxNDVI\",\n                \"format\": {\n                    \"type\": \"zarr/array\"\n                }\n            }]\n        },\n        \"evalscript\": evalscript\n    },\n    \"tilingGrid\": {\n        \"id\": 6,\n        \"resolution\": 100.0\n    },\n    \"zarrOutput\": {\n        \"path\": \"&lt;MyBucket&gt;/&lt;requestId&gt;\",\n        \"group\": {\n            \"zarr_format\": 2\n        },\n        \"arrayParameters\": {\n            \"dtype\": \"&lt;u2\",\n            \"order\": \"C\",\n            \"chunks\": [1, 1000, 1000],\n            \"fill_value\": 0\n        },\n        \"arrayOverrides\": {\n            \"maxNDVI\": {\n                \"dtype\": \"&lt;f4\",\n                \"fill_value\": \"NaN\"\n            },\n        }\n    },\n    \"description\": \"Max NDVI over Corsica with Zarr format output\"\n}\n\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = oauth.request(\"POST\", url, headers=headers, json = payload)\n\nresponse.json()\nExtracting the batch request id from the response:\nbatch_request_id = response.json()['id']\n\n\n\nGet information about all your batch processing requests\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\n\n\nGet information about a batch processing request\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\n\n\nGet current status of a batch processing request\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()['status']\n\n\nRequest detailed analysis (ANALYSE)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}/analyse\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code\n\n\nGet tiles for a batch processing request (optional)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}/tiles\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()\n\n\nRequest the start of processing (START)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}/start\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code\n\n\nGet the latest user's action for a batch processing request\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}\"\n\nresponse = oauth.request(\"GET\", url)\n\nresponse.json()['userAction']\n\n\nCancel a batch processing request (CANCEL)\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}/cancel\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code\n\n\nRestart a partially processed batch processing request (RESTART)\nIf case your batch processing fails only for some tiles while some are processed successfully (i.e. your batch processing request has status PARTIAL), you can restart the processing for all FAILED tiles by running the following code.\nurl = f\"https://sh.dataspace.copernicus.eu/api/v1/batch/process/{batch_request_id}/restartpartial\"\n\nresponse = oauth.request(\"POST\", url)\n\nresponse.status_code\n\n\nCreate a new batch collection\nAdd the parameters cogOutput and createCollection as true to your request output. Add also description\": \"&lt;Name&gt;\" to the request, to name your collection.\n\"description\": \"&lt;Name&gt;\",\n\"output\": {\n  \"defaultTilePath\": \"s3://&lt;MyBucket&gt;/&lt;MyFolder&gt;\",\n  \"cogOutput\": true,\n  \"createCollection\": true\n}\nNote that custom collections can only contain single-band TIFFs. To create a multi-band collection, return separate bands as multiple outputs in the evalscript and connect them to multiple identifiers in the request.\nThe output format of batch requests determines the data format of the collection. By default, the output format of batch requests will be in sampleType.AUTO, which means that batch results 0..1 will be scaled to 0..255 and stored as UINT8. Processing API request on the resulting collection will thus get values 0..255 as input. We recommend you instead use FLOAT32 as the sampleType for the batch request, so the batch request output is exactly the same as what you get with a process requests on the resulting collection."
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html",
    "href": "APIs/SentinelHub/Evalscript/V3.html",
    "title": "Evalscript V3",
    "section": "",
    "text": "Start your evalscript with //VERSION=3 so the system will interpret it as such.\nFor evalscript V3 you need to specify two functions (described in detail below):\nThis is an example of a simple V3 evalscript which returns a true color image:"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#setup-function",
    "href": "APIs/SentinelHub/Evalscript/V3.html#setup-function",
    "title": "Evalscript V3",
    "section": "setup function",
    "text": "setup function\nThis function is required as it sets up the input and output settings.\n\nSpecifics\nSetup needs to return a javascript object with the following properties:\n\ninput - an array of strings representing band names or an array of input objects.\noutput - a single output object or an array of output objects.\nmosaicking (optional) - `{=html} defines input sample     preparation, see [mosaicking](#mosaicking). Defaults toSIMPLE`.\n\n\nInput object properties\n\nbands - an array of strings representing band names\nunits (optional) - a string (all bands will use this unit) or an array of strings listing the units of each band. For a description of units see the documentation of the collection you are querying. Defaults to the default units for each band.\nmetadata (optional) - an array of strings representing properties which can be added to the metadata. Options:\n\n\"bounds\" - specifying this will add dataGeomtery and dataEnvelope to tiles\n\n\n\n\nOutput object properties\n\nid (optional) - any string of your choosing. Must be unique if multiple output objects are defined. Defaults to default.\nbands - the number of bands in this output.\nsampleType (optional) - sets the SampleType constant defining the returned raster sample type. Defaults to AUTO.\nnodataValue (optional) - sets the GDAL nodata metadata tag to the specified value. Only applicable for tiff files.\n\nNote that the number of bands represent the number of components in the output image. JPEG and PNG, for example, can only support 1 or 3 color components (plus an alpha channel for PNG, if set). The sampleType also needs to be compatible with the output raster format.\n\n\nMosaicking\nMosaicking defines how the source data is mosaicked. Not all collections support all these mosaicking types as it depends on how the source data is distributed. See the collection information pages to determine which ones are supported. It is a constant which is specified by a string. To use, for example, set: mosaicking: \"SIMPLE\".\n\nSIMPLE (default) - the simplest method, it flattens the mosaicked image so only a single sample is passed to evaluation. ``{=html}\nORBIT - the mosaicked image is flattened for each orbit so that there is only one sample per pixel per orbit. Multiple samples can therefore be present if there is more than one orbit for the selected time range at the pixel location.\nTILE - this is essentially the unflattened mosaic. It contains all data available for the selected time range. Multiple samples can be present as each sample comes from a single scene. What a scene is is defined by the datasource. ``{=html}\n\n\n\n\n\n\n\nNote\n\n\n\nORBIT mosaicking currently does not work exactly as described but generates a single scene for each day containing satellite data. For most requests this should not be an issue, however high latitude regions may have more than one acquisition per day. For these consider using TILE mosaicking if getting all available data is paramount. This will be corrected in future releases.\n\n\n\n\nSampleType\nSampleType defines the sample type of the output raster. This needs to be compatible with the raster format (e.g. JPEG cannot be FLOAT32). It is a constant which is specified by a string. To use, for example, set: sampleType: \"AUTO\".\n\nINT8 - signed 8-bit integer (values should range from -128 to 127)\nUINT8 - unsigned 8-bit integer (values should range from 0 to 255)\nINT16 - signed 16-bit integer (values should range from -32768 to\n\n\n\nUINT16 - unsigned 16-bit integer (values should range from 0 to\n\n\n\nFLOAT32 - 32-bit floating point (values have effectively no limits)\nAUTO (default) - values should range from 0-1, which will then automatically be stretched from the interval [0, 1] to [0, 255] and written into an UINT8 raster. Values below 0 and above 1 will be clamped to 0 and 255, respectively. This is the default if sampleType is not set in the output object.\n\nHandling SampleType in an Evalscript\nIt is the responsibility of the evalscript to return the values in the interval expected for the chosen sampleType. For integer SampleTypes, any floating point values will be rounded to the nearest integer and clamped to the value range of the SampleType. There is no need to do this yourself. For example, in case of UINT8 output, a value of 40.6 will be saved as 41, and a value of 310 will be saved as 255. If no sampleType is specified, AUTO is selected and the evalscript should return values ranging from 0-1. This is convenient as handling reflectance (e.g. Sentinel-2) data can be more intuitive.\n\n\n\nExamples\nThis simple Sentinel-2 setup() function gets bands B02, B03, B04 and returns (UINT16) 16 bit unsigned raster values.\nfunction setup() {\n  return {\n    input: [{\n      bands: [\"B02\", \"B03\", \"B04\"], // this sets which bands to use\n      units: \"DN\" // here we optionally set the units. All bands will be in this unit (in this case Digital numbers)\n    }],\n    output: { // this defines the output image type\n      bands: 3, // the output of this evalscript will have RGB colors\n      sampleType: \"UINT16\" // raster format will be UINT16\n    }\n  };\n}\nThis Sentinel-2 setup() function gets bands B02, B03, B04 and returns a single raster with 8-bit integer values. To return values in the correct interval for the UINT8 sampleType, the evaluatePixel() function multiplies the reflectance values by 255. A true color image is returned.\nfunction setup() {\n  return {\n    input: [{\n      bands: [\"B02\", \"B03\", \"B04\"], // this sets which bands to use\n    }],\n    output: {\n      bands: 3,\n      sampleType: \"UINT8\" // raster format will be UINT8\n    }\n  };\n}\nfunction evaluatePixel(sample) {\n  return [sample.B04 * 255, sample.B03 * 255, sample.B02 * 255]; // bands need to be multiplied by 255\n}\nIn case of UINT16, the multiplication factor in evaluatePixel() would be 65535 instead of 255.\nThe following example uses bands with different units and produces two rasters:\nfunction setup() {\n    return {\n      input: [{\n          bands: [\"B02\", \"B03\", \"B04\", \"B08\"],\n          units: [\"reflectance\", \"reflectance\", \"reflectance\", \"DN\"] // B08 will be in digital numbers, the rest reflectance\n      }],\n      output: [{ // this is now an array since there are multiple output objects\n          id: \"rgb\"\n          bands: 3\n      }, {\n          id: \"falseColor\"\n          bands: 3\n      }]\n    }\n}"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#evaluatepixel-function",
    "href": "APIs/SentinelHub/Evalscript/V3.html#evaluatepixel-function",
    "title": "Evalscript V3",
    "section": "evaluatePixel function",
    "text": "evaluatePixel function\nThe evaluatePixel function is a mapping which maps the input bands in their input units to the values in the output raster(s). The function is executed once for each output pixel.\n\nParameters\nThe evaluatePixel function has five positional parameters:\nfunction evaluatePixel(samples, scenes, inputMetadata, customData, outputMetadata)\nThe first two parameters can be objects or arrays depending on requested mosaicking as explained below. They are additionally changed for data fusion requests, which is documented separately here. The remaining parameters are always objects.\n\nsamples\n\nWhen mosaicking is SIMPLE:\n\nsamples - an object containing the band values of the single mosaicked sample, in the specified units, as its properties. The property names equal the names of all the input bands, pixel values of a band can be accessed as e.g. samples.B02.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen using mosaicking SIMPLE we usually call this parameter sample in our examples to emphasize that it is an object and not an array.\n\n\n\nWhen mosaicking is TILE or ORBIT:\n\nsamples - an array of samples as defined in the SIMPLE case. None1, one or multiple samples can therefore be present depending on how many orbits/tiles there are for the selected time range and area of interest. Pixel values of a band can be accessed for each sample as an item of the array, e.g. samples[0].B02.\n\n\n\n\nscenes\n\nWhen mosaicking is SIMPLE:\n\nscenes object is empty.\n\nWhen mosaicking is ORBIT:\n\nscenes - an object containing a property orbits. scenes.orbits is an array of objects, where each of them contains metadata for one orbit (day). The length of scenes.orbits array is always the same as the length of samples array. A property, for example dateFrom, can be accessed as scenes.orbits[0].dateFrom. Each object's properties include:\n\ndateFrom (string) - ISO date and time in \"YYYY-MM-DDTHH:MM:SSZ\" format. Together with orbits.dateTo it represents the time interval of one day. All tiles acquired on this day are mosaicked into this scene.\ndateTo (string) - ISO date and time in \"YYYY-MM-DDTHH:MM:SSZ\" format. Together with orbits.dateFrom it represents the time interval of one day. All tiles acquired on this day are mosaicked into this scene.\ntiles (array) - an array of metadata for each tile used for mosaicking of this orbit. Each element has the same properties as elements of scenes.tiles (listed just below for mosaicking TILE).\n\n\nWhen mosaicking is TILE:\n\nscenes - an object containing a property tiles. scenes.tiles is an array of objects, where each of them contains metadata for one tile. The length of scenes.tiles array is always the same as the length of samples array. A property, for example cloudCoverage, can be accessed as scenes.tiles[0].cloudCoverage. Which properties are available for each tiles element depends on requested data and is documented in the \"Scenes Object\" chapter for each data collection, e.g. here for Sentinel-2 L1C. All possible properties are:\n\ndate (string) - ISO date and time in \"YYYY-MM-DDTHH:MM:SSZ\" format. It represents a date when the tile was acquired.\ncloudCoverage (number) - Estimated percentage of pixels covered by clouds in the tile. This field is not available for all data collections. A value 2.09 means that 2.09% of pixels in the tile are cloudy.\ndataPath (string) - Path to where the tile is stored on a cloud. For example \"s3://sentinel-s2-l2a/tiles/33/T/VM/2020/9/15/0\".\ndataGeometry (geojson - like object, see example) - an optional property, added only when requested. Represents a geometry of data coverage within the tile.\ndataEnvelope (geojson - like object, see example) - an optional property, added only when requested. Represents a bbox of dataGeometry.\nshId (number) - Sentinel Hub internal identifier of the tile. For example 11583048.\n\n\n\nNOTE 1: Objects may contain also fields prefixed by __ (double underscore). Such fields are used internally by Sentinel Hub services. Evalscripts should not make use of them because they can be changed or removed at any time and must never modify or delete such fields. Doing so may cause your request to fail or return incorrect results.\nNOTE 2: In the first implementation, scenes was an array of objects, where each of them contained metadata for one orbit or tile (depending on selected mosaicking). It was possible to access metadata as e.g. scenes[0].date. This approach is now deprecated and we strongly advise to use scenes as described above.\n\n\ninputMetadata\ninputMetadata is an object containing metadata used for processing by Sentinel Hub. Its properties are:\n\nserviceVersion - the version of Sentinel Hub which was used for processing.\nnormalizationFactor - the factor used by Sentinel Hub to convert digital numbers (DN) to reflectance using REFLECTANCE = DN * normalizationFactor. This is useful when requesting bands for which both units - DN and REFLECTANCE - are supported.\n\n\n\ncustomData\ncustomData is an object reserved for possible future use.\n\n\noutputMetadata\noutputMetadata is an object which can be used to output any user defined metadata including passing scenes objects, user defined thresholds or ids of original tiles used for processing. It contains:\n\nuserData - is a property to which you can assign a generic object that can contain any data. This can be pushed to the API response by adding a userdata identified output response object to your API request (see this for details or an example here).\n\n\n\n\nReturns\nThe evaluatePixel function can return:\n\nAn object whose keys are the output ids and its values are arrays of numbers. The length of the array is bound by the output object bands number and the values by sampleType.\nAn array of numbers with the same rules as above. This option can be used only when a single image output is defined.\nNothing; the return statement is not specified. This is useful when only information in outputMetadata.userData is needed.\n\n\nInput Units and Output Values\nThe values of each sample is the units specified in the input object. See the input object documentation for more information. How the output values are written to the output raster depends on the sample type. AUTO will stretch values in the interval [0, 1] to [0, 255] and then write those values into an UINT8 raster. The remaining sample types expect values within the range of the sample format.\n\n\n\nExamples\nExample evaluatePixel script returns a simple True Color image based on bands B04, B03, B02:\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02];\n}\nWhen we have multiple outputs in the setup function we can provide them as such:\nfunction evaluatePixel(sample) {\n  return {\n    trueColor: [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02],\n    falseColor: [2.5 * sample.B08, 2.5 * sample.B04, 2.5 * sample.B03]\n  };\n}\nCalculate the average value of band B04 when using ORBIT or TILE mosaicking:\nfunction evaluatePixel(samples) {\n  var sum = 0;\n  var nonZeroSamples = 0;\n  for (var i = 0; i &lt; samples.length; i++) {\n    var value = samples[i].B04;\n    if (value != 0) {\n      sum += value;\n      nonZeroSamples++;\n    }\n  }\n  return [sum / nonZeroSamples];\n}"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#updateoutput-function-optional",
    "href": "APIs/SentinelHub/Evalscript/V3.html#updateoutput-function-optional",
    "title": "Evalscript V3",
    "section": "updateOutput function (optional)",
    "text": "updateOutput function (optional)\nThis function can be used to adjust the number of output bands. This is useful, for example, to request all observations in a given time period as bands of an output file. The function is executed after the setup and preProcessScenes functions but before the evaluatePixel.\n\nParameters\n\noutput - an object containing ids of all outputs and their number of bands as specified in the setup function (Note: This is not the same object as output in the setup function.). The number of bands of each output is stored under output.&lt;output id&gt;.bands where &lt;output id&gt; is equal to values in the setup.output object. For example:\n\n{\n    \"default\": {\n        \"bands\": 2\n    },\n    \"my_output\": {\n        \"bands\": 3\n    }\n}\n\ncollection - an object containing one array per requested data collection. The length of each array equals the number of scenes available for processing. If only one data collection is requested, use collection.scenes.length to get the number of available scenes. For data fusion requests, use collection.&lt;data collection identifier&gt;.scenes.length. Each element in an array has a property:\n\ndate (type Date) - the date when the corresponding scene was acquired.\n\n\n\n\nReturns\nThis function updates the number of output bands and does not return anything.\n\n\nExample\nSuppose we request sentinel-2-l1c data from January 2020 with a maximum of 50% cloud coverage. All of this is specified in the body of a request. We would then like to return all available scenes as bands of an output file. Since we generally do not know how many scenes are available, we can not set the number of output bands directly in a setup function. Using the updateOutput function we can get the number of available scenes from collection and assign it as the value of output.&lt;output id&gt;.bands:\n//VERSION=3\nfunction setup() {\n    return {\n        input: [{\n                bands: [\"B02\"],\n            }\n        ],\n        output: [{\n                id: \"my_output\",\n                bands: 1,\n                sampleType: SampleType.UINT16\n            }\n        ],\n        mosaicking: Mosaicking.ORBIT\n    }\n}\n\nfunction updateOutput(output, collection) {\n    output.my_output.bands = collection.scenes.length\n}\n\nfunction evaluatePixel(samples) {\n    var n_scenes = samples.length\n    let band_b02 = new Array(n_scenes)\n\n    // Arrange values of band B02 in an array\n    for (var i = 0; i &lt; n_scenes; i++){\n        band_b02[i] = samples[i].B02\n    }\n\n    return {\n        my_output: band_b02\n    }\n}"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#updateoutputmetadata-function-optional",
    "href": "APIs/SentinelHub/Evalscript/V3.html#updateoutputmetadata-function-optional",
    "title": "Evalscript V3",
    "section": "updateOutputMetadata function (optional)",
    "text": "updateOutputMetadata function (optional)\nThis function is optional and if present is called at the end of evalscript evaluation. It provides a convenient way to forward information pertaining to the returned data as a whole (as opposed to evaluatePixel which is run for each pixel) into an output object. Do this by assigning any object you require to the userData property of the outputMetadata parameter.\n\nParameters\nThese are the full parameters of the updateOutputMetadata function:\nfunction updateOutputMetadata(scenes, inputMetadata, outputMetadata)\nSee description of parameters in the \"evaluatePixel function\" chapter:\n\nscenes - scenes\ninputMetadata - inputMetadata\noutputMetadata - outputMetadata"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#preprocessscenes-function-optional",
    "href": "APIs/SentinelHub/Evalscript/V3.html#preprocessscenes-function-optional",
    "title": "Evalscript V3",
    "section": "preProcessScenes function (optional)",
    "text": "preProcessScenes function (optional)\nThis function is optional, and if present is called at the beginning of the script evaluation before the actual satellite data is processed. Use it when mosaicking is set to ORBIT or TILE. It provides additional filtering functionality for scenes, after the constraints set in the request parameters are already applied. This is useful, for example, to reduce the number of scenes needed, thereby reducing processing time and the number of processing units for the request.\n\nParameters\nThese are the full parameters of the preProcessScenes function:\nfunction preProcessScenes(collections)\n\ncollections\ncollections is an object, which contains different properties depending on which mosaicking option is selected.\n\nIf mosaicking is ORBIT, collections contains:\n\nfrom (type Date) - the value given as timeRange.from in the body of the request, representing the start of the search interval\nto (type Date) - the value given as timeRange.to in the body of the request, representing the end of the search interval\nscenes.orbits - corresponds to scenes.orbits as described for evalautePixel function and mosaicking ORBIT here but it doesn't contain tiles.\n\nIf mosaicking is TILE, collections contains:\n\nscenes.tiles - corresponds to scenes.tiles as described for evalautePixel function and mosaicking TILE here.\n\n\n\n\n\nReturns\nThe preProcessScenes function must return an objects of the same type as collections. Most often, a sub-set of the input collections will be returned, e.g. to keep only the data acquired before 1.2.2019:\nfunction preProcessScenes(collections){\n    collections.scenes.orbits = collections.scenes.orbits.filter(function (scene) {\n        return new Date(scene.dateFrom) &lt; new Date(\"2019-02-01T00:00:00Z\")\n    });\n    return collections\n}\n\n\nExamples\n\nFilter scenes by particular days\nIn this example, we use preProcessScenes function to select images acquired on two particular dates within the requested timeRange. This example was taken (and adopted) from the evalscript for delineation of burned areas, based on the comparison of Sentinel-2 images acquired before (i.e. on \"2017-05-15\") and after (i.e. on \"2017-06-24\") the event.\n\nIf mosaicking is ORBIT:\nfunction preProcessScenes (collections) {\n    var allowedDates = [\"2017-05-15\", \"2017-06-24\"]; //before and after Knysna fires\n    collections.scenes.orbits = collections.scenes.orbits.filter(function (orbit) {\n        var orbitDateFrom = orbit.dateFrom.split(\"T\")[0];\n        return allowedDates.includes(orbitDateFrom);\n    })\n    return collections\n}\n\n\nIf mosaicking is TILE:\nfunction preProcessScenes (collections) {\n    var allowedDates = [\"2017-05-15\", \"2017-06-24\"]; //before and after Knysna fires\n    collections.scenes.tiles = collections.scenes.tiles.filter(function (tile) {\n        var tileDate = tile.date.split(\"T\")[0];\n        return allowedDates.includes(tileDate);\n    })\n    return collections\n}\n\n\n\nFilter scenes by time interval\nHere, we filter out (= remove) all the scenes acquired between the two selected dates, which both fall within the requested time range.\n\nIf mosaicking is ORBIT:\nfunction preProcessScenes (collections) {\n    collections.scenes.orbits = collections.scenes.orbits.filter(function (orbit) {\n        return (new Date(orbit.dateFrom) &lt; new Date(\"2019-01-31T00:00:00Z\")) ||\n               (new Date(orbit.dateFrom) &gt;= new Date(\"2019-06-01T00:00:00Z\"))\n    })\n    return collections\n}\n\n\nIf mosaicking is TILE:\nfunction preProcessScenes (collections) {\n    collections.scenes.tiles = collections.scenes.tiles.filter(function (tile) {\n        return (new Date(tile.date) &lt; new Date(\"2019-01-31T00:00:00Z\")) ||\n               (new Date(tile.date) &gt;= new Date(\"2019-06-01T00:00:00Z\"))\n    })\n    return collections\n}\n\n\n\nSpecify the number of months taken into account\nValues of timeRange.from and timeRange.to parameters as given in the request, are available in the preProcessScenes function as collections.to and collections.from, respectively. Mosaicking must be ORBIT to use these parameters. They can be used to e.g. filter out scenes acquired more than 3 months before the given to date and time.\nfunction preProcessScenes (collections) {\n    collections.scenes.orbits = collections.scenes.orbits.filter(function (orbit) {\n        var orbitDateFrom = new Date(orbit.dateFrom)\n        return orbitDateFrom.getTime() &gt;= (collections.to.getTime()-3*31*24*3600*1000);\n    })\n    return collections\n}\nThe 3*31*24*3600*1000 represents the 3 months converted to milliseconds. This is needed, so that a 3-month time span can be compared to scene.dateFrom and collections.to, which are all returned as milliseconds since 1970-1-1 by the getTime() function. Note: The result is the same as if the timeRange.from parameter in the body of the request is set to 3 months prior to the timeRange.to.\n\n\nSelect one image per month\nIn this example, we filter the available scenes, so that only the first scene acquired in each month is sent to the evaluatePixel function:\n\nIf mosaicking is ORBIT:\nfunction preProcessScenes (collections) {\n    collections.scenes.orbits.sort(function (s1, s2) {\n            var date1 = new Date(s1.dateFrom);\n            var date2 = new Date(s2.dateFrom);\n            return date1 - date2}) // sort the scenes by dateFrom in ascending order\n\n    firstOrbitDate = new Date(collections.scenes.orbits[0].dateFrom)\n    var previousOrbitMonth = firstOrbitDate.getMonth() - 1\n    collections.scenes.orbits = collections.scenes.orbits.filter(function (orbit) {\n        var currentOrbitDate = new Date(orbit.dateFrom)\n        if (currentOrbitDate.getMonth() != previousOrbitMonth){\n            previousOrbitMonth = currentOrbitDate.getMonth();\n            return true;\n        } else return false;\n    })\n    return collections\n}\n\n\nIf mosaicking is TILE:\nfunction preProcessScenes (collections) {\n    collections.scenes.tiles.sort(function (s1, s2) {\n            var date1 = new Date(s1.date);\n            var date2 = new Date(s2.date);\n            return date1 - date2}) // sort the scenes by dateFrom in ascending order\n\n    firstTileDate = new Date(collections.scenes.tiles[0].date)\n    var previousTileMonth = firstTileDate.getMonth() - 1\n    collections.scenes.tiles = collections.scenes.tiles.filter(function (scene) {\n        var currentTileDate = new Date(scene.date)\n        if (currentTileDate.getMonth() != previousTileMonth){\n            previousTileMonth = currentTileDate.getMonth();\n            return true;\n        } else return false;\n    })\n    return collections\n}"
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#ogc-services-specifics",
    "href": "APIs/SentinelHub/Evalscript/V3.html#ogc-services-specifics",
    "title": "Evalscript V3",
    "section": "OGC services specifics",
    "text": "OGC services specifics\nThere are some specifics when using evalscript V3 with WMS, WTS, WCS services:\n\nThese services return only the default output. Only one image can be returned with each request and it is not possible to request metadata in JSON format.\nTRANSPARENCY and BGCOLOR parameters are ignored. You can use dataMask band in evalscript V3 to handle transparency, as described here.\nBit depth, which is given as the part of a FORMAT parameter (e.g. FORMAT=image/tiff;depth=8) is ignored. You can use sampleType in evalscript V3 to request the bit depth of your choice."
  },
  {
    "objectID": "APIs/SentinelHub/Evalscript/V3.html#footnotes",
    "href": "APIs/SentinelHub/Evalscript/V3.html#footnotes",
    "title": "Evalscript V3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn case samples is an empty array, calling samples[0].B02 will raise an error and it is up to users to handle this in their evalscript.↩︎"
  },
  {
    "objectID": "APIs/SentinelHub/Statistical/Examples.html",
    "href": "APIs/SentinelHub/Statistical/Examples.html",
    "title": "Examples of Statistical API",
    "section": "",
    "text": "The requests below are written in Python. To execute them you need to create an OAuth client as is explained here. It is named oauth in these examples.\n\nStatistics for one single-band output on a given day\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"output_B04\",\n        bands: 1,\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\nfunction evaluatePixel(samples) {\n    return {\n        output_B04: [samples.B04],\n        dataMask: [samples.dataMask]\n        }\n}\n\"\"\"\n\nstats_request = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [414315, 4958219, 414859, 4958819],\n    \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n        }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastRecent\"\n        },\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-07-04T00:00:00Z\",\n            \"to\": \"2020-07-05T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P1D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  }\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/json'\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\nresponse = oauth.request(\"POST\", url=url , headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-07-04T00:00:00Z',\n    'to': '2020-07-05T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.07970000058412552,\n        'max': 0.30959999561309814,\n        'mean': 0.11471141986778864,\n        'stDev': 0.034298170449733226,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}}],\n 'status': 'OK'}\n\n\nStatistics, histogram and percentiles for one single-band output\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"output_B04\",\n        bands: 1,\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\nfunction evaluatePixel(samples) {\n    return {\n        output_B04: [samples.B04],\n        dataMask: [samples.dataMask]\n        }\n}\n\"\"\"\n\nstats_request = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [414315, 4958219, 414859, 4958819],\n    \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n        }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastRecent\"\n        },\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-07-04T00:00:00Z\",\n            \"to\": \"2020-07-05T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P1D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  },\n  \"calculations\": {\n    \"default\": {\n      \"histograms\": {\n        \"default\": {\n          \"nBins\": 5,\n          \"lowEdge\": 0.0,\n          \"highEdge\": 0.3\n        }\n      },\n      \"statistics\": {\n        \"default\": {\n          \"percentiles\": {\n            \"k\": [ 33, 50, 75, 90 ]\n          }\n        }\n      }\n    }\n  }\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/json'\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\n\nresponse = oauth.request(\"POST\", url=url , headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-07-04T00:00:00Z',\n    'to': '2020-07-05T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.07970000058412552,\n        'max': 0.30959999561309814,\n        'mean': 0.11471141986778864,\n        'stDev': 0.034298170449733226,\n        'sampleCount': 3240,\n        'noDataCount': 0,\n        'percentiles': {'33.0': 0.09709999710321426,\n         '50.0': 0.10360000282526016,\n         '75.0': 0.11940000206232071,\n         '90.0': 0.16040000319480896}},\n       'histogram': {'bins': [{'lowEdge': 0.0, 'highEdge': 0.06, 'count': 0},\n         {'lowEdge': 0.06, 'highEdge': 0.12, 'count': 2458},\n         {'lowEdge': 0.12, 'highEdge': 0.18, 'count': 558},\n         {'lowEdge': 0.18, 'highEdge': 0.24, 'count': 177},\n         {'lowEdge': 0.24, 'highEdge': 0.3, 'count': 44}],\n        'overflowCount': 3,\n        'underflowCount': 0}}}}}}],\n 'status': 'OK'}\n\n\nStatistics for one single-band output for two months with 10 days aggregation period\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"output_B04\",\n        bands: 1,\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\nfunction evaluatePixel(samples) {\n    return {\n        output_B04: [samples.B04],\n        dataMask: [samples.dataMask]\n        }\n}\n\"\"\"\n\nstats_request = {\n  \"input\": {\n   \"bounds\": {\n      \"bbox\": [414315, 4958219, 414859, 4958819],\n    \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n        }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastRecent\"\n        }\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-06-01T00:00:00Z\",\n            \"to\": \"2020-07-31T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P10D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  }\n}\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/json'\n}\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\n\nresponse = oauth.request(\"POST\", url=url , headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-06-01T00:00:00Z',\n    'to': '2020-06-11T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.7892000079154968,\n        'max': 0.8303999900817871,\n        'mean': 0.804223583473102,\n        'stDev': 0.0067066009561434865,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}},\n  {'interval': {'from': '2020-06-11T00:00:00Z', 'to': '2020-06-21T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.016300000250339508,\n        'max': 0.5956000089645386,\n        'mean': 0.06240126554233315,\n        'stDev': 0.06266500670629409,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}},\n  {'interval': {'from': '2020-06-21T00:00:00Z', 'to': '2020-07-01T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.026000000536441803,\n        'max': 0.43799999356269836,\n        'mean': 0.06872379640174772,\n        'stDev': 0.056520330692016944,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}},\n  {'interval': {'from': '2020-07-01T00:00:00Z', 'to': '2020-07-11T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.07970000058412552,\n        'max': 0.30959999561309814,\n        'mean': 0.11471141986778864,\n        'stDev': 0.034298170449733226,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}},\n  {'interval': {'from': '2020-07-11T00:00:00Z', 'to': '2020-07-21T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.017400000244379044,\n        'max': 0.4187999963760376,\n        'mean': 0.062194598779473156,\n        'stDev': 0.06317700445712106,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}},\n  {'interval': {'from': '2020-07-21T00:00:00Z', 'to': '2020-07-31T00:00:00Z'},\n   'outputs': {'output_B04': {'bands': {'B0': {'stats': {'min': 0.13920000195503235,\n        'max': 0.4927999973297119,\n        'mean': 0.3146395680115182,\n        'stDev': 0.054700527707146035,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}}],\n 'status': 'OK'}\n\n\nBasic statistics of NDVI with water pixels excluded (custom output dataMask)\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\",\n        \"SCL\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"data\",\n        bands: 1\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\n\nfunction evaluatePixel(samples) {\n    let ndvi = (samples.B08 - samples.B04)/(samples.B08 + samples.B04)\n\n    var validNDVIMask = 1\n    if (samples.B08 + samples.B04 == 0 ){\n        validNDVIMask = 0\n    }\n\n    var noWaterMask = 1\n    if (samples.SCL == 6 ){\n        noWaterMask = 0\n    }\n\n    return {\n        data: [ndvi],\n        // Exclude nodata pixels, pixels where ndvi is not defined and water pixels from statistics:\n        dataMask: [samples.dataMask * validNDVIMask * noWaterMask]\n    }\n}\n\"\"\"\n\n\nstats_request = {\n  \"input\": {\n   \"bounds\": {\n      \"geometry\": {\n          \"type\": \"Polygon\",\n          \"coordinates\": [\n            [\n              [\n                458085.878866,\n                5097236.833044\n              ],\n              [\n                457813.834156,\n                5096808.351383\n              ],\n              [\n                457979.897062,\n                5096313.767184\n              ],\n              [\n                458146.639373,\n                5096405.411294\n              ],\n              [\n                458085.878866,\n                5097236.833044\n              ]\n            ]\n          ]\n        },\n    \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n        }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastCC\"\n        }\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n        \"from\": \"2020-01-01T00:00:00Z\",\n        \"to\": \"2020-12-31T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P30D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  }\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/json'\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\n\nresponse = oauth.request(\"POST\", url=url, headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-01-01T00:00:00Z',\n    'to': '2020-01-31T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.24306687712669373,\n        'max': 0.6244725584983826,\n        'mean': 0.4123224201824293,\n        'stDev': 0.055874589607421886,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-01-31T00:00:00Z', 'to': '2020-03-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.2451941967010498,\n        'max': 0.4233206510543823,\n        'mean': 0.3160828609431641,\n        'stDev': 0.0280772593636271,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-03-01T00:00:00Z', 'to': '2020-03-31T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.4236144721508026,\n        'max': 0.8021259307861328,\n        'mean': 0.5844831434836089,\n        'stDev': 0.05766820795482124,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-03-31T00:00:00Z', 'to': '2020-04-30T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.4647541046142578,\n        'max': 0.8266128897666931,\n        'mean': 0.6615912824901472,\n        'stDev': 0.05539347152437238,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-04-30T00:00:00Z', 'to': '2020-05-30T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.1761743128299713,\n        'max': 0.870899498462677,\n        'mean': 0.6880682412526884,\n        'stDev': 0.18833356676740057,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-05-30T00:00:00Z', 'to': '2020-06-29T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.6883189082145691,\n        'max': 0.8775584697723389,\n        'mean': 0.8230951517303176,\n        'stDev': 0.026851310273968688,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-06-29T00:00:00Z', 'to': '2020-07-29T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.8124191164970398,\n        'max': 0.9270430207252502,\n        'mean': 0.8977047195274247,\n        'stDev': 0.01321883825220214,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-07-29T00:00:00Z', 'to': '2020-08-28T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.750795304775238,\n        'max': 0.8925060033798218,\n        'mean': 0.8437445996058478,\n        'stDev': 0.017705930134783242,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-08-28T00:00:00Z', 'to': '2020-09-27T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.7094070315361023,\n        'max': 0.8823529481887817,\n        'mean': 0.8138526516467535,\n        'stDev': 0.020639924263070358,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-09-27T00:00:00Z', 'to': '2020-10-27T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.6416097283363342,\n        'max': 0.8256189227104187,\n        'mean': 0.7368144742384923,\n        'stDev': 0.02884084473079313,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-10-27T00:00:00Z', 'to': '2020-11-26T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': 0.5131579041481018,\n        'max': 0.9108409285545349,\n        'mean': 0.6912739742345253,\n        'stDev': 0.06273793790576106,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-11-26T00:00:00Z', 'to': '2020-12-26T00:00:00Z'},\n   'outputs': {'data': {'bands': {'B0': {'stats': {'min': -0.01446416787803173,\n        'max': 0.015364916995167732,\n        'mean': 0.0018048733875211391,\n        'stDev': 0.004322122712106793,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}}],\n 'status': 'OK'}\n\n\nStatistics of maximum monthly NDVI for a parcel in 2020\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\",\n        \"SCL\",\n        \"dataMask\"\n      ]\n    }],\n    mosaicking: \"ORBIT\",\n    output: [\n      {\n        id: \"data\",\n        bands: [\"monthly_max_ndvi\"]\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\n\nfunction evaluatePixel(samples) {\n    var max = 0;\n    var hasData = 0;\n    for (var i=0;i&lt;samples.length;i++) {\n      if (samples[i].dataMask == 1 && samples[i].SCL != 6 && samples[i].B04+samples[i].B08 != 0 ){\n        hasData = 1\n        var ndvi = (samples[i].B08 - samples[i].B04)/(samples[i].B08 + samples[i].B04);\n        max = ndvi &gt; max ? ndvi:max;\n      }\n    }\n\n    return {\n        data: [max],\n        dataMask: [hasData]\n    }\n}\n\"\"\"\n\nstats_request = {\n  \"input\": {\n   \"bounds\": {\n      \"geometry\": {\n          \"type\": \"Polygon\",\n          \"coordinates\": [\n            [\n              [\n                458085.878866,\n                5097236.833044\n              ],\n              [\n                457813.834156,\n                5096808.351383\n              ],\n              [\n                457979.897062,\n                5096313.767184\n              ],\n              [\n                458146.639373,\n                5096405.411294\n              ],\n              [\n                458085.878866,\n                5097236.833044\n              ]\n            ]\n          ]\n        },\n    \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n        }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastCC\"\n        }\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-01-01T00:00:00Z\",\n            \"to\": \"2021-01-01T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P1M\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  }\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n   'Accept': 'application/json'\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\n\nresponse = oauth.request(\"POST\", url=url, headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-01-01T00:00:00Z',\n    'to': '2020-02-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.4755639135837555,\n        'max': 0.881286084651947,\n        'mean': 0.6396090604381046,\n        'stDev': 0.06844923487502963,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-02-01T00:00:00Z', 'to': '2020-03-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.3580246865749359,\n        'max': 0.8721038103103638,\n        'mean': 0.5956351390500386,\n        'stDev': 0.07367438999713516,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-03-01T00:00:00Z', 'to': '2020-04-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.4486735761165619,\n        'max': 0.8021259307861328,\n        'mean': 0.5871563556072766,\n        'stDev': 0.057052289003643133,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-04-01T00:00:00Z', 'to': '2020-05-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.7103235721588135,\n        'max': 0.9151291251182556,\n        'mean': 0.8202670164519443,\n        'stDev': 0.029936259510749567,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-05-01T00:00:00Z', 'to': '2020-06-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.7955418825149536,\n        'max': 0.9187881350517273,\n        'mean': 0.8889340774162204,\n        'stDev': 0.013139359632348635,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-06-01T00:00:00Z', 'to': '2020-07-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.6883189082145691,\n        'max': 0.8775584697723389,\n        'mean': 0.8258738168990016,\n        'stDev': 0.025802682912912194,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-07-01T00:00:00Z', 'to': '2020-08-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.8329545259475708,\n        'max': 0.9370484948158264,\n        'mean': 0.9037947789513383,\n        'stDev': 0.01278601507445675,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-08-01T00:00:00Z', 'to': '2020-09-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.750795304775238,\n        'max': 0.8925060033798218,\n        'mean': 0.843880225772972,\n        'stDev': 0.017580399946741675,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-09-01T00:00:00Z', 'to': '2020-10-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.7121148109436035,\n        'max': 0.8823529481887817,\n        'mean': 0.8138710224835326,\n        'stDev': 0.02056652680651673,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-10-01T00:00:00Z', 'to': '2020-11-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.6416097283363342,\n        'max': 0.8256189227104187,\n        'mean': 0.7368144742384923,\n        'stDev': 0.02884084473079313,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-11-01T00:00:00Z', 'to': '2020-12-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.5424679517745972,\n        'max': 0.9108409285545349,\n        'mean': 0.7069293897671695,\n        'stDev': 0.05380689467103403,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}},\n  {'interval': {'from': '2020-12-01T00:00:00Z', 'to': '2021-01-01T00:00:00Z'},\n   'outputs': {'data': {'bands': {'monthly_max_ndvi': {'stats': {'min': 0.0683102235198021,\n        'max': 0.23551543056964874,\n        'mean': 0.1444664227123698,\n        'stDev': 0.027443079533455306,\n        'sampleCount': 3036,\n        'noDataCount': 1192}}}}}}],\n 'status': 'OK'}\n\n\nMultiple outputs with different dataMasks, multi-band output with custom bands' names and different histogram types\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\",\n        \"SCL\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"output_my_bands\",\n        bands: [\"only_band_B04\", \"only_band_B08\"],\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"output_my_indices\",\n        bands: 1,\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"output_scl\",\n        bands: 1,\n        sampleType: \"UINT8\"\n      },\n      {\n        id: \"dataMask\",\n        bands: [\"output_my_bands\", \"output_my_indices\"]\n      }]\n  }\n}\nfunction evaluatePixel(samples) {\n    let ndvi = (samples.B08 - samples.B04)/(samples.B08 + samples.B04)\n\n    var validNDVIMask = 1\n    if (samples.B08 + samples.B04 == 0 ){\n        validNDVIMask = 0\n    }\n\n    var noWaterMask = 1\n    if (samples.SCL == 6 ){\n        noWaterMask = 0\n    }\n\n    return {\n        output_my_bands: [samples.B04, samples.B08],\n        output_my_indices: [ndvi],\n        output_scl: [samples.SCL],\n        dataMask: [samples.dataMask, samples.dataMask * noWaterMask * validNDVIMask]\n    }\n}\n\"\"\"\n\nstats_request = {\n  \"input\": {\n   \"bounds\": {\n      \"bbox\": [414315, 4958219, 414859, 4958819],\n      \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n      }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-2-l2a\",\n        \"dataFilter\": {\n            \"mosaickingOrder\": \"leastRecent\"\n        }\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-07-01T00:00:00Z\",\n            \"to\": \"2020-07-15T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P5D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 20,\n    \"resy\": 20\n  },\n  \"calculations\": {\n    \"output_my_bands\": {\n      \"histograms\": {\n        \"only_band_B08\": {\n          \"nBins\": 3,\n          \"lowEdge\": 0.0,\n          \"highEdge\": 0.3\n        }\n      },\n      \"statistics\": {\n        \"only_band_B04\": {\n          \"percentiles\": {\n            \"k\": [33, 66,100],\n          }\n        }\n      }\n    },\n    \"output_scl\": {\n      \"histograms\": {\n        \"default\": {\n          \"bins\": [0,1,2,3,4,5,6,7,8,9,10,11]\n        }\n      }\n    },\n    \"default\": {\n      \"histograms\": {\n        \"default\": {\n          \"binWidth\": 0.05,\n          \"lowEdge\": 0.0\n        }\n      }\n    }\n  }\n}\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/json'\n}\n\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\n\nresponse = oauth.request(\"POST\", url=url , headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-07-01T00:00:00Z',\n    'to': '2020-07-06T00:00:00Z'},\n   'outputs': {'output_my_bands': {'bands': {'only_band_B04': {'stats': {'min': 0.0803999975323677,\n        'max': 0.2939999997615814,\n        'mean': 0.11451061716602186,\n        'stDev': 0.032769790113614555,\n        'sampleCount': 810,\n        'noDataCount': 0,\n        'percentiles': {'33.0': 0.09719999879598618,\n         '66.0': 0.11169999837875366,\n         '100.0': 0.2939999997615814}}},\n      'only_band_B08': {'stats': {'min': 0.0860000029206276,\n        'max': 0.34290000796318054,\n        'mean': 0.16518679009175594,\n        'stDev': 0.07128630441809644,\n        'sampleCount': 810,\n        'noDataCount': 0},\n       'histogram': {'bins': [{'lowEdge': 0.0,\n          'highEdge': 0.09999999999999999,\n          'count': 199},\n         {'lowEdge': 0.09999999999999999,\n          'highEdge': 0.19999999999999998,\n          'count': 270},\n         {'lowEdge': 0.19999999999999998, 'highEdge': 0.3, 'count': 332}],\n        'overflowCount': 9,\n        'underflowCount': 0}}}},\n    'output_scl': {'bands': {'B0': {'stats': {'min': 8.0,\n        'max': 10.0,\n        'mean': 9.75432098765432,\n        'stDev': 0.6555648554361158,\n        'sampleCount': 810,\n        'noDataCount': 0},\n       'histogram': {'bins': [{'lowEdge': 0, 'highEdge': 1, 'count': 0},\n         {'lowEdge': 1, 'highEdge': 2, 'count': 0},\n         {'lowEdge': 2, 'highEdge': 3, 'count': 0},\n         {'lowEdge': 3, 'highEdge': 4, 'count': 0},\n         {'lowEdge': 4, 'highEdge': 5, 'count': 0},\n         {'lowEdge': 5, 'highEdge': 6, 'count': 0},\n         {'lowEdge': 6, 'highEdge': 7, 'count': 0},\n         {'lowEdge': 7, 'highEdge': 8, 'count': 0},\n         {'lowEdge': 8, 'highEdge': 9, 'count': 99},\n         {'lowEdge': 9, 'highEdge': 10, 'count': 1},\n         {'lowEdge': 10, 'highEdge': 11, 'count': 710}],\n        'overflowCount': 0,\n        'underflowCount': 0}}}},\n    'output_my_indices': {'bands': {'B0': {'stats': {'min': -0.04050104320049286,\n        'max': 0.5338308215141296,\n        'mean': 0.14599402473584097,\n        'stDev': 0.15671216615792566,\n        'sampleCount': 810,\n        'noDataCount': 0},\n       'histogram': {'bins': [{'lowEdge': 0.0, 'highEdge': 0.05, 'count': 340},\n         {'lowEdge': 0.05, 'highEdge': 0.1, 'count': 71},\n         {'lowEdge': 0.1, 'highEdge': 0.15000000000000002, 'count': 50},\n         {'lowEdge': 0.15000000000000002, 'highEdge': 0.2, 'count': 26},\n         {'lowEdge': 0.2, 'highEdge': 0.25, 'count': 23},\n         {'lowEdge': 0.25, 'highEdge': 0.30000000000000004, 'count': 33},\n         {'lowEdge': 0.30000000000000004,\n          'highEdge': 0.35000000000000003,\n          'count': 64},\n         {'lowEdge': 0.35000000000000003, 'highEdge': 0.4, 'count': 81},\n         {'lowEdge': 0.4, 'highEdge': 0.45, 'count': 53},\n         {'lowEdge': 0.45, 'highEdge': 0.5, 'count': 6},\n         {'lowEdge': 0.5, 'highEdge': 0.55, 'count': 9}],\n        'overflowCount': 0,\n        'underflowCount': 54}}}}}},\n  {'interval': {'from': '2020-07-06T00:00:00Z', 'to': '2020-07-11T00:00:00Z'},\n   'outputs': {'output_my_bands': {'bands': {'only_band_B04': {'stats': {'min': 0.007499999832361937,\n        'max': 0.3788999915122986,\n        'mean': 0.05566148159990979,\n        'stDev': 0.060176196853468686,\n        'sampleCount': 810,\n        'noDataCount': 0,\n        'percentiles': {'33.0': 0.022700000554323196,\n         '66.0': 0.04439999908208847,\n         '100.0': 0.3788999915122986}}},\n      'only_band_B08': {'stats': {'min': 0.006500000134110451,\n        'max': 0.46369999647140503,\n        'mean': 0.12869839533864502,\n        'stDev': 0.1266643048401008,\n        'sampleCount': 810,\n        'noDataCount': 0},\n       'histogram': {'bins': [{'lowEdge': 0.0,\n          'highEdge': 0.09999999999999999,\n          'count': 450},\n         {'lowEdge': 0.09999999999999999,\n          'highEdge': 0.19999999999999998,\n          'count': 27},\n         {'lowEdge': 0.19999999999999998, 'highEdge': 0.3, 'count': 254}],\n        'overflowCount': 79,\n        'underflowCount': 0}}}},\n    'output_scl': {'bands': {'B0': {'stats': {'min': 2.0,\n        'max': 9.0,\n        'mean': 5.1716049382715985,\n        'stDev': 1.09834157450977,\n        'sampleCount': 810,\n        'noDataCount': 0},\n       'histogram': {'bins': [{'lowEdge': 0, 'highEdge': 1, 'count': 0},\n         {'lowEdge': 1, 'highEdge': 2, 'count': 0},\n         {'lowEdge': 2, 'highEdge': 3, 'count': 29},\n         {'lowEdge': 3, 'highEdge': 4, 'count': 0},\n         {'lowEdge': 4, 'highEdge': 5, 'count': 235},\n         {'lowEdge': 5, 'highEdge': 6, 'count': 103},\n         {'lowEdge': 6, 'highEdge': 7, 'count': 428},\n         {'lowEdge': 7, 'highEdge': 8, 'count': 13},\n         {'lowEdge': 8, 'highEdge': 9, 'count': 1},\n         {'lowEdge': 9, 'highEdge': 10, 'count': 1},\n         {'lowEdge': 10, 'highEdge': 11, 'count': 0}],\n        'overflowCount': 0,\n        'underflowCount': 0}}}},\n    'output_my_indices': {'bands': {'B0': {'stats': {'min': -0.18976545333862305,\n        'max': 0.858506441116333,\n        'mean': 0.47965881587323095,\n        'stDev': 0.25189343011256504,\n        'sampleCount': 810,\n        'noDataCount': 428},\n       'histogram': {'bins': [{'lowEdge': 0.0, 'highEdge': 0.05, 'count': 3},\n         {'lowEdge': 0.05, 'highEdge': 0.1, 'count': 3},\n         {'lowEdge': 0.1, 'highEdge': 0.15000000000000002, 'count': 15},\n         {'lowEdge': 0.15000000000000002, 'highEdge': 0.2, 'count': 36},\n         {'lowEdge': 0.2, 'highEdge': 0.25, 'count': 28},\n         {'lowEdge': 0.25, 'highEdge': 0.30000000000000004, 'count': 20},\n         {'lowEdge': 0.30000000000000004,\n          'highEdge': 0.35000000000000003,\n          'count': 17},\n         {'lowEdge': 0.35000000000000003, 'highEdge': 0.4, 'count': 6},\n         {'lowEdge': 0.4, 'highEdge': 0.45, 'count': 9},\n         {'lowEdge': 0.45, 'highEdge': 0.5, 'count': 24},\n         {'lowEdge': 0.5, 'highEdge': 0.55, 'count': 22},\n         {'lowEdge': 0.55, 'highEdge': 0.6000000000000001, 'count': 18},\n         {'lowEdge': 0.6000000000000001, 'highEdge': 0.65, 'count': 32},\n         {'lowEdge': 0.65, 'highEdge': 0.7000000000000001, 'count': 46},\n         {'lowEdge': 0.7000000000000001, 'highEdge': 0.75, 'count': 37},\n         {'lowEdge': 0.75, 'highEdge': 0.8, 'count': 29},\n         {'lowEdge': 0.8, 'highEdge': 0.8500000000000001, 'count': 21},\n         {'lowEdge': 0.8500000000000001, 'highEdge': 0.9, 'count': 2}],\n        'overflowCount': 0,\n        'underflowCount': 14}}}}}}],\n 'status': 'OK'}\n\n\nStatistics for Sentinel-1\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"VV\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"output_VV\",\n        bands: 1,\n        sampleType: \"FLOAT32\"\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  }\n}\nfunction evaluatePixel(samples) {\n    return {\n        output_VV: [samples.VV],\n        dataMask: [samples.dataMask]\n        }\n}\n\"\"\"\n\nstats_request = {\n  \"input\": {\n    \"bounds\": {\n      \"bbox\": [414315, 4958219, 414859, 4958819],\n    \"properties\": {\n        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/32633\"\n        }\n    },\n    \"data\": [\n      {\n        \"type\": \"sentinel-1-grd\",\n        \"dataFilter\": {\n        }\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n            \"from\": \"2020-07-01T00:00:00Z\",\n            \"to\": \"2020-07-10T00:00:00Z\"\n      },\n    \"aggregationInterval\": {\n        \"of\": \"P5D\"\n    },\n    \"evalscript\": evalscript,\n    \"resx\": 10,\n    \"resy\": 10\n  }\n}\nheaders = {\n  'Content-Type': 'application/json',\n  'Accept': 'application/json'\n}\nurl = \"https://sh.dataspace.copernicus.eu/api/v1/statistics\"\nresponse = oauth.request(\"POST\", url=url , headers=headers, json=stats_request)\nsh_statistics = response.json()\nsh_statistics\n{'data': [{'interval': {'from': '2020-07-01T00:00:00Z',\n    'to': '2020-07-06T00:00:00Z'},\n   'outputs': {'output_VV': {'bands': {'B0': {'stats': {'min': 0.0,\n        'max': 0.4447733759880066,\n        'mean': 0.046840328479290934,\n        'stDev': 0.05487441687888816,\n        'sampleCount': 3240,\n        'noDataCount': 0}}}}}}],\n 'status': 'OK'}"
  },
  {
    "objectID": "APIs/SentinelHub/Overview/Authentication.html",
    "href": "APIs/SentinelHub/Overview/Authentication.html",
    "title": "Authentication",
    "section": "",
    "text": "The Sentinel Hub API uses OAuth2 Authentication and requires that you have an access token. In essence, this is a piece of information you add to your requests so the server knows it's you. These tokens do not last forever for a multitude of reasons, but you can get new ones and when they expire from the Sentinel-Hub OAuth2 server at the token endpoint listed below. But first, if you do not have one already, you need to register an OAuth Client in your account settings. This is so the server can expect you to make such token requests.\n\nHow to use tokens\nOnce you have a token, do use it for authenticating all your requests within its validity period. While tokens do not last forever, they do last a reasonable amount of time, and sufficiently long that they can be reused. The information of how long each token lasts is embedded in the token itself in the exp claim, and can be read from there.\nDo not fetch a new token for each API request you make. Token requests are rate limited, so if you are getting an HTTP 429 error, that means you are requesting too many tokens.\nTokens are JSON Web Tokens (JWT), more information about them here or here.\n\n\nRegistering OAuth client\nTo register an OAuth client, open the “User Settings” tab in the Dashboard app and locate the “OAuth clients” section. Click on the Create button (1). Provide a name for your OAuth client (2). You can select the Expiry date for your client (3) or select “Never expire” if you want your OAuth client to be indefinite (4). For OAuth clients without expiration you will need to confirm your understanding of risks (5). If your OAuth client will be used by a single-page application (SPA), select the corresponding option (6). If the SPA option is selected, you'll be prompted to choose between \"Allow all domains\" (7) or specify the web origins (8) from which the OAuth Client will be used. Confirmation is required if you selected to use the OAuth client in a frontend application, acknowledging the risks of exposure (9).\nOnce all mandatory fields are filled, proceed by clicking the Create button (10).\nYour client ID and client secret will be displayed. You may now copy both client ID and the client secret (11). Ensure to copy the secret value and paste it securely as it won’t be retrievable once the pop-up closes! After copying, click Close(12). With the client ID and client secret in hand, you are now prepared to request tokens.\nUpon completion, you’ll find your newly created OAuth client with its corresponding ID and name (13) listed under the “OAuth clients” section.\n\n\nInactive clients\nOnce an OAuth client has expired, it is moved to a list of inactive clients. Similarly, deleted clients will also appear in this list as inactive clients.\n\n\n\n\nOAuth2 Endpoints\nToken Endpoint - for requesting tokens\nhttps://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\n\n\nToken Request Examples\n\ncURL\nThe following cURL request will return an access token, just make sure to replace &lt;your client id&gt; with your client ID and &lt;your client secret&gt; with your client secret:\ncurl --request POST --url https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token --header 'content-type: application/x-www-form-urlencoded' --data 'grant_type=client_credentials&client_id=&lt;your client id&gt;' --data-urlencode 'client_secret=&lt;your client secret&gt;'\n\n\nPostman\nIn the Postman request Authorization tab set the Type to OAuth 2.0 and Add auth data to to Request Headers. Set the Grant Type to Client Credentials, the access token URL to the token endpoint, then set the Client ID and Client Secret to the values of your OAuth Client. Scope can be blank. Set Client Authentication to Send client credentials in body. Click Get New Access Token button. You should get a new one immediately. To use this token to authorize your request, click Use Token. For more information see the Postman authorization documentation\n\n\nPython\nIn python the requests-oauthlib library can handle the retrieval of access tokens using your OAuth Client configuration.\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\n\n# Your client credentials\nclient_id = '&lt;client_id&gt;'\nclient_secret = '&lt;secret&gt;'\n\n# Create a session\nclient = BackendApplicationClient(client_id=client_id)\noauth = OAuth2Session(client=client)\n\n# Get token for the session\ntoken = oauth.fetch_token(token_url='https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token',\n                          client_secret=client_secret, include_client_id=True)\n\n# All requests using this session will have an access token automatically added\nresp = oauth.get(\"https://sh.dataspace.copernicus.eu/configuration/v1/wms/instances\")\nprint(resp.content)\nrequests-oauthlib doesn't check for status before checking if the response is ok. In case there's a server error, the user can receive an incorrect error, which falsely makes it seem as if the issue is on client side. Library's compliance hooks will prevent the invalid status response from being ignored, returning the correct error. To use them, add the following code:\ndef sentinelhub_compliance_hook(response):\n    response.raise_for_status()\n    return response\n\noauth.register_compliance_hook(\"access_token_response\", sentinelhub_compliance_hook)\n\n\nJavascript\nExample using axios.\nimport axios from \"axios\"\nimport qs from \"qs\"\n\nconst client_id = \"&lt;client_id&gt;\"\nconst client_secret = \"&lt;secret&gt;\"\n\nconst instance = axios.create({\n  baseURL: \"https://sh.dataspace.copernicus.eu\"\n})\n\nconst config = {\n  headers: {\n    'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8'\n  }\n}\n\nconst body = qs.stringify({\n  client_id,\n  client_secret,\n  grant_type: \"client_credentials\"\n})\n\n\n// All requests using this instance will have an access token automatically added\ninstance.post(\"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\", body, config).then(resp =&gt; {\n  Object.assign(instance.defaults, {headers: {authorization: `Bearer ${resp.data.access_token}`}})\n})"
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html",
    "title": "Processing Unit definition",
    "section": "",
    "text": "⚠ Costs marked with ** are not yet applied, they will come into effect in the future."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#general-data-processing---applicable-to-process-api-ogc-api-statistical-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#general-data-processing---applicable-to-process-api-ogc-api-statistical-api",
    "title": "Processing Unit definition",
    "section": "General data processing - applicable to Process API, OGC API, Statistical API",
    "text": "General data processing - applicable to Process API, OGC API, Statistical API\nEach request costs a proportional amount of processing unit(s), depending on what data and processing is requested. One processing unit (PU) is defined as a request for:\n\nan output (image) size of 512 x 512 pixels,\n3 collection input bands,\none data sample per pixel (see sample),\nan output (image) format not exceeding 16 bits per pixel,\nwithout additional processing (e.g. orthorectification) applied,\n\nIn addition to this:\n\nMinimal cost of a request is\n\n0.005 PU for Process API and OGC API,\n0.01 PU for Statistical API.\n\nThe number of remaining processing units is reduced only when a request successfully executes, i.e. when the response code is 2XX.\n\n\"Multiplication factors\" are used to calculate how many processing units are required for each request. The definition of 1 processing unit and the calculation rules are summarized in the following tables:\n\n\n\nParameter/API\nQuantity for 1 PU\nRules for multiplication factors\n\n\n\n\nArea of interest\n512 x 512 px\nThe multiplication factor is calculated by dividing requested input size (BBOX) by 512 x 512 (pixel size depends on the user-defined resolution of the request execution).  The minimum value of this multiplication factor is 0.01. This corresponds to an area of 0.25 km^2 for Sentinel-2 data at 10 m spatial resolution.\n\n\nNumber of input bands\n3\nThe multiplication factor is calculated by dividing the requested number of input bands by 3. An exception is requesting dataMask which is not counted, unless it is the only band included.\n\n\nOutput format\n8 bit or 16 bit TIFF/JPG/PNG\nRequesting 32 bit float TIFF will result in a multiplication factor of 2 due to larger memory consumption and data traffic.  Requesting application/octet-stream will result in a multiplication factor of 1.4 due to additional integration costs (This is used for integration with external tools such as xcube.).\n\n\nNumber of data samples\n1\nThe multiplication factor equals the number of data samples per pixel.\n\n\nData fusion\nN/A\nThe multiplication is only applied when data fusion is used. Multiplication factor is calculated as a sum of all collections within the same endpoint location and twice the sum of all remote collections, i.e. count(local_collections) + 2x count(remote_collections). Example: data fusion request executed on services.sentinel-hub.com endpoint, which includes Sentinel-2 L1C, Sentinel-2 L2A and Landsat-9 would have a multiplication factor of 4 (1 + 1 + 2).\n\n\n\nSurcharges\n'Surcharges' are used for non-standard requests, which impact on the execution costs.\n\n\n\nSurcharge\nRules for calculation\n\n\n\n\n** Evalscript execution time\nExecution of evalscript with duration shorter than 200ms, is covered within the base request. Execution of complex evalscripts (i.e. neural networks, large decision trees, etc.) with duration longer than 200ms there is a surcharge of 0.5 PU per each additional started 100ms interval."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#sentinel-1-data-processing---applicable-to-process-api-ogc-api-statistical-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#sentinel-1-data-processing---applicable-to-process-api-ogc-api-statistical-api",
    "title": "Processing Unit definition",
    "section": "Sentinel-1 data processing - applicable to Process API, OGC API, Statistical API",
    "text": "Sentinel-1 data processing - applicable to Process API, OGC API, Statistical API\nIn addition to General data processing rules defined above, the following optional multiplicators apply as well:\n\n\n\nParameter/API\nRules for multiplication factors\n\n\n\n\nOrthorectification\nRequesting orthorectification will result in a multiplication factor of 2 due to additional processing requirements .\n\n\nRadiometric Terrain Correction\nRequesting radiometric terrain correction will result in a multiplication factor of 2.5 due to additional processing requirements. The orthorectification factor is not additionally applied as it is a prerequisite.\n\n\nSpeckle Filtering\nRequesting speckle filtering will result in a multiplication factor of 2 due to additional processing requirements."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#data-querying---applicable-to-catalog-api-ogc-wfs",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#data-querying---applicable-to-catalog-api-ogc-wfs",
    "title": "Processing Unit definition",
    "section": "Data querying - applicable to Catalog API, OGC WFS",
    "text": "Data querying - applicable to Catalog API, OGC WFS\nEach request costs a proportional amount of processing unit(s) depending on what data and processing is requested. One processing unit (PU) is defined as a request for:\n\narea of 1000 x 1000 km\ntime period up to one month\n\nIn addition to this:\n\nMinimal cost of a request is 0.01 PU.\nMaximal cost of a request is 1 PU.\nThe number of remaining processing units is reduced only when a request successfully executes, i.e. when the response code is 2XX.\n\n\n\n\nParameter/API\nQuantity for 1 PU\nRules for multiplication factors\n\n\n\n\nArea of interest\n1 000 000 km2\nThe multiplication factor is calculated by dividing requested input area of interest (BBOX) by 1 000 000.The minimum value of this multiplication factor is 0.01. This corresponds to an area of 10 000 km2\n\n\nTime period\n1 month\nThe multiplication factor is calculated by ceiling requested time period in months."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#batch-processing-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#batch-processing-api",
    "title": "Processing Unit definition",
    "section": "Batch Processing API",
    "text": "Batch Processing API\n\"General data processing\" and \"Sentinel-1 data processing\" rules apply with the following exceptions:\n\nMinimal cost of a request is 100 PU.\nWhen using Batch Processing API, a multiplication factor of 1/3 will be applied to all tiles with a size of at least 10,000 px. Thus, up to three times more data can be processed compared to the Processing API for the same amount of PUs. If the tiling grid contains tiles smaller than 10,000 px, these tiles will be charged at the regular rate (no multiplication factor)\n** When data is delivered to a bucket in other region within the same system (i.e. Copernicus Data Space Ecosystem, AWS) there is additional cost of 0.03 PU per MB of data."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#batchv2-processing-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#batchv2-processing-api",
    "title": "Processing Unit definition",
    "section": "BatchV2 Processing API",
    "text": "BatchV2 Processing API\n\"General data processing\" and \"Sentinel-1 data processing\" rules apply with the following exceptions:\n\nMinimal cost of a request is 100 PU.\nProcessing with batch processing API will result in a multiplication factor of 1/3 (only applies if processed tiles is bigger than 10.000 px). Thus, three times more data can be processed comparing to process API for the same amount of PUs.\n** When data is delivered to a bucket in other region within the same system (i.e. Copernicus Data Space Ecosystem, AWS) there is additional cost of 0.03 PU per MB of transferred data."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#asynchronous-processing-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#asynchronous-processing-api",
    "title": "Processing Unit definition",
    "section": "Asynchronous Processing API",
    "text": "Asynchronous Processing API\n\"General data processing\" and \"Sentinel-1 data processing\" rules apply with the following exceptions:\n\nMinimal cost of a request is 10 PU.\nWhen using Asynchronous Processing API, a multiplication factor of 2/3 will be applied to all requests with an area of at least 10,000 px. Thus, up to 1.5 times more data can be processed compared to the Processing API for the same amount of PUs. If the request defines an area smaller than 10,000 px, this request will be charged at the regular rate (no multiplication factor).\nWhen data is delivered to a bucket in other region within the same system (i.e. Copernicus Data Space Ecosystem, AWS) there is an additional cost of 0.03 PU per MB of data."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#batch-statistical-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#batch-statistical-api",
    "title": "Processing Unit definition",
    "section": "Batch Statistical API",
    "text": "Batch Statistical API\n\"General data processing\" and \"Sentinel-1 data processing\" rules apply with the following exceptions:\n\nMinimal cost of a request is 100 PU.\n** When data is delivered to a bucket in other region within the same system (i.e. Copernicus Data Space Ecosystem, AWS) there is an additional cost of 0.03 PU per MB of data."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#third-party-data-order---applicable-to-third-party-data-import-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#third-party-data-order---applicable-to-third-party-data-import-api",
    "title": "Processing Unit definition",
    "section": "Third party data order - applicable to Third Party Data Import API",
    "text": "Third party data order - applicable to Third Party Data Import API\n\n** Each search request costs 1 PU.\n** Each thumbnail request costs 1 PU.\n** Each created order/subscription costs 5 PU.\n** Each processed order delivery costs 5 PU.\n** Each processed subscription delivery costs 2 PU."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#data-ingestion---applicable-to-bring-your-own-cog-api-and-zarr-api",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#data-ingestion---applicable-to-bring-your-own-cog-api-and-zarr-api",
    "title": "Processing Unit definition",
    "section": "Data ingestion - applicable to Bring your own COG API and Zarr API",
    "text": "Data ingestion - applicable to Bring your own COG API and Zarr API\n\nEach non-GET request to BYOC or Zarr API costs 1 PU.\nUsage of your BYOC and Zarr collections is billed the same as usage of public collections."
  },
  {
    "objectID": "APIs/SentinelHub/Overview/ProcessingUnit.html#request-cost-calculation-examples",
    "href": "APIs/SentinelHub/Overview/ProcessingUnit.html#request-cost-calculation-examples",
    "title": "Processing Unit definition",
    "section": "Request cost calculation examples",
    "text": "Request cost calculation examples\n\nSentinel-1 change detection\nAn example of calculation of processing units for a Sentinel-1 change detection request (e.g. comparison of two time slices) is presented in the table below.\n\n\n\nParameter\nQuantity\nMultiplication factor\nDetails\n\n\n\n\nOutput size (width x height)\n1024 x 1024 px\nx 4\nThe requested output size is 1024 x 1024 px which is 4 times larger than the output size for one PU (512 x 512 px). Hence the multiplication factor is 4.\n\n\nNumber of input bands\n4\nx 4/3\n4 input bands are requested, which is 4/3 times more than 3 input bands, which are included in one PU. The multiplication factor is thus 4/3.\n\n\nOutput format\n32-bit float\nx 2\nThe requested 32 bit float TIFF has a multiplication factor of 2.\n\n\nNumber of data samples\n2\nx 2\n2 data samples (one for each time slice) were requested for each pixel. Thus the multiplication factor is 2.\n\n\nOrthorectification\nYes\nx 2\nOrtorectification is requested, which results in a multiplication factor of 2.\n\n\n\nTotal\n42.667 processing units\nTo calculate the number of processing units for this request multiply all the individual multiplication factors: 4 x 4/3 x 2 x 2 x 2 = 42.667\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatistical API is also a multi-temporal request. The same rules for calculating multiplication factors apply.\n\n\n\n\nNDVI calculation for a parcel\nAn example of calculation of processing units of NDVI value over a 4 hectare large parcel at 10 m spatial resolution is presented in the table below.\n\n\n\nParameter\nQuantity\nMultiplication factor\nDetails\n\n\n\n\nOutput size (width x height)\n20 x 20 px\nx 0.01\nThe requested output size is 20 x 20 px which is smaller than the minimum area, thus the multiplication factor is 0.01.\n\n\nNumber of input bands\n2\nx 2/3\n2 input bands are requested, thus the multiplication factor is 2/3.\n\n\nOutput format\n16-bit tiff\nx 1\nThe same as in the definition of one processing unit, thus the multiplication factor is 1.\n\n\nNumber of data samples\n1\nx 1\nThe same as in the definition of one processing unit, thus the multiplication factor is 1.\n\n\nOrthorectification\nNo\nx 1\nThe same as in the definition of one processing unit, thus the multiplication factor is 1.\n\n\n\nTotal\n0.0067 processing units\nTo calculate the number of processing units for this request multiply all the individual multiplication factors:  0.01 x 2/3 x 1 x 1 = 0.0067"
  },
  {
    "objectID": "APIs/SentinelHub/OGC.html",
    "href": "APIs/SentinelHub/OGC.html",
    "title": "OGC service",
    "section": "",
    "text": "Our OGC services offer the access to the Sentinel Hub functionalists via interfaces, which conform to the Open Geospatial Consortium (OGC) standards: WMS, WCS, WFS, and WMTS.\nUsing the OGC services you can avoid the complexities of preprocessing of satellite data. No need to download the data, no dealing with the JP2 format, no re-projecting, or mosaicking. No need for large storage volumes and lots of processing power. Simply add a new data collection in your GIS application (ArcGIS, QGIS, OpenLayers, Google Earth or any other app supporting standard services) and start using the data right away! Find more information on:\n\nWMS - Web Mapping Service\nWCS - Web Coverage Service\nWFS - Web Feature Service\nWMTS - Web Mapping Tile Service\n\n\nConfiguration Instance and Authentication\nTo use any of our OGC services you will need a \"configuration instance\" (or shortly \"instance\"). A configuration instance defines which layers are part of your OGC service, how the data shall be processed and visualized for each of these layers, and its id is used to authenticate your OGC requests. You can create and edit configuration instances in the Sentinel Hub Dashboard in the \"Configuration Utility\" tab. \"Simple WMS Instance\" is a pre-created configuration instance, which comes with your Sentinel Hub account and you can use its id (\"9d559...\" in the example below but yours will have a different id) to run the OGC examples.\n\n\n\nTutorials and Other Related Materials\nTo get you started, we have prepared a webinar on OGC API with QGIS integration, explaining the structure of OGC requests, how to run them in web browser, Postman and Python and integrate them into your own GIS. November 4, 2020"
  },
  {
    "objectID": "APIs/On-Demand Production API.html",
    "href": "APIs/On-Demand Production API.html",
    "title": "On-Demand Production API",
    "section": "",
    "text": "On-Demand Production API provides access to two sets of data processing workflows:\nBoth production and processing workflows are available through the OData API described below, and the Data Workspace which provided a graphical user interface to the service.\nBasic interaction with the On-Demand Production service is following:"
  },
  {
    "objectID": "APIs/On-Demand Production API.html#on-demand-production",
    "href": "APIs/On-Demand Production API.html#on-demand-production",
    "title": "On-Demand Production API",
    "section": "On-Demand Production",
    "text": "On-Demand Production\nThe production of the Copernicus products is done using the recent IPF (Instrument Processing Facility) processor sets, used by the Production Services. Users can request production of any Sentinel-2 or Sentinel-3 product using the existing output product as reference.\nUsing the metadata of the reference product, the service identifies the necessary Level-0 and Auxiliary products to run the IPF.\nThe input products are retrieved from the Copernicus LTA (Long Term Archive) and submitted to a processing engine. The output of the production is stored in the temporary data storage and available to download for a period of 14 days.\nThe production workflows are available for Sentinel-1, Sentinel-2 and Sentinel-3.\nCheck the examples below to learn how to submit a production order.\n\nSentinel-1\nAll Sentinel-1 workflows start from Level-0 RAW products and require Level-0 product as input reference.\nThe available workflows\n\n\n\nWorkflow name\nInput product type\nOutput product type\n\n\n\n\nSentinel-1-L0-EW_GRDM_1S\nEW_RAW__0\nEW_GRDM_1S\n\n\nSentinel-1-L0-EW_OCN__2S\nEW_RAW__0\nEW_OCN__2S\n\n\nSentinel-1-L0-EW_SLC__1S\nEW_RAW__0\nEW_SLC__1S\n\n\nSentinel-1-L0-IW_GRDH_1S\nIW_RAW__0\nIW_GRDH_1S\n\n\nSentinel-1-L0-IW_OCN__2S\nIW_RAW__0\nIW_OCN__2S\n\n\nSentinel-1-L0-IW_SLC__1S\nIW_RAW__0\nIW_SLC__1S\n\n\nSentinel-1-L0-S1_GRDH_1S\nS1_RAW__0\nS1_GRDH_1S\n\n\nSentinel-1-L0-S1_OCN__2S\nS1_RAW__0\nS1_OCN_2S\n\n\nSentinel-1-L0-S1_SLC__1S\nS1_RAW__0\nS1_SLC__1S\n\n\nSentinel-1-L0-S2_GRDH_1S\nS2_RAW__0\nS2_GRDH_1S\n\n\nSentinel-1-L0-S2_OCN__2S\nS2_RAW__0\nS2_OCN__2S\n\n\nSentinel-1-L0-S2_SLC__1S\nS2_RAW__0\nS2_SLC__1S\n\n\nSentinel-1-L0-S3_GRDH_1S\nS3_RAW__0\nS3_GRDH_1S\n\n\nSentinel-1-L0-S3_OCN__2S\nS3_RAW__0\nS3_OCN__2S\n\n\nSentinel-1-L0-S3_SLC__1S\nS3_RAW__0\nS3_SLC__1S\n\n\nSentinel-1-L0-S4_GRDH_1S\nS4_RAW__0\nS4_GRDH_1S\n\n\nSentinel-1-L0-S4_OCN__2S\nS4_RAW__0\nS4_OCN__2S\n\n\nSentinel-1-L0-S4_SLC__1S\nS4_RAW__0\nS4_SLC__1S\n\n\nSentinel-1-L0-S5_GRDH_1S\nS5_RAW__0\nS5_GRDH_1S\n\n\nSentinel-1-L0-S5_OCN__2S\nS5_RAW__0\nS5_OCN__2S\n\n\nSentinel-1-L0-S5_SLC__1S\nS5_RAW__0\nS5_SLC__1S\n\n\nSentinel-1-L0-S6_GRDH_1S\nS6_RAW__0\nS6_GRDH_1S\n\n\nSentinel-1-L0-S6_OCN__2S\nS6_RAW__0\nS6_OCN__2S\n\n\nSentinel-1-L0-S6_SLC__1S\nS6_RAW__0\nS6_SLC__1S\n\n\nSentinel-1-L0-WV_OCN__2S\nWV_RAW__0\nWV_OCN__2S\n\n\nSentinel-1-L0-WV_SLC__1S\nWV_RAW__0\nWV_SLC__1S\n\n\n\nWorkflow options for Sentinel-1\n\n\n\n\n\n\n\n\nOption name\nDescription\nValues\n\n\n\n\nDEM\nIdentifier of the digital elevation model to use for processing.\n“coarse”, “getasse30v1”, “getasse30v2”\n\n\noutput_storage\nLocation of output product.\n“TEMPORARY”\n\n\n\n\n\nSentinel-2\nAll Sentinel-2 workflows start from Level-0 products and require Level-1/Level-2 (output) product as input reference. For Sentinel-2-L1-S2MSI2A the input is searched in the catalogue and it is possible that the baseline of the found product is different from the baseline provided in the WorkflowOptions.\nThe available workflows\n\n\n\nWorkflow name\nInput product type\nOutput product type\n\n\n\n\nSentinel-2-L0-S2MSI1C\nS2MSI1C\nS2MSI1C\n\n\nSentinel-2-L0-S2MSI2A\nS2MSI2A\nS2MSI2A\n\n\nSentinel-2-L1-S2MSI2A\nS2MSI2A\nS2MSI2A\n\n\n\nWorkflow options for Sentinel-2\n\n\n\n\n\n\n\n\nOption name\nDescription\nValues\n\n\n\n\nBaseline\nVersion number of the processor and static auxiliary files.\ndependent on the specific workflow\n\n\nBaseline Collection\nCompatible Baseline group.\ndependent on the specific workflow\n\n\noutput_storage\nLocation of output product.\n“TEMPORARY”\n\n\n\n\n\nSentinel-3\nThe Sentinel-3 workflows run from Level-0 products and require Level-1/Level-2 (output) product as input reference.\nThe available workflows\n\n\n\nWorkflow name\nInput product type\nOutput product type\n\n\n\n\nSentinel-3-L0-OL_1_EFR\nOL_0_EFR___\nOL_1_EFR___\n\n\nSentinel-3-L0-OL_1_ERR\nOL_0_EFR___\nOL_1_ERR___\n\n\nSentinel-3-L0-OL_2_LFR\nOL_0_EFR___\nOL_2_LFR___\n\n\nSentinel-3-L0-OL_2_LRR\nOL_0_EFR___\nOL_2_LRR___\n\n\nSentinel-3-L0-SL_1_RBT\nSL_0_SLT___\nSL_1_RBT___\n\n\nSentinel-3-L0-SL_2_FRP\nSL_0_SLT___\nSL_2_FRP___\n\n\nSentinel-3-L0-SL_2_LST\nSL_0_SLT___\nSL_2_LST___\n\n\nSentinel-3-L0-SR_1_SRA\nSR_0_SRA___\nSR_1_SRA___\n\n\nSentinel-3-L0-SR_2_LAN\nSR_0_SRA___,MW_0_MWR___\nSR_2_LAN___\n\n\nSentinel-3-L0-SY_2_AOD\nOL_0_EFR___,SL_0_SLT___\nSY_2_AOD___\n\n\nSentinel-3-L0-SY_2_SYN\nOL_0_EFR___,SL_0_SLT___\nSY_2_SYN___\n\n\nSentinel-3-L0-SY_2_VG1\nOL_1_EFR___,SL_1_RBT___\nSY_2_VG1___\n\n\nSentinel-3-L0-SR_2_LAN_LI\nSR_0_SRA___,MW_0_MWR___\nSR_2_LAN_LI\n\n\nSentinel-3-L0-SR_2_LAN_HY\nSR_0_SRA___,MW_0_MWR___\nSR_2_LAN_HY\n\n\nSentinel-3-L0-SR_2_LAN_SI\nSR_0_SRA___,MW_0_MWR___\nSR_2_LAN_SI\n\n\n\nWorkflow options for Sentinel-3\n\n\n\n\n\n\n\n\nOption name\nDescription\nValues\n\n\n\n\nBaseline\nVersion number of the processor and static auxiliary files.\ndependent on the specific workflow\n\n\nMode\nTimeliness condition : NTC (Non-Time Critical).\n“NTC”\n\n\noutput_storage\nLocation of output product.\n“TEMPORARY”"
  },
  {
    "objectID": "APIs/On-Demand Production API.html#on-demand-processing",
    "href": "APIs/On-Demand Production API.html#on-demand-processing",
    "title": "On-Demand Production API",
    "section": "On-Demand Processing",
    "text": "On-Demand Processing\nOn-demand processing capability for CARD-BS and CARD-COH6/12 is available in the Copernicus Data Space Ecosystem. This service is offered via a limited pool of resources, shared across all users, and can be used to process the data free of charge. This is suitable for users who need to process smaller batches of products. There is no guarantee that processing will be completed in a specific time. For commercial use, the price list is available in the CREODIAS portal. The service is available via an On-Demand Processing API and Data Workspace."
  },
  {
    "objectID": "APIs/On-Demand Production API.html#ondemand-production-api-with-odata-interface",
    "href": "APIs/On-Demand Production API.html#ondemand-production-api-with-odata-interface",
    "title": "On-Demand Production API",
    "section": "OnDemand Production API with OData interface",
    "text": "OnDemand Production API with OData interface\nThe OnDemand Processing API allows the users to interact with the service to issue and control the orders. It provides functionalities like creation, update, cancellation, pausing and monitoring of orders. This allows the users to have better control over the workflow execution process. The API is based on the Production Interface Delivery Point ICD, extended to provide more flexibility and additional functionality."
  },
  {
    "objectID": "APIs/On-Demand Production API.html#general-information",
    "href": "APIs/On-Demand Production API.html#general-information",
    "title": "On-Demand Production API",
    "section": "General information",
    "text": "General information\nThis documentation provides an overview of the OnDemand Processing (ODP) API based on the Open Data Protocol (OData) standard. The ODP API complements the Catalogue API used for accessing data and metadata from the Copernicus data catalogue.\nAccess to the API is limited by the Authentication service. Quotas are assigned according to the user typology and include limits on the number of concurrent orders and available processing workflows.\nThe API allows the discovery of all available workflows that can be run in the Copernicus Data Space Ecosystem platform, indicating which data types can be processed and what the available parameters and output modes are.\n\nAPI Endpoint\nThe ODP API endpoint is https://odp.dataspace.copernicus.eu/odata/v1.\nOpenAPI documentation is located at https://odp.dataspace.copernicus.eu/odata/docs\n\n\nAPI Operations\nThe ODP API supports the following operations:\n\nGET - This operation retrieves data and metadata from the ODP. The GET operation supports various query options to filter, order, and limit the data retrieved\nPOST - This operation creates new entities in the ODP. The POST operation requires a payload in JSON format that specifies the properties of the new entity\nPATCH - This operation is used to update existing entities in the ODP. The PATCH operation requires a JSON payload that specifies the entity’s properties to update\nDELETE - This operation deletes existing entities from the ODP. The DELETE operation requires the URL of the entity to be deleted\n\n\n\nAPI Resources\nThe ODP API provides access to the following resources:\n\nWorkflow - predefined processor which creates a single output product or series of products based on the input parameters provided by the user. Typical inputs are the name of the source product and parameters specific to the processing chain\nProduction Order - request for production using a Workflow chosen by the user\nBatch Order - request to produce multiple products using a chosen Workflow\nOrder Item - single processing job within and a Production Order or Batch Order\n\n\n\nAuthentication\n\n\n\n\n\n\nImportant\n\n\n\nTo access the ODP API, you need an authorization token, as only authorized users are allowed to interact with the processing service.\n\n\nTo get the token, you can use the following script:\nexport KEYCLOAK_TOKEN=$(curl -d 'client_id=cdse-public' \\\n-d 'username=&lt;LOGIN&gt;' \\\n-d 'password=&lt;PASSWORD&gt;' \\\n-d 'grant_type=password' \\\n'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token' | \\\npython -m json.tool | grep \"access_token\" | awk -F\\\" '{print $4}')\nOnce you have your token, you can execute a request to the API, including the token in the request header. For example, to list available Workflows, you can use the following command:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" 'https://odp.dataspace.copernicus.eu/odata/v1/Workflows'\nMore information on the tokens and authentication can be found here: https://documentation.dataspace.copernicus.eu/APIs/OData.html#product-download"
  },
  {
    "objectID": "APIs/On-Demand Production API.html#querying-the-api",
    "href": "APIs/On-Demand Production API.html#querying-the-api",
    "title": "On-Demand Production API",
    "section": "Querying the API",
    "text": "Querying the API\n\nWorkflows\n\nListing available Workflows\nTo list all processing Workflows available to the user:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/Workflows'\nTo search for specific Workflows you can use filters on the attributes. To find workflow named “coh”:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/Workflows?$filter=Name%20eq%20coh'\nIn a similar way to find all Workflows suitable for processing Sentinel-1 SLC products:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/Workflows?$filter=contains(InputProductType,'SLC')'\nDetails of the Workflow in the response list the parameters which are needed to create a new Production Order:\n{\n    \"Id\": \"47\",\n    \"Name\": \"card_coh12_public\",\n    \"DisplayName\": \"Sentinel-1: Coherence (12 days)\",\n    \"Documentation\": null,\n    \"Description\": \"The Sentinel-1 CARD COH12 (Copernicus Analysis Ready\n      Data Coherence) processor generates a Sentinel-1 Level 2 product describing \n      the coherence of a pair of images - 12 days apart. \n      Concurrently, a terrain-correction (using DEM) is performed. This processor \n      provided by the Joint Research Centre is based on a GPT graph that can be run \n      with ESA SNAP.\",\n    \"InputProductType\": \"SLC\",\n    \"InputProductTypes\": [\n        \"SLC\",\n        \"S1_SLC__1S\",\n        \"S2_SLC__1S\",\n        \"S3_SLC__1S\",\n        \"S4_SLC__1S\",\n        \"S5_SLC__1S\",\n        \"S6_SLC__1S\",\n        \"IW_SLC__1S\",\n        \"EW_SLC__1S\",\n        \"WV_SLC__1S\"\n    ],\n    \"OutputProductType\": \"CARD-COH12\",\n    \"WorkflowVersion\": \"3.1.0\"\n}\n\n\n\nProduction Orders\n\nCreate a new Production Order\nTo submit a new processing job, you need to use the POST method and send the parameters as a JSON message according to the requirements of a specific Workflows:\ncurl -X POST -H 'Content-Type:application/json' \\\n-H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder/OData.CSC.Order' \\\n-d '&lt;json_message&gt;'\nProduction orders accept the following general parameters:\n\nWorkflowName - the identifier of the workflow – “Name” attribute in the /Workflows response\nInputProductReference - information about the input data to be used by the processor\n\nReference - identifier of a single input product or multiple products (example – mosaicking processors)\nContentDate - time period which should be used by the Workflows (example - time-series based processors)\n\nWorkflowOptions - parameters specific to the Workflows (example – DEM version, cloud coverage)\nPriority - priority of the order in the users job queue. Orders with higher priority will be processed first\nNotificationEndpoint - (not used) URL of the endpoint which should receive the information once the order is completed (done or failed)\nName - non-unique name for the order to help identify the orders\n\nThe structure of the JSON message:\n{\n  \"WorkflowName\": \"string\",\n  \"InputProductReference\": {\n    \"Reference\": \"string\",\n    \"ContentDate\": {\n      \"Start\": \"YYYY-MM-DDTHH:mm:ss.SSSZ\",\n      \"End\": \"YYYY-MM-DDTHH:mm:ss.SSSZ\"\n    }\n  },\n  \"WorkflowOptions\": [\n    {\n      \"Name\": \"string\",\n      \"Value\": \"string\"\n    }\n  ],\n  \"Priority\": 0,\n  \"NotificationEndpoint\": \"string\",\n  \"Name\": \"string\"\n}\nExample: to submit an order for the Sentinel-1 CARD Backscatter product:\ncurl -X POST -H 'Content-Type:application/json' \\\n-H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder/OData.CSC.Order' \\\n-d '{ \\\n  \"WorkflowName\": \"card_bs\",\n  \"InputProductReference\": {\n    \"Reference\": \"S1A_IW_GRDH_1SDV_20230404T162838_20230404T162903_047949_05C333_B4FC.SAFE\"\n  },\n  \"Priority\": 1,\n  \"Name\": \"card_bs_order_1\"\n}'\nSample response after successful submission of the order:\n{\n  \"@odata.context\": \"$metadata#OData.CSC.Order\",\n  \"value\": {\n    \"Id\": \"9999999\",\n    \"Status\": \"queued\",\n    \"StatusMessage\": \"queued\",\n    \"SubmissionDate\": \"2023-04-05T10:03:25.474Z\",\n    \"Name\": \"S1A_IW_GRDH_1SDV_20230404T162838_20230404T162903_047949_05C333_B4FC.SAFE\",\n    \"EstimatedDate\": \"2023-04-05T10:33:16.161Z\",\n    \"InputProductReference\": {\n      \"Reference\": S1A_IW_GRDH_1SDV_20230404T162838_20230404T162903_047949_05C333_B4FC.SAFE\",\n      \"ContentDate\": null\n    },\n    \"WorkflowOptions\": [],\n    \"WorkflowName\": \"card_bs\",\n    \"WorkflowId\": null,\n    \"Priority\": 1\n  }\n}\n\n\nCheck list of Production Orders\nTo list all Production Orders requested by the user:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders'\nWhen looking for completed orders for a specific processor:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders?$filter=WorkflowName%20eq%20coh%20and%20Status%20eq%20completed'\n\n\nCheck the status of a single Production Order\nTo check details of the single order using the order Id:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders(&lt;order_id&gt;)'\n\n\nCancel a Production Order\nTo cancel an existing order:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(&lt;order_id&gt;)/OData.CSC.Cancel'\nThe orders that are in the queue and have not yet been processed will be removed instantly. For the orders in processing, the Order Items (single item within a Production Order) being processed will complete but the remaining part of the Order will be cancelled.\n\n\nDisplay details of the result\nThe order generates a new product which can be downloaded from the public repository or private storage. To check the details of the result:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(&lt;order_id&gt;)/Product'\n\n\nDownload the result\nOnce the order is successfully processed, the status changes to complete, and the result is ready for download. The user may choose to instruct the service to put the results in a specified location (mandatory if custom parameters have been passed to the Workflow), and standard results (for Workflows like CARD-BS or CARD-COH12) are stored in the CDSE public repository and can be retrieved through the API.\nTo download the result:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(&lt;order_id&gt;)/Product/$value' \\\n-o result.zip\n\n\n\nBatch Orders\n\nCreate a new Batch Order\nIn a way similar to a single Production Order, you can request processing of multiple input products as a Batch Order. The Batch Order will run a selected Workflow with the same parameters for all inputs and output the results to the same location. Using Batch Orders makes it easier to process time series or large AOIs.\nBatch Order endpoint uses the same mechanism as described for the Production Order:\ncurl -X POST -H 'Content-Type:application/json' \\\n-H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\ \n'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder/OData.CSC.Order' \\\n-d '&lt;json_message&gt;'\nBatch Orders accept the following general parameters:\n\nWorkflowName - the identifier of the workflow – “Name” attribute in the /Workflows response\nIdentifierList - information about the input data to be used by the processor - identifiers of products\nWorkflowOptions - parameters specific to the Workflows (example – DEM version, cloud coverage)\nPriority - priority of the order in the users job queue. Orders with higher priority will be processed first\nCallback - (not used) URL of the endpoint which should receive the information once the order is completed (done or failed)\nName - non-unique name for the order to help identify the orders\nBatchSize - maximum number of items in the batch\nBatchVolume - maximum size of output data\n\nThe structure of the JSON message:\n{\n  \"Name\": \"string\",\n  \"Priority\": 0,\n  \"WorkflowName\": \"string\",\n  \"Callback\": \"string\",\n  \"WorkflowOptions\": [\n    {\n      \"Name\": \"string\",\n      \"Value\": \"string\"\n    }\n  ],\n  \"IdentifierList\": [\n    \"string\"\n  ],\n  \"BatchSize\": 0,\n  \"BatchVolume\": 0\n}\n\n\nCheck list of Batch Orders\nTo list all Production Orders requested by the user:\ncurl -X POST -H 'Content-Type:application/json' \\\n-H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\ \n'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder'\nWhen looking for batch orders for a specific processor (in example ‘coh’):\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder?$filter=WorkflowName%20eq%20%coh'\n\n\nCheck the status of a single Batch Order\nTo check details of the single order using the order Id:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder(&lt;batch_order_id&gt;)'\n\n\nList products generated in a Batch Order\nWhen the batch order is processed, for each input product an output is generated. To list the output of a batch:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder(&lt;batch_order_id&gt;)/Products'\n\n\nDisplay details of the results\nEach item in the batch is an individual Production Order. To check the details of the single result:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/BatchOrder(&lt;batch_order_id&gt;)/Product(&lt;order_id&gt;)'"
  },
  {
    "objectID": "APIs/On-Demand Production API.html#usage-examples",
    "href": "APIs/On-Demand Production API.html#usage-examples",
    "title": "On-Demand Production API",
    "section": "Usage examples",
    "text": "Usage examples\nThe examples below show how to use the OnDemand Production API to order production of Sentinel data packages. The requests to the API endpoint can be made using standard Linux CLI tools like curl and wget, or by other programmatic methods.\nMore information and examples of using the OData APIs of the Copernicus Dataspace Ecosystem can be found above and in the Catalogue API documentation\n\nOn-Demand Production\n\nOrdering Sentinel-1 Level-1 IW_GRDH_1S using Production Order\nThe production of Sentinel-1 GRD products starts from Level-0 RAW product. As the Sentinel-1 products require single input product, the workflow can use Level-0 product as input reference.\nIn the following example we will order production of GRD product covering Brussels on 12th of May, 2024. First the required input Level-0 products needs to be identified using the CDSE catalogue:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=((ContentDate/Start%20ge%202024-05-12T00:00:00.000Z%20and%20ContentDate/Start%20le%202024-05-12T23:59:59.999Z)%20and%20(Online%20eq%20true)%20and%20(OData.CSC.Intersects(Footprint=geography%27SRID=4326;POLYGON%20((4.313869%2050.796245,%204.437046%2050.796245,%204.437046%2050.913716,%204.313869%2050.913716,%204.313869%2050.796245))%27))%20and%20(((((Collection/Name%20eq%20%27SENTINEL-1%27)%20and%20(((Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27productType%27%20and%20i0/Value%20eq%20%27RAW%27)))))))))&$expand=Attributes&$expand=Assets&$orderby=ContentDate/Start%20asc&$top=1'\nThe query breaks down to the following filters:\n\n(ContentDate/Start ge 2024-05-12T00:00:00.000Z and ContentDate/Start le 2024-05-12T23:59:59.999Z) - observation starting on 12/05/2024\n(Online eq true) - the product is online (ready to use) in the CDSE archive\n(OData.CSC.Intersects(Footprint=geography’SRID=4326;POLYGON ((4.313869 50.796245, 4.437046 50.796245, 4.437046 50.913716, 4.313869 50.913716, 4.313869 50.796245))’)) - product footprint intersects with the Brussels metropolitan area\n(Collection/Name eq ‘SENTINEL-1’) - we are looking for Sentinel-1 data\n(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name eq ‘productType’ and i0/Value eq ‘RAW’)) - the “RAW” product type is a group covers all Level-0 product types\n$top=1 - results in a single product\n\nThe result is S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE which will be used as input to the production order.\nThe next step is preparation of the body of the request:\n{\n    \"Name\":\"S1 IW_GRDH_1S getasse30v1 1\",\n    \"WorkflowName\":\"Sentinel-1-L0-IW_GRDH_1S\",\n    \"InputProductReference\": {\n        \"Reference\": \"S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE\"\n        },\n    \"Priority\": 1,\n    \"WorkflowOptions\":[\n        {\n        \"Name\":\"Dem\",\"Value\":\"getasse30v1\"\n        },\n        {\n        \"Name\":\"output_storage\",\"Value\":\"TEMPORARY\"\n        }\n    ]\n}\nand save it in a file called odp_request.json (example name).\nThe request contents break down to the following:\n\n“Name”:“S1 IW_GRDH_1S getasse30v1” - user provided string identifying the order\n“WorkflowName”:“Sentinel-1-L0-IW_GRDH_1S” - identifier of the workflow which will be used to process input data\n“Name”:“Dem”,“Value”:“getasse30v1” - GETASSE30 elevation model selected\n“Name”:“output_storage”,“Value”:“TEMPORARY” - the data will be stored in temporary storage (available for download for 14 days)\n“IdentifierList”:[“S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE”] - Level-0 RAW product used as input to the processor\n\nThe body of the request needs to be sent in a call to ODP API:\ncurl -X POST -H 'Content-Type:application/json' -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" -d @odp_request.json 'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder/OData.CSC.Order'\nAs a result new order should be created. The status of the order can be checked with the following request:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" 'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrders?$filter=Name%20eq%20%27S1%20IW_GRDH_1S%20getasse30v1%27'\nThe API response includes the Status and the ID:\n{\n  \"@odata.context\": \"$metadata#ProductionOrder/$entity\",\n  \"value\": [\n    {\n      \"Id\": \"2322968\",\n      \"Status\": \"completed\",\n      \"StatusMessage\": \"requested output product is available\",\n      \"SubmissionDate\": \"2024-05-21T05:34:52.840Z\",\n      \"Name\": \"S1 IW_GRDH_1S getasse30v1\",\n      \"EstimatedDate\": \"2024-05-21T08:33:31.103Z\",\n      \"InputProductReference\": {\n        \"Reference\": \"S1A_IW_RAW__0SDV_20240512T055035_20240512T055108_053834_068ADC_5C24.SAFE\",\n        \"ContentDate\": null\n      },\n...\nKey information in the response:\n\n“Id”: “2322968” - the identifier (ID) of the order\n“Status”: “completed” - status of the processing (queued, in progress, completed, failed)\n“StatusMessage”: “requested output product is available” - detailed information about the processing status. In case the production fails, the cause of error is provided\n\nOnce the order is in status “Completed” the result can be downloaded using the order ID:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://odp.dataspace.copernicus.eu/odata/v1/ProductionOrder(2322968)/Product/$value' -o result.zip\nThe output will be saved as result.zip in the local folder. It contains the output product stored as compressed SAFE archive.\nunzip -l result.zip\nArchive:  result.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n1771670525  2024-05-21 15:46   S1A_IW_GRDH_1SDV_20240512T055039_20240512T055104_053834_068ADC_A2BD.SAFE.zip\n---------                     -------\n1771670525                     1 file\n\n\nOrdering Sentinel-3 Level-2 SL_2_LST using Production Order\nThe production of Sentinel-2 and Sentinel-3 products starts from a set of Level-0 products chosen by the selection rules.\n\n\n\n\n\n\nImportant\n\n\n\nDifferently than in case of Sentinel-1, the reference input is an existing higher level product. The workflow based on the reference will identify required Level-0 inputs and produces an equivalent of the reference product using the chosen processor version and workflow options.\n\n\nIn this example we will order production of product covering Brussels on 12th of May, 2024.\nWhen choosing Sentinel-3 product it is important to select NTC (non-time critical) timeliness as the workflow does not generate NRT or STC products. Providing NRT or STC product as input reference will result in an error.\nFirst the required input Level-2 reference products needs to be identified using the CDSE catalogue:\ncurl -H \"Authorization: Bearer $KEYCLOAK_TOKEN\" \\\n'https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=((ContentDate/Start%20ge%202024-05-12T00:00:00.000Z%20and%20ContentDate/Start%20le%202024-05-12T23:59:59.999Z)%20and%20(Online%20eq%20true)%20and%20(OData.CSC.Intersects(Footprint=geography%27SRID=4326;POLYGON%20((4.313869%2050.796245,%204.437046%2050.796245,%204.437046%2050.913716,%204.313869%2050.913716,%204.313869%2050.796245))%27))%20and%20((Collection/Name%20eq%20%27SENTINEL-3%27)%20and%20(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27productType%27%20and%20i0/Value%20eq%20%27SL_2_LST___%27)))%20and%20(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27timeliness%27%20and%20i0/Value%20eq%20%27NT%27)))&$expand=Attributes&$expand=Assets&$orderby=ContentDate/Start%20asc&$top=1'\nFiltering parameters specific to this example (standard parameters as in Sentinel-1 example):\n\n(Collection/Name%20eq%20%27SENTINEL-3%27) - we are looking for Sentinel-3 data\n(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27productType%27%20and%20i0/Value%20eq%20%27SL_2_LST_%27)) - product type SL_2_LST___\n(Attributes/OData.CSC.StringAttribute/any(i0:i0/Name%20eq%20%27timeliness%27%20and%20i0/Value%20eq%20%27NT%27)) - product timeliness non-time critical (NTC)\n\nThe result is S3B_SL_2_LST____20240512T093034_20240512T093334_20240513T031156_0180_093_036_2160_PS2_O_NT_004.SEN3 which will be used as input to the production order.\nThe next step is preparation of the body of the request:\n{\n    \"Name\":\"S3 SL_2_LST 1\",\n    \"WorkflowName\":\"Sentinel-3-L0-SL_2_LST\",\n    \"InputProductReference\": {\n        \"Reference\": \"S3B_SL_2_LST____20240512T093034_20240512T093334_20240513T031156_0180_093_036_2160_PS2_O_NT_004.SEN3\"\n        },\n    \"Priority\": 1,\n    \"WorkflowOptions\":[\n        {\n        \"Name\":\"Baseline\",\"Value\":\"3.27\"\n        },\n        {\n        \"Name\":\"output_storage\",\"Value\":\"TEMPORARY\"\n        }\n    ]\n}\nand save it in a file called odp_request.json (example name).\nThe additional workflow option used in this example:\n\n“Name”:“Baseline”,“Value”:“3.27” - selects specific baseline (version of the processor)\n\nFurther steps to submit, check the status of the order and download results are the same as for the Sentinel-1 example."
  },
  {
    "objectID": "APIs/SentinelHub.html",
    "href": "APIs/SentinelHub.html",
    "title": "Sentinel Hub",
    "section": "",
    "text": "Sentinel Hub is a multi-spectral and multi-temporal big data satellite imagery service capable of fully automated archiving, real-time processing and distribution of remote sensing data and related EO products. Users can use APIs to retrieve satellite data over their AOI and specific time range from complete archives in a few seconds."
  },
  {
    "objectID": "APIs/S3.html",
    "href": "APIs/S3.html",
    "title": "Access to EO data via S3",
    "section": "",
    "text": "S3 API is one of the main access methods for EO data. It is suitable for Third Party applications that require high-performance parallel access and scalability. Moreover, any user who wants to connect from an external infrastructure to the Copernicus Data Space Ecosystem collection can do so through the S3 protocol."
  },
  {
    "objectID": "APIs/S3.html#object-storage-endpoints",
    "href": "APIs/S3.html#object-storage-endpoints",
    "title": "Access to EO data via S3",
    "section": "Object Storage endpoints",
    "text": "Object Storage endpoints\nAccess to EO data hosted on object storage is using an API compatible with S3.\nS3 is an object storage service with which you can retrieve data over HTTPS using REST API.\nThere are multiple S3 endpoints for accessing the EO Data. The default S3 endpoint address is: https://eodata.dataspace.copernicus.eu/\nThis endpoint will provide access to EO data which is stored on the Object Storage. A Global Service Load Balancer (GSLB) directs the request to either CloudFerro Cloud or OpenTelekom Cloud (OTC) S3 endpoint. The decision is based on the location of the DNS resolver. (Global Service Load Balancer)\nUsers who want to make sure they are forwarded to OTC S3 endpoint have to use the following URL: https://eodata.ams.dataspace.copernicus.eu/"
  },
  {
    "objectID": "APIs/S3.html#registration",
    "href": "APIs/S3.html#registration",
    "title": "Access to EO data via S3",
    "section": "Registration",
    "text": "Registration\nTo generate the necessary credentials, you must have a registered account on dataspace.copernicus.eu. If you don’t have an account, you can register here."
  },
  {
    "objectID": "APIs/S3.html#generate-secrets",
    "href": "APIs/S3.html#generate-secrets",
    "title": "Access to EO data via S3",
    "section": "Generate secrets",
    "text": "Generate secrets\nIn order to obtain secrets, visit page and log in with the Copernicus Data Space Ecosystem account for which the keys are to be generated. After successfully logging in, indicate the expiration date of the secrets using the “Add Credentials” button and click “Confirm”.\n\n\nNote that the Secret Key will be displayed only once, and you cannot view it again after clicking the Close button. Copy your secret key and keep it in a safe place."
  },
  {
    "objectID": "APIs/S3.html#example-access-using-s3cmd",
    "href": "APIs/S3.html#example-access-using-s3cmd",
    "title": "Access to EO data via S3",
    "section": "Example access using s3cmd",
    "text": "Example access using s3cmd\nThe example shown below assumes the use of a Linux environment.\nWith the access and secret key and the endpoint eodata.dataspace.copernicus.eu, you can use any tool to handle access via S3. Below is an example of how to access EO Data using the s3cmd.\nFirst, we recommend creating a configuration file. You can create it with tools like vi/vim or nano:\nvi .s3cfg\nvim .s3cfg\nnano .s3cfg\nCopy the following content to your configuration file with your access and secret key:\n[default]\naccess_key = &lt;access_key&gt;\nhost_base = eodata.dataspace.copernicus.eu\nhost_bucket = eodata.dataspace.copernicus.eu\nhuman_readable_sizes = False\nsecret_key = &lt;secret_key&gt;\nuse_https = true\ncheck_ssl_certificate = true\nThen you can run any s3cmd command pointing to the previously created configuration file with option -c:\n$ s3cmd -c .s3cfg ls s3://eodata/\nBelow is an example of downloading a product from the EO data repository using s3cmd:\n$ s3cmd -c ~/.s3cfg get s3://eodata/Sentinel-1/SAR/SLC/2016/12/28/S1A_IW_SLC__1SDV_20161228T044442_20161228T044509_014575_017AE8_4C26.SAFE/measurement/s1a-iw2-slc-vv-20161228t044442-20161228t044508-014575-017ae8-005.tiff\nIf the objects in the repository are archives, for example, such as S1B_IW_SLC__1SDV_20191013T155948_20191013T160015_018459_022C6B_13A2.SAFE use the resursive option ––recursive or -r to download whole product.\nExample with the recursive option:\n$ s3cmd -c ~/.s3cfg -r get s3://eodata/Sentinel-1/SAR/SLC/2019/10/13/S1B_IW_SLC__1SDV_20191013T155948_20191013T160015_018459_022C6B_13A2.SAFE/\nRequest type using command s3cmd get with –recursive option can be broken down as follows:\n\nSingle Request to Initiate: The initial command with –recursive is a single request to initiate the process of downloading files recursively.\nMultiple Requests for Each File: Despite the initial command being a single request to start the recursive download, S3 treats each file download as a separate request. Therefore, if downloaded product have 100 files in the package, S3 will count this as 100 requests (one for each file downloaded).\n\nSo, while s3cmd is executed in a single command, the –recursive option will cause that each downloaded file is counted as an individual request in terms of quota count."
  },
  {
    "objectID": "APIs/S3.html#example-script-to-download-product-using-boto3",
    "href": "APIs/S3.html#example-script-to-download-product-using-boto3",
    "title": "Access to EO data via S3",
    "section": "Example script to download product using boto3",
    "text": "Example script to download product using boto3\nimport boto3\nimport os\n\nsession = boto3.session.Session()\ns3 = boto3.resource(\n    's3',\n    endpoint_url='https://eodata.dataspace.copernicus.eu',\n    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n    region_name='default'\n)  # generated secrets\n\ndef download(bucket, product: str, target: str = \"\") -&gt; None:\n    \"\"\"\n    Downloads every file in bucket with provided product as prefix\n\n    Raises FileNotFoundError if the product was not found\n\n    Args:\n        bucket: boto3 Resource bucket object\n        product: Path to product\n        target: Local catalog for downloaded files. Should end with an `/`. Default current directory.\n    \"\"\"\n    files = bucket.objects.filter(Prefix=product)\n    if not list(files):\n        raise FileNotFoundError(f\"Could not find any files for {product}\")\n    for file in files:\n        os.makedirs(os.path.dirname(file.key), exist_ok=True)\n        if not os.path.isdir(file.key):\n            bucket.download_file(file.key, f\"{target}{file.key}\")\n\n# path to the product to download\ndownload(s3.Bucket(\"eodata\"), \"Sentinel-1/SAR/SLC/2019/10/13/S1B_IW_SLC__1SDV_20191013T155948_20191013T160015_018459_022C6B_13A2.SAFE/\")"
  },
  {
    "objectID": "APIs/S3.html#example-script-to-download-product-using-python",
    "href": "APIs/S3.html#example-script-to-download-product-using-python",
    "title": "Access to EO data via S3",
    "section": "Example script to download product using python",
    "text": "Example script to download product using python\n#!/usr/bin/env python3\n\nimport os\nimport json\nimport requests\nimport boto3\nfrom tqdm import tqdm\nimport time\nimport argparse\n\n# Set up command line argument parser\nparser = argparse.ArgumentParser(\n    description=\"Script to download EO product using OData and S3 protocol.\",\n    epilog=\"Example usage: python script.py -u &lt;username&gt; -p &lt;password&gt; &lt;eo_product_name&gt;\"\n)\nparser.add_argument('-u', '--username', type=str, help='Username for authentication (required)')\nparser.add_argument('-p', '--password', type=str, help='Password for authentication (required)')\nparser.add_argument('eo_product_name', type=str, help='Name of the Earth Observation product to be downloaded (required)')\nargs = parser.parse_args()\n\n# Prompt for missing credentials\nif not args.username:\n    args.username = input(\"Enter username: \")\nif not args.password:\n    args.password = input(\"Enter password: \")\n\n# Configuration parameters\nconfig = {\n    \"auth_server_url\": \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n    \"odata_base_url\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products\",\n    \"s3_endpoint_url\": \"https://eodata.dataspace.copernicus.eu\",\n}\n\ndef get_access_token(config, username, password):\n    \"\"\"\n    Retrieve an access token from the authentication server.\n    This token is used for subsequent API calls.\n    \"\"\"\n    auth_data = {\n        \"client_id\": \"cdse-public\",\n        \"grant_type\": \"password\",\n        \"username\": username,\n        \"password\": password,\n    }\n    response = requests.post(config[\"auth_server_url\"], data=auth_data, verify=True, allow_redirects=False)\n    if response.status_code == 200:\n        return json.loads(response.text)[\"access_token\"]\n    else:\n        print(f\"Failed to retrieve access token. Status code: {response.status_code}\")\n        exit(1)\n\ndef get_eo_product_details(config, headers, eo_product_name):\n    \"\"\"\n    Retrieve EO product details using the OData API to determine the S3 path.\n    \"\"\"\n    odata_url = f\"{config['odata_base_url']}?$filter=Name eq '{eo_product_name}'\"\n    response = requests.get(odata_url, headers=headers)\n    if response.status_code == 200:\n        eo_product_data = response.json()[\"value\"][0]\n        return eo_product_data[\"Id\"], eo_product_data[\"S3Path\"]\n    else:\n        print(f\"Failed to retrieve EO product details. Status code: {response.status_code}\")\n        exit(1)\n\ndef get_temporary_s3_credentials(headers):\n    \"\"\"\n    Create temporary S3 credentials by calling the S3 keys manager API.\n    \"\"\"\n    credentials_response = requests.post(\"https://s3-keys-manager.cloudferro.com/api/user/credentials\", headers=headers)\n    if credentials_response.status_code == 200:\n        s3_credentials = credentials_response.json()\n        print(\"Temporary S3 credentials created successfully.\")\n        print(f\"access: {s3_credentials['access_id']}\")\n        print(f\"secret: {s3_credentials['secret']}\")\n        return s3_credentials\n    else:\n        print(f\"Failed to create temporary S3 credentials. Status code: {credentials_response.status_code}\")\n        print(\"Product download aborted.\")\n        exit(1)\n\ndef format_filename(filename, length=40):\n    \"\"\"\n    Format a filename to a fixed length, truncating if necessary.\n    \"\"\"\n    if len(filename) &gt; length:\n        return filename[:length - 3] + '...'\n    else:\n        return filename.ljust(length)\n\ndef download_file_s3(s3, bucket_name, s3_key, local_path, failed_downloads):\n    \"\"\"\n    Download a file from S3 with a progress bar.\n    Track failed downloads in a list.\n    \"\"\"\n    try:\n        file_size = s3.head_object(Bucket=bucket_name, Key=s3_key)['ContentLength']\n        formatted_filename = format_filename(os.path.basename(local_path))\n        with tqdm(total=file_size, unit='B', unit_scale=True, desc=formatted_filename, ncols=80, bar_format='{desc:.40}|{bar:20}| {percentage:3.0f}% {n_fmt}/{total_fmt}B') as pbar:\n            def progress_callback(bytes_transferred):\n                pbar.update(bytes_transferred)\n\n            s3.download_file(bucket_name, s3_key, local_path, Callback=progress_callback)\n    except Exception as e:\n        print(f\"Failed to download {s3_key}. Error: {e}\")\n        failed_downloads.append(s3_key)\n\ndef traverse_and_download_s3(s3_resource, bucket_name, base_s3_path, local_path, failed_downloads):\n    \"\"\"\n    Traverse the S3 bucket and download all files under the specified prefix.\n    \"\"\"\n    bucket = s3_resource.Bucket(bucket_name)\n    files = bucket.objects.filter(Prefix=base_s3_path)\n\n    for obj in files:\n        s3_key = obj.key\n        relative_path = os.path.relpath(s3_key, base_s3_path)\n        local_path_file = os.path.join(local_path, relative_path)\n        local_dir = os.path.dirname(local_path_file)\n        os.makedirs(local_dir, exist_ok=True)\n        download_file_s3(s3_resource.meta.client, bucket_name, s3_key, local_path_file, failed_downloads)\n\ndef main():\n    # Step 1: Retrieve the access token\n    access_token = get_access_token(config, args.username, args.password)\n\n    # Step 2: Set up headers for API calls\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/json\"\n    }\n\n    # Step 3: Get EO product details (including S3 path)\n    eo_product_id, s3_path = get_eo_product_details(config, headers, args.eo_product_name)\n    bucket_name, base_s3_path = s3_path.lstrip('/').split('/', 1)\n\n    # Step 4: Get temporary S3 credentials\n    s3_credentials = get_temporary_s3_credentials(headers)\n\n    # Step 5: Set up S3 client and resource with temporary credentials\n    time.sleep(5)  # Ensure the key pair is installed\n    s3_resource = boto3.resource('s3',\n                                 endpoint_url=config[\"s3_endpoint_url\"],\n                                 aws_access_key_id=s3_credentials[\"access_id\"],\n                                 aws_secret_access_key=s3_credentials[\"secret\"])\n\n    # Step 6: Create the top-level folder and start download\n    top_level_folder = args.eo_product_name\n    os.makedirs(top_level_folder, exist_ok=True)\n    failed_downloads = []\n    traverse_and_download_s3(s3_resource, bucket_name, base_s3_path, top_level_folder, failed_downloads)\n\n    # Step 7: Print final status\n    if not failed_downloads:\n        print(\"Product download complete.\")\n    else:\n        print(\"Product download incomplete:\")\n        for failed_file in failed_downloads:\n            print(f\"- {failed_file}\")\n\n    # Step 7: Delete the temporary S3 credentials\n    delete_response = requests.delete(f\"https://s3-keys-manager.cloudferro.com/api/user/credentials/access_id/{s3_credentials['access_id']}\", headers=headers)\n    if delete_response.status_code == 204:\n        print(\"Temporary S3 credentials deleted successfully.\")\n    else:\n        print(f\"Failed to delete temporary S3 credentials. Status code: {delete_response.status_code}\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "APIs/S3.html#accessing-eodata-via-aws-cli",
    "href": "APIs/S3.html#accessing-eodata-via-aws-cli",
    "title": "Access to EO data via S3",
    "section": "Accessing EOData via AWS CLI",
    "text": "Accessing EOData via AWS CLI\nIn this section, you will access EOData resources using a Linux OS using AWS Command Line Interface (AWS CLI).\n\nPrerequirements\nCheck for system updates:\n$ sudo apt-get update -y\nInstall prerequirements:\n$ sudo apt-get install unzip groff less -y\nGenerate S3 credentials at CDSE:\nIn order to access EOData via S3 you must obtain secrets (access and secret key). Please visit following page and log in with Copernicus Data Space Ecosystem account for which the keys are to be generated.\nIf you dont have an account, you can register via following link.\n\n\nInstall or update the AWS CLI\nTo update your current installation of AWS CLI, download a new installer each time you update to overwrite previous versions. Follow these steps from the command line to install the AWS CLI on Linux.\n$ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nVerify if client is installed:\n$ aws --version\naws-cli/2.13.32 Python/3.11.6 Linux/5.15.0-75-generic exe/x86_64.ubuntu.22 prompt/off\n\n\nConfiguration\nEnter the following command:\n$ aws configure\nYou will be prompted for providing certain connection details. Below are the values which you should use during the configuration. Replace access_key and secret_key with appropriate keys you obtained previously.\nAWS Access Key ID [None]: &lt;access_key&gt;\nAWS Secret Key ID [None]: &lt;secret_key&gt;\nDefault region name [None]: default\nDefault output format [None]: text\n\n\nList and download products\nNow you can use AWS CLI commands (documentation available via following link: https://aws.amazon.com/cli/)\nSpecify the CDSE endpoint:\nUse following line to specify endpoint.\nexport AWS_ENDPOINT_URL=https://eodata.dataspace.copernicus.eu/\nList available buckets:\n$ aws s3 ls s3://eodata/\neouser@aws-cli:~$ aws s3 ls s3://eodata/\n                           PRE C3S/\n                           PRE CAMS/\n                           PRE CEMS/\n                           PRE CLMS/\n                           PRE CMEMS/\n                           PRE Envisat/\n                           PRE Envisat-ASAR/\n                           PRE Global-Mosaics/\n                           PRE Jason-3/\n                           PRE Landsat-5/\n                           PRE Landsat-7/\n                           PRE Landsat-8/\n                           PRE SMOS/\n                           PRE Sentinel-1/\n                           PRE Sentinel-1-COG/\n                           PRE Sentinel-1-RTC/\n                           PRE Sentinel-2/\n                           PRE Sentinel-3/\n                           PRE Sentinel-5P/\n                           PRE Sentinel-6/\n                           PRE auxdata/\nDownloading products:\nIn order to download products from repository follow the steps.\nBelow example is downloading product from Sentinel-2 named “S2A_MSIL2A_20240115T235221_N0510_R130_T55HGS_20240116T021554.SAFE” and saving it at user’s home location.\n$ aws s3 cp s3://eodata/Sentinel-2/MSI/L2A/2024/01/15/S2A_MSIL2A_20240115T235221_N0510_R130_T55HGS_20240116T021554.SAFE/ /home/eouser/S2A_MSIL2A_20240115T235221_N0510_R130_T55HGS_20240116T021554.SAFE\n\ndownload: s3://eodata/Sentinel-2/MSI/L2A/2024/01/15/S2A_MSIL2A_20240115T235221_N0510_R130_T55HGS_20240116T021554.SAFE/ to ./S2A_MSIL2A_20240115T235221_N0510_R130_T55HGS_20240116T021554.SAFE\nAfter this operation, you should downloaded product in specified path."
  },
  {
    "objectID": "APIs/openEO/authentication.html",
    "href": "APIs/openEO/authentication.html",
    "title": "Authentication in openEO",
    "section": "",
    "text": "While basic discovery of openEO collections and processes is possible without authentication, executing openEO workflows requires user authentication. This is necessary to manage user quotas, resources, and credit consumption effectively. User authentication in openEO is handled with the OpenID Connect protocol (often abbreviated as “OIDC”).\nThe openEO endpoint of the Copernicus Data Space Ecosystem is configured to use the identity provider service of the Copernicus Data Space Ecosystem. It is therefore recommended to complete the Copernicus Data Space Ecosystem registration before attempting to authenticate with openEO."
  },
  {
    "objectID": "APIs/openEO/authentication.html#sec-typical-authentication-flow",
    "href": "APIs/openEO/authentication.html#sec-typical-authentication-flow",
    "title": "Authentication in openEO",
    "section": "Authentication Essentials With The openEO Python Client",
    "text": "Authentication Essentials With The openEO Python Client\nA typical Copernicus Data Space Ecosystem openEO workflow, using the openEO Python client library, starts with setting up a connection like this:\nimport openeo\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\nconnection.authenticate_oidc()\nAfter connecting to the Copernicus Data Space Ecosystem openEO endpoint, Connection.authenticate_oidc() initiates the OpenID Connect authentication flow:\n\nBy default, the first time authenticate_oidc() is called, instructions to visit a certain URL will be printed, e.g.:\nVisit https://auth.example/?user_code=EAXD-RQXV to authenticate.\nVisit this URL (click or copy-paste it into a web browser) and follow the login flow using Copernicus Data Space Ecosystem credentials.\n\n\n\n\n\n\nTip\n\n\n\n\n\nVisit this URL using any preferred browser to complete the login procedure (e.g., on a laptop or smartphone). It does not need to be a browser on the same machine or network as the Python script/application.\n\n\n\nOnce authentication is complete, the Python script will receive the necessary authentication tokens and print\nAuthorized successfully.\nOther times, when valid (refresh) tokens are still present on the system, the openEO Python client library will automatically use these tokens. As no manual login interaction is involved here, the following will be printed immediately on successful authentication:\nAuthenticated using refresh token.\n\nIn any case, connection is now authenticated and ready to make download or processing requests."
  },
  {
    "objectID": "APIs/openEO/authentication.html#alternative-authentication-methods",
    "href": "APIs/openEO/authentication.html#alternative-authentication-methods",
    "title": "Authentication in openEO",
    "section": "Alternative Authentication Methods",
    "text": "Alternative Authentication Methods\nThe openEO Python client library documentation has a more in-depth information of various authentication concepts, and discusses alternative authentication methods."
  },
  {
    "objectID": "APIs/openEO/authentication.html#non-interactive-and-machine-to-machine-authentication",
    "href": "APIs/openEO/authentication.html#non-interactive-and-machine-to-machine-authentication",
    "title": "Authentication in openEO",
    "section": "Non-interactive And Machine-to-Machine Authentication",
    "text": "Non-interactive And Machine-to-Machine Authentication\nSection 1 describes the typical authentication flow for interactive use cases, e.g. when working in a Jupyter Notebook or manually running a script on a local machine. This section will discuss authentication approaches that are more fitting for non-interactive and machine-to-machine use cases.\n\n\n\n\n\n\nNote\n\n\n\nThe practical aspects will be based on the openEO Python client library, but the concepts are also generally applicable to other openEO client libraries.\n\n\n\nRefresh Tokens\nRefresh tokens are long-lived tokens (order of weeks or months) that can be used to obtain new access tokens without the need for the user to re-authenticate.\nAs mentioned in Section 1, Connection.authenticate_oidc() will require the user to go through an interactive login flow in a browser, when there is no (valid) refresh token available. But when the openEO Python client library can find a valid refresh token on their system, this will be a non-interactive operation. This makes it a viable option for non-interactive and machine-to-machine authentication, if it is feasible to produce a new refresh token once in a while using an interactive login flow.\nTo get this working, there are basically two aspects to cover (both of which have built-in support in the openEO Python client library):\n\nObtain and store a new refresh token.\nThere are several authentication methods on the Connection object (e.g. the often used authenticate_oidc() method) and most these have an option store_refresh_token to enable storing of the refresh token obtained during the authentication process. Note that this is enabled by default in authenticate_oidc(), but not in authenticate_oidc_device().\nThe refresh token is stored in a private JSON file, by default, in a folder within the user’s personal data directory (typically determined by environment variables such as XDG_DATA_HOME on Linux or APPDATA on Windows). The folder can also be configured directly using the OPENEO_CONFIG_HOME environment variable. The actual location can be verified with the openeo-auth command line tool.\nLoad and use the refresh token\nWhen a valid refresh token is stored in a location accessible to the openEO Python client library, the user can authenticate directly with:\nconnection.authenticate_oidc_refresh_token()\nAlternatively:\n\nIf a user wants to keep their authentication logic generic, the following can also be used\nconnection.authenticate_oidc()\nThis method will first try to use a refresh token if available, and fall back on other methods (e.g. device code flow) otherwise.\nIf a user wants to manage their refresh token themselves, it can be passed explicitly:\nconnection.authenticate_oidc_refresh_token(\n    refresh_token=your_refresh_token,\n)\n\n\n\n\n\n\n\n\nAdvanced refresh token storage\n\n\n\nFor advanced use cases, it is also possible to override the file-based refresh token storage with a custom implementation through the refresh_token_store parameter of Connection.\n\n\n\n\nClient Credentials Flow\nOpenID Connect also provides the so-called “client credentials flow”, which is a non-interactive flow, based on a static client id and a client secret, making it suitable for machine-to-machine authentication.\nSee the documentation page on client credentials for more information on client credentials usage in the context of the Copernicus Data Space Ecosystem, how to obtain them, pitfalls, and other considerations."
  },
  {
    "objectID": "APIs/openEO/File_formats.html",
    "href": "APIs/openEO/File_formats.html",
    "title": "File Formats",
    "section": "",
    "text": "File formats and their options are often very different between processing software. To improve interoperability, the openEO API supports the following file formats currently for saving your results."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html",
    "title": "Getting started with JavaScript client",
    "section": "",
    "text": "This Getting Started guide will give you just a simple overview of the capabilities of the openEO JavaScript client library."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#installation",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#installation",
    "title": "Getting started with JavaScript client",
    "section": "Installation",
    "text": "Installation\nThe openEO JavaScript Client can be used in all modern browsers (excludes Internet Explorer) and all maintained Node.js versions (&gt;= 10.x). It can also been used for mobile app development with the Ionic Framework, for example.\nThe easiest way to try out the client is using one of the examples. Alternatively, you can create an HTML file and include the client with the following HTML script tags:\n&lt;script src=\"https://cdn.jsdelivr.net/npm/axios@0.21/dist/axios.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/@openeo/js-client@2/openeo.min.js\"&gt;&lt;/script&gt;\nThis gives you a minified version for production environments. If you’d like a better development experience, use the following code:\n&lt;script src=\"https://cdn.jsdelivr.net/npm/axios@0.21/dist/axios.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/@openeo/js-client@2/openeo.js\"&gt;&lt;/script&gt;\nIf you are working on a Node.js application or you are using a Node.js-based build tool for web development (e.g. Webpack), you can install the client via npm by using the following command:\n\nnpm install @openeo/js-client\nAfterwards you can load the library. Depending on whether you are directly working in Node.js or are just using a Node.js build tool, the import can be different. Please inform yourself which import is suited for your project.\nThis is usually used directly in Node.js:\nconst { OpenEO } = require('@openeo/js-client');\nThis may be used in build tools such as Webpack:\nimport { OpenEO } from '@openeo/js-client';\nNow that the installation was successfully finished, we can now connect to openEO compliant back-ends. In the following chapters we quickly walk through the main features of the JavaScript client.\nIf you have trouble installing the client, feel free to create a ticket or leave an issue at the GitHub project."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#exploring-a-back-end",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#exploring-a-back-end",
    "title": "Getting started with JavaScript client",
    "section": "Exploring a back-end",
    "text": "Exploring a back-end\nFor this tutorial we will use the openEO instance of Copernicus Data Space Ecosystem, which is available at https://openeo.dataspace.copernicus.eu.\nFirst we need to establish a connection to the back-end.\nvar con = await OpenEO.connect(\"https://openeo.dataspace.copernicus.eu\");\n\n\n\n\n\n\nNote\n\n\n\nThe JavaScript client uses Promises (async/await). So there are two ways to express the code above:\nPromises:\nOpenEO.connect(\"https://openeo.dataspace.copernicus.eu\").then(function(con) {\n  // Success\n}).catch(function(error) {\n  // Error\n});\nasync/await:\ntry {\n  var con = await OpenEO.connect(\"https://openeo.dataspace.copernicus.eu\");\n  // Success\n} catch (error) {\n  // Error\n}\n\n\nTo simplify the code here, we use async/await in all examples and don’t catch errors. So we assume you run the code in an async function and also in a try/catch block.\nAfter establishing the connection to the back-end, it can be explored using the Connection object returned. The basic service’s metadata (capabilities) can be accessed via\nvar info = con.capabilities();\nThis allows to request a couple of different information, like API version, description, related links or the billing plans. You can print some of these information to the console as follows:\nconsole.log(\"API Version: \", info.apiVersion());\nconsole.log(\"Description: \", info.description());\n\nconsole.log(\"Billing plans:\");\ninfo.listPlans().forEach(plan =&gt; {\n  console.log(`${plan.name}: ${plan.url}`);\n});\n\nconsole.log(\"Related links:\");\ninfo.links().forEach(link =&gt; {\n  console.log(`${link.title}: ${link.href}`);\n});\n\nCollections\nCollections represent the basic data the back-end provides (e.g. Sentinel 2 collection). Collections are used as input data for job executions (more info on collections). With the following code snippet you can print all 400+ available collection names and their summary.\nconsole.log(\"Available Collections:\");\nvar response = await con.listCollections();\nresponse.collections.forEach(collection =&gt; {\n  console.log(`${collection.id}: ${collection.summary}`);\n});\nTo get detailed information about a single collection, you can pass any of the collection IDs requested earlier to describeCollection and get a full object of STAC compliant Collection metadata back. In this example we request information about the Sentinel-2 Level 1C data from Google:\nconsole.log(await con.describeCollection(\"COPERNICUS/S2\"));\nTo get the full set of metadata you should always use describeCollection.\n\n\nProcesses\nProcesses in openEO are small tasks that can be applied on (EO) data. The input of a process might be the output of another process, so that several connected processes form a new (user-defined) process itself. Therefore, a process resembles the smallest unit of task descriptions in openEO (more details on processes). With the following code snippet you can print all available process IDs and their summaries.\nconsole.log(\"Available Collections:\");\nvar response = await con.listProcesses();\nresponse.processes.forEach(process =&gt; {\n  console.log(`${process.id}: ${process.summary}`);\n});\nIn contrast to the collections, the process descriptions returned by listProcesses are complete. There’s no need to call describeProcess to get the full set of metadata. describeProcess is just a convenience function to get a single process from listProcesses. In this example we request the process specification for the apply process:\nconsole.log(await con.describeProcess(\"apply\"));\nFor a graphical overview of the openEO processes, there is an online documentation for general process descriptions and the openEO Hub for back-end specific process descriptions."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#authentication",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#authentication",
    "title": "Getting started with JavaScript client",
    "section": "Authentication",
    "text": "Authentication\nIn the code snippets above we did not need to log in since we just queried publicly available back-end information. However, to run non-trivial processing queries one has to authenticate so that permissions, resource usage, etc. can be managed properly.\nTo authenticate your account on the backend of the Copernicus Data Space Ecosystem, it is necessary for you to complete the registration process. Once registered, the OIDC (OpenID Connect)authentication method will be employed to verify your identity using an external service.\n\n\n\n\n\n\nWarning\n\n\n\nIf you have included the library using HTML script tags, then you need to include the following OIDC client before the openEO client:\n&lt;script src=\"https://cdn.jsdelivr.net/npm/oidc-client@1/lib/oidc-client.min.js\"&gt;&lt;/script&gt;\nNo further action is required, if you have installed the client via npm.\n\n\nAs OpenID Connect authentication is a bit more complex and depends on the environment your are using it in, please refer to the JavaScript client documentation for more information.\nCalling this method opens your system web browser, with which you can authenticate yourself on the back-end authentication system. After that the website will give you the instructions to go back to the JavaScript client, where your connection has logged your account in. This means that every call that comes after that via the con variable is executed by your user account."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#creating-a-user-defined-process",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#creating-a-user-defined-process",
    "title": "Getting started with JavaScript client",
    "section": "Creating a (user-defined) process",
    "text": "Creating a (user-defined) process\nNow that we know how to discover the back-end and how to authenticate, lets continue by creating a new batch job to process some data. First we need to create a user-defined process and for that a process builder is the easiest method.\n\nvar builder = await con.buildProcess();\nWith the builder, a datacube can be initialized by selecting a collection from the back-end with the process load_collection:\nvar datacube = builder.load_collection(\n        \"SENTINEL2_L2A\",\n        {west: 3.20, south: 51.19, east: 3.26, north: 51.21},\n        [\"2022-05-01\", \"2022-05-30\"],\n        [\"B04\", \"B03\", \"B02\"]\n);\nThis results in a datacube containing the “SENTINEL2_L2A” data restricted to the given spatial extent, the given temporal extend and the given bands .\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can also filter the datacube at a later stage by using the following filter methods:\ndatacube = builder.filter_bbox(datacube, {west: 3.20, south: 51.19, east: 3.26, north: 51.21});\ndatacube = builder.filter_temporal(datacube, [\"2022-05-01\", \"2022-05-30\"]);\ndatacube = builder.filter_bands(datacube, [\"B04\", \"B03\", \"B02\"]);\nStill, it is recommended to always use the filters in load_collection to avoid loading too much data upfront.\n\n\n\nHaving the input data ready, we want to apply a process on the datacube, which returns a datacube with the process applied:\n\nvar min = function(data) { return this.min(data); };\ndatacube = builder.reduce_dimension(datacube, min, \"t\");\nThe datacube is now reduced by the time dimension named t, by taking the minimum value of the timeseries values. Now the datacube has no time dimension left.\nOther so called “reducer” processes exist, e.g. for computing maximum and mean values.\n\n\n\n\n\n\nNote\n\n\n\nEverything applied to the datacube at this point is neither executed locally on your machine nor executed on the back-end. It just defines the input data and process chain the back-end needs to apply when it sends the datacube to the back-end and executes it there. How this can be done is the topic of the next chapter.\n\n\nAfter applying all processes you want to execute, we need to tell the back-end to export the datacube, for example as GeoTiff:\n\nvar result = builder.save_result(datacube, \"GTiff\");"
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#batch-job-management",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#batch-job-management",
    "title": "Getting started with JavaScript client",
    "section": "Batch Job Management",
    "text": "Batch Job Management\nAfter you finished working on your (user-defined) process, we can now send it to the back-end and start the execution. In openEO, an execution of a (user-defined) process (here defined using the process builder) is called a (batch) job. Therefore, we need to create a job at the back-end using our datacube, giving it the title Example Title.\n\nvar job = await con.createJob(result, \"Example Title\");\nThe createJob method sends all necessary information to the back-end and creates a new job, which gets returned. After this, the job is just created, but has not started the execution at the back-end yet. It needs to be queued for processing explicitly:\n\nawait job.startJob();\nNow the execution of the job can be monitored by requesting the job status and the log files every once in a while (30 seconds in this example):\n\nlet stopFn = job.monitorJob((job, logs) =&gt; {\n  console.log(job.status);\n  logs.forEach(log =&gt; console.log(`${log.level}: ${log.message}`));\n}, 30);\nThe monitoring stops automatically once the job has finished, was canceled or errored out. But with the return value of the monitorJob function, you can also stop monitoring the job manually:\n\nstopFn();\nWhen the job is finished, calling listResults gets you the URLs to the results.\n\nvar urls = await job.listResults();\n\n\n\n\n\n\nTip\n\n\n\n\n\nThis only works if the job execution has finished. We recommend to use listResults in combination with monitorJob, for example as follows:\n\nlet stopFn = job.monitorJob(async (job, logs) =&gt; {\n  if (job.status === \"finished\") {\n    var urls = await job.listResults();\n    urls.forEach(url =&gt; console.log(`Download result from: ${url.href}`));\n  }\n});\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s also the method downloadResults to download the results directly. Unfortunately, you can only download files from a Node.js environment where file access to your local drive is possible. In a Browser environment, it is also an option to download the STAC Item or Collection for the results using the getResultsAsStac method and point a STAC client to it for downloading.\n\n\nNow you know the general workflow of job executions."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#full-example",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#full-example",
    "title": "Getting started with JavaScript client",
    "section": "Full Example",
    "text": "Full Example\nIn this chapter we will show a full example of an earth observation use case using the JavaScript client in a Node.js environment. Instead of batch job processing, we compute the image synchronously. Synchronous processing means the result is directly returned in the response, which usually works only for smaller amounts of data.\nHere, we want to produce a monthly RGB composite of Sentinel 1 backscatter data over the area of Vienna, Austria for three months in 2017. This can be used for classification and crop monitoring.\nIn the following code example, we use inline code comments to describe what we are doing.\n// Make the client available to the Node.js script\n// Also include the Formula library for simple math expressions\nconst { OpenEO, Formula } = require('@openeo/js-client');\n\nasync function example() {\n  // Connect to the back-end\n  var con = await OpenEO.connect(\"https://openeo.dataspace.copernicus.eu\");\n  // Authenticate \n  await con.authenticateOIDC();\n  // Create a process builder\n  var builder = await con.buildProcess();\n  // We are now loading the Sentinel-1 data over the Area of Interest\n  var datacube = builder.load_collection(\n    \"SENTINEL1_GRD\",\n    {west: 16.06, south: 48.06, east: 16.65, north: 48.35},\n    [\"2017-03-01\", \"2017-06-01\"],\n    [\"VV\"]\n  );\n\n  // Since we are creating a monthly RGB composite, we need three separated time ranges (March aas R, April as G and May as G).\n  // Therefore, we split the datacube into three datacubes using a temporal filter.\n  var march = builder.filter_temporal(datacube, [\"2017-03-01\", \"2017-04-01\"]);\n  var april = builder.filter_temporal(datacube, [\"2017-04-01\", \"2017-05-01\"]);\n  var may = builder.filter_temporal(datacube, [\"2017-05-01\", \"2017-06-01\"]);\n\n  // We aggregate the timeseries values into a single image by reducing the time dimension using a mean reducer.\n  var mean = function(data) {\n    return this.mean(data);\n  };\n  march = builder.reduce_dimension(march, mean, \"t\");\n  april = builder.reduce_dimension(april, mean, \"t\");\n  may = builder.reduce_dimension(may, mean, \"t\");\n\n  // Now the three images will be combined into the temporal composite.\n  // We rename the bands to R, G and B as otherwise the bands are overlapping and the merge process would fail.\n  march = builder.rename_labels(march, \"bands\", [\"R\"], [\"VV\"]);\n  april = builder.rename_labels(april, \"bands\", [\"G\"], [\"VV\"]);\n  may = builder.rename_labels(may, \"bands\", [\"B\"], [\"VV\"]);\n\n  datacube = builder.merge_cubes(march, april);\n  datacube = builder.merge_cubes(datacube, may);\n\n  // To make the values match the RGB values from 0 to 255 in a PNG file, we need to scale them.\n  // We can simplify expressing math formulas using the openEO Formula parser.\n  datacube = builder.apply(datacube, new Formula(\"linear_scale_range(x, -20, -5, 0, 255)\"));\n\n  // Finally, save the result as PNG file.\n  // In the options we specify which band should be used for \"red\", \"green\" and \"blue\" color.\n  datacube = builder.save_result(datacube, \"PNG\", {\n    red: \"R\",\n    green: \"G\",\n    blue: \"B\"\n  });\n\n  // Now send the processing instructions to the back-end for (synchronous) execution and save the file as result.png\n  await con.downloadResult(datacube, \"../_images/result.png\");\n}\n\n// Run the example, write errors to the console.\nexample().catch(error =&gt; console.error(error));\nNow the resulting PNG file of the RGB backscatter composite is stored as result.png in the node.JS working directory and should look as follows:"
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#user-defined-functions",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#user-defined-functions",
    "title": "Getting started with JavaScript client",
    "section": "User Defined Functions",
    "text": "User Defined Functions\nIf your use case can not be accomplished with the default processes of openEO, you can define a user defined function. Unfortunately, you can only create Python and R functions at the moment. Therefore, this guide doesn’t get into detail. For more information check out the Python or R tutorials on UDFs."
  },
  {
    "objectID": "APIs/openEO/JavaScript_Client/JavaScript.html#useful-links",
    "href": "APIs/openEO/JavaScript_Client/JavaScript.html#useful-links",
    "title": "Getting started with JavaScript client",
    "section": "Useful links",
    "text": "Useful links\nAdditional information and resources about the openEO JavaScript Client Library:\n\nExamples\nDocumentation\nRepository"
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html",
    "href": "APIs/openEO/R_Client/R.html",
    "title": "Getting started with R client",
    "section": "",
    "text": "This Getting Started guide provides a simple overview of the capabilities offered by the openEO R client library."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#installation",
    "href": "APIs/openEO/R_Client/R.html#installation",
    "title": "Getting started with R client",
    "section": "Installation",
    "text": "Installation\nBefore installing the openEO R-client module, ensure to have atleast R of 3.6 version. Older versions might also work, but have not been tested.\nStable releases can be installed from CRAN:\ninstall.packages(\"openeo\")\n\n\n\n\n\n\nInstalling the development version\n\n\n\n\n\nTo install the development version, follow the GitHub installation instructions. This version might include additional features but could be less stable.\nEnsure ‘devtools’ is installed; if not, use install.packages(\"devtools\").\nNow, install_github from the devtools package can be used to install the development version:\n\ndevtools::install_github(repo=\"Open-EO/openeo-r-client\", dependencies=TRUE, ref=\"develop\")\nIf an error occurs, there may have been an issue during the installation process. Please review the requirements to troubleshoot the problem.\n\n\n\nIf problems are encountered when installing, create a ticket, post in forum or raise an issue on the GitHub project page.\nWith the successful completion of the installation, import it to use openEO-compliant back-ends. The following section provides a simple overview of the R client."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#exploring-a-back-end",
    "href": "APIs/openEO/R_Client/R.html#exploring-a-back-end",
    "title": "Getting started with R client",
    "section": "Exploring a back-end",
    "text": "Exploring a back-end\nFirst, establish a connection to the Copernicus Data Space Ecosystem backend. The endpoint is https://openeo.dataspace.copernicus.eu.\n\nlibrary(openeo)\nconnection = connect(host = \"https://openeo.dataspace.copernicus.eu\")\n\nCollections\nCollections are the data offered by the backend (e.g., Sentinel 1 collection) for further processing and analysis. For additional information on collections, please refer to the glossary entry here.\nThe code snippet below lists available collection names and their descriptions. To use a specific collection in a workflow, its Collection ID can access it. For a more detailed overview of any collection, utilize the describe_collection function.\n\n# Dictionary of the full metadata of the \"COPERNICUS/S2\" collection (dict)\ndescribe_collection(\"SENTINEL2_L2A\")\nGenerally, all metadata objects are structured as lists. Users can use str() to examine the structure of the list and access fields using the $ operator.\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhen using RStudio, metadata can be readily displayed as a web page in the viewer panel by executing collection_viewer(x=\"SENTINEL2_L2A\").\n\n\n\n\n\nProcesses\nProcesses in openEO are operations applied to (EO) data (see here), such as calculating the mean, pixel masking, spatial aggregations, etc. Furthermore, a process’s output can be another’s input in a workflow.\n# List of available openEO processes with full metadata\nprocesses = list_processes()\nprint(processes)\n\n# print metadata of the process with ID \"load_collection\"\nprint(processes$load_collection)\nThe list_processes() used above returns a list of available processes. Each process includes a unique identifier and additional metadata, such as expected arguments and return types.\n\n\n\n\n\n\nTip\n\n\n\n\n\nLike collections, processes can also be displayed as a web page in the viewer panel when using RStudio. To open the viewer, use process_viewer() with either a specific process (e.g., process_viewer(\"load_collection\")) or all processes (process_viewer(processes))."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#authentication",
    "href": "APIs/openEO/R_Client/R.html#authentication",
    "title": "Getting started with R client",
    "section": "Authentication",
    "text": "Authentication\nBasic metadata about collection and processes, as discussed above is publicly available and does not require being logged in. However, for downloading EO data or running processing workflows, it is necessary to authenticate so that permissions, resource usage, etc. can be managed properly.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to complete Copernicus Data Space Ecosystem registration before authentication.\n\n\nOnce registered, user can authenticate as shown below:\n\nlogin()\nCalling this method opens the system’s web browser for user authentication. Upon successful authentication, a message confirming the login success will appear on the R console."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#creating-a-datacube",
    "href": "APIs/openEO/R_Client/R.html#creating-a-datacube",
    "title": "Getting started with R client",
    "section": "Creating a datacube",
    "text": "Creating a datacube\nNow, let us create, fetch and process EO data using openEO. This involves working with “Datacubes,” which are fundamental in openEO for representing EO data. For more information on datacubes, refer to the glossary here.\nFirst we need to create a process builder object that carries all the available predefined openEO processes of the connected back-end as attached R functions with the parameters stated in the process metadata.\n\np = processes()\nA process builder is initialised. This object encapsulates all available predefined openEO processes from the CDSE backend as R functions, each with parameters defined in the process metadata.\ndatacube = p$load_collection(\n  id = \"SENTINEL1_GRD\",\n  spatial_extent=list(west = 16.06, south = 48.06, east = 16.65, north = 48.35),\n  temporal_extent=c(\"2017-03-01\", \"2017-04-01\"),\n  bands=c(\"VV\", \"VH\")\n)\nThis creates a process node representing a datacube with “SENTINEL1_GRD” data for the specified spatial extent, temporal extent, and bands.\n\n\n\n\n\n\nSample Data Retrieval\n\n\n\n\n\nIt is useful to check the actual data occasionally to understand the processing mechanisms and data structures used in the openEO Platform. The function get_sample helps in downloading data for a small spatial extent. This data is automatically loaded into R, for direct inspection with stars.\n\n\n\nA process can now be applied to the input datacube.\n\n# apply sar backscatter \n\ndatacube_sar = p$sar_backscatter(data = datacube, coefficient = \"sigma0-ellipsoid\")\n\nmin_reducer = function(data,context) { \n  return(p$min(data = data))\n}\n\nreduced = p$reduce_dimension(data = datacube_sar, reducer = min_reducer, dimension=\"t\")\nA SAR backscatter process is applied to the input datacube. The datacube is then reduced by the time dimension using min_reducer function.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that all processes applied so far have not yet been run locally or on the backend. An abstract representation of the algorithm (input data and processing chain) has been created and encapsulated in a local reduced object. To run the workflow, it must be explicitly sent to the backend.\n\n\nAfter applying all processes you want to execute, we need to tell the back-end to export the datacube, for example as GeoTiff:\n\nformats = list_file_formats()\n\nresult = p$save_result(data = reduced, format = formats$output$`GTIFF-ZIP`)\nThe initial line in the above snippet fetches the supported format backends. The following line creates the result node, which stores the data as a zipped GeoTiff."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#batch-job-management",
    "href": "APIs/openEO/R_Client/R.html#batch-job-management",
    "title": "Getting started with R client",
    "section": "Batch Job Management",
    "text": "Batch Job Management\nSimple openEO usage examples demonstrate synchronous downloading of results. This approach is preferred to a smaller extent or for a simple workflow. However, batch jobs are necessary for more demanding tasks, such as processing larger extents or intensive computations. For more information on these approaches visit here.\n# create a job at the back-end using our datacube, giving it the title `Example Title`\n\njob = create_job(graph=result,title = \"Example Title\")\nThe create_job method sends all required information to the backend and registers a new job. However, the job is created at this stage and to start, it must be explicitly queued for processing:\n\nstart_job(job = job)\nThe status updates can be obtained using the list_jobs() function. This function provides a list of jobs submitted to the backend. However, it is important to note that only list_jobs() refreshes this list. Therefore, to monitor a job, you need to iteratively call either describe_job() or update the job list using list_jobs().\n\njobs = list_jobs()\njobs # printed as a tibble or data.frame, but the object is a list\n\n# or use the job id (in this example 'cZ2ND0Z5nhBFNQFq') as index to get a particular job overview\njobs$cZ2ND0Z5nhBFNQFq\n\n# alternatively request detailed information about the job\ndescribe_job(job = job)\nOnce completed, download_results() allows the result to be retrieved. Alternatively, list_results() provides an overview of the created files, including download links and encountered error messages.\n\n# list the processed results\nlist_results(job = job)\n\n# download all the files into a folder on the file system\ndownload_results(job = job, folder = \"/some/folder/on/filesystem\")\nThis summarizes the complete openEO workflow using the R-client."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#full-example",
    "href": "APIs/openEO/R_Client/R.html#full-example",
    "title": "Getting started with R client",
    "section": "Full Example",
    "text": "Full Example\nTherefore, a complete example of an earth observation use case using the R client is outlined in this chapter.\nThe objective is to get a monthly RGB composite of Sentinel 1 backscatter data across Vienna, Austria for three months in 2017. This composite can support tasks like classification and crop monitoring.\nThe code example below includes inline comments explaining each step of the process:\nlibrary(openeo)\n\n# connect to the backend and authenticate\nconnection = connect(host = \"https://openeo.dataspace.copernicus.eu\")\nlogin()\n# get the process collection to use the predefined processes of the back-end\np = processes()\n\n# get the collection list to get easier access to the collection ids, via auto completion\ncollections = list_collections()\n\n# get the formats\nformats = list_file_formats()\n\n# load the initial data collection and limit the amount of data loaded\n# note: for the collection id and later the format you can also use the its character value\ndata = p$load_collection(id = collections$`SENTINEL1_GRD`,\n                         spatial_extent = list(west=16.06, \n                                               south=48.06,\n                                               east=16.65,\n                                               north=48.35),\n                         temporal_extent = c(\"2017-03-01\", \"2017-06-01\"),\n                         bands = c(\"VV\"))\n\n# apply the SAR backscatter process to the data\ndatacube_sar = p$sar_backscatter(data = data, coefficient = \"sigma0-ellipsoid\")\n\n\n# create three monthly sub-datasets, which will later be combined back into a single data cube\nmarch = p$filter_temporal(data = datacube_sar,\n                          extent = c(\"2017-03-01\", \"2017-04-01\"))\n\napril = p$filter_temporal(data = datacube_sar,\n                          extent = c(\"2017-04-01\", \"2017-05-01\"))\n\nmay = p$filter_temporal(data = datacube_sar,\n                        extent = c(\"2017-05-01\", \"2017-06-01\"))\n\n# The aggregation function for the following temporal reducer\nagg_fun_mean = function(data, context) {\n  mean(data)\n}\n\nmarch_reduced = p$reduce_dimension(data = march,\n                                   reducer = agg_fun_mean,\n                                   dimension = \"t\")\n\napril_reduced = p$reduce_dimension(data = april,\n                                   reducer = agg_fun_mean,\n                                   dimension = \"t\")\n\nmay_reduced = p$reduce_dimension(data = may,\n                                 reducer = agg_fun_mean,\n                                 dimension = \"t\")\n\n# Each band is currently called VV. We need to rename at least the label of one dimension, \n# because otherwise identity of the data cubes is assumed. The bands dimension consists \n# only of one label, so we can rename this to be able to merge those data cubes.\nmarch_renamed = p$rename_labels(data = march_reduced,\n                                dimension = \"bands\",\n                                target = c(\"R\"),\n                                source = c(\"VV\"))\n\napril_renamed = p$rename_labels(data = april_reduced,\n                                dimension = \"bands\",\n                                target = c(\"G\"),\n                                source = c(\"VV\"))\n\nmay_renamed = p$rename_labels(data = may_reduced,\n                              dimension = \"bands\",\n                              target = c(\"B\"),\n                              source = c(\"VV\"))\n\n# combine the individual data cubes into one\n# this is done one by one, since the dimensionalities have to match between each of the data cubes\nmerge_1 = p$merge_cubes(cube1 = march_renamed,cube2 = april_renamed)\nmerge_2 = p$merge_cubes(cube1 = merge_1, cube2 = may_renamed)\n\n# rescale the the back scatter measurements into 8Bit integer to view the results as PNG\nrescaled = p$apply(data = merge_2,\n        process = function(data,context) {\n          p$linear_scale_range(x=data, inputMin = -20,inputMax = -5, outputMin = 0, outputMax = 255)\n        })\n\n\n# store the results using the png format and set the create a job options\nresult = p$save_result(data = rescaled,format = formats$output$PNG, options = list(red=\"R\",green=\"G\",blue=\"B\"))\n\n# create a job\njob = create_job(graph = result, title = \"S1 Example R\", description = \"Getting Started example on openeo.org for R-client\")\n\n# then start the processing of the job and turn on logging (messages that are captured on the back-end during the process execution)\nstart_job(job = job, log = TRUE)\nThe resulting PNG file of the RGB backscatter composite is stored in the current working directory. Here is how it looks:"
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#user-defined-functions",
    "href": "APIs/openEO/R_Client/R.html#user-defined-functions",
    "title": "Getting started with R client",
    "section": "User Defined Functions",
    "text": "User Defined Functions\nIf a usecase cannot be defined using the default openEO processes, user can define them as a user defined function.\nThe workflow involves uploading a Python or R script to the user’s directory. Reference it by URL or name (e.g., /scripts/script1.R) in the run_udf function of openEO.\nFind out more about UDFs in the respective Python UDF and R UDF repositories with their documentation and examples."
  },
  {
    "objectID": "APIs/openEO/R_Client/R.html#useful-links",
    "href": "APIs/openEO/R_Client/R.html#useful-links",
    "title": "Getting started with R client",
    "section": "Useful links",
    "text": "Useful links\nAdditional information and resources about the openEO R Client Library:\n\nDocumentation\nVignettes\nCode Repository\nfor function documentation, use R’s ? function or see the online documentation"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html",
    "title": "Parcel delineation using Sentinel-2",
    "section": "",
    "text": "Authors:\nTuning:"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#introduction",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#introduction",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook we will be performing parcel delineation using Sentinel-2 data retrieved from- and processed on openEO. The models are generated using a U-Net and are pretrained. So in this notebook, we are dealing with the inference part of training a model. We will however also show how you can retrieve features from openEO, so that you know how the entire workflow looks like.\n\nfrom pathlib import Path\nimport json\nimport openeo\nfrom openeo import processes as eop\nfrom shapely.geometry import shape, box\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport xarray as xr\nfrom numpy import datetime_as_string\n\n\nopeneo.client_version()\n\n'0.27.0'\n\n\n\n## Output folder\nbase_path = Path(\"results\")"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#authentication",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#authentication",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Authentication",
    "text": "Authentication\nWe first need to connect to an openEO provider. Most providers require you to register an account, and provide you with a basic amount of processing credits. In this notebook we will use the Copernicus Data Scpace Ecosystem as openEO provider.\nAll the known openEO providers and their services: https://hub.openeo.org/\nMore info on authentication: https://open-eo.github.io/openeo-python-client/auth.html\nRun the authenticate_oidc() method again in case you can no longer connect. The token expires after a certain time.\n\nbackend_url = \"openeo.dataspace.copernicus.eu/\"\n\neoconn = openeo.connect(backend_url)\neoconn.authenticate_oidc()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#load-collection",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#load-collection",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Load collection",
    "text": "Load collection\nWe start off by loading in a collection. In this case, we are loading in Sentinel-2 L2A. More information on the collections available can be displayed with eoconn.list_collections(). Use eoconn.describe_collection(\"SENTINEL2_L2A\") for example to get the description of a specific collection.\nMore information on finding and loading data: https://open-eo.github.io/openeo-python-client/data_access.html\n\nbbox = [5.0, 51.2, 5.1, 51.3]\nyear = 2021\n\nstartdate = f\"{year}-01-01\"\nenddate = f\"{year}-09-30\"\n\n\ns2_bands = eoconn.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[startdate, enddate],\n    spatial_extent=dict(zip([\"west\", \"south\", \"east\", \"north\"], bbox)),\n    bands=[\"B04\", \"B08\", \"SCL\"],\n    max_cloud_cover=20,\n)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#select-usable-observations",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#select-usable-observations",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Select usable observations",
    "text": "Select usable observations\nThis pipeline will use 12 NDVI tiles in total. Depending on the time range selected, it is likely that your temporal dimension contains a lot more than 12 NDVI tiles. Therefore, we apply a selection procedure that selects the tiles with the highest number of clear pixels.\n\nnb_of_timesteps = 12\n\n\n## We use the SCL band to create a mask where clouded pixels are set to 1\n## and other pixels are set to 0\nscl_band = s2_bands.band(\"SCL\")\ns2_cloudmask = ( (scl_band == 8) | (scl_band == 9) | (scl_band == 3) ) * 1.0\n\n## Reduce the spacial dimension using the reducer \"mean\" to calculate the average cloud coverage\n#TODO: replace aggregate_spacial with reduce_spacial in a future openeo client version\nbbox_poly = box(*bbox)\navg_cloudmask = s2_cloudmask\\\n    .aggregate_spatial(geometries=bbox_poly, reducer=\"mean\") \n\n## Download the result for local sorting\navg_cloudmask.download(base_path / \"avg_cloudmask.nc\", format=\"NetCDF\")\n\n\n## Open the calculated cloudmask aggregation\navg_array = xr.open_dataset(base_path / \"avg_cloudmask.nc\")\n\n## Sort the timesteps by their cloud coverage and select the best ones\nbest_timesteps_dt64 = avg_array.squeeze(\"feature\")\\\n    .sortby(\"band_0\",ascending=True)\\\n    .coords[\"t\"].values[:nb_of_timesteps]\n\n## Close the dataset\navg_array.close()\n\n## Convert the timestep labels to iso format\nbest_timesteps = [datetime_as_string(t, unit=\"s\", timezone=\"UTC\") for t in best_timesteps_dt64]\n\n\n## Create a condition that checks if a date is one of the best timesteps\ncondition = lambda x : eop.any(\n    [\n        eop.date_between(\n            x = x,\n            min = timestep,\n            max = eop.date_shift(date=timestep, value=1, unit='day')) \n        for timestep in best_timesteps\n    ]\n)\n## Filter the bands using the condition\ns2_bands_reduced = s2_bands.filter_labels(\n    condition = condition,\n    dimension = \"t\"\n)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#calculate-ndvi",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#calculate-ndvi",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Calculate NDVI",
    "text": "Calculate NDVI\nThe delineation will be estimated based on the NDVI. The ndvi process can be used for these calculations.\n\nndviband = s2_bands_reduced.ndvi(red=\"B04\", nir=\"B08\")\n\nNote that the openEO Python clients generates an openEO process graph, which is then sent to one of the backends selected by the platform based on the operations that you are using. This process graph is then executed on selected platform. However, if you want to inspect intermediate results, you can, for example using ndviband.download(...) like below. Executing this line will allow you to inspect the NDVI images.\n\nndviband.download(base_path / \"ndvi.nc\")\n\nThe intermediate result (here only 3 of them are ploted) of the NDVI looks as shown below:\n\n## Load your dataset\nndvi_data = xr.open_dataset(base_path / \"ndvi.nc\")\n\n## Access the \"var\" variable\nvar = ndvi_data[\"var\"]\n\n## Select the top 3 time steps\nthree_steps = var[\"t\"][-3:]\n\n## Create a 1x3 horizontal plot for the top 3 time steps\nplt.figure(figsize=(16, 10))  # Adjust the figure size as needed\nfor i, t in enumerate(three_steps):\n    plt.subplot(1, 3, i + 1)\n    data_slice = var.sel(t=t)\n    dt = t.values.astype(\"M8[D]\").astype(\"O\")  # Convert to Python datetime object\n    formatted_date = dt.strftime(\"%Y-%m-%d\")  # Format the date\n    plt.imshow(data_slice, cmap=\"viridis\", origin=\"lower\", vmin=0, vmax=1)\n    plt.title(f\"Date: {formatted_date}\")\n    # Hide both horizontal and vertical ticks and labels\n    plt.xticks([])\n    plt.yticks([])\nplt.tight_layout()  # Ensures proper spacing between subplots\nplt.show()\n\n## Close the dataset\nndvi_data.close()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#apply-a-neural-network",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#apply-a-neural-network",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Apply a neural network",
    "text": "Apply a neural network\nWe now apply a neural network, that requires 128x128 pixel ‘chunks’ input. To avoid discontinuities between neighboring chunks, we work with an overlap of 32 pixels in all directions. The U-Net itself is trained on an input size of (x,y,1), that is, just one grayscale channel as an input, which is just one NDVI tile. However, we are using 12 NDVI images as an input (the 12 images selected in the previous step). That is because we are doing inference using 3 different models, 4 times per model. The final prediction is then the median of all predictions per pixel.\nAs you may know, a U-Net just like any other CNN applies a filter over an image. This can be done using apply_neighborhood, an openEO process defined here: https://openeo.org/documentation/1.0/processes.html#apply_neighborhood .\nFurther preprocessing of the data and the inference logic of the models is coded in an UDF. UDF’s are used to implement any custom code. It can therefore be used to quickly transfer code that you already wrote outside of openEO, or it can be used to implement features that are not present in openEO yet. It does however come at a cost of being slower than using openEO functionalities, due to optimization reasons. UDF’s are explained here: https://open-eo.github.io/openeo-python-client/udf.html\nThe pretrained models are converted to onnx models to allow for interoperability between different AI tools. More information on onnx and how to conveert your models from a specific framework can be found on their website. The onnxruntime package and other dependencies, used in udf_segmentation.py, have to be passed to the backend. This is done by passing a dependency archive as a job option, alongside the models.\n\ndependencies_url = \"https://artifactory.vgt.vito.be:443/auxdata-public/openeo/onnx_dependencies.zip\"\nmodels_url = \"https://artifactory.vgt.vito.be:443/artifactory/auxdata-public/openeo/parcelDelination/BelgiumCropMap_unet_3BandsGenerator_Models.zip\"\njob_options = {\n    \"udf-dependency-archives\": [\n        f\"{dependencies_url}#onnx_deps\",\n        f\"{models_url}#onnx_models\",\n    ]\n}\n\n\n## Apply the segmentation UDF using `apply_neighborhood`\n## An overlap of 32px is used, resulting in a 128x128 pixel input\nsegmentationband = ndviband.apply_neighborhood(\n    process=openeo.UDF.from_file(\"udf_segmentation.py\"),\n    size=[\n        {\"dimension\": \"x\", \"value\": 64, \"unit\": \"px\"},\n        {\"dimension\": \"y\", \"value\": 64, \"unit\": \"px\"},\n    ],\n    overlap=[\n        {\"dimension\": \"x\", \"value\": 32, \"unit\": \"px\"},\n        {\"dimension\": \"y\", \"value\": 32, \"unit\": \"px\"},\n    ],\n)\n\n\nsegmentation_job = segmentationband.create_job(\n    title=\"segmentation_onnx_job\", \n    out_format=\"NetCDF\", \n    job_options=job_options\n)\nsegmentation_job.start_and_wait()\nsegmentation_job.download_result(base_path / \"delineation.nc\")\n\n0:00:00 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': send 'start'\n0:00:24 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:00:29 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:00:39 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:00:47 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:00:57 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:01:10 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:01:26 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:01:46 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:02:12 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': queued (progress N/A)\n0:02:42 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:03:22 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:04:10 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:05:09 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:06:10 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:07:10 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:08:11 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:09:12 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:10:13 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': running (progress N/A)\n0:11:15 Job 'vito-j-24011501398f497fbf13cf76a0a1bbfb': finished (progress N/A)\n\n\nWindowsPath('results/parcels/delineation.nc')\n\n\nThe result of the U-Net is a map with more clearly defined boundaries, however the result is not optimal. We will therefore post-process our U-Net result by applying non-ML filters.\n\n## Load your dataset\nds = xr.open_dataset(base_path / \"delineation.nc\")\n\n## Access the \"var\" variable\nvar = ds[\"var\"]\n\n## Plot the data\nvar.plot(figsize=(10, 14), cmap=\"gray\")  # Use a colormap that suits your data\nplt.title(\"Parcel Delineation\")\nplt.show()\n\n## Close the dataset\nds.close()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#segmentation-postprocessing",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#segmentation-postprocessing",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Segmentation postprocessing",
    "text": "Segmentation postprocessing\nWe postprocess the output from the neural network using a sobel filter and Felzenszwalb’s algorithm, which are then merged. This time, we work on larger chunks, to reduce the need for stitching the vector output.\n\n## Apply the sobel felzenszwalb UDF using `apply_neighborhood`\nsobel_felzenszwalb = segmentationband.apply_neighborhood(\n    process=openeo.UDF.from_file(\"udf_sobel_felzenszwalb.py\"),\n    size=[\n        {\"dimension\": \"x\", \"value\": 256, \"unit\": \"px\"},\n        {\"dimension\": \"y\", \"value\": 256, \"unit\": \"px\"},\n    ],\n    overlap=[\n        {\"dimension\": \"x\", \"value\": 0, \"unit\": \"px\"},\n        {\"dimension\": \"y\", \"value\": 0, \"unit\": \"px\"},\n    ],\n)\n\n\nsobel_felzenszwalb_job = sobel_felzenszwalb.create_job(\n    title=\"sobel_felzenszwalb\",\n    out_format=\"NetCDF\",\n    job_options=job_options\n)\nsobel_felzenszwalb_job.start_and_wait()\nsobel_felzenszwalb_job.download_result(base_path / \"delineation_filtered.nc\")\n\n0:00:00 Job 'j-240206f8717f419b96cb35e01323ff77': send 'start'\n0:00:15 Job 'j-240206f8717f419b96cb35e01323ff77': created (progress N/A)\n0:00:21 Job 'j-240206f8717f419b96cb35e01323ff77': created (progress N/A)\n0:00:27 Job 'j-240206f8717f419b96cb35e01323ff77': created (progress N/A)\n0:00:36 Job 'j-240206f8717f419b96cb35e01323ff77': created (progress N/A)\n0:00:46 Job 'j-240206f8717f419b96cb35e01323ff77': created (progress N/A)\n0:00:58 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:01:14 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:01:33 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:01:57 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:02:28 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:03:05 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:03:57 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:04:56 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:05:57 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:06:57 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:07:57 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:08:58 Job 'j-240206f8717f419b96cb35e01323ff77': running (progress N/A)\n0:09:59 Job 'j-240206f8717f419b96cb35e01323ff77': finished (progress N/A)\n\n\nWindowsPath('results/delineation_filtered.nc')\n\n\n\n## Load your dataset\nds = xr.open_dataset(base_path / \"delineation_filtered.nc\")\n\n## Access the \"var\" variable\nvar = ds[\"var\"]\n\n## Plot the data\nvar.plot(figsize=(10, 14), cmap=\"gray\")  # Use a colormap that suits your data\nplt.title(\"Parcel Delineation - filtered\")\nplt.show()\n\n## Close the dataset\nds.close()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#convert-to-vector",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#convert-to-vector",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Convert to vector",
    "text": "Convert to vector\nWe are now ready to convert the raster image to vector. In openEO, we have an integrated process to do so, called raster_to_vector. You can read more about this method, or about other methods of interest, in our documentation: https://docs.openeo.cloud/processes/#raster_to_vector Note: This process is still in development, so results may not be perfect.\n\n## Convert the raster to a vector\nvectorization = sobel_felzenszwalb.raster_to_vector()\n\n\nvectorization_job = vectorization.create_job(\n    title=\"vectorization\",\n    out_format=\"json\",\n    job_options=job_options\n)\nvectorization_job.start_and_wait()\nvectorization_job.download_result(base_path / \"parcels.json\")\n\n0:00:00 Job 'j-240206b71829447fb44fe4b18bfce0eb': send 'start'\n0:00:16 Job 'j-240206b71829447fb44fe4b18bfce0eb': created (progress N/A)\n0:00:21 Job 'j-240206b71829447fb44fe4b18bfce0eb': created (progress N/A)\n0:00:29 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:00:37 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:00:47 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:00:59 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:01:15 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:01:34 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:01:59 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:02:29 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:03:07 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:03:54 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:04:53 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:05:54 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:06:54 Job 'j-240206b71829447fb44fe4b18bfce0eb': queued (progress N/A)\n0:07:55 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:08:55 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:09:56 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:10:56 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:11:57 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:13:17 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:14:17 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:15:18 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:16:18 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:17:18 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:18:19 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:19:20 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:20:20 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:21:21 Job 'j-240206b71829447fb44fe4b18bfce0eb': running (progress N/A)\n0:22:25 Job 'j-240206b71829447fb44fe4b18bfce0eb': finished (progress N/A)\n\n\nWindowsPath('results/parcels.json')\n\n\n\n## Load the vector data as a GeoDataFrame\nwith open(base_path / \"parcels.json\") as f:\n    polygons = json.load(f)\ngeom = [shape(p) for p in polygons]\nparcels_gdf = gpd.GeoDataFrame(geometry=geom, crs=\"EPSG:32631\")\n\n## Save the GeoDataFrame to a file\nparcels_gdf.to_file(str(base_path / \"parcels.gpkg\"), layer=\"parcels\", driver=\"GPKG\")\n\n## Plot the data\nparcels_gdf.plot(cmap=\"OrRd\", figsize=(10, 16), edgecolor=\"black\")\nplt.title(\"Vector Parcels\")\nplt.show()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#sampling",
    "href": "APIs/openEO/openeo-community-examples/python/ParcelDelineation/Parcel delineation.html#sampling",
    "title": "Parcel delineation using Sentinel-2",
    "section": "Sampling",
    "text": "Sampling\nIf you are training your own network, openEO can be used to get training data. In general, a workflow that is often used by researchers using our platform is: * data access, data preparation, feature engineering and sampling: openEO * model training: outside of openEO (e.g. on GPU’s) * model inference: in openEO using UDF’s\nIn this notebook we are only doing inference. But to show you how to sample data, this is a small section on how that would work. When you are training a CNN, you will likely already have a set of images (e.g. for this use case, a number of delineated fields at a certain date, with a certain spatial extent). Sampling would therefore consist of: * Loading in a collection, like we did before * Calculating whatever index, raw band, custom calculation, or collection of one of the aforementioned you need for your use case * Applying openEO processes filter_spatial and filter_temporal on your image corresponding to the labeled images you already have * Downloading the results using execute_batch() (batch processing), or download() (synchronous processing, if your images are fairly small)\nIf you are using a recurrent net or another ML model where your input is flat (for example, a random forest, a booster, an SVM, …) and you are sampling points rather than entire images, you can use filter_spatial to filter your feature cube to the points for which you actually have sampling data, and the option sample_by_feature=True to store them as a separate record. You can do the same thing with aggregate_spatial. You can find more information here as well as a notebook where it is applied, here.\n\n## We can reuse the `ndviband` from the previous steps to create training data\ntraining_data = ndviband.filter_spatial(\n    json.load(open(\"resources/soy_2019.geojson\"))\n)\n\n\ntraining_data_job = training_data.create_job(\n    title=\"training_data\",\n    out_format=\"NetCDF\",\n    sample_by_feature=True,\n)\ntraining_data_job.start_and_wait()\ntraining_data_job.get_results(base_path / \"training_data.nc\")"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html",
    "href": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html",
    "title": "Usecase showcasing Regional Benchmarking service of Anomaly Identification",
    "section": "",
    "text": "With the OpenEO-based Regional Benchmarking service you can check the crop growth on a field and compare it with a similar fields. It gives you an idea of whether your field is performing better or worse than other fields.\nIn this example, we have compared several fields with similar croptype available in our area of interest. The area of interest is derived as WFS from DLV service filtered by a croptype(here, croptype = ‘Zomergerst’). Nevertheless, if users have their polygons/parcels, they can use them with a note that they should be of similar crop type.\n# importing necessary packages\nimport openeo\nimport rasterio\nfrom rasterio.plot import show\n# Acquire more information about the service\nservice = \"Anomaly_Detection\"\nnamespace = \"vito\"\n\neoconn = openeo.connect('https://openeo.vito.be').authenticate_oidc()\neoconn.describe_process(service, namespace=namespace)\n\nAuthenticated using refresh token.\nAs mentioned earlier, though, in this example, we used parcels from a WFS; these parameters are specific to them. User can use their polygons/parcels based on their requirements.\n# Specific parameters\ncroptype = 'Zomergerst'\n\n# Bounding Box\nwest = 5.17\neast = 5.3\nsouth = 51.1\nnorth = 52.246\n# reading the json file (user can use this function if they have their features stored as json file)\n\nimport json\ndef read_json_str(json_txt: str) -&gt; dict:\n    field = json.loads(json_txt)\n    return field"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html#parse-the-data",
    "href": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html#parse-the-data",
    "title": "Usecase showcasing Regional Benchmarking service of Anomaly Identification",
    "section": "Parse the data",
    "text": "Parse the data\nHere, we tried in parsing WFS provided by https://lv.vlaanderen.be/en as parcels with their crop types.\n\n# requesting data over a region for a specific crop type\n\nimport urllib\nimport requests\n\nurl = f\"https://geo.api.vlaanderen.be/Landbgebrperc/wfs?service=WFS&request=getfeature&cql_filter=LBLHFDTLT='{croptype}'&outputformat=json&typename=Lbgebrperc&SRSName=urn:x-ogc:def:crs:EPSG:4326\"\nreq = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})\n\nwfs_request_url = requests.Request('GET', url,headers={'User-Agent': 'Mozilla/5.0'}).prepare().url\ndata = req.json()\n\nBefore proceeding forward, we want to ensure that the filtered data likes in the area of interest. Thus the following cell includes the method to display the parsed data as a dataframe and map.\n\nimport shapely\nimport geopandas as gpd\nfrom shapely.geometry import box\n\n\ndataframe = gpd.GeoDataFrame.from_features(data[\"features\"],crs='EPSG:4326')\narea=dataframe.to_crs(epsg=3857).area\ndataframe=dataframe[area&gt;200]\n\n# filter data within the bounding box\nbbox = box(west,south,east,north)\ndataframe = dataframe[dataframe.within(bbox)]\n\ndataframe = dataframe.head()\n# converting dataframe to geojson string\ngeojson_str = dataframe.to_json()\n\ndataframe\n\n\n\n\n\n\n\n\ngeometry\nUIDN\nOIDN\nALVID\nHFDTLT\nLBLHFDTLT\nGEWASGROEP\nPM\nLBLPM\n\n\n\n\n19\nPOLYGON ((5.17759388 51.13236902, 5.17754339 5...\n4092306\n1228344\n1211525956\n322\nZomergerst\nGranen, zaden en peulvruchten\n\n\n\n\n47\nPOLYGON ((5.24743582 51.10311003, 5.24843201 5...\n1882791\n1021017\n1319447853\n322\nZomergerst\nGranen, zaden en peulvruchten\n\n\n\n\n57\nPOLYGON ((5.17872085 51.17170835, 5.17848342 5...\n4340847\n1667648\n2074973981\n322\nZomergerst\nGranen, zaden en peulvruchten\n\n\n\n\n82\nPOLYGON ((5.21847254 51.27816026, 5.21847592 5...\n4355462\n1639446\n2070875228\n322\nZomergerst\nGranen, zaden en peulvruchten\n\n\n\n\n129\nPOLYGON ((5.2909333 51.20048401, 5.29085554 51...\n4367507\n1523096\n1860308032\n322\nZomergerst\nGranen, zaden en peulvruchten\n\n\n\n\n\n\n\n\n\n\n# plot the polygons\nimport folium\nmap = folium.Map( tiles=\"open street map\", zoom_start=12,location=[51.243,5.18])\npoints = folium.features.GeoJson(dataframe.to_crs('EPSG:4326').to_json())\nmap.add_child(points)\nmap.fit_bounds(map.get_bounds(), padding=(30, 30))\nmap\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html#apply-anomaly-detection-service",
    "href": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html#apply-anomaly-detection-service",
    "title": "Usecase showcasing Regional Benchmarking service of Anomaly Identification",
    "section": "Apply Anomaly Detection service",
    "text": "Apply Anomaly Detection service\n\n#parameters mandatory for this openeo-based service\naoi=read_json_str(geojson_str)\ndate = [\"2020-03-06\", \"2020-06-30\"]\n\n#accessing the openeo service\nanomaly = eoconn.datacube_from_process(service, namespace=namespace, date=date\n                                       , polygon=aoi)\n\n/home/pratixa/.local/lib/python3.6/site-packages/openeo/metadata.py:252: UserWarning: No cube:dimensions metadata\n  complain(\"No cube:dimensions metadata\")\n\n\n\n# synchronous download or batch process\nanomaly.download('RegionalBenchmarking_AD.json')\n\n# # batch processing\n# batch_job = anomaly.create_job(out_format = \"json\", title=\"Croptype\")\n# batch_job.start_and_wait()\n# results = batch_job.get_results()\n# results.download_files()\n\nThe service calculates the CropSAR fAPAR curve for each field and the regional average fAPAR curve calculated from comparable fields in the region during a given time period."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html#visualize-and-compare-the-final-result",
    "href": "APIs/openEO/openeo-community-examples/python/Anomaly_Detection/Anomaly_Detection.html#visualize-and-compare-the-final-result",
    "title": "Usecase showcasing Regional Benchmarking service of Anomaly Identification",
    "section": "Visualize and compare the final result",
    "text": "Visualize and compare the final result\n\nimport matplotlib.pyplot as plt\nimport json\n\ndata_json = json.load(open('RegionalBenchmarking_AD.json', 'r'))\n\n\nfrom matplotlib.pyplot import figure\nimport matplotlib.dates as mdates\n\nfigure(figsize=(18,9), dpi=300)\nfor i in data_json:\n    x_Axis = [key for key, value in data_json[i].items()]\n    y_Axis = [value for key, value in data_json[i].items()]\n    plt.plot(x_Axis,y_Axis, label = i)\n\nax = plt.gca()\nn = 7  # Keeps every 7th label\n[l.set_visible(False) for (i,l) in enumerate(ax.get_xticklabels()) if i % n != 0]\nplt.xlabel('variable')\nplt.xticks(rotation=90)\nplt.ylabel('value')\nplt.tight_layout()\nplt.legend()\nplt.show()\n\n\n\n\nThrough the visualized curved you can study the croptype behaviour of the field in comparision with the Regional average."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/OnnxMLInference/Onnx_ML_Inference.html",
    "href": "APIs/openEO/openeo-community-examples/python/OnnxMLInference/Onnx_ML_Inference.html",
    "title": "Example of running ML inference with an ONNX model in openEO",
    "section": "",
    "text": "This notebook shows how to run inference with a trained model in the ONNX format. In this example, we use a Convolutional Neural Network (CNN) that is applied on a window of 256x256 pixels of a Sentinel-2 band. After training, the model has been converted to the ONNX format. More information on how to do this for your own model can be found here: https://github.com/onnx/tutorials#converting-to-onnx-format\nWe first set up the connection with an openEO backend and load some data as usual.\n\nimport openeo\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\n\nspatial_extent = {\n    \"west\": 4.217639,\n    \"south\": 51.162882,\n    \"east\": 4.477856,\n    \"north\": 51.260197\n}\ntemporal_extent = [\"2022-06-01\", \"2022-08-01\"]\ns2_cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=temporal_extent,\n    spatial_extent=spatial_extent,\n    bands=[\"B04\"],\n    max_cloud_cover=20,\n)\n\nAfter setting up the connection with a back-end, we define where both the dependencies and the model are stored.\n\nDependencies\nIn order to remain lightweight, an openEO backend does not have the necessary dependencies to run the model. Therefore, we need to pass the extra dependencies to the backend. If you are using an ONNX model that can be run with ONNX Runtime version 1.16.3, you can use the dependencies url provided in the code below. If you are using a different version of ONNX Runtime, you need to provide the dependencies yourself.\nMore information on onnx version compatibility can be found here: https://github.com/onnx/tutorials#converting-to-onnx-format\nMore information on how to make a dependency archive for openeo can be found here: https://documentation.dataspace.copernicus.eu/APIs/openEO/job_config.html#custom-udf-dependencies\n\n\nModel\nThe model needs to be stored in a location that is accessible by the backend. In this example, we store the model in the same location as the dependencies. You could however also store the model in a public github repository and provide the url to the backend.\n\n# This will be required to import onnxruntime in the UDF.\ndependencies_url = \"https://artifactory.vgt.vito.be:443/auxdata-public/openeo/onnx_dependencies_1.16.3.zip\"\n# You can upload your own model to a public download link and paste the url here. An example of a public storage is github, or you can set up a public S3 bucket.\nmodel_url = \"https://artifactory.vgt.vito.be:443/auxdata-public/openeo/test_onnx_model.zip\"\n\nNext we load the udf that will run the model. Make sure to take a look at the udf code to understand how and why the loading of the model is cached to reduce your processing costs, how you can work with the named dimensions of an Xarray and how to run a model on each separate timestep.\nWe use the process apply_neighborhood to apply the udf. The model accepts a window of 256x256 pixels. In order to get a smooth result, we apply the model on overlapping windows. The values included in the overlap will not be modified.\nIn this case an overlap of 16 pixels is used on both sides, so the remaining size of 224x224 pixels will be modified each step.\nOther models might perform pixel-wise predictions (e.g. random forests) in which case you can simply use the apply process. The openEO backend will then automatically handle the chunking and stitching of the results.\nIn case you only want to keep your prediction, instead of using apply you can use the reduce dimension process to only keep the prediction and not the input bands.\n\n# Load the UDF from a file.\nudf = openeo.UDF.from_file(\n    \"onnx_udf.py\",\n    context=None, # you can pass extra arguments to the UDF here\n)\n\n# Apply the UDF to the data cube.\nonnx_result = s2_cube.apply_neighborhood(\n    udf,\n    size=[\n        {\"dimension\": \"x\", \"value\": 224, \"unit\": \"px\"},\n        {\"dimension\": \"y\", \"value\": 224, \"unit\": \"px\"},\n    ],\n    overlap=[\n        {\"dimension\": \"x\", \"value\": 16, \"unit\": \"px\"},\n        {\"dimension\": \"y\", \"value\": 16, \"unit\": \"px\"}\n    ],\n)\n\nmean_result = onnx_result.mean_time()\n\nFinally, we add the dependencies to the job-options and run the process on the back-end.\n\n# We pass the model and dependencies as zip files to the UDF through the job options.\n# The zip files provided in this list will be extracted in the UDF environment in a folder indicated by the name after the # symbol.\njob_options = {\n    \"udf-dependency-archives\": [\n        f\"{dependencies_url}#onnx_deps\",\n        f\"{model_url}#onnx_models\",\n    ],\n}\nmean_result.execute_batch(\"output.tiff\", job_options=job_options)\n\n\nimport rasterio\nfrom rasterio.plot import show\n\nwith rasterio.open(\"output.tiff\") as image:\n    show(image)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html",
    "href": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html",
    "title": "Wildfire mapping using Sentinel-2",
    "section": "",
    "text": "In this notebook, we want to show a use case of mapping wildfire using Sentinel-2 following the method discussed in the work of Nolde et al. (2020). Here, we want to replicate only a part of the workflow. Thus, it involves calculating cloud-free differences in NDVI for pre and post-fire data and comparing them.\nimport scipy\nimport numpy as np\n\nimport openeo\nfrom openeo.extra.spectral_indices import compute_indices\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.patches as mpatches\nimport rasterio\nfrom rasterio.plot import show\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\nFor this usecase we select an area in Jaen/Spain where fire was noted on 2017/08/03 (source: https://emergency.copernicus.eu/mapping/list-of-components/EMSR219)\nextent = {\"west\": -2.6910, \"south\": 38.2239, \"east\": -2.5921, \"north\": 38.3002}\nAs locations with forest fire often have cloud cover, which hinders the calculation of forest fire-affected areas, we want to get a cloud-free pre-event datacube. Thus, let us define a simple Python function to pre-process pre-event NDVI. Here we use the concept of BAP which means the best available cloud-free pixels right before or right after the forest fire event, this is obtained via cloud masking and temporal reduction with “first” in post event datacube and “last” in pre event datacube.ent date.\nHence: * We first create a cloud mask with an SCL band where we filter out pixels which have a cloud effect; * Subsequently, a Gaussian kernel is created since the cloud masks are frequently noisy and may not completely encompass the cloud region; * Next, we apply a smoothing function to the selected mask coupled with a morphometric dilation to increase and smooth the mask area. Here, for dilation, we select 11 pixels; * Following this, a binary mask is produced by applying a threshold of 0.1. This implies that to be classified as cloud-free pixels, the pixels that have undergone smoothing and dilation should exhibit a value below 0.1. * Once the required mask is generated, it is applied to the data cube; * Finally, once the masked datacube is available, we want to select each cloud-free pixel as near the forest fire date as possible. To achieve this, we reduce the time dimension (reduce_dimension) based on the “last” criterion, which selects the nearest cloud-free pixel to the event date.\n# define a function to identify cloud-free pixels in the available data-cube\n\n\ndef getBAP(scl, data, reducer=\"first\"):\n    mask = (scl == 3) | (scl == 8) | (scl == 9) | (scl == 10)\n\n    # mask is a bit noisy, so we apply smoothening\n    # 2D gaussian kernel\n    g = scipy.signal.windows.gaussian(11, std=1.6)\n    kernel = np.outer(g, g)\n    kernel = kernel / kernel.sum()\n\n    # Morphological dilation of mask: convolution + threshold\n    mask = mask.apply_kernel(kernel)\n    mask = mask &gt; 0.1\n\n    data_masked = data.mask(mask)\n\n    # now select Best Available Pixel based on the mask\n    return data_masked.reduce_dimension(reducer=reducer, dimension=\"t\")"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html#using-normalized-difference-vegetation-index-ndvi",
    "href": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html#using-normalized-difference-vegetation-index-ndvi",
    "title": "Wildfire mapping using Sentinel-2",
    "section": "Using Normalized Difference Vegetation Index (NDVI)",
    "text": "Using Normalized Difference Vegetation Index (NDVI)\n\nPre-event NDVI\nLet us load Sentinel-2 L2A as our pre-event datacube to calculate NDVI.\n\n# load S2 pre-collection\ns2pre = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2017-05-03\", \"2017-08-03\"],\n    spatial_extent=extent,\n    bands=[\"B04\", \"B08\", \"B12\"],\n    max_cloud_cover=90,\n)\n\ns2pre_scl = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2017-05-03\", \"2017-08-03\"],\n    spatial_extent=extent,\n    bands=[\"SCL\"],\n    max_cloud_cover=90,\n)\n\n# calculate ndvi\nndvi_pre = s2pre.ndvi()\n\n\n# Create a Pre-event cloud free mosiac\nndvi_pre = getBAP(s2pre_scl, ndvi_pre, reducer=\"last\")\n\n\nndvi_pre.download(\"NDVI.tiff\")\n\nDuring the forest fire, we want to know the location of the forest fire as soon as possible. However, it means that the images are often filled with smoke and clouds right after the fire events, which hinders its performance with many cloud-covered pixels where we cannot map the forest fire.\nTherefore, here, we want to show two modes of operation:\n(i) NRT mode and \n(ii) Post-event mode. \nIn NRT mode, we try to map based on the first available image right after the forest fire event. In post-event mode, we perform pre-processing similar to the pre-event data to obtain cloud-free images. The NRT mode is generally used for the first-response and quick actions, whereas the post mode is used for scientific studies.\n\n\nNear Real Time(NRT) Mode NDVI\n\n# load S2 Near-real-time(NRT) collection\ns2nrt = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2017-08-03\", \"2017-08-08\"],\n    spatial_extent=extent,\n    bands=[\"B04\", \"B08\", \"B12\"],\n    max_cloud_cover=90,\n)\n\ns2nrt_scl = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2017-08-03\", \"2017-08-08\"],\n    spatial_extent=extent,\n    bands=[\"SCL\"],\n    max_cloud_cover=90,\n)\n\nndvi_nrt = s2nrt.ndvi()\nndvi_nrt = getBAP(s2nrt_scl, ndvi_nrt, reducer=\"first\")\n\nNow, we have both pre and post event NDVI data we calculate dNDVI (differential NDVI) to find locations where forest fire has occured.\n\n# download NDVI for NRT mode for comparison\nndvi_nrt.download(\"NRT_NDVI.tiff\")\n\n\n# download signal of fire in near real time\nfire_nrt = ndvi_pre - ndvi_nrt\nfire_nrt.download(\"NRT_Fire.tiff\")\n\n\n\nPost-event NDVI\nIn post-event mode, similar to pre-event data, we collect long-term image time series and prepare the data to calculate cloud-free NDVI pixels. Notably, here we use the reducer as “first” mainly because this is a post-event time series, and we would like to obtain a cloud-free pixel with NDVI as soon as possible from the forest fire event, therefore selecting the first available cloud-free pixel ensures that we get the earliest cloud-free pixel.\n\n# load S2 post collection\ns2post = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2017-08-03\", \"2017-11-03\"],\n    spatial_extent=extent,\n    bands=[\"B04\", \"B08\", \"B12\"],\n    max_cloud_cover=90,\n)\n\ns2post_scl = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2017-08-03\", \"2017-11-03\"],\n    spatial_extent=extent,\n    bands=[\"SCL\"],\n    max_cloud_cover=90,\n)\n\n# calculate post ndvi mosaic\nndvi_post = s2post.ndvi()\nndvi_post = getBAP(s2post_scl, ndvi_post, reducer=\"first\")\n\n\n# download NDVI for post-event mode for comparison\nndvi_post.download(\"Post_NDVI.tiff\")\n\n\nfire_mosiac = ndvi_pre - ndvi_post\nfire_mosiac.download(\"Mosiac_Fire.tiff\")\n\n\n\nPlot the NDVI\n\nndvi = rasterio.open(\"NDVI.tiff\")\nndvi_nrt = rasterio.open(\"NRT_NDVI.tiff\")\nndvi_mosiac = rasterio.open(\"Post_NDVI.tiff\")\n\nf, axarr = plt.subplots(1, 3, dpi=100, figsize=(18, 6))\nim = show(ndvi.read(1), vmin=0, vmax=1, transform=ndvi.transform, ax=axarr[0])\naxarr[0].set_title(\"Pre Event NDVI\")\n\nim = show(ndvi_nrt.read(1), vmin=0, vmax=1, transform=ndvi_nrt.transform, ax=axarr[1])\naxarr[1].set_title(\"Post Event NDVI (NRT)\")\n\nim = show(\n    ndvi_mosiac.read(1), vmin=0, vmax=1, transform=ndvi_mosiac.transform, ax=axarr[2]\n)\naxarr[2].set_title(\"Post Event NDVI (Mosiac)\")\nplt.tight_layout()\n\n\n\n\nAs we can see there is a drop in NDVI at the locations with forest fire (see end of page for ground truth). This shows that with the adopted approach we can identify locations with forest fire.\nMoreover, we can also observe that in the NDVI NRT mode and NDVI Mosiac mode there is not much difference, the reason could be because the cloud cover was not present in the NRT image. This shows that if there is no cloud cover, both modes operate similarly. Now, based on these image we calculated the difference in NDVI which is the likely locations of forest fire; let’s visualize them, too.\n\nfire_nrt = rasterio.open(\"NRT_Fire.tiff\")\nfire_mosiac = rasterio.open(\"Mosiac_Fire.tiff\")\n\ncmap = matplotlib.colors.ListedColormap([\"black\", \"firebrick\"])\nvalues = [\"Absence\", \"Presence\"]\ncolors = [\"black\", \"firebrick\"]\n\nf, axarr = plt.subplots(1, 2, dpi=100, figsize=(12, 6))\n\nim = show(\n    fire_nrt.read(1) &gt; 0.5,\n    vmin=0,\n    vmax=1,\n    transform=fire_nrt.transform,\n    ax=axarr[0],\n    cmap=cmap,\n)\naxarr[0].set_title(\"Forest Fire NRT mode\")\n\nim = show(\n    fire_mosiac.read(1) &gt; 0.5,\n    vmin=0,\n    vmax=1,\n    transform=fire_mosiac.transform,\n    ax=axarr[1],\n    cmap=cmap,\n)\naxarr[1].set_title(\"Forest Fire Post Mode\")\npatches = [\n    mpatches.Patch(color=colors[i], label=\"Fire {l}\".format(l=values[i]))\n    for i in range(len(values))\n]\nf.legend(handles=patches, bbox_to_anchor=(0.95, 0.2), loc=1)\nplt.tight_layout()\n\n\n\n\nHere, we can see that the burnt area is quite nicely mapped in both modes and compared to the ground truth image shown at the end of this notebook, the burned area mapping is quite accurate. Moreover, this can still be improved with the NBR data, as suggested by Nolde et al.(2020). Let’s try the same with NBR."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html#with-normalized-burn-rationbr",
    "href": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html#with-normalized-burn-rationbr",
    "title": "Wildfire mapping using Sentinel-2",
    "section": "With Normalized Burn Ratio(NBR)",
    "text": "With Normalized Burn Ratio(NBR)\nThis whole process can also be done with NBR as the paper suggests that NBR produces higher accuracy, but due to the unavailability of SWIR band in medium-resolution satellites, they opted to use NDVI.\nQuoted: “Optionally, the system can utilize the Normalized Burn Ratio (NBR) index ([40], see Equations (3) and (4)) instead of the NDVI. This index features superior capabilities for burnt area discrimination compared to the NDVI”\nThus, further in this notebook, we also showcase the same process as above with NBR. In this case, all processing steps are the same except for the calculation of NBR instead of NDVI.\n\nnbr_pre = compute_indices(s2pre, indices=[\"NBR\"])\nnbr_pre = getBAP(s2pre_scl, nbr_pre, reducer=\"last\")\n\n\nnbr_pre.download(\"NBR.tiff\")\n\n\nNRT mode for NBR\n\nnbr_nrt = compute_indices(s2nrt, indices=[\"NBR\"])\nnbr_nrt = getBAP(s2nrt_scl, nbr_nrt, reducer=\"first\")\n\n\nnbr_nrt.download(\"NRT_NBR.tiff\")\n\n\n# download signal of fire in near real-time\nfire_nrt_nbr = nbr_pre - nbr_nrt\nfire_nrt_nbr.download(\"NRT_NBR_Fire.tiff\")\n\n\n\nPost-event mode for NBR\n\nnbr_post = compute_indices(s2post, indices=[\"NBR\"])\nnbr_post = getBAP(s2post_scl, nbr_post, reducer=\"first\")\n\n\nnbr_post.download(\"Post_NBR.tiff\")\n\n\nfire_nrt_mosiac = nbr_pre - nbr_post\nfire_nrt_mosiac.download(\"Mosiac_NBR_Fire.tiff\")\n\n\n\nPlot the NBR\nNow we have the NBR and its difference which can be treated as a proxy of forest fire. Let’s visualize them, first the NBR alone and then the mapped forest fire from it.\n\nnbr = rasterio.open(\"NBR.tiff\")\nnbr_nrt = rasterio.open(\"NRT_NBR.tiff\")\nnbr_mosiac = rasterio.open(\"Post_NBR.tiff\")\n\nf, axarr = plt.subplots(1, 3, dpi=100, figsize=(18, 6))\nim = show(nbr.read(1), vmin=0, vmax=1, transform=nbr.transform, ax=axarr[0])\naxarr[0].set_title(\"Pre Event NBR\")\n\nim = show(nbr_nrt.read(1), vmin=0, vmax=1, transform=nbr_nrt.transform, ax=axarr[1])\naxarr[1].set_title(\"Post Event NBR (NRT)\")\n\nim = show(\n    nbr_mosiac.read(1), vmin=0, vmax=1, transform=nbr_mosiac.transform, ax=axarr[2]\n)\naxarr[2].set_title(\"Post Event NBR (Mosiac)\")\nplt.tight_layout()\n\n\n\n\nAs we can see, the location of the forest fire is quite prominent in the post-event NBR images compared to that of the pre-event NBR images. This shows that the approach with NBR has better visibility of forest fire signals than the NDVI-based approach. Now, let’s see how the final output of the forest fire area looks with the NBR difference image.\n\nfire_nbr_nrt = rasterio.open(\"NRT_NBR_Fire.tiff\")\nfire_nbr_mosiac = rasterio.open(\"Mosiac_NBR_Fire.tiff\")\n\ncmap = matplotlib.colors.ListedColormap([\"black\", \"firebrick\"])\nvalues = [\"Absence\", \"Presence\"]\ncolors = [\"black\", \"firebrick\"]\n\nf, axarr = plt.subplots(1, 2, dpi=100, figsize=(12, 6))\n\nim = show(\n    fire_nbr_nrt.read(1) &gt; 0.5,\n    vmin=0,\n    vmax=1,\n    transform=fire_nbr_nrt.transform,\n    ax=axarr[0],\n    cmap=cmap,\n)\naxarr[0].set_title(\"Forest Fire NRT mode\")\n\nim = show(\n    fire_nbr_mosiac.read(1) &gt; 0.5,\n    vmin=0,\n    vmax=1,\n    transform=fire_nbr_mosiac.transform,\n    ax=axarr[1],\n    cmap=cmap,\n)\naxarr[1].set_title(\"Forest Fire Mosiac Mode\")\npatches = [\n    mpatches.Patch(color=colors[i], label=\"Fire {l}\".format(l=values[i]))\n    for i in range(len(values))\n]\nf.legend(handles=patches, bbox_to_anchor=(0.95, 0.2), loc=1)\nplt.tight_layout()\n\n\n\n\nWe can see here with the use of NBR, the final forest fire area map looks much more complete and less noisy/pixelated compared to the NDVI-based approach. In general, we can see that both NDVI and NBR-based approaches are good for mapping wildfires in near real-time as well as post-event analysis basis. This data can further be used for planning, mitigation and management of forest fire prevention."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html#ground-truth",
    "href": "APIs/openEO/openeo-community-examples/python/ForestFire/ForestFire.html#ground-truth",
    "title": "Wildfire mapping using Sentinel-2",
    "section": "Ground Truth",
    "text": "Ground Truth\nAs a validation of this workflow, we have a map provided by Copernicus Emergency Management Service (CEMS) is shown below:"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/WorldCereal/WorldCereal.html",
    "href": "APIs/openEO/openeo-community-examples/python/WorldCereal/WorldCereal.html",
    "title": "WorldCereal product download",
    "section": "",
    "text": "This example illustrates the use of openEO for combining and downloading data from the ESA WorldCereal project.\nThis project provides a global map of cereals for 2021 at 10m resolution! It can be used as an important base layer for agriculture use cases. Combined with the power of openEO, you can easily generate agricultural statistics over an area of interest, or use this data as a masking layer in an advanced workflow.\nIn this example, we’ll illustrate a fairly simple case of combining two collections into a single image file.\n\nimport openeo\n\nc = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nWe list the available collection id’s to make sure we have the right name.\n\n[col[\"id\"] for col in c.list_collections() if \"WORLDCEREAL\" in col[\"id\"]]\n\n['ESA_WORLDCEREAL_ACTIVECROPLAND',\n 'ESA_WORLDCEREAL_IRRIGATION',\n 'ESA_WORLDCEREAL_TEMPORARYCROPS',\n 'ESA_WORLDCEREAL_WINTERCEREALS',\n 'ESA_WORLDCEREAL_MAIZE',\n 'ESA_WORLDCEREAL_SPRINGCEREALS']\n\n\n\nc.describe_collection(\"ESA_WORLDCEREAL_MAIZE\")\n\n\n    \n    \n        \n    \n    \n\n\nIn the following block, we combine two WorldCereal collections into a single output. Both the maize and the wintercereal collection will have value 100 for their respective crops. This means that once the two datacubes are merged, it’s impossible to distinguish between the two. Therefore, a linear transformation is applied to the winter datacube. Furthermore, since both collections are mapped to a different point in time, the time dimension is reduced, such that the resulting map only contains one timestep with both crops shown. The formula used here is just an example, and can be made much more complex depending on your use case.\n\nextent = {'west': 3.0, 'south': 50.0, 'east': 4.0, 'north': 51.0, 'crs': 'EPSG:4326'}\n\ntemporal = ('2020-09-01T00:00:00Z', '2021-12-31T00:00:00Z')\n\nmaize = c.load_collection(\"ESA_WORLDCEREAL_MAIZE\",\n                         temporal_extent= temporal,\n                         spatial_extent=extent,\n                         bands=[\"CLASSIFICATION\"]).reduce_dimension(dimension=\"t\",reducer=\"mean\")\n\nwinter = c.load_collection(\"ESA_WORLDCEREAL_WINTERCEREALS\",\n                         temporal_extent= temporal,\n                         spatial_extent=extent,\n                         bands=[\"CLASSIFICATION\"]).apply(lambda x:100*(x+10)).reduce_dimension(dimension=\"t\",reducer=\"mean\")\n\ncombined = maize.merge_cubes(winter, overlap_resolver=\"sum\")\n\nWe now have defined what openEO calls a ‘process graph’, but still need to execute it. We will use an ‘asynchronous’ batch job that gets sent to the server, as it can take a longer time to execute.\nWhen finished, this command will automatically download the result from openEO to your local working directory. You can also follow the progress and view results in the openEO web editor.\n\njob = combined.execute_batch(\n    title = \"Worldcereal example Terrascope\",\n    out_format=\"GTiff\",\n)\n\n0:00:00 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': send 'start'\n0:00:22 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:00:28 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:00:35 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:00:43 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:01:18 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:01:31 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:01:47 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:02:06 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:02:30 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:03:00 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': queued (progress N/A)\n0:03:54 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': running (progress N/A)\n0:04:42 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': running (progress N/A)\n0:05:40 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': running (progress N/A)\n0:06:41 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': running (progress N/A)\n0:07:41 Job 'vito-j-0e6bd54acb9d4de2b6b6e03a5bb120dd': finished (progress N/A)\n\n\n\njob.get_results().download_files()"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html",
    "href": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html",
    "title": "Heatwave in the Netherlands",
    "section": "",
    "text": "As an impact of global warming, an increase in temperature has been reported, which can lead to potential health risks and environmental stress. Several research studies have been presented over the years, such as in this paper, where they explain the change in Land surface temperature over several regions.\nThus, in this notebook, we want to showcase a tool for mapping heatwaves using Sentinel-3 products. For this, we focused on the Netherlands and used specific conditions proposed by the “National Heatwave Plan” in the Netherlands. The condition implies:\nMoreover, in De Bilt, a municipality in the province of Utrecht had a temperature of 25°C at least five days in a row, with at least three days hotter than 30 °C\nhttps://nltimes.nl/2023/09/08/dutch-heat-record-broken-third-day-row-warm-sunny-weekend-ahead\nimport openeo\nimport json\nfrom pathlib import Path\nimport folium\nconnection = openeo.connect(\n    \"openeo.dataspace.copernicus.eu\"\n).authenticate_oidc()\n\nAuthenticated using refresh token.\nLet us load 5 month data for the area of interest\ndef read_json(filename: str) -&gt; dict:\n    with open(filename) as input:\n        field = json.load(input)\n    return field\n\n\ndate = [\"2023-06-01\", \"2023-10-30\"]\naoi = read_json(\"Netherlands_polygon.geojson\")\nm = folium.Map([52.2, 5], zoom_start=7)\nfolium.GeoJson(aoi).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nlst = connection.load_collection(\n    \"SENTINEL3_SLSTR_L2_LST\",\n    temporal_extent=date,\n    spatial_extent=aoi,\n    bands=[\"LST\"],\n)\n# apply cloud masking\n\nmask = connection.load_collection(\n    \"SENTINEL3_SLSTR_L2_LST\",\n    temporal_extent=date,\n    spatial_extent=aoi,\n    bands=[\"confidence_in\"],\n)\n\nmask = mask &gt;= 16384\n\nlst.mask(mask)\nNext, we define a User Defined Function (UDF) that takes in the datacube and checks whether both specified conditions are satisfied. Given that the unit of the Land Surface Temperature (LST) layer is Kelvin, the conditions are applied accordingly. If the values meet the designated threshold and continue for more than two days, a new resulting array is generated and returned.\nPlease note that, here we gave 298.15 and 303.15 Kelvin as two thresholds since we chose a relatively small region. You can change match your requirement.\nudf = openeo.UDF(\n    \"\"\"\nimport xarray\nimport numpy as np\nfrom openeo.udf import inspect\n\ndef apply_datacube(cube: xarray.DataArray, context: dict) -&gt; xarray.DataArray:\n    \n    array = cube.values\n    inspect(data=[array.shape], message = \"Array dimensions\")\n    res_arr=np.zeros(array.shape)\n    for i in range(array.shape[0]-4):\n        ar_sub=np.take(array,  range(i, i+5), axis=0)\n        res_arr[i]=(np.all(ar_sub&gt;295,axis=0)) & (np.nansum(ar_sub&gt;300,axis=0)&gt;2)\n    return xarray.DataArray(res_arr, dims=cube.dims, coords=cube.coords)\n\"\"\"\n)\nheatwave_loc = lst.apply_dimension(process=udf, dimension=\"t\")\nNow let us use sum as the reducer to count the total number of times each pixels had a heat wave in the Netherlands during June-September 2023.\nheatwave_loc = heatwave_loc.reduce_dimension(reducer=\"sum\", dimension=\"t\")\nSince the workflow covers an entire country, we assume that the processing might take a longer time and the default amount of CPU and memory resources assigned might not be sufficient. Therefore, we can use the job configuration capabilities provided in openEO to execute the workflow when using batch job-based methods.\nYou can find more information on Job configuration on this page.\njob_options = {\n    \"executor-memory\": \"3G\",\n    \"executor-memoryOverhead\": \"4G\",\n    \"executor-cores\": \"2\",\n}\n# execute using batch job\nheatwave_job = heatwave_loc.execute_batch(\n    title=\"Heatwave Locations in the Netherlands\",\n    outputfile=\"Heatwave_NL.nc\"\n)\n\n0:00:00 Job 'j-240520f051fb4bd7be300b1c0958e2ce': send 'start'\n0:00:14 Job 'j-240520f051fb4bd7be300b1c0958e2ce': created (progress 0%)\n0:00:20 Job 'j-240520f051fb4bd7be300b1c0958e2ce': created (progress 0%)\n0:00:26 Job 'j-240520f051fb4bd7be300b1c0958e2ce': created (progress 0%)\n0:00:34 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:00:44 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:00:57 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:01:14 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:01:33 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:01:57 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:02:27 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:03:05 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:03:52 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:04:51 Job 'j-240520f051fb4bd7be300b1c0958e2ce': running (progress N/A)\n0:05:51 Job 'j-240520f051fb4bd7be300b1c0958e2ce': finished (progress 100%)\nprint(f\"\"\" The total openEO credits consumed when executing heatwave workflow is \n        {heatwave_job.describe()['costs']} \n        credits.\"\"\"\n     )\n\n The total openEO credits consumed when executing heatwave workflow is \n        3.0 \n        credits.\nHowever, please note that the cost mentioned above was incurred during the preparation of this notebook and could change over time."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html#lets-plot-the-results",
    "href": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html#lets-plot-the-results",
    "title": "Heatwave in the Netherlands",
    "section": "Let’s plot the results",
    "text": "Let’s plot the results\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport xarray as xr\nimport numpy as np\n\n\nheatwave = xr.load_dataset(\"Heatwave_NL.nc\")\n\n\ndata = heatwave[[\"LST\"]].to_array(dim=\"bands\")[0]\ndata.values[data==0] = np.nan\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 304, x: 431)&gt; Size: 524kB\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ...,  1.,  1., nan],\n       [nan, nan, nan, ...,  1., nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\nCoordinates:\n  * x        (x) float64 3kB 3.375 3.384 3.393 3.402 ... 7.188 7.197 7.206 7.215\n  * y        (y) float64 2kB 53.46 53.45 53.44 53.43 ... 50.78 50.77 50.76 50.76\n    bands    &lt;U3 12B 'LST'\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platform - Geotrellis backend: 0.33.1a1\n    description:  \n    title:        xarray.DataArrayy: 304x: 431nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ...,  1.,  1., nan],\n       [nan, nan, nan, ...,  1., nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)Coordinates: (3)x(x)float643.375 3.384 3.393 ... 7.206 7.215standard_name :longitudelong_name :longitudeunits :degrees_eastarray([3.375334, 3.384263, 3.393191, ..., 7.196763, 7.205691, 7.21462 ])y(y)float6453.46 53.45 53.44 ... 50.76 50.76standard_name :latitudelong_name :latitudeunits :degrees_northarray([53.461366, 53.452437, 53.443509, ..., 50.773866, 50.764937, 50.756009])bands()&lt;U3'LST'array('LST', dtype='&lt;U3')Indexes: (2)xPandasIndexPandasIndex(Float64Index([3.3753342857142856,  3.384262857142857, 3.3931914285714275,\n              3.4021199999999987, 3.4110485714285694, 3.4199771428571406,\n               3.428905714285712, 3.4378342857142825, 3.4467628571428537,\n              3.4556914285714244,\n              ...\n              7.1342628571426765,  7.143191428571248,  7.152119999999818,\n               7.161048571428389,   7.16997714285696, 7.1789057142855315,\n               7.187834285714103,  7.196762857142674,  7.205691428571245,\n               7.214619999999815],\n             dtype='float64', name='x', length=431))yPandasIndexPandasIndex(Float64Index([ 53.46136571428571,  53.45243714285714,  53.44350857142857,\n                        53.43458,  53.42565142857143,  53.41672285714286,\n               53.40779428571429,  53.39886571428571,  53.38993714285714,\n               53.38100857142857,\n              ...\n               50.83636571428584,  50.82743714285727,   50.8185085714287,\n              50.809580000000125, 50.800651428571555, 50.791722857142986,\n               50.78279428571442,  50.77386571428584,  50.76493714285727,\n                50.7560085714287],\n             dtype='float64', name='y', length=304))Attributes: (4)Conventions :CF-1.9institution :openEO platform - Geotrellis backend: 0.33.1a1description :title :"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html#interactive-plot",
    "href": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html#interactive-plot",
    "title": "Heatwave in the Netherlands",
    "section": "Interactive plot",
    "text": "Interactive plot\nUsing Folium, we can easily create an interactive map with a background that allows us to easily spot areas affected by heatwaves.\n\nlon, lat = np.meshgrid(data.x.values.astype(np.float64), data.y.values.astype(np.float64))\ncm = matplotlib.colormaps.get_cmap('hot_r')\ncolored_data = cm(data/10)\n\n\nm = folium.Map(location=[lat.mean(), lon.mean()], zoom_start=8)\nfolium.raster_layers.ImageOverlay(colored_data,\n                     [[lat.min(), lon.min()], [lat.max(), lon.max()]],\n                     mercator_project=True,\n                     opacity=0.5).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html#static-plot",
    "href": "APIs/openEO/openeo-community-examples/python/Heatwave/HeatwaveNL.html#static-plot",
    "title": "Heatwave in the Netherlands",
    "section": "Static plot",
    "text": "Static plot\nUsing cartopy, we can create a static plot.\n\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\naxes = plt.axes(projection=ccrs.PlateCarree())\naxes.coastlines()\naxes.add_feature(cfeature.BORDERS, linestyle=':')\ndata.plot.imshow(vmin=0, vmax=10, ax=axes, cmap=\"hot_r\")\naxes.set_title(\"# of Days with Heatwave in 2023\")\n\nText(0.5, 1.0, '# of Days with Heatwave in 2023')\n\n\n\n\n\nThe above plot shows the number of days with a heatwave in the area of interest in the specified time interval."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/FloodSAR/flood_sar_udf.html",
    "href": "APIs/openEO/openeo-community-examples/python/FloodSAR/flood_sar_udf.html",
    "title": "Flood extent using Sentinel 1",
    "section": "",
    "text": "Flood extent can be determined using a change detection approach on Sentinel-1 data. In this process, we have tried adopting UN SPIDER’s recommended practice for computing flood extents by implementing an openEO UDF.\n\n# import necessary packages\nimport openeo\nfrom openeo.api.process import Parameter\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport rasterio\nimport numpy as np\n\n# connect with the backend\neoconn = openeo.connect(\"openeo.vito.be\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nUser can choose among different backend available here to connect to the backend. Rrgarding the authentication process OpenID connect (oidc) is recommended, but not always straightforward to use. In cases where you are unable to connect with the backend use basic authentication method explained here.\n\n# function to load geojson file\ndef read_json(path: Path) -&gt; dict:\n    with open(path) as input:\n        field = json.load(input)\n        input.close()\n    return field\n\nTo use the data collection, a user must use the correct backend with the data collection. Then using load_collection, they can specify bands, temporal extent (i.e. interested time interval) and even spatial extent.\n\n# load before flash flood params\nbefore_date = [\"2021-05-12\",\"2021-05-12\"]\nafter_date = [\"2021-06-18\", \"2021-06-18\"]\nspatial_param = read_json(\"aoi/cologne_all.geojson\")\n\n\n# using S1 data from Sentinelhub (https://hub.openeo.org/) directly instead of downloading\n\nbefore_cube = eoconn.load_collection(\n                            \"SENTINEL1_GAMMA0_SENTINELHUB\",\n                            temporal_extent = before_date,\n                            spatial_extent = spatial_param,\n                            bands = ['VV'],\n                            properties={\"sat:orbit_state\": lambda v: v==\"ascending\"}\n                            )\nafter_cube = eoconn.load_collection(\n                            \"SENTINEL1_GAMMA0_SENTINELHUB\",\n                            temporal_extent = after_date,\n                            spatial_extent = spatial_param,\n                            bands = ['VV'],\n                            properties={\"sat:orbit_state\": lambda v: v==\"ascending\"}\n                            )\n\nSince now we have details on temporal dimension we can perform dimension reduction. As we loaded our collection for specific time intervals, it can include multiple time dimensions. Thus reduce_dimension applies a reducer to a data cube dimension by collapsing all the pixel values along the time dimension into an output value computed by the reducer.\n\n#'reduce_dimension' to reduce temporal dimension\nrbefore_cube = before_cube.reduce_dimension(dimension=\"t\", reducer=\"mean\")\nrafter_cube = after_cube.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n\n\n# calculating the ratio of post and pre datacube as mentioned in the UNSPIDER documentation for flood extent using Sentinel 1\ndifference = rafter_cube.divide(rbefore_cube)\n\nHere we tried in presenting a method to create and use UDF as an openEO feature. In a similar manner user can create their own UDF as needed to apply to their data cube. More information on UDF.\nOur UDF is designed to perform thresholding to the final result obtained by comparing pre and post datacubes and returns a cube that assigns 1 to region with higher value than threshold otherwise 0.\n\n# define a udf that will perform thresholding of the dataset\nudf = openeo.UDF(\"\"\"\nfrom openeo.udf import XarrayDataCube\n\ndef apply_datacube(cube: XarrayDataCube, context: dict) -&gt; XarrayDataCube:\n    array = cube.get_array()\n    \n    # UN defined difference threshold\n    array.values = np.where(array &gt; 1.5, 1, 0)\n    return cube\n\"\"\")\n\n# Apply the UDF to a cube.\nthreshold_cube = difference.apply(process=udf)\n\nOnce the process is completed, you can also save it as your process using save_user_defined_process that can later be used for a similar task. Otherwise, you can download the result either by direct download (in case of the small spatial extent with few processing) or perform create a batch job in case it is a heavy task over a large extent.\n\n# download your result either syncronous or proceed as batch job\nthreshold_cube.download(\"s1_diff.tiff\")"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/BurntMapping/burntmapping_chunks.html",
    "href": "APIs/openEO/openeo-community-examples/python/BurntMapping/burntmapping_chunks.html",
    "title": "Burnt area mapping using chunk_polygon on UDF",
    "section": "",
    "text": "In this notebook classical Normalized Burnt Ratio(NBR) difference is performedon a chunk of polygons. You can find ways to develop your process and use chunk_polygon on a usecase. The method followed in this notebook to compute DNBR is inspired from UN SPIDER’s recommended preactices.\n(To be noted: chunk_polygon are experimental at the moment)\n\n# import necessary packages\nimport openeo\nfrom openeo.api.process import Parameter\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport rasterio\nimport numpy as np\n\n# connect with the backend\neoconn = openeo.connect(\"openeo.vito.be\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nUser can choose among different backend available here to connect to their choice of backend. Regarding the authentication process OpenID connect (oidc) is recommended, but not always straightforward to use. In cases where you are unable to connect with the backend use basic authentication method explained here.\n\n# function to load geojson file\ndef read_json(path: Path) -&gt; dict:\n    with open(path) as input:\n        field = json.load(input)\n        input.close()\n    return field\n\nTo use the data collection, a user must use the correct backend with the data collection. Then using load_collection, they can specify bands, temporal extent (i.e. interested time interval) and even spatial extent. In this example, we have loaded the entire collection so that process (including UDF) can later be applied to spatial chunks.\n\n# load datacube for beforeand after fire \nbefore_date = [\"2021-01-12\",\"2021-03-12\"]\nafter_date = [\"2021-05-18\", \"2021-07-18\"]\n\nbefore_cube = eoconn.load_collection(\n                            \"SENTINEL2_L1C_SENTINELHUB\",\n                            temporal_extent = before_date,\n                            bands = ['B08','B12']\n                            )\nafter_cube = eoconn.load_collection(\n                            \"SENTINEL2_L1C_SENTINELHUB\",\n                            temporal_extent = after_date,\n                            bands = ['B08','B12'],\n                            )\n\nHere we tried in presenting a method to create and use UDF as an openEO feature. In a similar manner user can create their own UDF as needed to apply to their data cube. More information on UDF. The reason to create UDF openEO, is similar to creating a function in general python i.e to avoid recursive script.\nOur UDF computes Normalised Burnt Ratio (NBR) from the selected band by performing simple band computation and returns a NBR datacube.\n\n# Create a UDF object from inline source code for computing nbr\nmy_code = \"\"\"\nfrom openeo.udf import XarrayDataCube\n\n\ndef apply_datacube(cube: XarrayDataCube, context: dict) -&gt; XarrayDataCube:\n    # access the underlying xarray\n    inarr = cube.get_array()\n\n    # nbr\n    nir = inarr.loc[:,'B08']\n    swir = inarr.loc[:,'B12']\n    nbr = (nir-swir)/(nir+swir)\n    \n    # extend bands dim\n    nbr=nbr.expand_dims(dim='bands', axis=-3).assign_coords(bands=['nbr'])\n    \n    # wrap back to datacube and return\n    return XarrayDataCube(nbr)\n\"\"\"\nudf_process = lambda data: data.run_udf(udf=my_code,runtime='python')\n\nWe used the chunk_polygon method to apply our UDF over a spatial chunk of the datacube. In the case of a simple process that does not require UDF, you can directly load your spatial extent in the dataset.\n\n#specify aoi chunks\nspatial_param = read_json(\"cal_aoi_v2.geojson\") \n\n# compute nbr for pre and post datacube\npre_nbr = before_cube.chunk_polygon(chunks=spatial_param,process=udf_process)\npost_nbr = after_cube.chunk_polygon(chunks=spatial_param,process=udf_process)\n\nFurthermore, since we loaded our collection for specific time intervals, it can include multiple time dimensions. Thus reduce_dimension applies a reducer to a data cube dimension by collapsing all the pixel values along the time dimension into an output value computed by the reducer.\n\n# perform time dimension reduction\npre_n = pre_nbr.reduce_dimension(dimension=\"t\", reducer=\"mean\")\npost_n = post_nbr.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n\n# find the difference between pre and post image\nsub = post_n-pre_n\n\nOnce the process is completed, you can also save it as your process using save_user_defined_process that can later be used for a similar task. Otherwise, you can download the result either by direct download (in case of the small spatial extent with few processing) or perform create a batch job in case it is a heavy task over a large extent.\n\n#download your output\nsub.download(\"sub_nbr_udf.tiff\")\n\n\n#set colors for plotting and classes based on UN SPIDER recommended practices\nimport matplotlib\nimg = rasterio.open(\"sub_nbr_udf.tiff\").read()\ncmap = matplotlib.colors.ListedColormap(['green','yellow','orange','red','purple'])\nbounds = [-0.5, 0.1, 0.27, 0.440, 0.660, 1.3] \nnorm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\ncmap.set_over('purple')\ncmap.set_under('white')\nfig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'xticks': [], 'yticks': []})\ncax = ax.imshow(img[0], cmap=cmap,norm=norm)\nplt.title('Burn Severity Map')\ncbar = fig.colorbar(cax, ax=ax, fraction=0.035, pad=0.04, ticks=bounds)\ncbar.ax.set_yticklabels(['Unburned', 'Low Severity', 'Moderate-low Severity', 'Moderate-high Severity', 'High Severity'])\nplt.show()\n\n\n\n\nThe bound set for the legend are based on the description provided in the UN SPIDER guideline."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/GlobalFloodMonitoring/Global_Flood_Monitoring.html",
    "href": "APIs/openEO/openeo-community-examples/python/GlobalFloodMonitoring/Global_Flood_Monitoring.html",
    "title": "openEO Flood Monitoring - Pakistan flooding of 2022",
    "section": "",
    "text": "The flooding in Pakistan in 2022 was a devastating natural disaster that affected millions of people across the country. Heavy monsoon rains led to overflowing rivers, flash floods, and widespread destruction of homes, infrastructure, and agricultural land. It is referred to as the worst flooding in the history of Pakistan. The disaster highlighted the vulnerability of Pakistan’s population to extreme weather events and underscored the need for improved disaster preparedness and climate resilience strategies in the country.\nSatellite data plays a crucial role in monitoring and understanding the impact of flooding events. It can be used to create accurate maps showing the extent of flooded areas. This information is essential for identifying affected regions, assessing the scale of the disaster, and planning rescue and relief operations.\nIn this notebook, we explore the Global Flood Mornitoring product to get a first overview of the flooding in the area around Digri Tehsil. We combine the flood extent data with the Global Human Settlement Built-up layer to get an estimate of the affected population in the region."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/GlobalFloodMonitoring/Global_Flood_Monitoring.html#gfm",
    "href": "APIs/openEO/openeo-community-examples/python/GlobalFloodMonitoring/Global_Flood_Monitoring.html#gfm",
    "title": "openEO Flood Monitoring - Pakistan flooding of 2022",
    "section": "GFM",
    "text": "GFM\nThe Global Flood Monitoring (GFM) product is a component of the EU’s Copernicus Emergency Management Service (CEMS) that provides continuous monitoring of floods worldwide, by processing and analysing in near real-time all incoming Sentinel-1 SAR acquisitions over land.\nThe operational implementation the GFM product includes the following key elements: - Downloading of worldwide Sentinel-1 SAR acquisitions (Level-1 IW GRDH) - Pre-processing of the downloaded Sentinel-1 data to backscatter data (SIG0) - Operational application of three fully automated flood mapping algorithms. - An ensemble-based approach is then used to combine the three flood extent outputs of the individual flood algorithms. - Generation of the required GFM output layers, including Observed flood extent, Reference water mask, Exclusion Mask and Likelihood Values - Web service-based access and dissemination of the GFM product output layers\n\nOutput layers used in this notebook\n\nObserved flood extent (ENSEMBLE of all three individual flood outputs)\nReference water mask (permament and seasonal water bodies)\n\n\n\nLinks\nhttps://extwiki.eodc.eu/GFM\n\n\nConnect and authenticate to openEO\n\nimport openeo\nfrom openeo.processes import *\n\nconn = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n\n\nDescribe the GFM collection\n\nconn.describe_collection(\"GFM\")\n\n\n    \n    \n        \n    \n    \n\n\n\n\nTemporal sum of flooded pixels\nIn this example, we have a closer look at an area in Pakistan, which was revaged by the unprecedented floods of 2022. We compute the sum of flooded pixels over time.\n\nspatial_extent  = {'west': 67.5, 'east': 70, 'south': 24.5, 'north': 26}\ntemporal_extent = [\"2022-09-01\", \"2022-10-01\"] \ncollection      = 'GFM'\n\ngfm_data = conn.load_collection(\n    collection, \n    spatial_extent=spatial_extent, \n    temporal_extent=temporal_extent, \n    bands = [\"flood_extent\"]\n)\ngfm_sum = gfm_data.reduce_dimension(dimension=\"t\", reducer=sum)\n\ngfm_sum_tiff = gfm_sum.save_result(format=\"GTiff\", options={\"tile_grid\": \"wgs84-1degree\"})\n\n\njob = gfm_sum_tiff.create_job(title = \"UC11\").start_job()\n\n\njob.status()\n\n'finished'\n\n\n\njob.get_results().download_files(\"./gfm/flood_extent_wgs/\")\n\n[PosixPath('gfm/flood_extent_wgs/WGS84_E67N23_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E67N24_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E67N25_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E67N26_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E68N23_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E68N24_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E68N25_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E68N26_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E69N23_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E69N24_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E69N25_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E69N26_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E70N23_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E70N24_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E70N25_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/WGS84_E70N26_20220901T010907.tif'),\n PosixPath('gfm/flood_extent_wgs/job-results.json')]\n\n\n\n\nLoad the downloaded result files\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xarray as xr\n\n\npath = \"./gfm/flood_extent_wgs/WGS84_E69N25_20220901T010907.tif\"\nflood_extent = xr.open_dataset(path).band_data\n\n\n\nExplore how the flood extent relates to the Global Human Settlement Built-up layer\nWe display the flood extent next to the Global Human Settlement Built-up layer.\nThe Global Human Settlement Layer (GHSL) project produces global spatial information about the human presence on the planet over time in the form of built-up maps, population density maps and settlement maps.\nHere, the GHS-BUILT-S spatial raster dataset at 10m resolution is used which depicts the distribution of built-up surfaces, expressed as number of square metres.\nValues are between 0 and 100 and represent the amount of square metres of built-up surface in the cell.\nThe data was downloaded from: https://ghsl.jrc.ec.europa.eu/about.php\n\nghsl = xr.open_dataarray(\"ghsl.nc\")\ndisplay(ghsl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'built' (y: 750, x: 500)&gt;\n[375000 values with dtype=int8]\nCoordinates:\n    band     int64 ...\n  * x        (x) float64 69.1 69.1 69.1 69.1 69.1 ... 69.14 69.14 69.14 69.14\n  * y        (y) float64 25.2 25.2 25.2 25.2 25.2 ... 25.14 25.14 25.14 25.14\nAttributes:\n    AREA_OR_POINT:  Area\n    grid_mapping:   spatial_refxarray.DataArray'built'y: 750x: 500...[375000 values with dtype=int8]Coordinates: (3)band()int64...[1 values with dtype=int64]x(x)float6469.1 69.1 69.1 ... 69.14 69.14axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([69.100027, 69.100107, 69.100187, ..., 69.139774, 69.139854, 69.139934])y(y)float6425.2 25.2 25.2 ... 25.14 25.14axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([25.199972, 25.199892, 25.199812, ..., 25.140232, 25.140152, 25.140072])Indexes: (2)xPandasIndexPandasIndex(Index([69.10002710430724, 69.10010707836112,   69.100187052415,\n       69.10026702646887, 69.10034700052275, 69.10042697457662,\n       69.10050694863048, 69.10058692268436, 69.10066689673823,\n       69.10074687079211,\n       ...\n       69.13921439070573,  69.1392943647596, 69.13937433881348,\n       69.13945431286736, 69.13953428692123, 69.13961426097511,\n       69.13969423502897, 69.13977420908284, 69.13985418313672,\n        69.1399341571906],\n      dtype='float64', name='x', length=500))yPandasIndexPandasIndex(Index([25.199972280901584,  25.19989230684771, 25.199812332793833,\n       25.199732358739958, 25.199652384686082,  25.19957241063221,\n       25.199492436578335,  25.19941246252446, 25.199332488470585,\n       25.199252514416713,\n       ...\n       25.140791481034476, 25.140711506980605,  25.14063153292673,\n       25.140551558872854, 25.140471584818982, 25.140391610765107,\n       25.140311636711232, 25.140231662657357,  25.14015168860348,\n       25.140071714549606],\n      dtype='float64', name='y', length=750))Attributes: (2)AREA_OR_POINT :Areagrid_mapping :spatial_ref\n\n\n\nmin_lat, max_lat = np.min(ghsl.y.values), np.max(ghsl.y.values)\nmin_lon, max_lon = np.min(ghsl.x.values), np.max(ghsl.x.values)\n\nflood_extent = flood_extent.sel(x=slice(min_lon, max_lon), y=slice(max_lat, min_lat), band = 1)\nflood_extent = xr.where(flood_extent == 0, np.nan, flood_extent)\nflood_extent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'band_data' (y: 333, x: 222)&gt;\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\nCoordinates:\n    band         int64 1\n  * x            (x) float64 69.1 69.1 69.1 69.1 ... 69.14 69.14 69.14 69.14\n  * y            (y) float64 25.2 25.2 25.2 25.2 ... 25.14 25.14 25.14 25.14\n    spatial_ref  int64 0xarray.DataArray'band_data'y: 333x: 222nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)Coordinates: (4)band()int641array(1)x(x)float6469.1 69.1 69.1 ... 69.14 69.14array([69.10011, 69.10029, 69.10047, ..., 69.13953, 69.13971, 69.13989])y(y)float6425.2 25.2 25.2 ... 25.14 25.14array([25.19991, 25.19973, 25.19955, ..., 25.14051, 25.14033, 25.14015])spatial_ref()int640array(0)Indexes: (2)xPandasIndexPandasIndex(Index([         69.10011,          69.10029,          69.10047,\n                69.10065,          69.10083,          69.10101,\n                69.10119,          69.10137,          69.10155,\n                69.10173,\n       ...\n       69.13826999999999, 69.13844999999999, 69.13862999999999,\n       69.13880999999999, 69.13898999999999,          69.13917,\n                69.13935,          69.13953,          69.13971,\n                69.13989],\n      dtype='float64', name='x', length=222))yPandasIndexPandasIndex(Index([25.199910000000003, 25.199730000000002, 25.199550000000002,\n       25.199370000000002,           25.19919,           25.19901,\n       25.198830000000005, 25.198650000000004, 25.198470000000004,\n       25.198290000000004,\n       ...\n       25.141770000000005, 25.141590000000004, 25.141410000000004,\n       25.141230000000004, 25.141050000000003, 25.140870000000003,\n       25.140690000000003, 25.140510000000003, 25.140330000000002,\n       25.140150000000002],\n      dtype='float64', name='y', length=333))Attributes: (0)\n\n\n\nplt.figure(figsize=(14,10))\nplt.title(\"Global Human Settlement - Flood Extent\", fontsize=15)\nX, Y = np.meshgrid(ghsl.x.values, ghsl.y.values)\ng = plt.contourf(X, Y, ghsl,cmap='YlOrRd', levels=10)\nplt.colorbar(label=\"GHSL\")\nX, Y = np.meshgrid(flood_extent.x.values, flood_extent.y.values)\nf = plt.contourf(X, Y, flood_extent,cmap='Blues', levels=5)\nplt.colorbar(label=\"Flood extent\")\n\n&lt;matplotlib.colorbar.Colorbar at 0x7fee14dfdca0&gt;\n\n\n\n\n\nEstimate of how the built-up surface was effected by the flood in Pakistan in September 2022. Some of the highest values of the GHSL can be found around 25.16 N 69.11 E, which marks Digri Tehsil, the second largest town of Mirpurkhas District, Pakistan. The sum over the temporal extent shows the areas that were affected the most.\n\n\nObserved water (flood_extent + refwater)\nThe observed water combines both flood extent and the reference water mask. The reference water mask represents permanent or seasonal water bodies, which are clearly distinct from flood events.\n\nspatial_extent  = {'west': 67.5, 'east': 70, 'south': 24.5, 'north': 26}\ntemporal_extent = [\"2022-09-01\", \"2022-10-01\"] \ncollection      = 'GFM'\n\ngfm_data = conn.load_collection(\n    collection, \n    spatial_extent=spatial_extent, \n    temporal_extent=temporal_extent, \n    bands = [\"flood_extent\", \"refwater\"]\n)\n\n# retrieve all pixels which have been detected as water during the given period\n# -&gt; observed water\nobserved_water = gfm_data.reduce_dimension(dimension=\"bands\", reducer=any).reduce_dimension(dimension=\"t\", reducer=any)\n\n# Save the result in Equi7Grid and as GeoTiff\nobserved_water_tif = observed_water.save_result(format=\"NetCDF\", options={\"tile_grid\": \"equi7\"})\n\nopenEO allows us to choose a tile grid, which matches the coordinate reference system. The original crs of the dataset is the Equi7, so we store our results accordingly.\n\njob = observed_water_tif.create_job(title = \"UC11\").start_job()\n\n\njob.status()\n\n'finished'\n\n\n\njob.get_results().download_files(\"./gfm/observed_water/\")\n\n[PosixPath('gfm/observed_water/AS020M_E015N024T3_20220901T010907.nc'),\n PosixPath('gfm/observed_water/AS020M_E015N027T3_20220901T010907.nc'),\n PosixPath('gfm/observed_water/AS020M_E018N024T3_20220901T010907.nc'),\n PosixPath('gfm/observed_water/AS020M_E018N027T3_20220901T010907.nc'),\n PosixPath('gfm/observed_water/job-results.json')]\n\n\n\npath = \"./gfm/observed_water/\"\nfiles = [path + file for file in os.listdir(path) if file.startswith(\"AS\")]\n# we expect only 0 and 1 -&gt; bool\ndata = xr.open_mfdataset(files).name.astype(\"bool\")\ndisplay(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'name' (y: 11879, x: 14489)&gt;\ndask.array&lt;astype, shape=(11879, 14489), dtype=bool, chunksize=(9506, 8967), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * y            (y) float64 2.89e+06 2.89e+06 2.89e+06 ... 2.653e+06 2.653e+06\n  * x            (x) float64 1.621e+06 1.621e+06 1.621e+06 ... 1.91e+06 1.91e+06\n    time         datetime64[ns] 2022-09-01T01:09:07\n    spatial_ref  int64 0\nAttributes:\n    nodata:                        -9999\n    filepaths:                     []\n    snapshot_STAC_collection_URL:  https://dev.stac.eodc.eu/api/v1/collection...\n    gfm:                           https://extwiki.eodc.eu/GFM\n    crs:                           PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS...xarray.DataArray'name'y: 11879x: 14489dask.array&lt;chunksize=(9506, 8967), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n164.14 MiB\n81.29 MiB\n\n\nShape\n(11879, 14489)\n(9506, 8967)\n\n\nCount\n20 Tasks\n4 Chunks\n\n\nType\nbool\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (4)y(y)float642.89e+06 2.89e+06 ... 2.653e+06units :metreresolution :-20.0crs :PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"false_easting\",4340913.84808],PARAMETER[\"false_northing\",4812712.92347],PARAMETER[\"longitude_of_center\",94.0],PARAMETER[\"latitude_of_center\",47.0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]array([2890110., 2890090., 2890070., ..., 2652590., 2652570., 2652550.])x(x)float641.621e+06 1.621e+06 ... 1.91e+06units :metreresolution :20.0crs :PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"false_easting\",4340913.84808],PARAMETER[\"false_northing\",4812712.92347],PARAMETER[\"longitude_of_center\",94.0],PARAMETER[\"latitude_of_center\",47.0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]array([1620670., 1620690., 1620710., ..., 1910390., 1910410., 1910430.])time()datetime64[ns]2022-09-01T01:09:07array('2022-09-01T01:09:07.000000000', dtype='datetime64[ns]')spatial_ref()int640inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84false_easting :4340913.84808projected_crs_name :Azimuthal_Equidistanthorizontal_datum_name :World Geodetic System 1984false_northing :4812712.92347semi_major_axis :6378137.0latitude_of_projection_origin :47.0prime_meridian_name :Greenwichlongitude_of_projection_origin :94.0GeoTransform :1500000.0 20.0 0.0 3000000.0 0.0 -20.0semi_minor_axis :6356752.314245179longitude_of_prime_meridian :0.0geographic_crs_name :WGS 84spatial_ref :PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"false_easting\",4340913.84808],PARAMETER[\"false_northing\",4812712.92347],PARAMETER[\"longitude_of_center\",94],PARAMETER[\"latitude_of_center\",47],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]grid_mapping_name :azimuthal_equidistantcrs_wkt :PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"false_easting\",4340913.84808],PARAMETER[\"false_northing\",4812712.92347],PARAMETER[\"longitude_of_center\",94],PARAMETER[\"latitude_of_center\",47],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]array(0)Indexes: (2)yPandasIndexPandasIndex(Index([2890110.0, 2890090.0, 2890070.0, 2890050.0, 2890030.0, 2890010.0,\n       2889990.0, 2889970.0, 2889950.0, 2889930.0,\n       ...\n       2652730.0, 2652710.0, 2652690.0, 2652670.0, 2652650.0, 2652630.0,\n       2652610.0, 2652590.0, 2652570.0, 2652550.0],\n      dtype='float64', name='y', length=11879))xPandasIndexPandasIndex(Index([1620670.0, 1620690.0, 1620710.0, 1620730.0, 1620750.0, 1620770.0,\n       1620790.0, 1620810.0, 1620830.0, 1620850.0,\n       ...\n       1910250.0, 1910270.0, 1910290.0, 1910310.0, 1910330.0, 1910350.0,\n       1910370.0, 1910390.0, 1910410.0, 1910430.0],\n      dtype='float64', name='x', length=14489))Attributes: (5)nodata :-9999filepaths :[]snapshot_STAC_collection_URL :https://dev.stac.eodc.eu/api/v1/collections/GFMgfm :https://extwiki.eodc.eu/GFMcrs :PROJCS[\"Azimuthal_Equidistant\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Azimuthal_Equidistant\"],PARAMETER[\"false_easting\",4340913.84808],PARAMETER[\"false_northing\",4812712.92347],PARAMETER[\"longitude_of_center\",94],PARAMETER[\"latitude_of_center\",47],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n\n\n\ndata.x.values, data.y.values\n\n(array([1620670., 1620690., 1620710., ..., 1910390., 1910410., 1910430.]),\n array([2890110., 2890090., 2890070., ..., 2652590., 2652570., 2652550.]))\n\n\n\nplt.figure(figsize=(10,10))\nplt.title(\"Observed water\", fontsize=15)\nd_small = data.sel(x=slice(1700000,1900000), y=slice(2800000,2700000))\nX, Y = np.meshgrid(d_small.x.values, d_small.y.values)\nplt.contourf(X, Y, d_small, cmap='Blues')\n\n&lt;matplotlib.contour.QuadContourSet at 0x7fee2ca5a4f0&gt;\n\n\n\n\n\nThe notebook gives an example of how to use the GFM dataset in openEO to explore the flooded areas of Pakistan in 2022. Based on the notebook, further processing can be done by adapting or expanding the spatio-temporal requests."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/AirQuality/AirQuality.html",
    "href": "APIs/openEO/openeo-community-examples/python/AirQuality/AirQuality.html",
    "title": "Explore Sentinel-5P products with openEO",
    "section": "",
    "text": "Sentinel-5P is the first Copernicus mission to monitor the atmosphere and global air quality. It conducts precise atmospheric measurements to support air quality assessments, ozone and UV radiation monitoring, as well as climate forecasting.\nFor those interested in Sentinel-5P offerings, openEO provides a convenient means to retrieve these products without the hassle of dealing with file locations or downloading entire collections. Users can effortlessly access the products relevant to their specified area and timeframe with the openEO API. Moreover, users can conduct analyses on these products through built-in processes.\nThus, within this notebook, we illustrate the application of openEO for accessing Sentinel-5P products for comprehensive analysis. While we won’t delve into detailed analyses and possibilities in this notebook, we aim to demonstrate the fundamental tasks that users interested in atmospheric monitoring can perform using openEO.\nReference: * https://sentinels.copernicus.eu/web/sentinel/missions/sentinel-5p * https://www.sciencedirect.com/science/article/pii/S1352231097004457\n\nimport openeo\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# authenticate your account\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nLet’s have a quick understanding of the available SENTINEL_5P_L2 accessed through openEO in the Copernicus Data Space Ecosystem.\n\nconnection.describe_collection(\"SENTINEL_5P_L2\")\n\n\n    \n    \n        \n    \n    \n\n\nIn this scenario, we’ve selected a bounding box covering Belgium for 2020. We chose Belgium purely out of interest, but users can pick any geographical area or time frame they desire for their analysis.\n\n# define area and time of interest\nextent = {\"west\":2.146728, \"south\": 49.446978, \"east\": 6.497314, \"north\": 51.651837}\ntime = [\"2020-01-01\", \"2021-01-01\"]\n\nNext, we create a function called fetch_collection to retrieve a list of datacubes. Each datacube in the list represents an individual air quality product from the Sentinel-5P L2 dataset. These datacubes are then combined using the merge_cubes process to generate a single cube containing all these products as a separate layer.\nPlease note that users can only load one product at a time, even if provided within the same Sentinel 5P collection.\n\ndef fetch_collection(bands, time, extent) -&gt; list:\n    cubes = []\n    \n    for band in bands:\n        datacube = connection.load_collection(\n            \"SENTINEL_5P_L2\",\n            temporal_extent=time,\n            spatial_extent=extent,\n            bands=[band]\n        )\n        cubes.append(datacube)\n\n    return cubes\n\n\nband=[\"AER_AI_340_380\", \"AER_AI_354_388\", \"CO\", \"HCHO\", \"NO2\", \"O3\", \"SO2\", \"CH4\"]\n\ncubes = fetch_collection(band,time,extent)\nmerged_cube = cubes[0]\nfor cube in cubes[1:]:\n    merged_cube = merged_cube.merge_cubes(cube)\n\nAs an additional step in this workflow, we apply the aggregate_temporal_period process available in openEO, which combines data over a specified time period. In this case, we have set the period to dekad, meaning a 10-day period.\n\nsimplified_cube = merged_cube.aggregate_temporal_period(reducer=\"mean\", period=\"dekad\")\n\nSince the workflow covers an entire year with eight different bands over Belgium, we assume that the processing might be computationally intensive. Therefore, we should leverage the job configuration capabilities provided in openEO to execute the workflow using batch job-based methods.\n\njob_options = {\n        \"executor-memory\": \"3G\",\n        \"executor-memoryOverhead\": \"4G\",\n        \"executor-cores\": \"2\"\n}\n\n\n# execute the process\nsimplified_cube.execute_batch(title=\"Sentinel-5P Belgium\",outputfile=\"Sentinel5P_Belgium.nc\", job_options=job_options)\n\n\nNow, let’s analyse the output.\n\n#load the saved file\ndataset = xr.load_dataset(\"Sentinel5P_Belgium.nc\")\n\nLet us create a map showing the average values of each variable across time. These maps can provide insights into how the variables behave within the specified area of interest.\n\nband=[\"AER_AI_340_380\", \"AER_AI_354_388\", \"CO\", \"HCHO\", \"NO2\", \"O3\", \"SO2\", \"CH4\"]\nunits = [\"\", \"\", \"(mol/m$^{2}$)\", \"(mol/m$^{2}$)\", \"(mol/m$^{2}$)\", \"(mol/m$^{2}$)\", \"(mol/m$^{2}$)\", \"(ppb)\"]\nnrows = 2\nncols = 4\n\nfig, axes = plt.subplots(2,4, figsize=(15, 8), dpi=100, sharey=True)\n\nfor idx, (i, u) in enumerate(zip(band, units)):\n    row = idx // ncols\n    col = idx % ncols\n    \n    ds = dataset[[i]].to_array(dim=\"bands\")\n    ds.mean(dim=\"t\")[0].plot.imshow(ax=axes[row, col])\n    axes[row, col].set_title(\"mean \" + i  + u )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nAdditionally, to understand the trend of these variables over time, let us calculate the mean of the images and analyze it for selected timestamps by plotting the variables across time intervals.\n\n#calculate mean\ndataset_mean = dataset[band].mean(dim=['x','y'])\n\n\nnrows = 2\nncols = 4\n\nfig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 8), dpi=100, sharex=True)\n\nfor idx, (i, unit) in enumerate(zip(band, units)):\n    row = idx // ncols\n    col = idx % ncols\n    xr.plot.line(dataset_mean[i], ax=axs[row][col]) \n    axs[row][col].tick_params(axis='x', labelrotation=45)\n    axs[row][col].set_ylabel(i + unit )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFrom the plot above, we can observe the air quality trends of Belgium for the year 2020. There is a noticeable increase in CO and NO2 levels at the beginning of the year, followed by a decrease during the lockdown months from May to September. Conversely, there is a decreasing trend in O3 concentration, and a similar pattern is observed for SO2 levels.\nGiven that these variables are present in the atmosphere, it’s reasonable to assume there could be some correlation among them. An overview of the correlation could help in future research and workflows.\n\n# generate pandas dataframe\ndf = dataset.to_dataframe()\n\n#correlation matrix\ncorr = df[band].corr()\n\n# mask the same comparison twice for better visualisation\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\ncorr[mask] = np.nan\n(corr\n .style\n .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n .highlight_null(color='#f1f1f1') \n .format(precision=2))\n\n\n\n\n\n\n \nAER_AI_340_380\nAER_AI_354_388\nCO\nHCHO\nNO2\nO3\nSO2\nCH4\n\n\n\n\nAER_AI_340_380\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nAER_AI_354_388\n0.98\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nCO\n0.26\n0.30\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nHCHO\n-0.00\n0.00\n-0.12\nnan\nnan\nnan\nnan\nnan\n\n\nNO2\n0.20\n0.22\n0.38\n-0.01\nnan\nnan\nnan\nnan\n\n\nO3\n-0.24\n-0.22\n0.29\n-0.11\n-0.02\nnan\nnan\nnan\n\n\nSO2\n0.15\n0.15\n0.08\n-0.11\n0.11\n-0.05\nnan\nnan\n\n\nCH4\n0.11\n0.14\n-0.02\n-0.00\n0.36\n-0.30\n0.12\nnan\n\n\n\n\n\nAs observed in the plot above, no significant or strong correlation exists between products except the two air quality indexes. However, a slight positive correlation exists between CH4, CO, and NO2.\nPlease note that this correlation coefficient is calculated independently for a specific region, and no spatio-temporal or seasonal factors are considered in this analysis. This example illustrates potential correlations, and users can use any correlation matrix generation tool they prefer."
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html",
    "title": "Rank composites",
    "section": "",
    "text": "Optical satellite imagery contains gaps due to clouds, and the observation scenario. Many methods rely on having gap-free data available at regular time intervals. The most common technique to achieve this is to combine pixels from different observations, which is also referred to as compositing.\nVarious compositing approaches exist, in this notebook, we demonstrate ‘rank composites’ or more specifically the ‘max NDVI’ composite. A rank composite uses a single ‘rank band’ to decide if pixels of other bands are included in the composite. The advantage of rank composites over compositing per band is that the spectral signal represented by the different bands has been observed in reality in a single observation, and is not a combination of spectral values that occurred at different points in time.\nThis method is used in various peer reviewed publications, and some of its properties have been validated based on specific sensors: https://www.tandfonline.com/doi/abs/10.1080/01431168608948945\nIn this case the ‘rank band’ is a simple NDVI, which we ‘score’ based on the maximum value. The rank band can also be a combination of values leading to a more complex score, such as distance to cloud and observation angles. This variant is called a ‘best available pixel’ composite. (https://doi.org/10.1080/07038992.2014.945827)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html#openeo-implementation",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html#openeo-implementation",
    "title": "Rank composites",
    "section": "openEO implementation",
    "text": "openEO implementation\nThe steps to implement this method in openEO are relatively simple, but may be different from the steps in a ‘traditional’ programming language:\n\nWe load and compute the rank band separately\nThe rank band is converted into a mask, retaining only pixels that we want to select\nA datacube with raw bands is loaded, and the rank band mask is applied to it\naggregate_temporal(_period) is used to create a composite at regular intervals if needed\n\nMost methods require composites for multiple time periods as input. For instance, one composite per month, or every 10 days. We can compute these in one process graph, using apply_neighborhood, so that the result is also an immediate input for further processing.\n\nspatial_extent = {'west': 4.45, 'east': 4.50, 'south': 51.16, 'north': 51.17, 'crs': 'epsg:4326'}\n\n\nimport openeo\nimport xarray\nimport numpy as np\nimport io\nimport requests\n\nimport panel as pn\n\nimport pyproj\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n%matplotlib inline\n\n\nc=openeo.connect(\"openeo.dataspace.copernicus.eu\")\nc.authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n&lt;Connection to 'https://openeocloud.vito.be/openeo/1.0.0/' with OidcBearerAuth&gt;\n\n\nWe first create a binary cloud mask, as we don’t want to consider clouded pixels. This is also a good way to avoid loading too much data, which is costly.\n\nscl = c.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent = [\"2022-06-04\", \"2022-08-01\"],\n    bands = [\"SCL\"],\n    max_cloud_cover=95\n)\n\ncloud_mask = scl.process(\n    \"to_scl_dilation_mask\",\n    data=scl,\n    kernel1_size=17, kernel2_size=77,\n    mask1_values=[2, 4, 5, 6, 7],\n    mask2_values=[3, 8, 9, 10, 11],\n    erosion_kernel_size=3)\n\n\n/home/driesj/python/openeo-python-client/openeo/rest/connection.py:1133: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): eo:cloud_cover.\n  return DataCube.load_collection(\n\n\nNow we load the bands required to compute NDVI, apply the cloud mask, and compute NDVI. The NDVI will be our ‘rank band’ in this example.\n\nndvi_bands = c.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent = [\"2022-06-04\", \"2022-08-01\"],\n    bands = [\"B04\", \"B08\", \"SCL\"],\n    max_cloud_cover=95\n)\n\nndvi_bands = ndvi_bands.mask(cloud_mask)\n\nndvi = ndvi_bands.ndvi(nir=\"B08\",red=\"B04\")\n\nThe next step is the most difficult one, and constructs the final mask that will be used to load the full datacube, but with only the observations where the NDVI is equal to the maximum.\nWe first create a function that computes the maximum NDVI from a series of values, and then loops again over those values to set values to 1 if they are equal to the maximum, and zero otherwise.\nThe apply_neighborhood process is used here to define the groups of NDVI values on which we want to run this function. We want to create multiple monthly max-NDVI composites, so we specify that the size of groups along the time dimension should correspond to 1 month. Apply_neighborhood is one of the more complex processes, but once you understand it, you’ll notice it’s quite versatile and useful.\n\ndef max_ndvi_selection(ndvi):\n    max_ndvi = ndvi.max()\n    return ndvi.array_apply(lambda x:x!=max_ndvi)\n\nrank_mask = ndvi.apply_neighborhood(\n        max_ndvi_selection,\n        size=[{'dimension': 'x', 'unit': 'px', 'value': 1}, {'dimension': 'y', 'unit': 'px', 'value': 1},\n              {'dimension': 't', 'value': \"month\"}],\n        overlap=[]\n    )\n\nAt this point, we download our mask for inspection. This is just an intermediate result, and is not needed in a real use case.\n\nrank_mask.filter_bbox(spatial_extent).execute_batch(\"the_mask.nc\")\n\n0:00:00 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': send 'start'\n0:00:20 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:00:28 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:00:35 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:00:43 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:00:53 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:01:06 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:01:22 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:01:41 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:02:06 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': queued (progress N/A)\n0:02:36 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': running (progress N/A)\n0:03:14 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': running (progress N/A)\n0:04:01 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': running (progress N/A)\n0:05:00 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': running (progress N/A)\n0:06:01 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': running (progress N/A)\n0:07:02 Job 'vito-j-ca99a93e1d7641d1b4e7a4714d7cfe90': finished (progress N/A)\n\n\n\n    \n    \n        \n    \n    \n\n\n\nmask_ds = xarray.open_dataset('the_mask.nc')\nmask_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (t: 12, x: 352, y: 119)\nCoordinates:\n  * t        (t) datetime64[ns] 2022-06-04 2022-06-11 ... 2022-07-24 2022-07-29\n  * x        (x) float64 6.014e+05 6.014e+05 6.014e+05 ... 6.049e+05 6.049e+05\n  * y        (y) float64 5.67e+06 5.67e+06 5.67e+06 ... 5.669e+06 5.669e+06\nData variables:\n    crs      |S1 ...\n    var      (t, y, x) uint8 ...\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platform - Geotrellis backend: 0.16.2a1\n    description:  \n    title:        xarray.DatasetDimensions:t: 12x: 352y: 119Coordinates: (3)t(t)datetime64[ns]2022-06-04 ... 2022-07-29standard_name :tlong_name :taxis :Tarray(['2022-06-04T00:00:00.000000000', '2022-06-11T00:00:00.000000000',\n       '2022-06-14T00:00:00.000000000', '2022-06-16T00:00:00.000000000',\n       '2022-06-29T00:00:00.000000000', '2022-07-01T00:00:00.000000000',\n       '2022-07-04T00:00:00.000000000', '2022-07-11T00:00:00.000000000',\n       '2022-07-16T00:00:00.000000000', '2022-07-19T00:00:00.000000000',\n       '2022-07-24T00:00:00.000000000', '2022-07-29T00:00:00.000000000'],\n      dtype='datetime64[ns]')x(x)float646.014e+05 6.014e+05 ... 6.049e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([601375., 601385., 601395., ..., 604865., 604875., 604885.])y(y)float645.67e+06 5.67e+06 ... 5.669e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5669795., 5669785., 5669775., 5669765., 5669755., 5669745., 5669735.,\n       5669725., 5669715., 5669705., 5669695., 5669685., 5669675., 5669665.,\n       5669655., 5669645., 5669635., 5669625., 5669615., 5669605., 5669595.,\n       5669585., 5669575., 5669565., 5669555., 5669545., 5669535., 5669525.,\n       5669515., 5669505., 5669495., 5669485., 5669475., 5669465., 5669455.,\n       5669445., 5669435., 5669425., 5669415., 5669405., 5669395., 5669385.,\n       5669375., 5669365., 5669355., 5669345., 5669335., 5669325., 5669315.,\n       5669305., 5669295., 5669285., 5669275., 5669265., 5669255., 5669245.,\n       5669235., 5669225., 5669215., 5669205., 5669195., 5669185., 5669175.,\n       5669165., 5669155., 5669145., 5669135., 5669125., 5669115., 5669105.,\n       5669095., 5669085., 5669075., 5669065., 5669055., 5669045., 5669035.,\n       5669025., 5669015., 5669005., 5668995., 5668985., 5668975., 5668965.,\n       5668955., 5668945., 5668935., 5668925., 5668915., 5668905., 5668895.,\n       5668885., 5668875., 5668865., 5668855., 5668845., 5668835., 5668825.,\n       5668815., 5668805., 5668795., 5668785., 5668775., 5668765., 5668755.,\n       5668745., 5668735., 5668725., 5668715., 5668705., 5668695., 5668685.,\n       5668675., 5668665., 5668655., 5668645., 5668635., 5668625., 5668615.])Data variables: (2)crs()|S1...crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32631\"]]spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32631\"]][1 values with dtype=|S1]var(t, y, x)uint8...long_name :varunits :grid_mapping :crs[502656 values with dtype=uint8]Indexes: (3)tPandasIndexPandasIndex(DatetimeIndex(['2022-06-04', '2022-06-11', '2022-06-14', '2022-06-16',\n               '2022-06-29', '2022-07-01', '2022-07-04', '2022-07-11',\n               '2022-07-16', '2022-07-19', '2022-07-24', '2022-07-29'],\n              dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Float64Index([601375.0, 601385.0, 601395.0, 601405.0, 601415.0, 601425.0,\n              601435.0, 601445.0, 601455.0, 601465.0,\n              ...\n              604795.0, 604805.0, 604815.0, 604825.0, 604835.0, 604845.0,\n              604855.0, 604865.0, 604875.0, 604885.0],\n             dtype='float64', name='x', length=352))yPandasIndexPandasIndex(Float64Index([5669795.0, 5669785.0, 5669775.0, 5669765.0, 5669755.0, 5669745.0,\n              5669735.0, 5669725.0, 5669715.0, 5669705.0,\n              ...\n              5668705.0, 5668695.0, 5668685.0, 5668675.0, 5668665.0, 5668655.0,\n              5668645.0, 5668635.0, 5668625.0, 5668615.0],\n             dtype='float64', name='y', length=119))Attributes: (4)Conventions :CF-1.9institution :openEO platform - Geotrellis backend: 0.16.2a1description :title :\n\n\nWe inspect the mask by filtering out nodata and plotting.\n\nmask_ds['var'] = mask_ds['var'].where(mask_ds['var']!=129)\nmask_ds['var'].plot(vmin=0,vmax=1,col=\"t\",col_wrap=4)\n\n&lt;xarray.plot.facetgrid.FacetGrid at 0x7f72f4674ed0&gt;"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html#creating-and-downloading-your-composite",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html#creating-and-downloading-your-composite",
    "title": "Rank composites",
    "section": "Creating and downloading your composite",
    "text": "Creating and downloading your composite\nNow it’s time to create and load the actual composite, which is very simply once a compositing mask has been created. It’s very important in this step to use the exact same ‘load_collection’ paratemers as were used to create the mask(s), to have a correct output. Note that we still use aggregate_temporal_period, which has 2 effects: - after masking, observations will still have their original dates, this process will generate equitemporal intervals. - In the case where multiple observations have the same NDVI value, the first observation will be retained.\n\nrgb_bands = c.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent = [\"2022-06-04\", \"2022-08-01\"],\n    bands = [\"B02\", \"B03\",\"B04\"],\n    max_cloud_cover=95\n)\n\ncomposite = rgb_bands.mask(rank_mask).aggregate_temporal_period(\"month\",\"first\")\n\ncomposite.filter_bbox(spatial_extent).execute_batch(\"composite.nc\")\n\n\ncomposite = xarray.open_dataset('composite.nc')\ncomposite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (t: 2, x: 352, y: 119)\nCoordinates:\n  * t        (t) datetime64[ns] 2022-06-01 2022-07-01\n  * x        (x) float64 6.014e+05 6.014e+05 6.014e+05 ... 6.049e+05 6.049e+05\n  * y        (y) float64 5.67e+06 5.67e+06 5.67e+06 ... 5.669e+06 5.669e+06\nData variables:\n    crs      |S1 ...\n    B02      (t, y, x) float32 ...\n    B03      (t, y, x) float32 ...\n    B04      (t, y, x) float32 ...\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platform - Geotrellis backend: 0.16.2a1\n    description:  \n    title:        xarray.DatasetDimensions:t: 2x: 352y: 119Coordinates: (3)t(t)datetime64[ns]2022-06-01 2022-07-01standard_name :tlong_name :taxis :Tarray(['2022-06-01T00:00:00.000000000', '2022-07-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')x(x)float646.014e+05 6.014e+05 ... 6.049e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([601375., 601385., 601395., ..., 604865., 604875., 604885.])y(y)float645.67e+06 5.67e+06 ... 5.669e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5669795., 5669785., 5669775., 5669765., 5669755., 5669745., 5669735.,\n       5669725., 5669715., 5669705., 5669695., 5669685., 5669675., 5669665.,\n       5669655., 5669645., 5669635., 5669625., 5669615., 5669605., 5669595.,\n       5669585., 5669575., 5669565., 5669555., 5669545., 5669535., 5669525.,\n       5669515., 5669505., 5669495., 5669485., 5669475., 5669465., 5669455.,\n       5669445., 5669435., 5669425., 5669415., 5669405., 5669395., 5669385.,\n       5669375., 5669365., 5669355., 5669345., 5669335., 5669325., 5669315.,\n       5669305., 5669295., 5669285., 5669275., 5669265., 5669255., 5669245.,\n       5669235., 5669225., 5669215., 5669205., 5669195., 5669185., 5669175.,\n       5669165., 5669155., 5669145., 5669135., 5669125., 5669115., 5669105.,\n       5669095., 5669085., 5669075., 5669065., 5669055., 5669045., 5669035.,\n       5669025., 5669015., 5669005., 5668995., 5668985., 5668975., 5668965.,\n       5668955., 5668945., 5668935., 5668925., 5668915., 5668905., 5668895.,\n       5668885., 5668875., 5668865., 5668855., 5668845., 5668835., 5668825.,\n       5668815., 5668805., 5668795., 5668785., 5668775., 5668765., 5668755.,\n       5668745., 5668735., 5668725., 5668715., 5668705., 5668695., 5668685.,\n       5668675., 5668665., 5668655., 5668645., 5668635., 5668625., 5668615.])Data variables: (4)crs()|S1...crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32631\"]]spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 3.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32631\"]][1 values with dtype=|S1]B02(t, y, x)float32...long_name :B02units :grid_mapping :crs[83776 values with dtype=float32]B03(t, y, x)float32...long_name :B03units :grid_mapping :crs[83776 values with dtype=float32]B04(t, y, x)float32...long_name :B04units :grid_mapping :crs[83776 values with dtype=float32]Indexes: (3)tPandasIndexPandasIndex(DatetimeIndex(['2022-06-01', '2022-07-01'], dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Float64Index([601375.0, 601385.0, 601395.0, 601405.0, 601415.0, 601425.0,\n              601435.0, 601445.0, 601455.0, 601465.0,\n              ...\n              604795.0, 604805.0, 604815.0, 604825.0, 604835.0, 604845.0,\n              604855.0, 604865.0, 604875.0, 604885.0],\n             dtype='float64', name='x', length=352))yPandasIndexPandasIndex(Float64Index([5669795.0, 5669785.0, 5669775.0, 5669765.0, 5669755.0, 5669745.0,\n              5669735.0, 5669725.0, 5669715.0, 5669705.0,\n              ...\n              5668705.0, 5668695.0, 5668685.0, 5668675.0, 5668665.0, 5668655.0,\n              5668645.0, 5668635.0, 5668625.0, 5668615.0],\n             dtype='float64', name='y', length=119))Attributes: (4)Conventions :CF-1.9institution :openEO platform - Geotrellis backend: 0.16.2a1description :title :\n\n\n\n\nrgb_array=composite.to_array(dim=\"bands\").sel(bands=[\"B04\",\"B03\",\"B02\"]).astype(np.float32)/10000\nrgb_array\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (bands: 3, t: 2, y: 119, x: 352)&gt;\narray([[[[0.1007, 0.1026, 0.0713, ..., 0.0566, 0.0427, 0.0598],\n         [0.0868, 0.101 , 0.0811, ..., 0.083 , 0.0714, 0.104 ],\n         [0.087 , 0.0872, 0.0858, ..., 0.13  , 0.097 , 0.1382],\n         ...,\n         [0.0996, 0.096 , 0.0677, ..., 0.0165, 0.0178, 0.0228],\n         [0.0599, 0.059 , 0.0776, ..., 0.015 , 0.0168, 0.023 ],\n         [0.0532, 0.0518, 0.0499, ..., 0.0145, 0.0156, 0.0192]],\n\n        [[   nan,    nan, 0.0833, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [0.1098,    nan,    nan, ...,    nan,    nan,    nan],\n         ...,\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan]]],\n\n\n       [[[0.0966, 0.0972, 0.0716, ..., 0.0665, 0.0536, 0.0714],\n         [0.0845, 0.0871, 0.0762, ..., 0.0773, 0.072 , 0.1106],\n         [0.083 , 0.083 , 0.0882, ..., 0.1206, 0.1004, 0.1616],\n...\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan]]],\n\n\n       [[[0.0732, 0.0776, 0.0601, ..., 0.0479, 0.0394, 0.0458],\n         [0.0676, 0.078 , 0.0606, ..., 0.0708, 0.0594, 0.0787],\n         [0.0746, 0.0784, 0.0802, ..., 0.1036, 0.0727, 0.145 ],\n         ...,\n         [0.0742, 0.0708, 0.0469, ..., 0.0196, 0.018 , 0.0236],\n         [0.0438, 0.0426, 0.0546, ..., 0.0188, 0.0182, 0.0208],\n         [0.0429, 0.0431, 0.0418, ..., 0.0179, 0.0188, 0.0206]],\n\n        [[   nan,    nan, 0.093 , ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [0.1146,    nan,    nan, ...,    nan,    nan,    nan],\n         ...,\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan]]]],\n      dtype=float32)\nCoordinates:\n  * t        (t) datetime64[ns] 2022-06-01 2022-07-01\n  * x        (x) float64 6.014e+05 6.014e+05 6.014e+05 ... 6.049e+05 6.049e+05\n  * y        (y) float64 5.67e+06 5.67e+06 5.67e+06 ... 5.669e+06 5.669e+06\n  * bands    (bands) object 'B04' 'B03' 'B02'xarray.DataArraybands: 3t: 2y: 119x: 3520.1007 0.1026 0.0713 0.0976 0.1158 0.0876 ... nan nan nan nan nan nanarray([[[[0.1007, 0.1026, 0.0713, ..., 0.0566, 0.0427, 0.0598],\n         [0.0868, 0.101 , 0.0811, ..., 0.083 , 0.0714, 0.104 ],\n         [0.087 , 0.0872, 0.0858, ..., 0.13  , 0.097 , 0.1382],\n         ...,\n         [0.0996, 0.096 , 0.0677, ..., 0.0165, 0.0178, 0.0228],\n         [0.0599, 0.059 , 0.0776, ..., 0.015 , 0.0168, 0.023 ],\n         [0.0532, 0.0518, 0.0499, ..., 0.0145, 0.0156, 0.0192]],\n\n        [[   nan,    nan, 0.0833, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [0.1098,    nan,    nan, ...,    nan,    nan,    nan],\n         ...,\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan]]],\n\n\n       [[[0.0966, 0.0972, 0.0716, ..., 0.0665, 0.0536, 0.0714],\n         [0.0845, 0.0871, 0.0762, ..., 0.0773, 0.072 , 0.1106],\n         [0.083 , 0.083 , 0.0882, ..., 0.1206, 0.1004, 0.1616],\n...\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan]]],\n\n\n       [[[0.0732, 0.0776, 0.0601, ..., 0.0479, 0.0394, 0.0458],\n         [0.0676, 0.078 , 0.0606, ..., 0.0708, 0.0594, 0.0787],\n         [0.0746, 0.0784, 0.0802, ..., 0.1036, 0.0727, 0.145 ],\n         ...,\n         [0.0742, 0.0708, 0.0469, ..., 0.0196, 0.018 , 0.0236],\n         [0.0438, 0.0426, 0.0546, ..., 0.0188, 0.0182, 0.0208],\n         [0.0429, 0.0431, 0.0418, ..., 0.0179, 0.0188, 0.0206]],\n\n        [[   nan,    nan, 0.093 , ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [0.1146,    nan,    nan, ...,    nan,    nan,    nan],\n         ...,\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n         [   nan,    nan,    nan, ...,    nan,    nan,    nan]]]],\n      dtype=float32)Coordinates: (4)t(t)datetime64[ns]2022-06-01 2022-07-01standard_name :tlong_name :taxis :Tarray(['2022-06-01T00:00:00.000000000', '2022-07-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')x(x)float646.014e+05 6.014e+05 ... 6.049e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([601375., 601385., 601395., ..., 604865., 604875., 604885.])y(y)float645.67e+06 5.67e+06 ... 5.669e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5669795., 5669785., 5669775., 5669765., 5669755., 5669745., 5669735.,\n       5669725., 5669715., 5669705., 5669695., 5669685., 5669675., 5669665.,\n       5669655., 5669645., 5669635., 5669625., 5669615., 5669605., 5669595.,\n       5669585., 5669575., 5669565., 5669555., 5669545., 5669535., 5669525.,\n       5669515., 5669505., 5669495., 5669485., 5669475., 5669465., 5669455.,\n       5669445., 5669435., 5669425., 5669415., 5669405., 5669395., 5669385.,\n       5669375., 5669365., 5669355., 5669345., 5669335., 5669325., 5669315.,\n       5669305., 5669295., 5669285., 5669275., 5669265., 5669255., 5669245.,\n       5669235., 5669225., 5669215., 5669205., 5669195., 5669185., 5669175.,\n       5669165., 5669155., 5669145., 5669135., 5669125., 5669115., 5669105.,\n       5669095., 5669085., 5669075., 5669065., 5669055., 5669045., 5669035.,\n       5669025., 5669015., 5669005., 5668995., 5668985., 5668975., 5668965.,\n       5668955., 5668945., 5668935., 5668925., 5668915., 5668905., 5668895.,\n       5668885., 5668875., 5668865., 5668855., 5668845., 5668835., 5668825.,\n       5668815., 5668805., 5668795., 5668785., 5668775., 5668765., 5668755.,\n       5668745., 5668735., 5668725., 5668715., 5668705., 5668695., 5668685.,\n       5668675., 5668665., 5668655., 5668645., 5668635., 5668625., 5668615.])bands(bands)object'B04' 'B03' 'B02'array(['B04', 'B03', 'B02'], dtype=object)Indexes: (4)tPandasIndexPandasIndex(DatetimeIndex(['2022-06-01', '2022-07-01'], dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Float64Index([601375.0, 601385.0, 601395.0, 601405.0, 601415.0, 601425.0,\n              601435.0, 601445.0, 601455.0, 601465.0,\n              ...\n              604795.0, 604805.0, 604815.0, 604825.0, 604835.0, 604845.0,\n              604855.0, 604865.0, 604875.0, 604885.0],\n             dtype='float64', name='x', length=352))yPandasIndexPandasIndex(Float64Index([5669795.0, 5669785.0, 5669775.0, 5669765.0, 5669755.0, 5669745.0,\n              5669735.0, 5669725.0, 5669715.0, 5669705.0,\n              ...\n              5668705.0, 5668695.0, 5668685.0, 5668675.0, 5668665.0, 5668655.0,\n              5668645.0, 5668635.0, 5668625.0, 5668615.0],\n             dtype='float64', name='y', length=119))bandsPandasIndexPandasIndex(Index(['B04', 'B03', 'B02'], dtype='object', name='bands'))Attributes: (0)"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html#final-result-conclusion",
    "href": "APIs/openEO/openeo-community-examples/python/RankComposites/rank_composites.html#final-result-conclusion",
    "title": "Rank composites",
    "section": "Final result & conclusion",
    "text": "Final result & conclusion\nWe finally plot composites generated for 2 months. While this notebook ends here, for most real world cases, this is the point where you can start your actual work, as you now have gap-free equitemporal composites. Note that in openEO, things happen on the fly, and this method is quite efficient, so there’s no need to actually store these intermediate composites somewhere before movin on, you can just take the composited data cube, and go from there!\n\nNote on performance\nThis approach was designed with efficiency in mind: we explicitly load a minimum amount of bands to construct the mask. As a result if observations are not used in the composite at all, they do not need to be loaded for the final result. The max-NDVI approach is however somewhat limited in this regard, because the plots of the mask show that often many chunks are needed for the final output.\nOne other remark is that it could be an option to construct the mask at lower resolution: this would further reduce data loading times, at the cost of a somewhat less accurate max-NDVI composite.\n\nxarray.plot.imshow(rgb_array.isel(t=0),vmin=0,vmax=0.18,rgb=\"bands\",col_wrap=2)\n\n&lt;matplotlib.image.AxesImage at 0x7f72af386890&gt;"
  },
  {
    "objectID": "APIs/openEO/openeo-community-examples/python/FloodNDWI/flood_ndwi.html",
    "href": "APIs/openEO/openeo-community-examples/python/FloodNDWI/flood_ndwi.html",
    "title": "Comparitive study of before and after flash flood in Cologne using NDWI service available in EOplaza",
    "section": "",
    "text": "In this notebook, we tried in performing comparative study between pre and post image for Cologne during 2021 flood. A simple technique to subtract pre and post image is done to know the change in water content due to flood in that region. Refernce: https://labo.obs-mip.fr/multitemp/the-ndwi-applied-to-the-recent-flooding-in-the-central-us/\n\n# import necessary packages\nimport openeo\nfrom openeo.api.process import Parameter\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport rasterio\nimport numpy as np\n\n# connect with the backend\neoconn = openeo.connect(\"openeo.vito.be\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\nUser can choose among different backend available here to connect to the backend. Rrgarding the authentication process OpenID connect (oidc) is recommended, but not always straightforward to use. In cases where you are unable to connect with the backend use basic authentication method explained here.\n\n# function to load geojson file\ndef read_json(path: Path) -&gt; dict:\n    with open(path) as input:\n        field = json.load(input)\n        input.close()\n    return field\n\nSince this is an already published service available service, they need not be concerned with selecting the backend. They can directly execute the process by providing time and area of interest.\n\nbefore_date = [\"2021-05-12\",\"2021-05-12\"]\nafter_date = [\"2021-06-18\", \"2021-06-18\"]\naoi = read_json(\"cologne_all.geojson\")\n\n# Create a processing graph from the NDWI process using an active openEO connection\nbefore_ndwi = eoconn.datacube_from_process(\"NDWI\", namespace=\"vito\", date=before_date\n                                        ,polygon=aoi)\n# Create a processing graph from the NDWI process using an active openEO connection\nafter_ndwi = eoconn.datacube_from_process(\"NDWI\", namespace=\"vito\", date=after_date\n                                        ,polygon=aoi)\n\n/home/pratixa/.local/lib/python3.6/site-packages/openeo/metadata.py:252: UserWarning: No cube:dimensions metadata\n  complain(\"No cube:dimensions metadata\")\n\n\nAs you can see a userwarning for missing data pops up which might not be an issue in normal case but here we wish to further evaluate our result in our process thus defining metadata is needed.\nNot all the available service requires updating metadata. If a service lack metadata then performing further computation on output of the service could be an issue. In such case user can update the metadata based on it’s status.\n\n# check available information is available in metadata or not\nbefore_ndwi.metadata\n\n\n    \n    \n        \n    \n    \n\n\n\n# updating our metadata\nfrom openeo.metadata import CollectionMetadata\n\nbefore_ndwi.metadata = CollectionMetadata({\"cube:dimensions\":{\"t\":{\"type\":\"temporal\"}}})\nafter_ndwi.metadata = CollectionMetadata({\"cube:dimensions\":{\"t\":{\"type\":\"temporal\"}}})\n\nOnce the metadata is updated you can perform further operations like subtraction or sum etc as done for this use case.\nSince now we have details on temporal dimension we can perform dimension reduction. As we loaded our collection for specific time intervals, it can include multiple time dimensions. Thus reduce_dimension applies a reducer to a data cube dimension by collapsing all the pixel values along the time dimension into an output value computed by the reducer.\nIt is then followed by subtracting our before datacube from the later.\n\n# compute the change between pre and post image\nmerging_cubes = after_ndwi.merge_cubes(-before_ndwi)\ndifferenced_cube = merging_cubes.reduce_dimension(dimension=\"t\",reducer='sum')\n\nOnce the process is completed, you can also save it as your process using save_user_defined_process that can later be used for a similar task. Otherwise, you can download the result either by direct download (in case of the small spatial extent with few processing) or perform create a batch job in case it is a heavy task over a large extent.\n\n# download your result either using synchronous method or batch\n# synchronous download\ndifferenced_cube.download(\"changed_ndwi.tiff\")\n# \n# # Or perform batch processing if area is comparatively large\n# batch_job = Rrescaled_chunks.create_job(out_format = \"GTiff\", title=\"changed_ndwi\")\n# batch_job.start_and_wait()\n# results = batch_job.get_results()\n# results.download_files()"
  },
  {
    "objectID": "APIs/openEO/openeo_deployment.html",
    "href": "APIs/openEO/openeo_deployment.html",
    "title": "openEO public service deployment",
    "section": "",
    "text": "The openEO deployment of the Copernicus Data Space Ecosystem public service has the aim to protect against lock-in in various ways: - The software is 100% open source, preventing dependency on a vendor. - openEO is an open standard with a web service API. Allowing to switch to other APIs that implement openEO immediately. - openEO workflows use standardized processes. Avoiding your workflows being strongly coupled to a specific technology.\n\n\n\n\nThe service runs on Cloudferro and T-Systems infrastructure, close to the storage systems that store the Sentinel archives. It also reads from these archives directly and processes the data on the fly. For complex products like Sentinel-1, we have the sar_backscatter process that performs on-the-fly orthorectification.\nFor products like Sentinel-2, with lots of overlap, we have custom code in place to resolve this as efficiently and correctly as possible.\nWe tune the settings of data access libraries like GDAL to the specific needs of the clouds that we work with. This is a tedious process and subject to change. When you observe suboptimal data access performance, it may be good to enquire about this.\n\n\nSome datasets are accessed via the Sentinelhub API instead of the raw files. This is done to provide a cost-effective deployment, and be able to offer a wide choice of datasets, which would otherwise be impossible.\n\n\n\n\nProcessing happens on a Kubernetes cluster with auto-scaling capabilities. In that cluster, your jobs run as Apache Spark applications. This is a very widely used and mature big data processing framework. It is comparable to Dask, which mainly focuses on being a pure Python implementation.\nWe currently do not have benchmarks that rigorously compare the performance of openEO with other processing frameworks, but we can make some relevant observations:\n\nThere’s no technical evidence that, by design, other processing approaches are somehow better or faster.\nWe have validated the backend by running large-scale processing tasks and achieved results that matched and sometimes even exceeded the expectations with respect to processing budgets.\nUsing a service such as openEO implies significant cost savings in terms of personnel costs compared to an approach based on infrastructure as a service (IAAS).\nThe main cost driver and performance bottleneck of many EO workflows is the data access performance, which is relatively independent of the software, assuming that tuning parameters are properly set.\n\nThe openEO backend will try to determine performance-sensitive parameters of your processing job automatically. This includes settings for memory, cores and data partitioning/chunking. It is, however, possible that the automatic tuning needs some fixing for your use case. A typical situation is that workflows require more memory, but sometimes, more advanced tuning, such as partitioning, needs to be applied.\n\n\n\nJust like any other backend, you can run your batch jobs using open-source software and the public docker image.\nAt openEO.org, you can find a list of open-source implementations. There’s also some guidance for getting started as a service provider.\nTo run your small-scale testing locally, we can recommend the Pangeo based ‘local processing’ feature.\n\n\n\nThe openEO Hub maintains a list of other openEO services:\nhttps://hub.openeo.org"
  },
  {
    "objectID": "APIs/openEO/openeo_deployment.html#deployment",
    "href": "APIs/openEO/openeo_deployment.html#deployment",
    "title": "openEO public service deployment",
    "section": "",
    "text": "The service runs on Cloudferro and T-Systems infrastructure, close to the storage systems that store the Sentinel archives. It also reads from these archives directly and processes the data on the fly. For complex products like Sentinel-1, we have the sar_backscatter process that performs on-the-fly orthorectification.\nFor products like Sentinel-2, with lots of overlap, we have custom code in place to resolve this as efficiently and correctly as possible.\nWe tune the settings of data access libraries like GDAL to the specific needs of the clouds that we work with. This is a tedious process and subject to change. When you observe suboptimal data access performance, it may be good to enquire about this.\n\n\nSome datasets are accessed via the Sentinelhub API instead of the raw files. This is done to provide a cost-effective deployment, and be able to offer a wide choice of datasets, which would otherwise be impossible.\n\n\n\n\nProcessing happens on a Kubernetes cluster with auto-scaling capabilities. In that cluster, your jobs run as Apache Spark applications. This is a very widely used and mature big data processing framework. It is comparable to Dask, which mainly focuses on being a pure Python implementation.\nWe currently do not have benchmarks that rigorously compare the performance of openEO with other processing frameworks, but we can make some relevant observations:\n\nThere’s no technical evidence that, by design, other processing approaches are somehow better or faster.\nWe have validated the backend by running large-scale processing tasks and achieved results that matched and sometimes even exceeded the expectations with respect to processing budgets.\nUsing a service such as openEO implies significant cost savings in terms of personnel costs compared to an approach based on infrastructure as a service (IAAS).\nThe main cost driver and performance bottleneck of many EO workflows is the data access performance, which is relatively independent of the software, assuming that tuning parameters are properly set.\n\nThe openEO backend will try to determine performance-sensitive parameters of your processing job automatically. This includes settings for memory, cores and data partitioning/chunking. It is, however, possible that the automatic tuning needs some fixing for your use case. A typical situation is that workflows require more memory, but sometimes, more advanced tuning, such as partitioning, needs to be applied.\n\n\n\nJust like any other backend, you can run your batch jobs using open-source software and the public docker image.\nAt openEO.org, you can find a list of open-source implementations. There’s also some guidance for getting started as a service provider.\nTo run your small-scale testing locally, we can recommend the Pangeo based ‘local processing’ feature.\n\n\n\nThe openEO Hub maintains a list of other openEO services:\nhttps://hub.openeo.org"
  },
  {
    "objectID": "APIs/openEO/federation/backends/design.html",
    "href": "APIs/openEO/federation/backends/design.html",
    "title": "Documentation",
    "section": "",
    "text": "The openEO Aggregator is the central component that receives all incoming openEO requests for the federation. It mainly contains the logic that decides to which backends requests need to be forwarded. It does not perform any processing. To prevent bottlenecks, the aggregator does not handle any data transfers, either to the user or between backends.\nMaintained by the openEO community, the aggregator is an open source component with its code hosted on GitHub:\nhttps://github.com/Open-EO/openeo-aggregator/\n\n\n\nEach of the underlying back-ends of the federation can define its own set of available processes, but there is in practice a very large common ground across these back-ends. As such, the federation’s listing of available processes is the intersection of the process sets of each of the underlying back-ends. This is the most straightforward combination with the least surprise.\n\nAdvanced/experimental usage A savvy user that knows which underlying back-end will execute their job can however still submit process graphs with processes that are available on that back-end but fall outside the intersection, as the federation will just forward the process graph as-is to that back-end.\n\n\nNote For the technical discussion on process federation, see Open-EO/openeo-aggregator#4\n\n\n\n\nThe federation currently lists the union of import/export file formats available at each of the underlying back-ends.\n\nNote For the technical discussion on file format federation, see Open-EO/openeo-aggregator#1\n\n\n\n\nThe current routing logic is based on the collections and processes involved in a request. In almost all cases, this combination will allow the system to select a single backend. If more than one backend can manage the request, the aggregator will select backends according to a fixed preference, which makes the overall system deterministic. However, users have the option to indicate a preferred backend in their openEO process graph.\nCurrently, there’s no established way to evenly distribute load within the federation. However, such more advanced mechanisms are certainly possible and can be implemented by the community or anyone who is interested in such a feature.\n\n\n\nBy default, this request will not be accepted. However, an experimental setting can be enabled to activate this feature. When enabled, the process graph will be split into independent subgraphs. These subgraphs will be submitted, resulting in STAC collections that are not considered finished (as the processing will take some time). These results, called ‘partials’, can be used in a load_stac process. This approach regenerates the initial process graph where load_collection processes with unavailable data are replaced with load_stac containing a partial result.\nThis final process graph also gets submitted. The receiving backend will wait for partial results to become available before finishing the full computation."
  },
  {
    "objectID": "APIs/openEO/federation/backends/design.html#openeo-federation-design",
    "href": "APIs/openEO/federation/backends/design.html#openeo-federation-design",
    "title": "Documentation",
    "section": "",
    "text": "The openEO Aggregator is the central component that receives all incoming openEO requests for the federation. It mainly contains the logic that decides to which backends requests need to be forwarded. It does not perform any processing. To prevent bottlenecks, the aggregator does not handle any data transfers, either to the user or between backends.\nMaintained by the openEO community, the aggregator is an open source component with its code hosted on GitHub:\nhttps://github.com/Open-EO/openeo-aggregator/\n\n\n\nEach of the underlying back-ends of the federation can define its own set of available processes, but there is in practice a very large common ground across these back-ends. As such, the federation’s listing of available processes is the intersection of the process sets of each of the underlying back-ends. This is the most straightforward combination with the least surprise.\n\nAdvanced/experimental usage A savvy user that knows which underlying back-end will execute their job can however still submit process graphs with processes that are available on that back-end but fall outside the intersection, as the federation will just forward the process graph as-is to that back-end.\n\n\nNote For the technical discussion on process federation, see Open-EO/openeo-aggregator#4\n\n\n\n\nThe federation currently lists the union of import/export file formats available at each of the underlying back-ends.\n\nNote For the technical discussion on file format federation, see Open-EO/openeo-aggregator#1\n\n\n\n\nThe current routing logic is based on the collections and processes involved in a request. In almost all cases, this combination will allow the system to select a single backend. If more than one backend can manage the request, the aggregator will select backends according to a fixed preference, which makes the overall system deterministic. However, users have the option to indicate a preferred backend in their openEO process graph.\nCurrently, there’s no established way to evenly distribute load within the federation. However, such more advanced mechanisms are certainly possible and can be implemented by the community or anyone who is interested in such a feature.\n\n\n\nBy default, this request will not be accepted. However, an experimental setting can be enabled to activate this feature. When enabled, the process graph will be split into independent subgraphs. These subgraphs will be submitted, resulting in STAC collections that are not considered finished (as the processing will take some time). These results, called ‘partials’, can be used in a load_stac process. This approach regenerates the initial process graph where load_collection processes with unavailable data are replaced with load_stac containing a partial result.\nThis final process graph also gets submitted. The receiving backend will wait for partial results to become available before finishing the full computation."
  },
  {
    "objectID": "APIs/openEO/federation/backends/fileformats.html",
    "href": "APIs/openEO/federation/backends/fileformats.html",
    "title": "File Formats",
    "section": "",
    "text": "The openEO federation offers different file formats for importing and exporting of data. This page details the best practices and agreements for utilizing these file formats.\n\n\nDepending on the data cube that your process graph generates and the overall use case, some file formats are more suitable to export your data (save_result). Following is a brief overview of the most common use cases.\n\n\n\nJPEG/PNG: data formats that are well suited for use in media/printing\n\nNot georeferenced and therefore only of limited use for further analysis\nLimited to 3 (JPEG) or 4 (PNG) output bands (image channels)\nUsually contains data from a single timestamp\n\nnetCDF: ideal for time series data as it stores data in multi-dimensional arrays\n\nGeoreferenced (x/y dimensions)\nCan store multiple bands (band dimension)\nCan store multiple timestamps (time dimension)\nSelf-describing, portable and scalable\n\nGeoTiff: ideal for storing several bands in a single, cloud optimized file\n\nGeoreferenced\nCan store multiple bands\nA single GeoTiff corresponds to one timestamp (in combination with STAC, multi-temporal collections can be supported)\nCloud optimized\n\n\n\n\n\n\nIf backends support identical file formats for both import and export, it is required to align them.\nFor example, when files are exported through save_result, the output parameters and the structure of the data, which will be written to storage, needs to be defined. Such an agreement has been achieved for the following file formats:\n\nGeoTiff\nnetCDF\n\nThe objective of these guidelines is to maximize the alignment with capabilities of these formats and corresponding toolchains.\n\n\nDefaults:\n\nA single GeoTiff corresponds to one timestamp (in combination with STAC, multi-temporal collections can be supported).\nAll datacube bands are stored in the same geotiff.\nThe full spatial extent is written to the same geotiff.\nCloud optimized\nFor ideal support in the openEO Web Editor (and other tools), the following guide is recommended to be followed: https://github.com/Open-EO/openeo-web-editor/blob/master/docs/geotiff.md\n\n\n\n\nDefaults:\n\nThe full datacube is written to a single netCDF.\nThe openEO dimension metadata is preserved in the netCDF file.\nCF conventions (https://cfconventions.org/) are used where applicable.\nData is chunked and compressed."
  },
  {
    "objectID": "APIs/openEO/federation/backends/fileformats.html#best-practices-file-formats",
    "href": "APIs/openEO/federation/backends/fileformats.html#best-practices-file-formats",
    "title": "File Formats",
    "section": "",
    "text": "Depending on the data cube that your process graph generates and the overall use case, some file formats are more suitable to export your data (save_result). Following is a brief overview of the most common use cases.\n\n\n\nJPEG/PNG: data formats that are well suited for use in media/printing\n\nNot georeferenced and therefore only of limited use for further analysis\nLimited to 3 (JPEG) or 4 (PNG) output bands (image channels)\nUsually contains data from a single timestamp\n\nnetCDF: ideal for time series data as it stores data in multi-dimensional arrays\n\nGeoreferenced (x/y dimensions)\nCan store multiple bands (band dimension)\nCan store multiple timestamps (time dimension)\nSelf-describing, portable and scalable\n\nGeoTiff: ideal for storing several bands in a single, cloud optimized file\n\nGeoreferenced\nCan store multiple bands\nA single GeoTiff corresponds to one timestamp (in combination with STAC, multi-temporal collections can be supported)\nCloud optimized"
  },
  {
    "objectID": "APIs/openEO/federation/backends/fileformats.html#federation-agreement-file-formats",
    "href": "APIs/openEO/federation/backends/fileformats.html#federation-agreement-file-formats",
    "title": "File Formats",
    "section": "",
    "text": "If backends support identical file formats for both import and export, it is required to align them.\nFor example, when files are exported through save_result, the output parameters and the structure of the data, which will be written to storage, needs to be defined. Such an agreement has been achieved for the following file formats:\n\nGeoTiff\nnetCDF\n\nThe objective of these guidelines is to maximize the alignment with capabilities of these formats and corresponding toolchains.\n\n\nDefaults:\n\nA single GeoTiff corresponds to one timestamp (in combination with STAC, multi-temporal collections can be supported).\nAll datacube bands are stored in the same geotiff.\nThe full spatial extent is written to the same geotiff.\nCloud optimized\nFor ideal support in the openEO Web Editor (and other tools), the following guide is recommended to be followed: https://github.com/Open-EO/openeo-web-editor/blob/master/docs/geotiff.md\n\n\n\n\nDefaults:\n\nThe full datacube is written to a single netCDF.\nThe openEO dimension metadata is preserved in the netCDF file.\nCF conventions (https://cfconventions.org/) are used where applicable.\nData is chunked and compressed."
  },
  {
    "objectID": "APIs/openEO/federation/backends/processes.html",
    "href": "APIs/openEO/federation/backends/processes.html",
    "title": "Processes",
    "section": "",
    "text": "The openEO project defines a large number of processes, which can be found at https://processes.openeo.org. Given the multitude of processes, it is necessary to define a subset of processes (known as a profile) that must be implemented on each backend to ensure a certain level of interoperability.\nIn addition to the general openEO Processes specification and their Processes Profiles, the CDSE openEO federation requires to implement an additional Process profile, namely L2P: Required - openEO. This requires one of the following openEO profiles L2A: Recommended (Raster) or L2B: Recommended (Vector). Additionally, there are two openEO Platform specific profiles with additional requirements: L3P: Advanced - openEO  and L4P: Complete - openEO Platform.\n\n\n\nAn overview of the openEO and CDSE openEO Federation Processes profiles.\n\n\n\n\nThe profiles below only contain the requirements that are not covered by their corresponding openEO profiles.\nAlthough in the openEO Profiles experimental processes shall only lead to a “warning”, experimental processes in profiles that L2P depends on are required to be available (which currently are inspect and nan).\n\n\n\nRequires openEO Profile L2: Recommended and additionally:\n\nadd_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\naggregate_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\naggregate_temporal\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\naggregate_temporal_period\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\napply\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\napply_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\napply_kernel\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\narray_contains\ndimension_labels\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\ndrop_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_bands\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_bbox\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_temporal\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_collection\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nmask\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nmask_polygon\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nmerge_cubes\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nndvi\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nreduce_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nrename_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nrename_labels\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nresample_cube_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nresample_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nsave_result\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\n\n\n\n\nRequires openEO Profile L3: Advanced and additionally:\n\napply_neighborhood\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nard_surface_reflectance (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_labels (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_stac (load_result) (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nreduce_spatial (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nresample_cube_temporal\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nrun_udf\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\n\n\n\n\nRequires openEO Profile L4: Above and Beyond and additionally:\n\natmospheric_correction (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_uploaded_files (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_url (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nsar_backscatter (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)"
  },
  {
    "objectID": "APIs/openEO/federation/backends/processes.html#l2p-required---openeo-federation",
    "href": "APIs/openEO/federation/backends/processes.html#l2p-required---openeo-federation",
    "title": "Processes",
    "section": "",
    "text": "Requires openEO Profile L2: Recommended and additionally:\n\nadd_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\naggregate_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\naggregate_temporal\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\naggregate_temporal_period\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\napply\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\napply_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\napply_kernel\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\narray_contains\ndimension_labels\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\ndrop_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_bands\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_bbox\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_temporal\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_collection\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nmask\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nmask_polygon\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nmerge_cubes\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nndvi\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nreduce_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nrename_dimension\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nrename_labels\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nresample_cube_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nresample_spatial\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nsave_result\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)"
  },
  {
    "objectID": "APIs/openEO/federation/backends/processes.html#l3p-advanced---openeo-federation",
    "href": "APIs/openEO/federation/backends/processes.html#l3p-advanced---openeo-federation",
    "title": "Processes",
    "section": "",
    "text": "Requires openEO Profile L3: Advanced and additionally:\n\napply_neighborhood\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nard_surface_reflectance (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nfilter_labels (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_stac (load_result) (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nreduce_spatial (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nresample_cube_temporal\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nrun_udf\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)"
  },
  {
    "objectID": "APIs/openEO/federation/backends/processes.html#l4p-complete---openeo-federation",
    "href": "APIs/openEO/federation/backends/processes.html#l4p-complete---openeo-federation",
    "title": "Processes",
    "section": "",
    "text": "Requires openEO Profile L4: Above and Beyond and additionally:\n\natmospheric_correction (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_uploaded_files (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nload_url (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)\n\nsar_backscatter (experimental)\n\nhas been tested on &gt; 100x100km at 10m resolution (or equivalent)"
  },
  {
    "objectID": "APIs/openEO/federation/openeo_federation.html",
    "href": "APIs/openEO/federation/openeo_federation.html",
    "title": "Copernicus Data Space Ecosystem openEO Federation",
    "section": "",
    "text": "The openEO standard offers a unique capability to provide access to datasets and processing capacity beyond the offering of CDSE. This is accomplished through a single endpoint and interface for your workflows. This eliminates the need to register across multiple platforms and study their APIs and offerings. This function, referred to as the openEO federation, is available through the following endpoint:\nhttps://openeofed.dataspace.copernicus.eu\nCurrently, we are actively encouraging early adopters and projects with specific needs to try this new capability. While it has already been extensively tested by users of other projects, the configuration in the CDSE is still new. As a result, we are offering this feature as a ‘beta’ service at first. This approach allows us to still adjust the configuration based on your feedback.\nWhen using the federated endpoint, you will be asked to give your consent to share basic user information with the other providers of the federation. Additionally, you will be requested to accept the new terms and conditions. You can withdraw your consent at any time through your CDSE account dashboard.\nAll external providers comply with the regulations of the federation, but are not necessarily part of the CDSE service. This means that while they are part of the broader Copernicus ecosystem, they are either fully independent providers, or are governed and/or funded by other parties. This mainly impacts their currently foreseen lifetime, but also how they handle aspects that are not part of federation governance.\nYou can find a description of the technical design here, as well as a more extensive overview of federation governance.\n\n\nThe usage of processing capacity in the federation is likely to lead to credit consumption. This usage will be reported in the openEO algorithm plaza. To support basic processing within the federation, external providers can allow a limited amount of credits, similar to the free openEO credits offered by CDSE. It is important to note that there is clear separation of your credits amongst the external providers. This means that using an external provider will not lead to a reduction in the free credits supplied by CDSE.\n\n\n\nAll providers in the federation will post important information about changes to their service in the openEO category of the forum, We encourage users to subscribe to this topic to receive important updates via email. Important breaking changes are announced here with a lead time of 3 months, allowing users to adjust their workflows if needed.\n\n\n\nWe encourage users to use the forum, a place where service providers and users can meet, to ask questions about the federation. Our support team is not required to help with issues related to external, non-CDSE openEO services. Additionally, the free credits offered by the service provider do not include support. However, the dedicated CDSE openEO endpoint does include support under normal situations.\n\n\n\nIt is important to note that external providers within the federation may not have the same long term guarantees as the CDSE service. Providers may discontinue their service, or may be removed from the federation if they become non-compliant with our agreements or SLA’s. If this continuity is important to your project or your organization, we recommend to contact the provider(s) directly to discuss their long term commitment.\n\n\n\nWe list the members of the federation here. Collections in the federated endpoint are offered by one or more of these members.\n\n\nThe CDSE openEO instance is also part of the federated instance, ensuring that users can combine the core CDSE offering with data and processing offered at various other providers.\nThis is currently the ‘default’ provider, so many requests to the federation will simply run on the CDSE openEO instance. This should make it possible for users to easily switch to the federation.\n\n\n\nThe Terrascope platform is a Belgian initiative that offers access to Copernicus products, the full archive of the PROBA-V mission, and a range of other interesting datasets. The platform also offers its own processing capacity close to the datasets, and is an early implementor of the openEO open standard.\nTerrascope is operated by VITO Remote Sensing, an independent research and technology leader. The platform is guaranteed to operate until 2028.\nCredit usage on Terrascope is similar to the CDSE openEO service, as it is based on the consumed cpu and memory hours. However, actual costs for similar jobs may differ due to varying performance characteristics of underlying data centers."
  },
  {
    "objectID": "APIs/openEO/federation/openeo_federation.html#federation-credit-usage",
    "href": "APIs/openEO/federation/openeo_federation.html#federation-credit-usage",
    "title": "Copernicus Data Space Ecosystem openEO Federation",
    "section": "",
    "text": "The usage of processing capacity in the federation is likely to lead to credit consumption. This usage will be reported in the openEO algorithm plaza. To support basic processing within the federation, external providers can allow a limited amount of credits, similar to the free openEO credits offered by CDSE. It is important to note that there is clear separation of your credits amongst the external providers. This means that using an external provider will not lead to a reduction in the free credits supplied by CDSE."
  },
  {
    "objectID": "APIs/openEO/federation/openeo_federation.html#openeoannounce",
    "href": "APIs/openEO/federation/openeo_federation.html#openeoannounce",
    "title": "Copernicus Data Space Ecosystem openEO Federation",
    "section": "",
    "text": "All providers in the federation will post important information about changes to their service in the openEO category of the forum, We encourage users to subscribe to this topic to receive important updates via email. Important breaking changes are announced here with a lead time of 3 months, allowing users to adjust their workflows if needed."
  },
  {
    "objectID": "APIs/openEO/federation/openeo_federation.html#getting-help",
    "href": "APIs/openEO/federation/openeo_federation.html#getting-help",
    "title": "Copernicus Data Space Ecosystem openEO Federation",
    "section": "",
    "text": "We encourage users to use the forum, a place where service providers and users can meet, to ask questions about the federation. Our support team is not required to help with issues related to external, non-CDSE openEO services. Additionally, the free credits offered by the service provider do not include support. However, the dedicated CDSE openEO endpoint does include support under normal situations."
  },
  {
    "objectID": "APIs/openEO/federation/openeo_federation.html#long-term-continuity",
    "href": "APIs/openEO/federation/openeo_federation.html#long-term-continuity",
    "title": "Copernicus Data Space Ecosystem openEO Federation",
    "section": "",
    "text": "It is important to note that external providers within the federation may not have the same long term guarantees as the CDSE service. Providers may discontinue their service, or may be removed from the federation if they become non-compliant with our agreements or SLA’s. If this continuity is important to your project or your organization, we recommend to contact the provider(s) directly to discuss their long term commitment."
  },
  {
    "objectID": "APIs/openEO/federation/openeo_federation.html#federation-members",
    "href": "APIs/openEO/federation/openeo_federation.html#federation-members",
    "title": "Copernicus Data Space Ecosystem openEO Federation",
    "section": "",
    "text": "We list the members of the federation here. Collections in the federated endpoint are offered by one or more of these members.\n\n\nThe CDSE openEO instance is also part of the federated instance, ensuring that users can combine the core CDSE offering with data and processing offered at various other providers.\nThis is currently the ‘default’ provider, so many requests to the federation will simply run on the CDSE openEO instance. This should make it possible for users to easily switch to the federation.\n\n\n\nThe Terrascope platform is a Belgian initiative that offers access to Copernicus products, the full archive of the PROBA-V mission, and a range of other interesting datasets. The platform also offers its own processing capacity close to the datasets, and is an early implementor of the openEO open standard.\nTerrascope is operated by VITO Remote Sensing, an independent research and technology leader. The platform is guaranteed to operate until 2028.\nCredit usage on Terrascope is similar to the CDSE openEO service, as it is based on the consumed cpu and memory hours. However, actual costs for similar jobs may differ due to varying performance characteristics of underlying data centers."
  },
  {
    "objectID": "APIs/openEO/Processes.html",
    "href": "APIs/openEO/Processes.html",
    "title": "openEO Processes",
    "section": "",
    "text": "A process in openEO is an operation that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages.\nThe following is the list of Processes currently supported in openEO for performing several operations in your datacube."
  },
  {
    "objectID": "APIs/openEO/job_config.html",
    "href": "APIs/openEO/job_config.html",
    "title": "Job Configuration",
    "section": "",
    "text": "Jobs running on the cloud get assigned a default amount of CPU and memory resources. This may not always be enough for your job, for instance, when using UDFs. Also, for very large jobs, you may want to optimise your resource settings for cost optimisation.\nThe example below shows how to start a job with all options set to their default values. It is important to highlight that default settings are subject to change by the backend whenever needed.\nThis is a short overview of the various options:"
  },
  {
    "objectID": "APIs/openEO/job_config.html#validity-of-signed-urls-in-batch-job-results",
    "href": "APIs/openEO/job_config.html#validity-of-signed-urls-in-batch-job-results",
    "title": "Job Configuration",
    "section": "Validity of signed URLs in batch job results",
    "text": "Validity of signed URLs in batch job results\nBatch job results are accessible to the user via signed URLs stored in the result assets. Within the platform, these URLs have a validity (expiry time) of 7 days. Within these 7 days, the results of a batch job can be accessed by any person with the URL. Each time a user requests the results from the job endpoint (GET /jobs/{job_id}/results), a freshly signed URL (valid for 7 days) is created for the result assets.\n\nLearning more\nThe topic of resource optimisation is a complex one, and here we just give a short summary. The goal of openEO is to hide most of these details from the user, but we realize that advanced users sometimes want to have a bit more insight, so in the spirit of being open, we give some hints.\nTo learn more about these options, we point to the piece of code that handles this.\nMost memory related options are translated to Apache Spark configuration settings, which are documented here."
  },
  {
    "objectID": "APIs/TCP.html",
    "href": "APIs/TCP.html",
    "title": "TCP Stack Configuration",
    "section": "",
    "text": "In order to improve the download performance of EO products from the Copernicus Data Space Ecosystem platform, below is the recommended configuration of TCP stack for a machine with Linux operating system:\nsysctl -w net.core.rmem_max=\"2147483647\"\nsysctl -w net.core.wmem_max=\"2147483647\"\nsysctl -w net.ipv4.tcp_rmem=\"8192 262144 536870912\"\nsysctl -w net.ipv4.tcp_wmem=\"4096 65536 536870912\"\nsysctl -w net.core.default_qdisc=fq\nsysctl -w net.ipv4.tcp_congestion_control=bbr\nsysctl -w net.core.netdev_max_backlog=300000\nsysctl -w net.core.somaxconn=65535"
  },
  {
    "objectID": "APIs/STAC.html",
    "href": "APIs/STAC.html",
    "title": "STAC product catalogue",
    "section": "",
    "text": "STAC (SpatioTemporal Asset Catalog) is a relatively new web service specification for catalogs that is increasingly used and supported. STAC data have become a de-facto standard in the EO community, also being onboarded to OGC at the moment. STAC items are provided for all online products, as well as for products generated by users within the Copernicus Data Space Ecosystem.\nThe Copernicus Data Space Ecosystem STAC API was implemented as a web service interface to query over a group of STAC collections held in a database. All fields included in Data Space API are consistent with the STAC Specification.\nThe service implements the STAC API version v1.0.0. The version exposed in the Copernicus Data Space Ecosystem is still subject to change as the quality of STAC metadata is still improving. Nevertheless, it already supports basic product search."
  },
  {
    "objectID": "APIs/STAC.html#endpoint-url",
    "href": "APIs/STAC.html#endpoint-url",
    "title": "STAC product catalogue",
    "section": "Endpoint URL",
    "text": "Endpoint URL\nThe Copernicus Data Space Ecosystem STAC API Catalog can be accessed using the following URL:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac"
  },
  {
    "objectID": "APIs/STAC.html#available-collections",
    "href": "APIs/STAC.html#available-collections",
    "title": "STAC product catalogue",
    "section": "Available Collections",
    "text": "Available Collections\nThe data are organized in so-called collections corresponding to various satellites.\nThe following collections are currently available via STAC API:\n\nCopernicus Sentinel Mission\n\nSENTINEL-1\nSENTINEL-2\nSENTINEL-3\nSENTINEL-5P\nSENTINEL-6\nSENTINEL-1-RTC (Sentinel-1 Radiometric Terrain Corrected)\n\nComplementary data\n\nGLOBAL-MOSAICS (Sentinel-1 and Sentinel-2 Global Mosaics)\nSMOS (Soil Moisture and Ocean Salinity)\nENVISAT (ENVISAT- Medium Resolution Imaging Spectrometer - MERIS)\nLANDSAT-5\nLANDSAT-7\nLANDSAT-8-ESA\nCOP-DEM (Copernicus DEM)\nTERRAAQUA (Terra MODIS and Aqua MODIS)\nS2GLC (S2GLC 2017)"
  },
  {
    "objectID": "APIs/STAC.html#stac-collections-search",
    "href": "APIs/STAC.html#stac-collections-search",
    "title": "STAC product catalogue",
    "section": "STAC Collections Search",
    "text": "STAC Collections Search\nSTAC Collections endpoint lets users get information about collections available in the Copernicus Data Space Ecosystem catalogue.\nTo access the information about all STAC API Collections:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections\n\n\n\nTo access the information about a specified STAC API Collection (e.g. SENTINEL-2):\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-2"
  },
  {
    "objectID": "APIs/STAC.html#stac-items-search",
    "href": "APIs/STAC.html#stac-items-search",
    "title": "STAC product catalogue",
    "section": "STAC Items Search",
    "text": "STAC Items Search\nSearch for items is possible among all collections Items Search in all STAC Collections or in one specified collection only Items Search in a STAC Collection.\n\n\n\n\n\n\nNote\n\n\n\nTo accelerate the query performance, it is recommended to search for Items within one specified collection, e.g.:\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-3/items\n\n\n\nItems Search in a STAC Collection\n\nSearch for items in a collection\nTo list items in a given collection:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items\n\n\n\nBy default, the catalogue will limit the number of shown items to 20. It can be changed by filtering with the limit option as described below Limit option.\n\n\nSearch for a specific item\nTo list a specific item in a collection:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items/S1A_IW_SLC__1SDV_20221231T100709_20221231T100736_046574_0594DE_0A58.SAFE\n\n\n\n\n\nSearch for items by attributes\nTo list items with a given attribute:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?datetime=2022-12-31T09:59:31.293Z/\n\n\n\nCurrently, those attributes are supported:\n\nbbox\ndatetime\nids(Items Ids)\n\n\nSearch Items by bbox\nAttribute bbox will list all products from a given collection within the Area of Interest (AOI). This attribute requires between 4 and 6 values (coordinates) where a comma separates each coordinate.\nTo search for items by bbox:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?bbox=-80.673805,-0.52849,-78.060341,1.689651\n\n\n\n\n\nSearch Items by datetime\nAttribute datetime will list all products within a specified time interval.\nTo search for items within specified datetime:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?datetime=2021-12-31T09:59:31.293Z/2023-12-31T09:59:31.293Z\n\n\n\nAttribute datetime can search for several different formats:\n\n2022-12-31T00:00:00Z\n2022-12-31T00:00:00\n2022-12-31T16:00:00-08:00\n2022-12-31T00:00:00+01:00\n2022-12-31T00:00:00.000Z\n2022-12-31T00:00:00.000\n\ndatatime intervals:\n\n/2021-12-31T23:59:59Z (open start interval)\n2021-12-31T23:59:59Z/ (open end interval)\n2022-12-30T00:00:00Z/2022-12-31T23:59:59Z (closed interval)\n\nPlease note that those are example values and might not return anything if input.\n\n\nSearch Items by ids\nTo search for products by their Ids:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-2/items?ids=S2A_MSIL2A_20150715T094306_N0204_R036_T33SXA_20150715T094315.SAFE,S2A_MSIL2A_20150715T112846_N0204_R037_T29RLH_20150715T112845.SAFE\n\n\n\n\n\n\nSearch Items by two or more attributes\nTo list items by two or more attributes:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?bbox=-80.673805,-0.52849,-78.060341,1.689651&datetime=2014-10-13T23:28:54.650Z\n\n\n\n\n\nLimit option\nThe limit option allows users to increase or decrease the number of items shown. \nThe default value is set to 20.\nThe acceptable arguments for this option: Integer &lt;0,1000&gt;\nTo list a limited number of items:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?datetime=2022-12-31T09:59:31.293Z/&limit=10\n\n\n\n\n\nSortby option\nThe sortby option allows users to define the fields by which to sort results.\nThe acceptable arguments for this option are:\n\nend_datetime\nstart_datetime\ndatetime\n\nTo set the sort order, the prefix should be added to the sort parameter:\n\n+ for ascending (in https standard + sign should be encoded with %2B)\n- for descending\n\nIf no prefix is provided, ascending order is assumed.\nTo sort items within a specified collection:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?datetime=2021-12-31T09:59:31.293Z/&sortby=-start_datetime\n\n\n\n\n\nPage option\nThe page option determines the page of results.\nThe acceptable arguments for this option: Integer &lt;1,100&gt;\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?datetime=2022-12-31T09:59:31.293Z/&page=32\n\n\n\n\n\n\nItems Search in all STAC Collections\nIf users would like to list items from any collection, they can use search option, which will search all collections.\nTo list items in any collection:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/search?\n\n\n\nThis endpoint enables searching with simple filtering by:\n\ncollectionId\nids(Items Ids)\ndatetime\nbbox\n\nAlso, the following options are supported:\n\nlimit\nsortby\npage\n\n\nSearch by attributes\nListing by attributes and using the limit option works the same way as before:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/search?ids=S2A_MSIL2A_20150715T094306_N0204_R036_T33SXA_20150715T094315.SAFE,S2A_MSIL2A_20150715T112846_N0204_R037_T29RLH_20150715T112845.SAFE&datetime=2015-07-15T00:00:00.000Z/&limit=10\n\n\n\nCurrently those attributes are available:\n\nbbox\ndatetime\nids(Items Ids)\n\n\n\nSearch items for many collections\nUsing /search option allows the user to list more than one collection.\nTo list more than one collection:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/search?collections=SENTINEL-1,SENTINEL-2"
  },
  {
    "objectID": "APIs/STAC.html#stac-advanced-filters",
    "href": "APIs/STAC.html#stac-advanced-filters",
    "title": "STAC product catalogue",
    "section": "STAC Advanced filters",
    "text": "STAC Advanced filters\nSTAC Filter extensions allow users to search for items based on their attributes.\nCurrently, those advanced filters are supported:\n\nCQL2 JSON\nFilter parameters: filter, filter-crs (default and only accepted CRS84), filter-lang (json)\n\nEndpoint URL for the POST method:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/search\n\n\n\nFollowing endpoints have been added to allow users to check for available parameters when writing filter expressions.\nTo access queryable names for STAC API Item Search filter across the entire catalogue:\n\nHTTPS Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/queryables\n\n\n\nTo check acceptable queryable names for Copernicus Sentinel Missions:\n\nSENTINEL-1SENTINEL-2SENTINEL-3SENTINEL-5PSENTINEL-6SENTINEL-1-RTC\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-2/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-3/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-5P/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-6/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1-RTC/queryables\n\n\n\nTo check acceptable queryable names for Complementary data:\n\nGLOBAL-MOSAICSSMOSENVISATLANDSAT-5LANDSAT-7LANDSAT-8-ESACOP-DEMTERRAAQUAS2GLC\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/GLOBAL-MOSAICS/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SMOS/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/ENVISAT/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/LANDSAT-5/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/LANDSAT-7/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/LANDSAT-8-ESA/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/COP-DEM/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/TERRAAQUA/queryables\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/S2GLC/queryables\n\n\n\n\nQuery structure\nTo search for products using STAC Advanced filters, users must define a query in a request body and use the POST method. Simplest expression, looking for a specific item:\nPOST\nhttps://catalogue.dataspace.copernicus.eu/stac/search\nRequest body:\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"collection\"\n                    },\n                    \"SENTINEL-1\"\n                ]\n            },\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"id\"\n                    },\n                    \"S1A_IW_RAW__0SDV_20230106T110100_20230106T110132_046662_0597C3_F5B4.SAFE\"\n                ]\n            }\n        ]\n    }\n}\nTo search within a specific collection, its name must be included in the request body.\n\n\nAvailable operators\nCurrently, those operators are supported:\n\nComparison Operators: Equal to, Not Equal to, Less than, Less than or equal to, Greater than, Greater than or equal to, Is Null\nAdvanced Comparison Operators: Like, Between, In, And, Or, Not\nSpatial Operators: S_CONTAINS, S_CROSSES, S_DISJOINT, S_EQUALS, S_INTERSECTS, S_OVERLAPS, S_TOUCHES, S_WITHIN\nTemporal Operators: T_AFTER, T_BEFORE, T_CONTAINS, T_DISJOINT, T_DURING, T_EQUALS, T_FINISHEDBY, T_FINISHES, T_INTERSECTS, T_MEETS, T_METBY, T_OVERLAPPEDBY, T_OVERLAPS, T_STARTEDBY, T_STARTS\n\nThe following table describes available operators for given attributes:\n\n\nExamples\nFor each group of operators, examples are provided in the request body for the POST method and the following endpoint:\nPOST\nhttps://catalogue.dataspace.copernicus.eu/stac/search\n\nComparison Operators\n\nEQUAL TONOT EQUAL TOLESS THANLESS THAN OR EQUAL TOGREATER THANGREATER THAN OR EQUAL TOIS NULL\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"=\",\n        \"args\": [\n            {\n                \"property\": \"cloudCover\"\n            },\n            10\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"&lt;&gt;\",\n        \"args\": [\n            {\n                \"property\": \"cloudCover\"\n            },\n            10\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"&lt;\",\n        \"args\": [\n            {\n                \"property\": \"cloudCover\"\n            },\n            10\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"&lt;=\",\n        \"args\": [\n            {\n                \"property\": \"cloudCover\"\n            },\n            10\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"&gt;\",\n        \"args\": [\n            {\n                \"property\": \"cloudCover\"\n            },\n            10\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"&gt;=\",\n        \"args\": [\n            {\n                \"property\": \"cloudCover\"\n            },\n            10\n        ]\n    }\n}\n\n\n{\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"collection\"\n                    },\n                    \"SENTINEL-3\"\n                ]\n            },\n            {\n                \"op\": \"isNull\",\n                \"args\": [\n                    {\n                        \"property\": \"cloudCover\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n\n\n\n\n\nAdvanced Comparison Operators\n\nLIKEBETWEENINANDORNOT\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"collection\"\n                    },\n                    \"SENTINEL-2\"\n                ]\n            },\n            {\n                \"op\": \"like\",\n                \"args\": [\n                    {\n                        \"property\": \"productType\"\n                    },\n                    \"AUX%\"\n                ]\n            }\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"collection\"\n                    },\n                    \"SENTINEL-2\"\n                ]\n            },\n            {\n                \"op\": \"between\",\n                \"args\": [\n                    {\n                        \"property\": \"cloudCover\"\n                    },\n                    10,\n                    50\n                ]\n            }\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"in\",\n        \"args\": [\n            {\n                \"property\": \"operationalMode\"\n            },\n            [\n                \"IW\",\n                \"EW\",\n                \"WV\"\n            ]\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"id\"\n                    },\n                    \"S2A_MSIL1C_20240512T230341_N0510_R101_T59PRL_20240513T014351.SAFE\"\n                ]\n            },\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"collection\"\n                    },\n                    \"SENTINEL-2\"\n                ]\n            }\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"or\",\n        \"args\": [\n            {\n                \"op\": \"&gt;\",\n                \"args\": [\n                    {\n                        \"property\": \"orbitNumber\"\n                    },\n                    4000\n                ]\n            },\n            {\n                \"op\": \"&lt;\",\n                \"args\": [\n                    {\n                        \"property\": \"cloudCover\"\n                    },\n                    10\n                ]\n            }\n        ]\n    }\n}\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"not\",\n        \"args\": [\n            {\n                \"op\": \"=\",\n                \"args\": [\n                    {\n                        \"property\": \"collection\"\n                    },\n                    \"SENTINEL-3\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\n\n\nSpatial Operators\nSpatial operators specify a set of advanced functions based on Dimensionally Extended Nine-intersection Model (DE-9IM), enabling precise spatial queries and relationships between geometries in STAC API. The queries within spatial operators are supported for:\n\nBbox\nPoint\nLineString\nPolygon\nMultiPoint\nMultiLineString\nMultiPolygon\n\nCheck if the item’s geometry contains a specified geometry:\n\nS_CONTAINS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_contains\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            21.0,\n                            52.0\n                        ],\n                        [\n                            21.0,\n                            52.5\n                        ],\n                        [\n                            21.5,\n                            52.5\n                        ],\n                        [\n                            21.5,\n                            52.0\n                        ],\n                        [\n                            21.0,\n                            52.0\n                        ]\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry crosses within a specified geometry:\n\nS_CROSSES\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_crosses\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"LineString\",\n                \"coordinates\": [\n                    [\n                        20.5,\n                        52.0\n                    ],\n                    [\n                        21.5,\n                        52.5\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry doesn’t intersect with a specified geometry:\n\nS_DISJOINT\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_disjoint\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            21.5,\n                            53.0\n                        ],\n                        [\n                            21.5,\n                            53.5\n                        ],\n                        [\n                            22.0,\n                            53.5\n                        ],\n                        [\n                            22.0,\n                            53.0\n                        ],\n                        [\n                            21.5,\n                            53.0\n                        ]\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry exactly matches a specified geometry:\n\nS_EQUALS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_equals\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            10.700306,\n                            49.665051\n                        ],\n                        [\n                            14.264574,\n                            50.068405\n                        ],\n                        [\n                            13.874475,\n                            51.689091\n                        ],\n                        [\n                            10.182405,\n                            51.28339\n                        ],\n                        [\n                            10.700306,\n                            49.665051\n                        ]\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry intersects with a specified geometry:\n\nS_INTERSECTS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_intersects\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"Point\",\n                \"coordinates\": [\n                    36.319836,\n                    32.288087\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry overlaps with a specified geometry:\n\nS_OVERLAPS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_overlaps\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            21.2,\n                            52.1\n                        ],\n                        [\n                            21.2,\n                            52.4\n                        ],\n                        [\n                            21.6,\n                            52.4\n                        ],\n                        [\n                            21.6,\n                            52.1\n                        ],\n                        [\n                            21.2,\n                            52.1\n                        ]\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry touches a specified geometry:\n\nS_TOUCHES\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_touches\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"LineString\",\n                \"coordinates\": [\n                    [\n                        21.0,\n                        52.0\n                    ],\n                    [\n                        21.5,\n                        52.0\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s geometry is within a specified geometry:\n\nS_WITHIN\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"s_within\",\n        \"args\": [\n            {\n                \"property\": \"geometry\"\n            },\n            {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [\n                            21.2,\n                            52.1\n                        ],\n                        [\n                            21.2,\n                            52.4\n                        ],\n                        [\n                            21.6,\n                            52.4\n                        ],\n                        [\n                            21.6,\n                            52.1\n                        ],\n                        [\n                            21.2,\n                            52.1\n                        ]\n                    ]\n                ]\n            }\n        ]\n    }\n}\n\n\n\n\n\nTemporal Operators\nTemporal operators provide a set of advanced functions for working with datetime types, offering greater expressiveness while querying STAC API.\nCheck if the item’s datetime is after a specified datetime:\n\nT_AFTER\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_after\",\n        \"args\": [\n            {\n                \"property\": \"datetime\"\n            },\n            {\n                \"timestamp\": \"2023-01-01T00:00:00Z\"\n            }\n        ]\n    }\n}\n\n\n\nThis query will retrieve items where the datetime property is later than 2023-01-01T00:00:00Z. It can be used with a timestamp, as in the example above, or with an interval e.g.:\n\nT_AFTER\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_after\",\n        \"args\": [\n            {\n                \"property\": \"datetime\"\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-31T23:59:59Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime is before a specified datetime:\n\nT_BEFORE\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_before\",\n        \"args\": [\n            {\n                \"property\": \"datetime\"\n            },\n            {\n                \"timestamp\": \"2023-01-01T00:00:00Z\"\n            }\n        ]\n    }\n}\n\n\n\nThis query will retrieve items where the datetime property is before 2023-01-01T00:00:00Z. It can be used with a timestamp or an interval.\nCheck if a specified interval contains the item’s datetime:\n\nT_CONTAINS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_contains\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-31T23:59:59Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime is outside a specified interval:\n\nT_DISJOINT\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_disjoint\",\n        \"args\": [\n            {\n                \"property\": \"datetime\"\n            },\n            {\n                \"timestamp\": \"2024-02-28T14:21:54.265255Z\"\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime falls within a specified interval:\n\nT_DURING\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_during\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"end_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-01T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime exactly matches a specified datetime:\n\nT_EQUALS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_equals\",\n        \"args\": [\n            {\n                \"property\": \"datetime\"\n            },\n            {\n                \"timestamp\": \"2023-01-01T00:00:00Z\"\n            }\n        ]\n    }\n}\n\n\n\nCheck if a specified interval is finished by the item’s datetime:\n\nT_FINISHEDBY\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_finishedby\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-30T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime finishes a specified interval:\n\nT_FINISHES\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_finishes\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-15T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime intersects with a specified interval:\n\nT_INTERSECTS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_intersects\",\n        \"args\": [\n            {\n                \"property\": \"datetime\"\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-31T23:59:59Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime meets a specified interval:\n\nT_MEETS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_meets\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-10T00:00:00Z\",\n                    \"2023-01-15T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if a specified interval meets the item’s datetime:\n\nT_METBY\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_metby\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-10T00:00:00Z\",\n                    \"2023-01-15T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if a specified interval meets the item’s datetime:\n\nT_OVERLAPPEDBY\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_overlappedby\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-10T00:00:00Z\",\n                    \"2023-01-20T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime overlaps with a specified interval:\n\nT_OVERLAPS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_overlaps\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-10T00:00:00Z\",\n                    \"2023-01-20T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if a specified interval is started by the item’s datetime:\n\nT_STARTEDBY\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_startedby\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-15T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}\n\n\n\nCheck if the item’s datetime starts a specified interval:\n\nT_STARTS\n\n\n{\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"t_starts\",\n        \"args\": [\n            {\n                \"interval\": [\n                    {\n                        \"property\": \"start_datetime\"\n                    },\n                    {\n                        \"property\": \"end_datetime\"\n                    }\n                ]\n            },\n            {\n                \"interval\": [\n                    \"2023-01-01T00:00:00Z\",\n                    \"2023-01-15T00:00:00Z\"\n                ]\n            }\n        ]\n    }\n}"
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html",
    "href": "APIs/Others/UpcomingChanges.html",
    "title": "Upcoming changes",
    "section": "",
    "text": "This page includes the list of upcoming changes to Catalog APIs."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#catalogue-api-change-parameters-limits",
    "href": "APIs/Others/UpcomingChanges.html#catalogue-api-change-parameters-limits",
    "title": "Upcoming changes",
    "section": "Catalogue API Change: parameters’ limits",
    "text": "Catalogue API Change: parameters’ limits\nWe would like to inform you about an upcoming change to our OData, OpenSearch and STAC API interfaces, effective 12 November 2024.\nStarting from 12 November 2024, the number of skipped results will be limited to 10 000 items. Currently, there is no limit on the number of skipped items.\nThis limit will affect the following parameters in each Catalogue API interface:\n\nOData interface: maximum value for the skip parameter will be set to 10 000\nOpenSearch interface: maximum value for ‘(page - 1) * maxRecords + index - 1’ will be set to 10 000, where by deafult maxRecords = 20, page = 1 and index = 1; maximum value for ‘index’ will be set to 10001\nSTAC interface: maximum value for ‘(page - 1) * limit’ will be set to 10 000, where by deafult page = 1 and limit = 20\n\nPlease find below a list of parameters for each Catalogue API that will be affected by the change as well as the response which will be returned when the limit is exceeded.\nThe affected query parameters for OData Catalogue interface:\nOData Catalogue API\nskip\nOData API request\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-3' and ContentDate/Start gt 2024-09-01&$skip=10001&$count=True\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n{\n    \"@odata.context\": \"$metadata#Products\",\n    \"@odata.count\": 457588,\n    \"value\": [...],\n    \"@odata.nextLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?%24filter=Collection%2FName+eq+%27SENTINEL-3%27+and+ContentDate%2FStart+gt+2024-09-01&%24skip=10021&%24count=True\"\n}\n\n\n422 Unprocessable Entity\n \n{\n    \"detail\": [\n        {\n            \"type\": \"less_than_equal\",\n            \"loc\": [\n                \"query\",\n                \"$skip\"\n            ],\n            \"msg\": \"Input should be less than or equal to 10000\",\n            \"input\": \"10001\",\n            \"ctx\": {\n                \"le\": 10000\n            }\n        }\n    ]\n}\n\n\n\nThe @odata.nextLink field, which is normally included in the API response to provide the next link for pagination, will not be shown if the skip parameter in the next link exceeds a limit of 10,000. Here’s a step-by-step breakdown:\n1. OData query Example:\n{url}odata/v1/Products?$skip=10000&$top=1\nIn this query:\n\n$skip=10000 means you’re telling the API to skip the first 10,000 records.\n$top=1 means you want to retrieve only 1 record in the response.\n\n2. Pagination Logic: The OData API usually provides a @odata.nextLink in the response, which gives the URL to retrieve the next set of records in the sequence. This next link works by adjusting the skip value to keep moving forward through the data.\n3. Skip + Top: In this example, skip + top = 10000 + 1 = 10001. This value exceeds the skip limit of 10,000.\n4. Impact: Because this skip value (10001) exceeds the limit of 10,000, the API will not include the @odata.nextLink in the response. Essentially, the API is signaling that it cannot paginate beyond this point.\nImportant Note\nAfter the change, when the skip value exceeds 10,000, the API will no longer return the @odata.nextLink for further pagination, meaning you can’t retrieve records beyond this limit via pagination using @odata.nextLink.\nThe affected query parameters for OpenSearch interface:\nOpenSearch Catalogue API\nmaxRecords\npage\nindex\nOpenSearch API request\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&page=667&productType=IW_SLC__1S\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"a70c9f16-fc45-5ecb-b1e9-4433f14946f4\",\n    \"totalResults\": 2381730,\n    \"exactCount\": 1,\n    \"startIndex\": 10002,\n    \"itemsPerPage\": 15,\n    \"query\": {\n      \"originalFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 10.239139726\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&page=667&productType=IW_SLC__1S\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"previous\",\n        \"type\": \"application/json\",\n        \"title\": \"previous\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=666\"\n      },\n      {\n        \"rel\": \"first\",\n        \"type\": \"application/json\",\n        \"title\": \"first\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=1\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=668\"\n      },\n      {\n        \"rel\": \"last\",\n        \"type\": \"application/json\",\n        \"title\": \"last\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?maxRecords=15&exactCount=1&index=12&productType=IW_SLC__1S&page=158782\"\n      }\n    ]\n  },\n    \"features\": [...]\n}\n\n\n400 Bad Request\n \n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"page\",\n                    \"maxRecords\",\n                    \"index\"\n                ],\n                \"msg\": \"The '(page - 1) * maxRecords + index - 1' should be less than 10000. By default, 'maxRecords' is set to 20, while both 'page' and 'index' are set to 1 if not specified.\"\n            }\n        ],\n        \"RequestID\": \"5ec27fc6-2b2d-4513-b61e-ccc09b8f0bba\"\n    }\n}\n\n\n\nIndex limit in OpenSearch Catalogue API\nOpenSearch API request\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?&index=10002&productType=IW_SLC__1S\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"a70c9f16-fc45-5ecb-b1e9-4433f14946f4\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 10002,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"productType\": \"IW_SLC__1S\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.296336496\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?&index=10002&productType=IW_SLC__1S\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?index=10002&productType=IW_SLC__1S&page=2\"\n      }\n    ]\n  },\n  \"features\": [...]\n}\n\n\n400 Bad Request\n \n{\n  \"detail\": {\n    \"ErrorMessage\": \"Validation error.\",\n    \"ErrorCode\": 400,\n    \"ErrorDetail\": [\n      {\n        \"loc\": [\n          \"index\"\n        ],\n        \"msg\": \"Input should be less than or equal to 10001.\"\n      }\n    ],\n    \"RequestID\": \"09ff2890-c925-497a-a11d-95b308437b8e\"\n  }\n}\n\n\n\nImportant Note\nAfter the change, when the ‘(page - 1) * maxRecords + index - 1’ value exceeds 10,000, the API will no longer return the ‘next’ link for further pagination, meaning you can’t retrieve records beyond this limit via pagination using ‘next’ link.\nThe affected query parameters for STAC Catalogue API interface:\nSTAC Catalogue API\nlimit\npage\nSTAC API request\nhttps://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=12&sortby=datetime\n\nCurrent ResponseNew Response\n\n\n200 OK\n \n\"type\": \"FeatureCollection\",\n\"features\": [...],\n\"links\": [\n    {\n        \"rel\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=13&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"prev\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=11&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"first\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=1&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/search?limit=1000&page=12&sortby=datetime\",\n        \"type\": \"application/json\"\n    },\n    {\n        \"rel\": \"root\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/stac\",\n        \"type\": \"application/json\"\n    }\n ]\n} \n\n\n400 Bad Request\n \n{\n    \"code\": \"400\",\n    \"description\": \"The '(page - 1) * limit' should be less than 10000. By default, 'limit' is set to 20 and 'page' is set to 1 if not specified.\"\n    \"request_id\": \"4e681b43-5b47-4437-a0b1-f11b3c36a24a\"\n}\n\n\n\nImportant Note\nAfter the change, when the ‘(page - 1) * limit’ value exceeds 10,000, the API will no longer return the ‘next’ link for further pagination, meaning you can’t retrieve records beyond this limit via pagination using ‘next’ link.\nWe recommend reviewing the upcoming changes to Catalogue OData, OpenSearch and STAC API interfaces described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#odata-catalogue-api-response-format-change-rescheduled",
    "href": "APIs/Others/UpcomingChanges.html#odata-catalogue-api-response-format-change-rescheduled",
    "title": "Upcoming changes",
    "section": "OData Catalogue API: Response Format Change Rescheduled",
    "text": "OData Catalogue API: Response Format Change Rescheduled\nPlease be informed that OData Catalogue API Response Format Change was rescheduled to 24th September 2024. Therefore, all changes will be implemented starting from 24th September 2024."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#odata-catalogue-api-response-format-change",
    "href": "APIs/Others/UpcomingChanges.html#odata-catalogue-api-response-format-change",
    "title": "Upcoming changes",
    "section": "OData Catalogue API: Response Format Change",
    "text": "OData Catalogue API: Response Format Change\nWe would like to inform you about an upcoming change to our OData API response format, effective 24th September 2024 10th September 2024.\nStarting from 24th September 2024 10th September 2024, the format of the @odata.context property in the OData API responses will be updated when using the $expand query option. The new @odata.context format will list all expanded entities separated by commas. This change is aimed at aligning Catalogue API with the OData standards and improving the consistency and clarity of Catalogue OData API responses.\n\nCurrent FormatNew Format\n\n\n\"@odata.context\": \"$metadata#Products(Attributes())(Locations())(Assets())\"\n\n\n\"@odata.context\": \"$metadata#Products(Attributes(),Locations(),Assets())\"\n\n\n\nBelow please find the example of ODate Catalogue API response before and after the described change:\n\nHTTPS RequestCurrent ResponseNew Response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Name eq 'S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE'&$expand=Attributes&$expand=Locations&$expand=Assets\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes())(Locations())(Assets())\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"72ac8773-c55d-526a-a9a9-22ddb9b595ef\",\n      \"Name\": \"S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 0,\n      \"OriginDate\": \"2021-02-20T13:38:02.110Z\",\n      \"PublicationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"ModificationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"9999-12-31T23:59:59.999Z\",\n      \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"Checksum\": [\n         \n      ],\n      \"ContentDate\": {\n        \"Start\": \"2016-01-07T11:00:59.171Z\",\n        \"End\": \"2016-01-07T11:01:26.112Z\"\n      },\n      \"Footprint\": \"geography'SRID=4326;MULTIPOLYGON (((-79.007256 -3.315278, -78.661591 -1.686242, -80.881958 -1.204264, -81.230759 -2.827857, -79.007256 -3.315278)))'\",\n      \"GeoFootprint\": {\n        \"type\": \"MultiPolygon\",\n        \"coordinates\": [\n          [\n            [\n              [\n                -79.007256,\n                -3.315278\n              ],\n              [\n                -78.661591,\n                -1.686242\n              ],\n              [\n                -80.881958,\n                -1.204264\n              ],\n              [\n                -81.230759,\n                -2.827857\n              ],\n              [\n                -79.007256,\n                -3.315278\n              ]\n            ]\n          ]\n        ]\n      },\n      \"Attributes\": [\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"authority\",\n          \"Value\": \"ESA\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"coordinates\",\n          \"Value\": \"-3.315278,-79.007256 -2.827857,-81.230759 -1.204264,-80.881958 -1.686242,-78.661591 -3.315278,-79.007256\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"orbitNumber\",\n          \"Value\": 9387,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productType\",\n          \"Value\": \"IW_SLC__1S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"sliceNumber\",\n          \"Value\": 3,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productClass\",\n          \"Value\": \"S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"endingDateTime\",\n          \"Value\": \"2016-01-07T11:01:26.112Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"orbitDirection\",\n          \"Value\": \"DESCENDING\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productGroupId\",\n          \"Value\": \"55660\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"operationalMode\",\n          \"Value\": \"IW\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"processingLevel\",\n          \"Value\": \"LEVEL1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"swathIdentifier\",\n          \"Value\": \"IW1 IW2 IW3\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"beginningDateTime\",\n          \"Value\": \"2016-01-07T11:00:59.171Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformShortName\",\n          \"Value\": \"SENTINEL-1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"spatialResolution\",\n          \"Value\": \"2.3\",\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"instrumentShortName\",\n          \"Value\": \"SAR\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"relativeOrbitNumber\",\n          \"Value\": 40,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"polarisationChannels\",\n          \"Value\": \"VV\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformSerialIdentifier\",\n          \"Value\": \"A\",\n          \"ValueType\": \"String\"\n        }\n      ],\n      \"Assets\": [\n         \n      ],\n      \"Locations\": [\n        {\n          \"FormatType\": \"Extracted\",\n          \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(72ac8773-c55d-526a-a9a9-22ddb9b595ef)/$value\",\n          \"ContentLength\": 0,\n          \"Checksum\": [\n             \n          ],\n          \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\"\n        }\n      ]\n    }\n  ]\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes(),Locations(),Assets())\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"72ac8773-c55d-526a-a9a9-22ddb9b595ef\",\n      \"Name\": \"S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 0,\n      \"OriginDate\": \"2021-02-20T13:38:02.110Z\",\n      \"PublicationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"ModificationDate\": \"2021-02-20T13:52:41.446Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"9999-12-31T23:59:59.999Z\",\n      \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\",\n      \"Checksum\": [\n         \n      ],\n      \"ContentDate\": {\n        \"Start\": \"2016-01-07T11:00:59.171Z\",\n        \"End\": \"2016-01-07T11:01:26.112Z\"\n      },\n      \"Footprint\": \"geography'SRID=4326;MULTIPOLYGON (((-79.007256 -3.315278, -78.661591 -1.686242, -80.881958 -1.204264, -81.230759 -2.827857, -79.007256 -3.315278)))'\",\n      \"GeoFootprint\": {\n        \"type\": \"MultiPolygon\",\n        \"coordinates\": [\n          [\n            [\n              [\n                -79.007256,\n                -3.315278\n              ],\n              [\n                -78.661591,\n                -1.686242\n              ],\n              [\n                -80.881958,\n                -1.204264\n              ],\n              [\n                -81.230759,\n                -2.827857\n              ],\n              [\n                -79.007256,\n                -3.315278\n              ]\n            ]\n          ]\n        ]\n      },\n      \"Attributes\": [\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"authority\",\n          \"Value\": \"ESA\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"coordinates\",\n          \"Value\": \"-3.315278,-79.007256 -2.827857,-81.230759 -1.204264,-80.881958 -1.686242,-78.661591 -3.315278,-79.007256\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"orbitNumber\",\n          \"Value\": 9387,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productType\",\n          \"Value\": \"IW_SLC__1S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"sliceNumber\",\n          \"Value\": 3,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productClass\",\n          \"Value\": \"S\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"endingDateTime\",\n          \"Value\": \"2016-01-07T11:01:26.112Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"orbitDirection\",\n          \"Value\": \"DESCENDING\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"productGroupId\",\n          \"Value\": \"55660\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"operationalMode\",\n          \"Value\": \"IW\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"processingLevel\",\n          \"Value\": \"LEVEL1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"swathIdentifier\",\n          \"Value\": \"IW1 IW2 IW3\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n          \"Name\": \"beginningDateTime\",\n          \"Value\": \"2016-01-07T11:00:59.171Z\",\n          \"ValueType\": \"DateTimeOffset\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformShortName\",\n          \"Value\": \"SENTINEL-1\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"spatialResolution\",\n          \"Value\": \"2.3\",\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"instrumentShortName\",\n          \"Value\": \"SAR\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n          \"Name\": \"relativeOrbitNumber\",\n          \"Value\": 40,\n          \"ValueType\": \"Integer\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"polarisationChannels\",\n          \"Value\": \"VV\",\n          \"ValueType\": \"String\"\n        },\n        {\n          \"@odata.type\": \"#OData.CSC.StringAttribute\",\n          \"Name\": \"platformSerialIdentifier\",\n          \"Value\": \"A\",\n          \"ValueType\": \"String\"\n        }\n      ],\n      \"Assets\": [\n         \n      ],\n      \"Locations\": [\n        {\n          \"FormatType\": \"Extracted\",\n          \"DownloadLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(72ac8773-c55d-526a-a9a9-22ddb9b595ef)/$value\",\n          \"ContentLength\": 0,\n          \"Checksum\": [\n             \n          ],\n          \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2016/01/07/S1A_IW_SLC__1SSV_20160107T110059_20160107T110126_009387_00D96C_5CE9.SAFE\"\n        }\n      ]\n    }\n  ]\n}\n\n\n\nWe recommend reviewing the upcoming changes to Catalogue OData API described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#datetime-precision-change-for-odata-opensearch-and-stac-apis",
    "href": "APIs/Others/UpcomingChanges.html#datetime-precision-change-for-odata-opensearch-and-stac-apis",
    "title": "Upcoming changes",
    "section": "Datetime precision change for OData, OpenSearch and STAC APIs",
    "text": "Datetime precision change for OData, OpenSearch and STAC APIs\nWe would like to inform you about an upcoming change to our OData, OpenSearch and STAC API interfaces, effective 2 September 2024.\nStarting from 2 September 2024, the display precision of all attributes in datetime format within OData, OpenSearch and STAC API interfaces will be extended to 6 digits. This change is being made to align with the values provided in the product’s metadata and by data sources as well as to standardize the precision across all datetime attributes.\nThe searching queries for all datetime attributes in OData, OpenSearch ans STAC API interfaces will remain unchanged.\nPlease find below a list of attributes within our API interfaces that can be affected by this change:\n\nOdata API\n\n\nContentDate/Start\nContentDate/End\nOriginDate\nPublicationDate\nModificationDate\nEvictionDate\nbeginningDateTime\nendingDateTime\nprocessingDate\nproductGeneration\nsegmentStartTime\n\n\n\n\nOpeanSerach API\n\n\nstartDate\ncompletionDate\nupdated\npublished\n\n\n\n\nSTAC API\n\n\ndatetime\nstart_datetime\nend_datetime\n\n\n\nBelow please find some examples showing the upociming changes on the exemplary product with id=521bd8f9-48a5-4e1f-9435-58f97cb64d39.\n\nOData API RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products(521bd8f9-48a5-4e1f-9435-58f97cb64d39)?$expand=Attributes\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes())/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n  \"Name\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 2121095339,\n  \"OriginDate\": \"2024-06-04T12:12:03.404Z\",\n  \"PublicationDate\": \"2024-06-04T12:18:40.284Z\",\n  \"ModificationDate\": \"2024-06-04T12:19:02.061Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"\",\n  \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"4eec48737ecab7aab2bf590a8dec23b1\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-06-04T12:18:58.614664Z\"\n    },\n    {\n      \"Value\": \"d4e3b3f4fb3e076edc9a841fe74d2a33833fba6685c7854e22026a341038e291\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-06-04T12:19:01.850023Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-06-04T11:01:00.495Z\",\n    \"End\": \"2024-06-04T11:01:31.090Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((113.605408 -29.713636, 116.147186 -29.10652, 115.581909 -27.285719, 113.080147 -27.882109, 113.605408 -29.713636))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"Attributes\": [\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"origin\",\n      \"Value\": \"ESA\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"datatakeID\",\n      \"Value\": 431760,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"timeliness\",\n      \"Value\": \"Fast-24h\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"cycleNumber\",\n      \"Value\": 324,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"orbitNumber\",\n      \"Value\": 54172,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"sliceNumber\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"totalSlices\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productClass\",\n      \"Value\": \"S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorName\",\n      \"Value\": \"Sentinel-1 IPF\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"orbitDirection\",\n      \"Value\": \"ASCENDING\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"processingDate\",\n      \"Value\": \"2024-06-04T12:03:49.113620+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"operationalMode\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingLevel\",\n      \"Value\": \"LEVEL1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"swathIdentifier\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingCenter\",\n      \"Value\": \"Production Service-SERCO\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorVersion\",\n      \"Value\": \"003.71\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"segmentStartTime\",\n      \"Value\": \"2024-06-04T10:59:16.794000+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.BooleanAttribute\",\n      \"Name\": \"sliceProductFlag\",\n      \"Value\": false,\n      \"ValueType\": \"Boolean\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformShortName\",\n      \"Value\": \"SENTINEL-1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productComposition\",\n      \"Value\": \"Slice\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"instrumentShortName\",\n      \"Value\": \"SAR\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"relativeOrbitNumber\",\n      \"Value\": 25,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"polarisationChannels\",\n      \"Value\": \"VV&VH\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformSerialIdentifier\",\n      \"Value\": \"A\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"instrumentConfigurationID\",\n      \"Value\": 7,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"startTimeFromAscendingNode\",\n      \"Value\": 5419578.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"completionTimeFromAscendingNode\",\n      \"Value\": 5450173.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productType\",\n      \"Value\": \"IW_GRDH_1S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"beginningDateTime\",\n      \"Value\": \"2024-06-04T11:01:00.495Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"endingDateTime\",\n      \"Value\": \"2024-06-04T11:01:31.090Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    }\n  ]\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products(Attributes())/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n  \"Name\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 2121095339,\n  \"OriginDate\": \"2024-06-04T12:12:03.404000Z\",\n  \"PublicationDate\": \"2024-06-04T12:18:40.284131Z\",\n  \"ModificationDate\": \"2024-06-04T12:19:02.060894Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"\",\n  \"S3Path\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"4eec48737ecab7aab2bf590a8dec23b1\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-06-04T12:18:58.614664Z\"\n    },\n    {\n      \"Value\": \"d4e3b3f4fb3e076edc9a841fe74d2a33833fba6685c7854e22026a341038e291\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-06-04T12:19:01.850023Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-06-04T11:01:00.494767Z\",\n    \"End\": \"2024-06-04T11:01:31.089640Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((113.605408 -29.713636, 116.147186 -29.10652, 115.581909 -27.285719, 113.080147 -27.882109, 113.605408 -29.713636))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"Attributes\": [\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"origin\",\n      \"Value\": \"ESA\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"datatakeID\",\n      \"Value\": 431760,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"timeliness\",\n      \"Value\": \"Fast-24h\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"cycleNumber\",\n      \"Value\": 324,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"orbitNumber\",\n      \"Value\": 54172,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"sliceNumber\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"totalSlices\",\n      \"Value\": 5,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productClass\",\n      \"Value\": \"S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorName\",\n      \"Value\": \"Sentinel-1 IPF\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"orbitDirection\",\n      \"Value\": \"ASCENDING\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"processingDate\",\n      \"Value\": \"2024-06-04T12:03:49.113620+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"operationalMode\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingLevel\",\n      \"Value\": \"LEVEL1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"swathIdentifier\",\n      \"Value\": \"IW\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processingCenter\",\n      \"Value\": \"Production Service-SERCO\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"processorVersion\",\n      \"Value\": \"003.71\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"segmentStartTime\",\n      \"Value\": \"2024-06-04T10:59:16.794000+00:00\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.BooleanAttribute\",\n      \"Name\": \"sliceProductFlag\",\n      \"Value\": false,\n      \"ValueType\": \"Boolean\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformShortName\",\n      \"Value\": \"SENTINEL-1\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productComposition\",\n      \"Value\": \"Slice\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"instrumentShortName\",\n      \"Value\": \"SAR\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"relativeOrbitNumber\",\n      \"Value\": 25,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"polarisationChannels\",\n      \"Value\": \"VV&VH\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"platformSerialIdentifier\",\n      \"Value\": \"A\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.IntegerAttribute\",\n      \"Name\": \"instrumentConfigurationID\",\n      \"Value\": 7,\n      \"ValueType\": \"Integer\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"startTimeFromAscendingNode\",\n      \"Value\": 5419578.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DoubleAttribute\",\n      \"Name\": \"completionTimeFromAscendingNode\",\n      \"Value\": 5450173.0,\n      \"ValueType\": \"Double\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.StringAttribute\",\n      \"Name\": \"productType\",\n      \"Value\": \"IW_GRDH_1S\",\n      \"ValueType\": \"String\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"beginningDateTime\",\n      \"Value\": \"2024-06-04T11:01:00.494767Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    },\n    {\n      \"@odata.type\": \"#OData.CSC.DateTimeOffsetAttribute\",\n      \"Name\": \"endingDateTime\",\n      \"Value\": \"2024-06-04T11:01:31.089640Z\",\n      \"ValueType\": \"DateTimeOffset\"\n    }\n  ]\n}\n\n\n\n\nOpenSearch API RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=521bd8f9-48a5-4e1f-9435-58f97cb64d39\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"24391a89-5880-56c2-bdf5-f35f3a41178b\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.030846153\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=521bd8f9-48a5-4e1f-9435-58f97cb64d39\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [\n              113.605408,\n              -29.713636\n            ],\n            [\n              116.147186,\n              -29.10652\n            ],\n            [\n              115.581909,\n              -27.285719\n            ],\n            [\n              113.080147,\n              -27.882109\n            ],\n            [\n              113.605408,\n              -29.713636\n            ]\n          ]\n        ]\n      },\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2024-06-04T11:01:00.495Z\",\n        \"completionDate\": \"2024-06-04T11:01:31.090Z\",\n        \"productType\": \"IW_GRDH_1S\",\n        \"processingLevel\": \"LEVEL1\",\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": \"IW\",\n        \"orbitNumber\": 54172,\n        \"quicklook\": null,\n        \"thumbnail\": \"https://catalogue.dataspace.copernicus.eu/get-object?path=/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE/preview/thumbnail.png\",\n        \"updated\": \"2024-06-04T12:19:02.061Z\",\n        \"published\": \"2024-06-04T12:18:40.284Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": \"&lt;gml:Polygon srsName=\\\"EPSG:4326\\\"&gt;&lt;gml:outerBoundaryIs&gt;&lt;gml:LinearRing&gt;&lt;gml:coordinates&gt;113.605408,-29.713636 116.147186,-29.10652 115.581909,-27.285719 113.080147,-27.882109 113.605408,-29.713636&lt;/gml:coordinates&gt;&lt;/gml:LinearRing&gt;&lt;/gml:outerBoundaryIs&gt;&lt;/gml:Polygon&gt;\",\n        \"centroid\": {\n          \"type\": \"Point\",\n          \"coordinates\": [\n            114.604262987064,\n            -28.4994608096117\n          ]\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"orbitDirection\": \"ASCENDING\",\n        \"timeliness\": \"Fast-24h\",\n        \"relativeOrbitNumber\": 25,\n        \"processingBaseline\": 3.71,\n        \"polarisation\": \"VV&VH\",\n        \"swath\": \"IW\",\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 2121095339\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/521bd8f9-48a5-4e1f-9435-58f97cb64d39.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"24391a89-5880-56c2-bdf5-f35f3a41178b\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.030846153\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=521bd8f9-48a5-4e1f-9435-58f97cb64d39\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [\n              113.605408,\n              -29.713636\n            ],\n            [\n              116.147186,\n              -29.10652\n            ],\n            [\n              115.581909,\n              -27.285719\n            ],\n            [\n              113.080147,\n              -27.882109\n            ],\n            [\n              113.605408,\n              -29.713636\n            ]\n          ]\n        ]\n      },\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2024-06-04T11:01:00.494767Z\",\n        \"completionDate\": \"2024-06-04T11:01:31.089640Z\",\n        \"productType\": \"IW_GRDH_1S\",\n        \"processingLevel\": \"LEVEL1\",\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": \"IW\",\n        \"orbitNumber\": 54172,\n        \"quicklook\": null,\n        \"thumbnail\": \"https://catalogue.dataspace.copernicus.eu/get-object?path=/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE/preview/thumbnail.png\",\n        \"updated\": \"2024-06-04T12:19:02.060894Z\",\n        \"published\": \"2024-06-04T12:18:40.284131Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": \"&lt;gml:Polygon srsName=\\\"EPSG:4326\\\"&gt;&lt;gml:outerBoundaryIs&gt;&lt;gml:LinearRing&gt;&lt;gml:coordinates&gt;113.605408,-29.713636 116.147186,-29.10652 115.581909,-27.285719 113.080147,-27.882109 113.605408,-29.713636&lt;/gml:coordinates&gt;&lt;/gml:LinearRing&gt;&lt;/gml:outerBoundaryIs&gt;&lt;/gml:Polygon&gt;\",\n        \"centroid\": {\n          \"type\": \"Point\",\n          \"coordinates\": [\n            114.604262987064,\n            -28.4994608096117\n          ]\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n        \"orbitDirection\": \"ASCENDING\",\n        \"timeliness\": \"Fast-24h\",\n        \"relativeOrbitNumber\": 25,\n        \"processingBaseline\": 3.71,\n        \"polarisation\": \"VV&VH\",\n        \"swath\": \"IW\",\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 2121095339\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 521bd8f9-48a5-4e1f-9435-58f97cb64d39\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/521bd8f9-48a5-4e1f-9435-58f97cb64d39.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n\n\nSTAC API RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items?ids=S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\n\n\n{\n  \"type\": \"Feature\",\n  \"stac_version\": \"1.0.0\",\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/alternate-assets/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/storage/v1.0.0/schema.json\"\n  ],\n  \"id\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"collection\": \"SENTINEL-1\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"properties\": {\n    \"origin\": \"ESA\",\n    \"datatakeID\": 431760,\n    \"timeliness\": \"Fast-24h\",\n    \"cycleNumber\": 324,\n    \"orbitNumber\": 54172,\n    \"sliceNumber\": 5,\n    \"totalSlices\": 5,\n    \"productClass\": \"S\",\n    \"processorName\": \"Sentinel-1 IPF\",\n    \"orbitDirection\": \"ASCENDING\",\n    \"processingDate\": \"2024-06-04T12:03:49.113620+00:00\",\n    \"operationalMode\": \"IW\",\n    \"processingLevel\": \"LEVEL1\",\n    \"swathIdentifier\": \"IW\",\n    \"processingCenter\": \"Production Service-SERCO\",\n    \"processorVersion\": \"003.71\",\n    \"segmentStartTime\": \"2024-06-04T10:59:16.794000+00:00\",\n    \"sliceProductFlag\": false,\n    \"platformShortName\": \"SENTINEL-1\",\n    \"productComposition\": \"Slice\",\n    \"instrumentShortName\": \"SAR\",\n    \"relativeOrbitNumber\": 25,\n    \"polarisationChannels\": \"VV&VH\",\n    \"platformSerialIdentifier\": \"A\",\n    \"instrumentConfigurationID\": 7,\n    \"startTimeFromAscendingNode\": 5419578.0,\n    \"completionTimeFromAscendingNode\": 5450173.0,\n    \"datetime\": \"2024-06-04T11:01:00.495Z\",\n    \"end_datetime\": \"2024-06-04T11:01:31.090Z\",\n    \"start_datetime\": \"2024-06-04T11:01:00.495Z\",\n    \"productType\": \"IW_GRDH_1S\"\n  },\n  \"bbox\": [\n    113.080147,\n    -29.713636,\n    116.147186,\n    -27.285719\n  ],\n  \"links\": [\n    {\n      \"rel\": \"root\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac\"\n    },\n    {\n      \"rel\": \"self\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\"\n    },\n    {\n      \"rel\": \"collection\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1\"\n    }\n  ],\n  \"assets\": {\n    \"QUICKLOOK\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(28feb491-5c4f-41d4-8444-1e5d8426c478)/$value\",\n      \"title\": \"QUICKLOOK\",\n      \"type\": \"image/png\"\n    },\n    \"PRODUCT\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(521bd8f9-48a5-4e1f-9435-58f97cb64d39)/$value\",\n      \"title\": \"Product\",\n      \"type\": \"application/octet-stream\",\n      \"alternate\": {\n        \"s3\": {\n          \"href\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n          \"storage:platform\": \"CLOUDFERRO\",\n          \"storage:region\": \"waw\",\n          \"storage:requester_pays\": false,\n          \"storage:tier\": \"Online\"\n        }\n      }\n    }\n  }\n}\n\n\n{\n  \"type\": \"Feature\",\n  \"stac_version\": \"1.0.0\",\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/alternate-assets/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/storage/v1.0.0/schema.json\"\n  ],\n  \"id\": \"S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n  \"collection\": \"SENTINEL-1\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [\n        [\n          113.605408,\n          -29.713636\n        ],\n        [\n          116.147186,\n          -29.10652\n        ],\n        [\n          115.581909,\n          -27.285719\n        ],\n        [\n          113.080147,\n          -27.882109\n        ],\n        [\n          113.605408,\n          -29.713636\n        ]\n      ]\n    ]\n  },\n  \"properties\": {\n    \"origin\": \"ESA\",\n    \"datatakeID\": 431760,\n    \"timeliness\": \"Fast-24h\",\n    \"cycleNumber\": 324,\n    \"orbitNumber\": 54172,\n    \"sliceNumber\": 5,\n    \"totalSlices\": 5,\n    \"productClass\": \"S\",\n    \"processorName\": \"Sentinel-1 IPF\",\n    \"orbitDirection\": \"ASCENDING\",\n    \"processingDate\": \"2024-06-04T12:03:49.113620+00:00\",\n    \"operationalMode\": \"IW\",\n    \"processingLevel\": \"LEVEL1\",\n    \"swathIdentifier\": \"IW\",\n    \"processingCenter\": \"Production Service-SERCO\",\n    \"processorVersion\": \"003.71\",\n    \"segmentStartTime\": \"2024-06-04T10:59:16.794000+00:00\",\n    \"sliceProductFlag\": false,\n    \"platformShortName\": \"SENTINEL-1\",\n    \"productComposition\": \"Slice\",\n    \"instrumentShortName\": \"SAR\",\n    \"relativeOrbitNumber\": 25,\n    \"polarisationChannels\": \"VV&VH\",\n    \"platformSerialIdentifier\": \"A\",\n    \"instrumentConfigurationID\": 7,\n    \"startTimeFromAscendingNode\": 5419578.0,\n    \"completionTimeFromAscendingNode\": 5450173.0,\n    \"datetime\": \"2024-06-04T11:01:00.494767Z\",\n    \"end_datetime\": \"2024-06-04T11:01:31.089640Z\",\n    \"start_datetime\": \"2024-06-04T11:01:00.494767Z\",\n    \"productType\": \"IW_GRDH_1S\"\n  },\n  \"bbox\": [\n    113.080147,\n    -29.713636,\n    116.147186,\n    -27.285719\n  ],\n  \"links\": [\n    {\n      \"rel\": \"root\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac\"\n    },\n    {\n      \"rel\": \"self\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1/items/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\"\n    },\n    {\n      \"rel\": \"collection\",\n      \"type\": \"application/json\",\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-1\"\n    }\n  ],\n  \"assets\": {\n    \"QUICKLOOK\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Assets(28feb491-5c4f-41d4-8444-1e5d8426c478)/$value\",\n      \"title\": \"QUICKLOOK\",\n      \"type\": \"image/png\"\n    },\n    \"PRODUCT\": {\n      \"href\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products(521bd8f9-48a5-4e1f-9435-58f97cb64d39)/$value\",\n      \"title\": \"Product\",\n      \"type\": \"application/octet-stream\",\n      \"alternate\": {\n        \"s3\": {\n          \"href\": \"/eodata/Sentinel-1/SAR/IW_GRDH_1S/2024/06/04/S1A_IW_GRDH_1SDV_20240604T110100_20240604T110131_054172_069690_E33E.SAFE\",\n          \"storage:platform\": \"CLOUDFERRO\",\n          \"storage:region\": \"waw\",\n          \"storage:requester_pays\": false,\n          \"storage:tier\": \"Online\"\n        }\n      }\n    }\n  }\n}\n\n\n\nWe recommend reviewing the upcoming changes to Catalogue OData, OpenSearch and STAC API interfaces described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#odata-catalogue-api-evictiondate-attribute-update-for-null-values",
    "href": "APIs/Others/UpcomingChanges.html#odata-catalogue-api-evictiondate-attribute-update-for-null-values",
    "title": "Upcoming changes",
    "section": "OData Catalogue API: EvictionDate Attribute Update for Null Values",
    "text": "OData Catalogue API: EvictionDate Attribute Update for Null Values\nWe would like to inform you about an upcoming change to our OData API interface, effective July 31, 2024.\nStarting from July 31, 2024, the EvictionDate attribute will be set as a date far in the future for cases where the values are empty, which indicates that the products are not going to be deleted from the Catalogue. The new value will be: \"EvictionDate\": \"9999-12-31T23:59:59.999Z\". Currently, for the products without an eviction date set, the Catalogues returns empty strings \"EvictionDate\": \"\".\nBelow please find the example of ODate API response before and after the described change:\n\nHTTPS RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products(02807be7-482e-460e-8a29-7e4ab2320758)\n\n\n\n{\n  \"@odata.context\": \"$metadata#Products/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"02807be7-482e-460e-8a29-7e4ab2320758\",\n  \"Name\": \"S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 28102178,\n  \"OriginDate\": \"2024-07-09T08:35:54.000Z\",\n  \"PublicationDate\": \"2024-07-09T08:44:52.873Z\",\n  \"ModificationDate\": \"2024-07-09T08:45:31.989Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"\",\n  \"S3Path\": \"/eodata/Sentinel-2/MSI/L1C/2024/07/09/S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"3b44d7346a9e8c2e59487f8a9cb86a3f\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.631883Z\"\n    },\n    {\n      \"Value\": \"1c83b6242f72f7daa6a90c350853df95060c755a611542d1b825de781dfe0de2\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.698943Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-07-09T07:46:19.024Z\",\n    \"End\": \"2024-07-09T07:46:19.024Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((62.65582103662547 71.92607834342967, 63.06489612730844 71.91662516221517, 63.18741013834509 72.27252875746295, 63.1849607572785 72.27097856029074, 62.97493608061198 72.13567642459181, 62.76758219658625 72.00017809777046, 62.65582103662547 71.92607834342967))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [...\n    ]\n  }\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products/$entity\",\n  \"@odata.mediaContentType\": \"application/octet-stream\",\n  \"Id\": \"02807be7-482e-460e-8a29-7e4ab2320758\",\n  \"Name\": \"S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"ContentType\": \"application/octet-stream\",\n  \"ContentLength\": 28102178,\n  \"OriginDate\": \"2024-07-09T08:35:54.000Z\",\n  \"PublicationDate\": \"2024-07-09T08:44:52.873Z\",\n  \"ModificationDate\": \"2024-07-09T08:45:31.989Z\",\n  \"Online\": true,\n  \"EvictionDate\": \"9999-12-31T23:59:59.999Z\",\n  \"S3Path\": \"/eodata/Sentinel-2/MSI/L1C/2024/07/09/S2B_MSIL1C_20240709T074619_N0510_R135_T40XFF_20240709T080457.SAFE\",\n  \"Checksum\": [\n    {\n      \"Value\": \"3b44d7346a9e8c2e59487f8a9cb86a3f\",\n      \"Algorithm\": \"MD5\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.631883Z\"\n    },\n    {\n      \"Value\": \"1c83b6242f72f7daa6a90c350853df95060c755a611542d1b825de781dfe0de2\",\n      \"Algorithm\": \"BLAKE3\",\n      \"ChecksumDate\": \"2024-07-09T08:45:31.698943Z\"\n    }\n  ],\n  \"ContentDate\": {\n    \"Start\": \"2024-07-09T07:46:19.024Z\",\n    \"End\": \"2024-07-09T07:46:19.024Z\"\n  },\n  \"Footprint\": \"geography'SRID=4326;POLYGON ((62.65582103662547 71.92607834342967, 63.06489612730844 71.91662516221517, 63.18741013834509 72.27252875746295, 63.1849607572785 72.27097856029074, 62.97493608061198 72.13567642459181, 62.76758219658625 72.00017809777046, 62.65582103662547 71.92607834342967))'\",\n  \"GeoFootprint\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [...\n    ]\n  }\n}\n\n\n\nWe recommend reviewing the upcoming changes to Catalogue OData API described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#opensearch-catalogue-api-geometry-attribute-handling-updated-for-null-values",
    "href": "APIs/Others/UpcomingChanges.html#opensearch-catalogue-api-geometry-attribute-handling-updated-for-null-values",
    "title": "Upcoming changes",
    "section": "OpenSearch Catalogue API: Geometry Attribute Handling Updated for Null Values",
    "text": "OpenSearch Catalogue API: Geometry Attribute Handling Updated for Null Values\nWe would like to inform you about an upcoming change to our OpenSearch API interface, effective 8th July, 2024.\nStarting from 8th July 2024, the geometry attribute in case of empty product geometries will return null (\"geometry\": null,) instead of empty array (\"geometry\": [ ],). This update is designed to improve the clarity and consistency of the data returned by our APIs. For non-empty products geometries, the behaviour will remain unchanged.\nBelow please find the example of OpenSearch API response before and after the described change:\n\nHTTPS RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=cf1a597c-ec22-11ee-8006-fa163e7968e5\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"8cf9566f-ad45-5ed7-b586-7ff0f8d6c677\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.121144462\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=cf1a597c-ec22-11ee-8006-fa163e7968e5\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n      \"geometry\": [\n         \n      ],\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2014-09-08T00:00:00.000Z\",\n        \"completionDate\": \"2014-09-08T00:00:00.000Z\",\n        \"productType\": \"AUX_CAL\",\n        \"processingLevel\": null,\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": null,\n        \"orbitNumber\": 0,\n        \"quicklook\": null,\n        \"thumbnail\": null,\n        \"updated\": \"2024-03-27T10:15:07.353Z\",\n        \"published\": \"2024-03-27T10:15:06.733Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": null,\n        \"centroid\": {\n          \"type\": null,\n          \"coordinates\": null\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/AUX/AUX_CAL/2014/09/08/S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE.zip\",\n        \"orbitDirection\": null,\n        \"timeliness\": null,\n        \"relativeOrbitNumber\": 0,\n        \"processingBaseline\": 0,\n        \"polarisation\": null,\n        \"swath\": null,\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 505960\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/cf1a597c-ec22-11ee-8006-fa163e7968e5.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"8cf9566f-ad45-5ed7-b586-7ff0f8d6c677\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 20,\n    \"query\": {\n      \"originalFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"identifier\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.121144462\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?identifier=cf1a597c-ec22-11ee-8006-fa163e7968e5\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n      \"geometry\": null,\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2014-09-08T00:00:00.000Z\",\n        \"completionDate\": \"2014-09-08T00:00:00.000Z\",\n        \"productType\": \"AUX_CAL\",\n        \"processingLevel\": null,\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": null,\n        \"orbitNumber\": 0,\n        \"quicklook\": null,\n        \"thumbnail\": null,\n        \"updated\": \"2024-03-27T10:15:07.353Z\",\n        \"published\": \"2024-03-27T10:15:06.733Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": null,\n        \"centroid\": {\n          \"type\": null,\n          \"coordinates\": null\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/AUX/AUX_CAL/2014/09/08/S1A_AUX_CAL_V20140908T000000_G20240327T101157.SAFE.zip\",\n        \"orbitDirection\": null,\n        \"timeliness\": null,\n        \"relativeOrbitNumber\": 0,\n        \"processingBaseline\": 0,\n        \"polarisation\": null,\n        \"swath\": null,\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 505960\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for cf1a597c-ec22-11ee-8006-fa163e7968e5\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/cf1a597c-ec22-11ee-8006-fa163e7968e5.json\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\n\n\nWe recommend reviewing the upcoming changes to Catalogue OpenSearch API described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#default-timezone-change-for-odata-and-opensearch-apis",
    "href": "APIs/Others/UpcomingChanges.html#default-timezone-change-for-odata-and-opensearch-apis",
    "title": "Upcoming changes",
    "section": "Default Timezone Change for OData and OpenSearch APIs",
    "text": "Default Timezone Change for OData and OpenSearch APIs\nWe would like to inform you about an upcoming change to our OData and OpenSearch API interfaces, effective 27th May 2024.\nStarting from 27th May 2024, all API requests without a specified timezone will be treated by default as datetime provided in UTC format.\nCurrently, if a client does not specify a timezone in their date request, it defaults to Warsaw local time. However, as of 27th May 2024, all API requests without a specified timezone will default to datetime provided in UTC format.\nThis change is aimed at standardizing our API responses and ensuring uniformity. Please review your systems and update your requests with datetime accordingly to accommodate this change.\nTo specify a timezone within the request:\nOData API (e.g. UTC-4)\n\nOData Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start gt 2022-05-03T00:00:00.000-04:00 and ContentDate/Start lt 2022-05-04T00:00:00.000-04:00&$top=2\n\n\n\nOpenSearch API (e.g. UTC+1)\n\nOpenSearch Request\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00+01:00&maxRecords=2\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note while making an API request, any special characters need to be encoded in a specific way so that they can be interpreted correctly by the server. In the case of the plus sign (+), it’s a reserved character in URLs and is interpreted as a space. So, if you want to represent a literal plus sign within your API request, you need to encode it as ‘%2b’.\n\n\nExamples of API requests without the timezone and API responses before and after the change:\nOData API Example\n\nHTTP RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-1' and ContentDate/Start gt 2022-05-03T00:00:00.000 and ContentDate/Start lt 2022-05-04T00:00:00.000&$top=2\n\n\n{\n  \"@odata.context\": \"$metadata#Products\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n      \"Name\": \"S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 8231197123,\n      \"OriginDate\": \"2022-05-02T23:30:02.126Z\",\n      \"PublicationDate\": \"2022-05-02T23:40:02.825Z\",\n      \"ModificationDate\": \"2024-03-16T03:19:06.436Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"\",\n      \"S3Path\": \"/eodata/Sentinel-1/SAR/SLC/2022/05/02/S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n      \"Checksum\": [\n        {\n          \"Value\": \"65940707f71f444b0fa05141657cc387\",\n          \"Algorithm\": \"MD5\",\n          \"ChecksumDate\": \"2024-03-16T03:18:49.857391Z\"\n        },\n        {\n          \"Value\": \"3d2d07a95aad14f1fb77ea5ba49485b6efee667578a74257d07b9edbd9d4912a\",\n          \"Algorithm\": \"BLAKE3\",\n          \"ChecksumDate\": \"2024-03-16T03:19:07.058832Z\"\n        }\n      ],\n      \"ContentDate\": {\n        \"Start\": \"2022-05-02T22:06:17.548Z\",\n        \"End\": \"2022-05-02T22:06:47.359Z\"\n      },\n      \"Footprint\": \"geography'SRID=4326;POLYGON ((-57.750202 -2.026322, -57.366844 -3.828814, -55.15321 -3.341953, -55.540607 -1.545508, -57.750202 -2.026322))'\",\n      \"GeoFootprint\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [\n              -57.750202,\n              -2.026322\n            ],\n            [\n              -57.366844,\n              -3.828814\n            ],\n            [\n              -55.15321,\n              -3.341953\n            ],\n            [\n              -55.540607,\n              -1.545508\n            ],\n            [\n              -57.750202,\n              -2.026322\n            ]\n          ]\n        ]\n      }\n    },\n    {...\n    }\n  ],\n  \"@odata.nextLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?%24filter=Collection%2FName+eq+%27SENTINEL-1%27+and+ContentDate%2FStart+gt+2022-05-03T00%3A00%3A00.000+and+ContentDate%2FStart+lt+2022-05-04T00%3A00%3A00.000&%24top=2&%24skip=2\"\n}\n\n\n{\n  \"@odata.context\": \"$metadata#Products\",\n  \"value\": [\n    {\n      \"@odata.mediaContentType\": \"application/octet-stream\",\n      \"Id\": \"1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n      \"Name\": \"S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n      \"ContentType\": \"application/octet-stream\",\n      \"ContentLength\": 2663000,\n      \"OriginDate\": \"2022-05-10T02:30:11.130Z\",\n      \"PublicationDate\": \"2023-10-25T13:45:19.736Z\",\n      \"ModificationDate\": \"2023-11-14T22:50:17.708Z\",\n      \"Online\": true,\n      \"EvictionDate\": \"\",\n      \"S3Path\": \"/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n      \"Checksum\": [\n        {\n          \"Value\": \"6a99572d2baaa3c9a83bd851ba3ba70f\",\n          \"Algorithm\": \"MD5\",\n          \"ChecksumDate\": \"2023-11-14T22:50:17.595702Z\"\n        },\n        {\n          \"Value\": \"d42f79c1ab8840db09d7596dea4ee40b175df7795dd186f812eafe4a5fa21aab\",\n          \"Algorithm\": \"BLAKE3\",\n          \"ChecksumDate\": \"2023-11-14T22:50:17.616477Z\"\n        }\n      ],\n      \"ContentDate\": {\n        \"Start\": \"2022-05-03T00:00:04.000Z\",\n        \"End\": \"2022-05-03T23:59:54.000Z\"\n      },\n      \"Footprint\": null,\n      \"GeoFootprint\": null\n    },\n    {...\n    }\n  ],\n  \"@odata.nextLink\": \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?%24filter=Collection%2FName+eq+%27SENTINEL-1%27+and+ContentDate%2FStart+gt+2022-05-03T00%3A00%3A00.000Z+and+ContentDate%2FStart+lt+2022-05-04T00%3A00%3A00.000Z&%24top=2&%24skip=2\"\n}\n\n\n\nOpenSearch API Example\n\nHTTP RequestCurrent responseNew response\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00&maxRecords=2\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"04139de2-34f6-56d0-b36f-122f1a3c290a\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 2,\n    \"query\": {\n      \"originalFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.163432102\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00&maxRecords=2\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00%3A00%3A00&maxRecords=2&page=2\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [...\n        ]\n      },\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/SAR/SLC/2022/05/02/S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": \"ESA\",\n        \"startDate\": \"2022-05-02T22:06:17.548Z\",\n        \"completionDate\": \"2022-05-02T22:06:47.359Z\",\n        \"productType\": \"IW_SLC__1S\",\n        \"processingLevel\": \"LEVEL1\",\n        \"platform\": \"S1A\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 2.3,\n        \"sensorMode\": \"IW\",\n        \"orbitNumber\": 43037,\n        \"quicklook\": null,\n        \"thumbnail\": \"https://catalogue.dataspace.copernicus.eu/get-object?path=/Sentinel-1/SAR/SLC/2022/05/02/S1A_IW_SLC__1SDV_20220502T220617_20220502T220647_043037_052392_1E9A.SAFE/preview/thumbnail.png\",\n        \"updated\": \"2024-03-16T03:19:06.436Z\",\n        \"published\": \"2022-05-02T23:40:02.825Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": \"&lt;gml:Polygon srsName=\\\"EPSG:4326\\\"&gt;&lt;gml:outerBoundaryIs&gt;&lt;gml:LinearRing&gt;&lt;gml:coordinates&gt;-57.750202,-2.026322 -57.366844,-3.828814 -55.15321,-3.341953 -55.540607,-1.545508 -57.750202,-2.026322&lt;/gml:coordinates&gt;&lt;/gml:LinearRing&gt;&lt;/gml:outerBoundaryIs&gt;&lt;/gml:Polygon&gt;\",\n        \"centroid\": {\n          \"type\": \"Point\",\n          \"coordinates\": [\n            -56.45314692566288,\n            -2.68610524638\n          ]\n        },\n        \"orbitDirection\": \"ASCENDING\",\n        \"timeliness\": \"Fast-24h\",\n        \"relativeOrbitNumber\": 90,\n        \"processingBaseline\": 0,\n        \"polarisation\": \"VV&VH\",\n        \"swath\": \"IW1 IW2 IW3\",\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 8231197123\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 896aeef0-eee1-5e28-acaa-7f420bb23e8c\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/896aeef0-eee1-5e28-acaa-7f420bb23e8c.json\"\n          }\n        ]\n      }\n    },\n    {...\n    }\n  ]\n}\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"properties\": {\n    \"id\": \"1f7387ef-7456-5a77-ba63-fa036a7659cd\",\n    \"totalResults\": null,\n    \"exactCount\": 0,\n    \"startIndex\": 1,\n    \"itemsPerPage\": 2,\n    \"query\": {\n      \"originalFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00Z\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"appliedFilters\": {\n        \"startDate\": \"2022-05-03T00:00:00Z\",\n        \"collection\": \"SENTINEL-1\"\n      },\n      \"processingTime\": 0.032169373\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"type\": \"application/json\",\n        \"title\": \"self\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00:00:00Z&maxRecords=2\"\n      },\n      {\n        \"rel\": \"search\",\n        \"type\": \"application/opensearchdescription+xml\",\n        \"title\": \"OpenSearch Description Document\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/describe.xml\"\n      },\n      {\n        \"rel\": \"next\",\n        \"type\": \"application/json\",\n        \"title\": \"next\",\n        \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel1/search.json?startDate=2022-05-03T00%3A00%3A00Z&maxRecords=2&page=2\"\n      }\n    ]\n  },\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": \"1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n      \"geometry\": [\n         \n      ],\n      \"properties\": {\n        \"collection\": \"SENTINEL-1\",\n        \"status\": \"ONLINE\",\n        \"license\": {\n          \"licenseId\": \"unlicensed\",\n          \"hasToBeSigned\": \"never\",\n          \"grantedCountries\": null,\n          \"grantedOrganizationCountries\": null,\n          \"grantedFlags\": null,\n          \"viewService\": \"public\",\n          \"signatureQuota\": -1,\n          \"description\": {\n            \"shortName\": \"No license\"\n          }\n        },\n        \"productIdentifier\": \"/eodata/Sentinel-1/AUX/AUX_GNSSRD/2022/05/03/S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n        \"parentIdentifier\": null,\n        \"title\": \"S1A_OPER_AUX_GNSSRD_POD__20220510T020122_V20220502T235946_20220503T235936\",\n        \"description\": \"The Sentinel-1 mission is the European Radar Observatory for the Copernicus joint initiative of the European Commission (EC) and the European Space Agency (ESA). The Sentinel-1 mission includes C-band imaging operating in four exclusive imaging modes with different resolution (down to 5 m) and coverage (up to 400 km). It provides dual polarization capability, short revisit times and rapid product delivery. Additionally, precise measurements of spacecraft position and attitude are available for every observation [https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-1].\",\n        \"organisationName\": null,\n        \"startDate\": \"2022-05-03T00:00:04.000Z\",\n        \"completionDate\": \"2022-05-03T23:59:54.000Z\",\n        \"productType\": \"AUX_GNSSRD\",\n        \"processingLevel\": null,\n        \"platform\": \"SENTINEL-1\",\n        \"instrument\": \"SAR\",\n        \"resolution\": 0,\n        \"sensorMode\": null,\n        \"orbitNumber\": 0,\n        \"quicklook\": null,\n        \"thumbnail\": null,\n        \"updated\": \"2023-11-14T22:50:17.708Z\",\n        \"published\": \"2023-10-25T13:45:19.736Z\",\n        \"snowCover\": 0,\n        \"cloudCover\": 0,\n        \"gmlgeometry\": null,\n        \"centroid\": {\n          \"type\": null,\n          \"coordinates\": null\n        },\n        \"orbitDirection\": null,\n        \"timeliness\": null,\n        \"relativeOrbitNumber\": 0,\n        \"processingBaseline\": 0,\n        \"polarisation\": null,\n        \"swath\": null,\n        \"services\": {\n          \"download\": {\n            \"url\": \"https://catalogue.dataspace.copernicus.eu/download/1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n            \"mimeType\": \"application/octet-stream\",\n            \"size\": 2663000\n          }\n        },\n        \"links\": [\n          {\n            \"rel\": \"self\",\n            \"type\": \"application/json\",\n            \"title\": \"GeoJSON link for 1d42f2d3-2456-485f-a93e-92f08bdd5c51\",\n            \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/collections/SENTINEL-1/1d42f2d3-2456-485f-a93e-92f08bdd5c51.json\"\n          }\n        ]\n      }\n    },\n    {...\n    }\n  ]\n}\n\n\n\nWe kindly ask that you ensure your date requests include the appropriate timezone information to prevent any disruptions to your services."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#opensearch-api-new-error-handling-release",
    "href": "APIs/Others/UpcomingChanges.html#opensearch-api-new-error-handling-release",
    "title": "Upcoming changes",
    "section": "OpenSearch API new error handling release",
    "text": "OpenSearch API new error handling release\nPlease be informed that the OpenSearch API error handling update was successfully implemented on 24th October 2023. The details of the change are explained here."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#opensearch-api-error-handling-update-new-date",
    "href": "APIs/Others/UpcomingChanges.html#opensearch-api-error-handling-update-new-date",
    "title": "Upcoming changes",
    "section": "OpenSearch API error handling update new date",
    "text": "OpenSearch API error handling update new date\nPlease be informed that the OpenSearch API error handling update has been rescheduled for 24th October 2023. The details of the change are explained here."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#opensearch-api-error-handling-update-date",
    "href": "APIs/Others/UpcomingChanges.html#opensearch-api-error-handling-update-date",
    "title": "Upcoming changes",
    "section": "OpenSearch API error handling update date",
    "text": "OpenSearch API error handling update date\nPlease be informed that the OpenSearch API error handling update is planned for 17th of October 2023. The details of the change are explained here."
  },
  {
    "objectID": "APIs/Others/UpcomingChanges.html#opensearch-api-error-handling-update",
    "href": "APIs/Others/UpcomingChanges.html#opensearch-api-error-handling-update",
    "title": "Upcoming changes",
    "section": "OpenSearch API error handling update",
    "text": "OpenSearch API error handling update\nPlease be informed that the OpenSearch API error handling will be updated soon.\nPlease also note that new responses with errors will provide the RequestId, which is intended to help identify the requests with errors. It is strongly recommended to include the RequestId in the issues you submitted to the support team in case of Catalog API problems.\nThe new error handling is described below.\n\nIncorrect collection name\n\nCurrent responseNew response\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"loadFromStore - Not Found\",\n        \"ErrorCode\": 404\n    }\n}\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown collection.\",\n        \"ErrorCode\": 404,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"collection\"\n                ],\n                \"msg\": \"Collection '&lt;collection name presented in query&gt;' does not exist.\",\n            },\n        ],\n        \"RequestId\": &lt;request_id&gt;,\n    }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinl2/search.json\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown collection.\",\n        \"ErrorCode\": 404,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"collection\"\n                ],\n                \"msg\": \"Collection 'Sentinl2' does not exist.\"\n            },\n        ],\n        \"RequestID\": \"70970f42-e374-4e26-8778-41a1463e700d\"\n    }\n}\n\n\n\n\n\nIncorrect name of the query parameter\n(when the collection is not specified)\n\nCurrent responseNew response\n\n\nNo error is returned. The incorrect query parameter is ignored and not reflected in appliedFilters.\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of unexisting parameters&gt;],\n                \"msg\": \"Query parameters do not exist.\",\n            },\n        ],\n        \"RequestId\": &lt;request_id&gt;,\n    }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?productsType=S2MSI1C\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": {\n            \"loc\": [\n                \"productsType\"\n            ],\n            \"msg\": \"Query parameters do not exist.\"\n        },\n        \"RequestID\": \"d9f22173-4d56-44fd-ab18-35d6018c49d7\"\n    }\n}\n\n\n\n\n\nIncorrect name of the query parameter\n(when the collection is specified)\n\nCurrent responseNew response\n\n\nNo error is returned. The incorrect query parameter is ignored and not reflected in appliedFilters.\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of unexisting parameters&gt;],\n                \"msg\": \"Query parameters do not exist or are not available for specified collection.\",\n            },\n        ],\n        \"RequestId\": &lt;request_id&gt;,\n  }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?productType=S2MSI1C&startDat=2023-06-11&completionDte=2023-06-22\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Unknown query parameter(s).\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": {\n            \"loc\": [\n                \"startDat\",\n                \"completionDte\",\n                        ],\n            \"msg\": \"Query parameters do not exist or are not available for specified collection.\"\n        },\n        \"RequestID\": \"25d522af-ba4e-4152-a368-9635d560e649\"\n    }\n}\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note that the dataset parameter will not be supported anymore. Any query with the dataset parameter will result in an error.\n\n\n\n\nIncorrect value of the query parameter\n(maxRecords, index, page, sortParam, sortOrder, exactCount, geometry, box, lon, lat, radius, startDate, completionDate, updated, published, publishedAfter, publishedBefore, status)\n\nCurrent responseNew response\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": &lt;error message&gt;,\n        \"ErrorCode\": 400\n    }\n}\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of parameters that error \"msg\" field relate to&gt;],\n                \"msg”: &lt;error message&gt;}&gt;,\n            },\n        ]\n        \"RequestId\": &lt;request_id&gt;,\n  }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?startDate=2021-07-01T00:00:00Z&completionDate=2021-07-31T23:59:59Z&maxRecords=2001\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"maxRecords\"\n                ],\n                \"msg\": \"Input should be less than or equal to 2000.\"\n            }\n        ],\n        \"RequestID\": \"b3b4c0bb-9697-4ff8-b90c-4eb1b97a9914\"\n    }\n}\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe kindly remind you that for the status parameter, the only acceptable values will be:\n\nONLINE\nOFFLINE\nALL\n\nAny other value will result in an error.\n\n\n\n\nIncorrect value type of the query parameter\n\nCurrent responseNew response\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": &lt;error message&gt;,\n        \"ErrorCode\": 400\n    }\n}\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [&lt;list of parameters that error \"msg\" field relate to&gt;],\n                \"msg”: &lt;error message&gt;}&gt;,\n            },\n        ]\n        \"RequestId\": &lt;request_id&gt;,\n  }\n}\n\n\n\nExample\n\nHTTP RequestNew response example\n\n\nhttps://catalogue.dataspace.copernicus.eu/resto/api/collections/search.json?orbitNumber=ascending\n\n\n{\n    \"detail\": {\n        \"ErrorMessage\": \"Validation error.\",\n        \"ErrorCode\": 400,\n        \"ErrorDetail\": [\n            {\n                \"loc\": [\n                    \"orbitNumber\"\n                ],\n                \"msg\": \"Proper value types for specified attribute query parameters are: 'orbitNumber'-integer\"\n            }\n        ],\n        \"RequestID\": \"33e3ebb0-7d44-4dcd-8cb2-f60216c11cef\"\n    }\n}\n\n\n\nPlease also note about the following change:\n\nupdate of the last link\n\nThe last link will be provided only when exactCount is used in the request.\n\nLink last example\n\n\n{\n    \"rel\": \"last\",\n    \"type\": \"application/json\",\n    \"title\": \"last\",\n    \"href\": \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?page=19168&processingLevel=S2MSI1C&startDate=2023-07-01&completionDate=2023-07-31&sortParam=startDate&exactCount=1\"\n}\n\n\n\nWe recommend reviewing the upcoming changes to Catalog OpenSearch API described above to avoid disruption to your current scripts or apps."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data",
    "section": "",
    "text": "This section provides an overview of the EO data available from Copernicus Data Space Ecosystem.\nThe data offer will gradually extend starting from January 2023\nFor the latest information about available satellite data, users and stakeholders can follow them in Copernicus Sentinel Operations Dashboard.\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSentinel-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-5P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Atmosphere Monitoring Service (CAMS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Marine Environment Monitoring Service (CMEMS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandsat-8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandsat-7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoil Moisture and Ocean Salinity (SMOS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Land Monitoring Service (CLMS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandsat-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Emergency Management Service (CEMS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENVISAT- Medium Resolution Imaging Spectrometer (MERIS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Contributing Missions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-5P\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopernicus Contributing Missions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2FA.html",
    "href": "2FA.html",
    "title": "Setting up Two-Factor Authentication (2FA)",
    "section": "",
    "text": "This documentation provides information on how to set up two-factor authentication (2FA) on the Copernicus Data Space Ecosystem."
  },
  {
    "objectID": "2FA.html#registration",
    "href": "2FA.html#registration",
    "title": "Setting up Two-Factor Authentication (2FA)",
    "section": "Registration",
    "text": "Registration\nTo set up two-factor authentication, you must have a registered account on dataspace.copernicus.eu. If you don’t have an account, you can register here. If you encounter any issues during the registration process, please refer to the documentation User registration and authentication."
  },
  {
    "objectID": "2FA.html#two-factor-authentication",
    "href": "2FA.html#two-factor-authentication",
    "title": "Setting up Two-Factor Authentication (2FA)",
    "section": "Two-factor authentication",
    "text": "Two-factor authentication\nGo to the website, then simply click on the “LOGIN” button located in the top right corner.\n\nTo continue, log in using your credentials (Email and Password) by providing them in the “Login to access your account” section. Once you have provided your credentials, you can click on the “LOGIN” button to proceed with the login process.\n\nAfter successfully logging in, you’ll be redirected to the website displayed on the screen below. Click on the “MY ACCOUNT” button located in the top right corner.\n\nIn the “Account Security” section, locate the option labeled “Signing in”. Click on this option to configure ways to sign in.\n\nBelow “Two-factor authentication”, you’ll see the “Set up authenticator application” button with a plus sign. Click on it to access the “Mobile Authenticator Setup”.\n\nTo set up two-factor authentication, you need to first install either FreeOTP or Google Authenticator on your mobile device. Once installed, open the application and scan the provided barcode using the app’s scanning feature. If you encounter difficulty scanning the barcode, you can select the “Unable to scan?” option and manually enter the provided key into the authenticator application. After scanning or entering the key, you need to enter the one-time code provided by the application and provide a Device Name for managing your OTP devices. In this example, the Device Name is “Device Name”. Finally, click on the “SUBMIT” button to finish the setup.\n\nAfter successfully setting up two-factor authentication, you will see your OTP device listed below the “Two-factor authentication” section.\n\nThe setup process is complete. You can now log in using your credentials and the one-time code provided by the authenticator application."
  },
  {
    "objectID": "logos.html",
    "href": "logos.html",
    "title": "Documentation",
    "section": "",
    "text": "```{=html}\n&lt;div class=\"logos\"&gt;\n    &lt;a href=\"https://dataspace.copernicus.eu\" target=\"_blank\"&gt;\n        &lt;img src=\"_images/logos/EU.svg\"&gt;\n    &lt;/a&gt;\n    &lt;a href=\"https://dataspace.copernicus.eu\" target=\"_blank\"&gt;\n        &lt;img src=\"_images/logos/copernicus-white-BL.svg\"&gt;\n    &lt;/a&gt;\n    &lt;a href=\"https://dataspace.copernicus.eu\" target=\"_blank\"&gt;\n        &lt;img src=\"_images/logos/ESA_White.svg\"&gt;\n    &lt;/a&gt;\n    \n    ```"
  },
  {
    "objectID": "ResearchNetwork.html",
    "href": "ResearchNetwork.html",
    "title": "Access through Research Network",
    "section": "",
    "text": "This section provides the most important information for users intending to access the Copernicus Data Space Ecosystem data repository through the GEANT network."
  },
  {
    "objectID": "ResearchNetwork.html#introduction",
    "href": "ResearchNetwork.html#introduction",
    "title": "Access through Research Network",
    "section": "Introduction",
    "text": "Introduction\nTo ease your first steps in the new Copernicus Data Space Ecosystem environment, we prepared a Step-by-Step guide for your reference. It is addressed to users who want to use data retrieval services for downloading products using the research networks (GEANT connectivity).\nWe would like to inform that the current setup for downloading Copernicus Data utilising research network connectivity (via NREN+GEANT) is still in a process of optimisation to achieve best possible performance. This primarily concerns the load balancing mechanism which is based on a Global Service Load Balancer (GSLB) using the requesting party’s DNS resolver as a criterion where to direct the request."
  },
  {
    "objectID": "ResearchNetwork.html#instructions",
    "href": "ResearchNetwork.html#instructions",
    "title": "Access through Research Network",
    "section": "Instructions",
    "text": "Instructions\n\n1. Introduction to Copernicus Data Space environment\nDiscover the new services on the Copernicus Data Space Ecosystem portal.\nUse the documentation portal.\n\n\n2. Become a registered user\nIf you don’t have an account, you need to register as a new user. Follow the steps on User registration and authentication.\n\n\n3. Request a KeyCloak token\nThis is required to access to OData catalogue API.\nTo get the token you can follow the steps on Access token.\n\n\n4. Use OData catalogue API to search the products\nAs a default configuration you can use the general URL https://catalogue.dataspace.copernicus.eu/odata/v1/Products.\nThe prerequisite for using this URL is the correct set up of the DNS resolver as described below in Verification of data download via the research network. This is the recommended setup as it will take advantage of the Global Service Load Balancer (GSLB) mechanism.\nAlternatively, you can also use https://catalogue.ams.dataspace.copernicus.eu/odata/v1/Products.\nThis URL directs you to the designated endpoint utilizing GEANT connectivity.\n\n\n5. Use script for data download\nOnce you have your token and product Id, you can download the product using a script:\n\ncURL\n\n\ncurl -O -J -k --header \"Authorization: Bearer ${KEYCLOAK_TOKEN}\" 'https://catalogue.ams.dataspace.copernicus.eu/odata/v1/Products(d7e6cd54-7d53-5569-8a9e-d148bcb8917e)/$value' \n\n\n\nalternative download is possible\n\ncURL\n\n\ncurl -O -J -k --header \"Authorization: Bearer ${KEYCLOAK_TOKEN}\" 'https://download.ams.dataspace.copernicus.eu/odata/v1/Products(d7e6cd54-7d53-5569-8a9e-d148bcb8917e)/$value' \n\n\n\n\n\n6. S3 access (optional)\nTo access EO data via S3, please refer to S3 Access."
  },
  {
    "objectID": "ResearchNetwork.html#verification-of-data-download-via-the-research-network",
    "href": "ResearchNetwork.html#verification-of-data-download-via-the-research-network",
    "title": "Access through Research Network",
    "section": "Verification of data download via the research network",
    "text": "Verification of data download via the research network\nTo verify that your service has been set up correctly, please perform following steps:\n\ncommand\n\n\nhost catalogue.ams.dataspace.copernicus.eu\n\n\n\nShould resolve IP addresses to\n\ndownload.ams.dataspace.copernicus.eu has address 80.158.97.159\n\n\ndownload.ams.dataspace.copernicus.eu has address 80.158.97.12\n\n\ndownload.ams.dataspace.copernicus.eu has address 80.158.97.1\n\n\ndownload.ams.dataspace.copernicus.eu has address 80.158.97.169\n\nIf your DNS resolver is utilizing an IPv4 assigned to the Geant network, you should expect to receive an identical set of IP addresses. However, if this is not the case, it indicates that the resolver needs to be added to our service configuration."
  },
  {
    "objectID": "ResearchNetwork.html#global-service-load-balancer",
    "href": "ResearchNetwork.html#global-service-load-balancer",
    "title": "Access through Research Network",
    "section": "Global Service Load Balancer",
    "text": "Global Service Load Balancer\nTo ensure proper routing of user traffic through the Geant network to the designated endpoint, it is essential for the DNS resolver to be operational within the Geant network. Alternatively, we kindly request you to provide us with the public address of your resolver to enhance our configuration. It is essential to emphasize that it is not recommended to utilize widely known resolvers such as Google’s 8.8.8.8 or CloudFlare’s 1.1.1.1 in this particular context. In such cases, traffic directed to those resolvers will be routed to public endpoints via the Internet rather than through the Geant connectivity.\nOnce the DNS set up has been configured in line with above requirements the following general URL should be used to access the OData catalogue.\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products\nThe GSLB automatically routes users to the foreseen endpoint. The mechanism will also be used for failover in case of outages on one of the cloud platforms (Open Telekom Cloud or Cloud Ferro Cloud)."
  },
  {
    "objectID": "ResearchNetwork.html#relevant-ip-addresses",
    "href": "ResearchNetwork.html#relevant-ip-addresses",
    "title": "Access through Research Network",
    "section": "Relevant IP Addresses",
    "text": "Relevant IP Addresses\nIn case your firewall setting blocks IP addresses relevant for the data download function please find following IP Address ranges which are relevant. You may have to change your firewall rules to allow traffic from those Addresses.\n❯ host catalogue.ams.dataspace.copernicus.eu\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.1\n\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.159\n\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.12\n\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.169\n\n❯ host download.ams.dataspace.copernicus.eu\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.1\n\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.159\n\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.12\n\n\ncatalogue.ams.dataspace.copernicus.eu has address 80.158.97.169\n\n❯ host eodata.ams.dataspace.copernicus.eu\n\neodata.ams.dataspace.copernicus.eu has address 80.158.97.24\n\n\neodata.ams.dataspace.copernicus.eu has address 80.158.97.202"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "",
    "text": "The Sentinel Hub API is a RESTful API interface that provides access to various satellite imagery archives. It allows you to access raw satellite data, rendered images, statistical analysis, and other features.\n# Utilities\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport getpass\n\nfrom sentinelhub import (\n    SHConfig,\n    DataCollection,\n    SentinelHubCatalog,\n    SentinelHubRequest,\n    SentinelHubStatistical,\n    BBox,\n    bbox_to_dimensions,\n    CRS,\n    MimeType,\n    Geometry,\n)\n\nfrom utils import plot_image"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#credentials",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#credentials",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "Credentials",
    "text": "Credentials\nCredentials for Sentinel Hub services (client_id & client_secret) can be obtained in your Dashboard. In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant documentation page.\nNow that you have your client_id & client_secret, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found here. Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won’t need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook.\nIf you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:\n\n# Only run this cell if you have not created a configuration.\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n# config.save(\"cdse\")\n\nHowever, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing &lt;profile_name&gt;.\n\n# config = SHConfig(\"profile_name\")"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#setting-an-area-of-interest",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#setting-an-area-of-interest",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "Setting an area of interest",
    "text": "Setting an area of interest\nThe bounding box in WGS84 coordinate system is [(longitude and latitude coordinates of lower left and upper right corners)]. You can get the bbox for a different area at the bboxfinder website.\nAll requests require a bounding box to be given as an instance of sentinelhub.geometry.BBox with corresponding Coordinate Reference System (sentinelhub.constants.CRS). In our case it is in WGS84 and we can use the predefined WGS84 coordinate reference system from sentinelhub.constants.CRS.\n\naoi_coords_wgs84 = [15.461282, 46.757161, 15.574922, 46.851514]\n\nWhen the bounding box bounds have been defined, you can initialize the BBox of the area of interest. Using the bbox_to_dimensions utility function, you can provide the desired resolution parameter of the image in meters and obtain the output image shape.\n\nresolution = 10\naoi_bbox = BBox(bbox=aoi_coords_wgs84, crs=CRS.WGS84)\naoi_size = bbox_to_dimensions(aoi_bbox, resolution=resolution)\n\nprint(f\"Image shape at {resolution} m resolution: {aoi_size} pixels\")\n\nImage shape at 10 m resolution: (860, 1054) pixels"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#catalog-api",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#catalog-api",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "Catalog API",
    "text": "Catalog API\nTo search and discover data, you can use the Catalog API. Sentinel Hub Catalog API (or shortly “Catalog”) is an API implementing the STAC Specification, providing geospatial information for data available in Sentinel Hub. Firstly, to initialise the SentinelHubCatalog class we will use:\n\ncatalog = SentinelHubCatalog(config=config)\n\nNow we can build the Catalog API request; to do this we use the aoi_bbox we defined earlier as well as time_interval and insert these into the request:\n\naoi_bbox = BBox(bbox=aoi_coords_wgs84, crs=CRS.WGS84)\ntime_interval = \"2022-07-01\", \"2022-07-20\"\n\nsearch_iterator = catalog.search(\n    DataCollection.SENTINEL2_L2A,\n    bbox=aoi_bbox,\n    time=time_interval,\n    fields={\"include\": [\"id\", \"properties.datetime\"], \"exclude\": []},\n)\n\nresults = list(search_iterator)\nprint(\"Total number of results:\", len(results))\n\nresults\n\nTotal number of results: 8\n\n\n[{'id': 'S2B_MSIL2A_20220719T095559_N0400_R122_T33TWM_20220719T113943.SAFE',\n  'properties': {'datetime': '2022-07-19T10:07:53.062Z'}},\n {'id': 'S2B_MSIL2A_20220716T094549_N0400_R079_T33TWM_20220716T114017.SAFE',\n  'properties': {'datetime': '2022-07-16T09:57:56.26Z'}},\n {'id': 'S2A_MSIL2A_20220714T100041_N0400_R122_T33TWM_20220714T175057.SAFE',\n  'properties': {'datetime': '2022-07-14T10:08:00.748Z'}},\n {'id': 'S2A_MSIL2A_20220711T095041_N0400_R079_T33TWM_20220711T142927.SAFE',\n  'properties': {'datetime': '2022-07-11T09:58:04.522Z'}},\n {'id': 'S2B_MSIL2A_20220709T100029_N0400_R122_T33TWM_20220709T114004.SAFE',\n  'properties': {'datetime': '2022-07-09T10:07:52.974Z'}},\n {'id': 'S2B_MSIL2A_20220706T095039_N0400_R079_T33TWM_20220706T113052.SAFE',\n  'properties': {'datetime': '2022-07-06T09:57:56.689Z'}},\n {'id': 'S2A_MSIL2A_20220704T100041_N0400_R122_T33TWM_20220704T141618.SAFE',\n  'properties': {'datetime': '2022-07-04T10:08:01.243Z'}},\n {'id': 'S2A_MSIL2A_20220701T095041_N0400_R079_T33TWM_20220701T141709.SAFE',\n  'properties': {'datetime': '2022-07-01T09:58:04.669Z'}}]"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#process-api",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#process-api",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "Process API",
    "text": "Process API\n\nExample 1: True Color Image\nWe build the request according to the API Reference, using the SentinelHubRequest class. Each Process API request also needs an evalscript. An evalscript (or “custom script”) is a piece of Javascript code which defines how the satellite data shall be processed by Sentinel Hub and what values the service shall return. It is a required part of any process, batch processing or OGC request.\nThe information that we specify in the SentinelHubRequest object is: - an evalscript, - a list of input data collections with time interval, - a format of the response, - a bounding box and its size (size or resolution). - mosaickingOrder (optional): in this example we have used leastCC which will return pixels from the least cloudy acquisition in the specified time period.\nThe evalscript in the example is used to select the appropriate bands. We return the RGB (B04, B03, B02) Sentinel-2 L2A bands.\nThe least cloudy image from the time period is downloaded. Without any additional parameters in the evalscript, the downloaded data will correspond to reflectance values in UINT8 format (values in 0-255 range).\n\nevalscript_true_color = \"\"\"\n    //VERSION=3\n\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B02\", \"B03\", \"B04\"]\n            }],\n            output: {\n                bands: 3\n            }\n        };\n    }\n\n    function evaluatePixel(sample) {\n        return [sample.B04, sample.B03, sample.B02];\n    }\n\"\"\"\n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                name=\"s2l2a\", service_url=\"https://sh.dataspace.copernicus.eu\"\n            ),\n            time_interval=(\"2022-05-01\", \"2022-05-20\"),\n            other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=aoi_bbox,\n    size=aoi_size,\n    config=config,\n)\n\nThe method get_data() will always return a list of length 1 with the available image from the requested time interval in the form of numpy arrays.\n\ntrue_color_imgs = request_true_color.get_data()\n\n\nprint(\n    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n)\nprint(\n    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n)\n\nReturned data is of type = &lt;class 'list'&gt; and length 1.\nSingle element in the list is of type &lt;class 'numpy.ndarray'&gt; and has shape (1054, 860, 3)\n\n\n\nimage = true_color_imgs[0]\nprint(f\"Image type: {image.dtype}\")\n\n# plot function\n# factor 1/255 to scale between 0-1\n# factor 3.5 to increase brightness\nplot_image(image, factor=3.5 / 255, clip_range=(0, 1))\n\nImage type: uint8\n\n\n\n\n\n\n\nExample 2: NDVI Image\nSecondly, we will also show you an example of how to calculate and visualise NDVI using the same API. NDVI is a very commonly used spectral vegetation index for vegetation monitoring, for example, monitoring crop growth and yields. As you will notice in the codeblock below, the evalscript has changed substantially: - we are only using Band 4 and Band 8 as an input into our script. - In the evaluatePixel() function, we calculate NDVI and visualise this using the imgVals array.\n\nevalscript_ndvi = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\",\n        \"dataMask\"\n      ]\n    }],\n    output: {\n      bands: 4\n    }\n  }\n}\n  \n\nfunction evaluatePixel(sample) {\n    let val = (sample.B08 - sample.B04) / (sample.B08 + sample.B04);\n    let imgVals = null;\n    \n    if (val&lt;-1.1) imgVals = [0,0,0];\n    else if (val&lt;-0.2) imgVals = [0.75,0.75,0.75];\n    else if (val&lt;-0.1) imgVals = [0.86,0.86,0.86];\n    else if (val&lt;0) imgVals = [1,1,0.88];\n    else if (val&lt;0.025) imgVals = [1,0.98,0.8];\n    else if (val&lt;0.05) imgVals = [0.93,0.91,0.71];\n    else if (val&lt;0.075) imgVals = [0.87,0.85,0.61];\n    else if (val&lt;0.1) imgVals = [0.8,0.78,0.51];\n    else if (val&lt;0.125) imgVals = [0.74,0.72,0.42];\n    else if (val&lt;0.15) imgVals = [0.69,0.76,0.38];\n    else if (val&lt;0.175) imgVals = [0.64,0.8,0.35];\n    else if (val&lt;0.2) imgVals = [0.57,0.75,0.32];\n    else if (val&lt;0.25) imgVals = [0.5,0.7,0.28];\n    else if (val&lt;0.3) imgVals = [0.44,0.64,0.25];\n    else if (val&lt;0.35) imgVals = [0.38,0.59,0.21];\n    else if (val&lt;0.4) imgVals = [0.31,0.54,0.18];\n    else if (val&lt;0.45) imgVals = [0.25,0.49,0.14];\n    else if (val&lt;0.5) imgVals = [0.19,0.43,0.11];\n    else if (val&lt;0.55) imgVals = [0.13,0.38,0.07];\n    else if (val&lt;0.6) imgVals = [0.06,0.33,0.04];\n    else imgVals = [0,0.27,0];\n    \n    \n    imgVals.push(sample.dataMask)\n    \n    return imgVals\n}\n\"\"\"\n\nrequest_ndvi_img = SentinelHubRequest(\n    evalscript=evalscript_ndvi,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                name=\"s2l2a\", service_url=\"https://sh.dataspace.copernicus.eu\"\n            ),\n            time_interval=(\"2022-05-01\", \"2022-05-20\"),\n            other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=aoi_bbox,\n    size=aoi_size,\n    config=config,\n)\n\nThe same method as before is used to request and then visualise the data. In the visualisation, the lighter greens indicate a higher NDVI value (vegetation, forest) and the darker greens (urban areas and water bodies) represent areas with lower NDVI values.\n\nndvi_img = request_ndvi_img.get_data()\n\n\nprint(\n    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n)\nprint(\n    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n)\n\nReturned data is of type = &lt;class 'list'&gt; and length 1.\nSingle element in the list is of type &lt;class 'numpy.ndarray'&gt; and has shape (1054, 860, 3)\n\n\n\nimage = ndvi_img[0]\nprint(f\"Image type: {image.dtype}\")\n\n# plot function\nplot_image(image, factor=1 / 255)\n\nImage type: uint8"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#statistical-api",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#statistical-api",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "Statistical API",
    "text": "Statistical API\nIn the Process API examples, we have seen how to obtain satellite imagery. Statistical API can be used in a very similar way. The main difference is that the results of Statistical API are aggregated statistical values of satellite data instead of entire images. In many use cases, such values are all that we need. By using Statistical API we can avoid downloading and processing large amounts of satellite data.\nAll general rules for building evalscripts apply. However, there are some specifics when using evalscripts with the Statistical API:\n\nThe evaluatePixel() function must, in addition to other output, always return a dataMask output. This output defines which pixels are excluded from calculations. For more details and an example, see here.\nThe default value of sampleType is FLOAT32.\nThe output.bands parameter in the setup() function can be an array. This makes it possible to specify custom names for the output bands and different output dataMask for different outputs, see this example.\n\n\nRequesting, and plotting an NDVI time series for a single field\nIn the example here, we will calculate NDVI for a specific field of interest and then plot the mean NDVI and standard deviation over the requested time period. First we define our evalscript:\n\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\n        \"B04\",\n        \"B08\",\n        \"dataMask\"\n      ]\n    }],\n    output: [\n      {\n        id: \"ndvi\",\n        bands: 1\n      },\n      {\n        id: \"dataMask\",\n        bands: 1\n      }]\n  };\n}\n\nfunction evaluatePixel(samples) {\n    let index = (samples.B08 - samples.B04) / (samples.B08+samples.B04);\n    return {\n        ndvi: [index],\n        dataMask: [samples.dataMask],\n    };\n}\n\n\"\"\"\n\nIn this example, we will compare two fields within the area we requested using Process API:\n\nfield1 = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [15.541723001099184, 46.820368115848446],\n            [15.541756949727985, 46.82037740810231],\n            [15.54192669287196, 46.82008470133467],\n            [15.542211861353849, 46.81964331510048],\n            [15.539394125163792, 46.81905789197882],\n            [15.539251540922846, 46.819805931503055],\n            [15.541723001099184, 46.820368115848446],\n        ]\n    ],\n}\n\nfield2 = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [15.507170086710744, 46.83938135202761],\n            [15.508086699688228, 46.83921879483953],\n            [15.50831755036404, 46.839576420004114],\n            [15.508582349668648, 46.83992939835186],\n            [15.508874307876296, 46.840221997066486],\n            [15.50860950857169, 46.840514594187695],\n            [15.50842618597619, 46.84082112279607],\n            [15.508113858591262, 46.840639992466144],\n            [15.50781511065786, 46.84039384001332],\n            [15.50739414766079, 46.83981328730921],\n            [15.507149717533464, 46.83939064099493],\n            [15.507170086710744, 46.83938135202761],\n        ]\n    ],\n}\n\nNow we have defined the evalscript and the two fields of interest, we can build the first Statistical API Request, before returning the response for the first field. In this request, as part of the payload we define some input parameters: - time_interval this defines the time range of our request. - aggregation_interval this defines the length of time each interval is. In this case, the interval is 10 days. he aggregation intervals should be at least one day long (e.g. “P5D”, “P30D”). You can only use period OR time designator not both. - dataFilter: {maxCloudCoverage} this is an additional argument in our request which filters out image acquisitions that have a cloud coverage percentage above 10%.\nNOTE: If a timeRange is not divisible by an aggregationInterval, the last (“not full”) time interval will be dismissed by default (SKIP option). The user can instead set the lastIntervalBehavior to SHORTEN (shortens the last interval so that it ends at the end of the provided time range) or EXTEND (extends the last interval over the end of the provided time range so that all the intervals are of equal duration).\n\ngeometry = Geometry(geometry=field1, crs=CRS.WGS84)\n\nrequest = SentinelHubStatistical(\n    aggregation=SentinelHubStatistical.aggregation(\n        evalscript=evalscript,\n        time_interval=(\"2022-04-01T00:00:00Z\", \"2022-08-30T23:59:59Z\"),\n        aggregation_interval=\"P10D\",\n        size=[368.043, 834.345],\n    ),\n    input_data=[\n        SentinelHubStatistical.input_data(\n            DataCollection.SENTINEL2_L1C.define_from(\n                name=\"s2l1c\", service_url=\"https://sh.dataspace.copernicus.eu\"\n            ),\n            other_args={\"dataFilter\": {\"maxCloudCoverage\": 10}},\n        ),\n    ],\n    geometry=geometry,\n    config=config,\n)\n\nresponse1 = request.get_data()\nresponse1\n\n[{'data': [{'interval': {'from': '2022-04-21T00:00:00Z',\n     'to': '2022-05-01T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.08167635649442673,\n         'max': 0.39603960514068604,\n         'mean': 0.13346635554959452,\n         'stDev': 0.06778421108052068,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-05-11T00:00:00Z', 'to': '2022-05-21T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.07600757479667664,\n         'max': 0.4349462389945984,\n         'mean': 0.11771845381622796,\n         'stDev': 0.06006468950382084,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-05-21T00:00:00Z', 'to': '2022-05-31T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.12070990353822708,\n         'max': 0.22220245003700256,\n         'mean': 0.1623609989287693,\n         'stDev': 0.02119876493505649,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-05-31T00:00:00Z', 'to': '2022-06-10T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.30697834491729736,\n         'max': 0.5585442781448364,\n         'mean': 0.3871644425922805,\n         'stDev': 0.036877585538162914,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-06-10T00:00:00Z', 'to': '2022-06-20T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.4932262897491455,\n         'max': 0.7623130679130554,\n         'mean': 0.7166323115162642,\n         'stDev': 0.03632912872686905,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-06-20T00:00:00Z', 'to': '2022-06-30T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.5235322713851929,\n         'max': 0.8472102880477905,\n         'mean': 0.8134408265822731,\n         'stDev': 0.04334495262826597,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-06-30T00:00:00Z', 'to': '2022-07-10T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.5385261178016663,\n         'max': 0.8165295124053955,\n         'mean': 0.7346462549343704,\n         'stDev': 0.05007425808442363,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-07-10T00:00:00Z', 'to': '2022-07-20T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.48085325956344604,\n         'max': 0.7764950394630432,\n         'mean': 0.6345127327117525,\n         'stDev': 0.037435679050829576,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-07-20T00:00:00Z', 'to': '2022-07-30T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.3827281594276428,\n         'max': 0.7180401086807251,\n         'mean': 0.47361417948967777,\n         'stDev': 0.04486725306919478,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-07-30T00:00:00Z', 'to': '2022-08-09T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.25120440125465393,\n         'max': 0.6704408526420593,\n         'mean': 0.3448654317316632,\n         'stDev': 0.060250767466331526,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}},\n   {'interval': {'from': '2022-08-09T00:00:00Z', 'to': '2022-08-19T00:00:00Z'},\n    'outputs': {'ndvi': {'bands': {'B0': {'stats': {'min': 0.24285714328289032,\n         'max': 0.6134668588638306,\n         'mean': 0.33002391002395726,\n         'stDev': 0.05309494318283593,\n         'sampleCount': 306912,\n         'noDataCount': 137731}}}}}}],\n  'status': 'OK'}]\n\n\nHowever, as it is clear to see, our response is not that useful in json format. It’s difficult to read from a human perspective. So, let’s transform it into a pandas dataframe. To help us achieve this, let’s define some helper functions.\n\n# define functions to extract statistics for all acquisition dates\ndef extract_stats(date, stat_data):\n    d = {}\n    for key, value in stat_data[\"outputs\"].items():\n        stats = value[\"bands\"][\"B0\"][\"stats\"]\n        if stats[\"sampleCount\"] == stats[\"noDataCount\"]:\n            continue\n        else:\n            d[\"date\"] = [date]\n            for stat_name, stat_value in stats.items():\n                if stat_name == \"sampleCount\" or stat_name == \"noDataCount\":\n                    continue\n                else:\n                    d[f\"{key}_{stat_name}\"] = [stat_value]\n    return pd.DataFrame(d)\n\n\ndef read_acquisitions_stats(stat_data):\n    df_li = []\n    for aq in stat_data:\n        date = aq[\"interval\"][\"from\"][:10]\n        df_li.append(extract_stats(date, aq))\n    return pd.concat(df_li)\n\n\nresult_df1 = read_acquisitions_stats(response1[0][\"data\"])\nresult_df1\n\n\n\n\n\n\n\n\ndate\nndvi_min\nndvi_max\nndvi_mean\nndvi_stDev\n\n\n\n\n0\n2022-04-21\n0.081676\n0.396040\n0.133466\n0.067784\n\n\n0\n2022-05-11\n0.076008\n0.434946\n0.117718\n0.060065\n\n\n0\n2022-05-21\n0.120710\n0.222202\n0.162361\n0.021199\n\n\n0\n2022-05-31\n0.306978\n0.558544\n0.387164\n0.036878\n\n\n0\n2022-06-10\n0.493226\n0.762313\n0.716632\n0.036329\n\n\n0\n2022-06-20\n0.523532\n0.847210\n0.813441\n0.043345\n\n\n0\n2022-06-30\n0.538526\n0.816530\n0.734646\n0.050074\n\n\n0\n2022-07-10\n0.480853\n0.776495\n0.634513\n0.037436\n\n\n0\n2022-07-20\n0.382728\n0.718040\n0.473614\n0.044867\n\n\n0\n2022-07-30\n0.251204\n0.670441\n0.344865\n0.060251\n\n\n0\n2022-08-09\n0.242857\n0.613467\n0.330024\n0.053095\n\n\n\n\n\n\n\nWe can take this another step further, and display the data in a time series using the Matplotlib python library:\n\nfig_stat, ax_stat = plt.subplots(1, 1, figsize=(12, 6))\nt1 = result_df1[\"date\"]\nndvi_mean_field1 = result_df1[\"ndvi_mean\"]\nndvi_std_field1 = result_df1[\"ndvi_stDev\"]\nax_stat.plot(t1, ndvi_mean_field1, label=\"field 1 mean\")\nax_stat.fill_between(\n    t1,\n    ndvi_mean_field1 - ndvi_std_field1,\n    ndvi_mean_field1 + ndvi_std_field1,\n    alpha=0.3,\n    label=\"field 1 stDev\",\n)\nax_stat.tick_params(axis=\"x\", labelrotation=30, labelsize=12)\nax_stat.tick_params(axis=\"y\", labelsize=12)\nax_stat.set_xlabel(\"Date\", size=15)\nax_stat.set_ylabel(\"NDVI/unitless\", size=15)\nax_stat.legend(loc=\"lower right\", prop={\"size\": 12})\nax_stat.set_title(\"NDVI time series\", fontsize=20)\nfor label in ax_stat.get_xticklabels()[1::2]:\n    label.set_visible(False)\n\n\n\n\n\n\nComparing different fields\nNow that we have learnt how to plot the data for the first field, let’s take this another step forward and compare the NDVI time series of the first field with the second field. We will now run the same request for our second field and then transform the response into a second Pandas dataframe.\n\ngeometry = Geometry(geometry=field2, crs=CRS.WGS84)\n\nrequest = SentinelHubStatistical(\n    aggregation=SentinelHubStatistical.aggregation(\n        evalscript=evalscript,\n        time_interval=(\"2022-04-01T00:00:00Z\", \"2022-08-30T23:59:59Z\"),\n        aggregation_interval=\"P10D\",\n        size=[368.043, 834.345],\n    ),\n    input_data=[\n        SentinelHubStatistical.input_data(\n            DataCollection.SENTINEL2_L1C.define_from(\n                name=\"s2l1c\", service_url=\"https://sh.dataspace.copernicus.eu\"\n            ),\n            other_args={\"dataFilter\": {\"maxCloudCoverage\": 10}},\n        ),\n    ],\n    geometry=geometry,\n    config=config,\n)\n\nresponse2 = request.get_data()\nresult_df2 = read_acquisitions_stats(response2[0][\"data\"])\n\nNow we have requested the statistics for both fields and transformed them into Pandas dataframes, let’s plot the two time series and visualise this in the same plot:\n\nfig_stat, ax_stat = plt.subplots(1, 1, figsize=(12, 6))\nt1 = result_df1[\"date\"]\nt2 = result_df1[\"date\"]\nndvi_mean_field1 = result_df1[\"ndvi_mean\"]\nndvi_std_field1 = result_df1[\"ndvi_stDev\"]\nndvi_mean_field2 = result_df2[\"ndvi_mean\"]\nndvi_std_field2 = result_df2[\"ndvi_stDev\"]\nax_stat.plot(t1, ndvi_mean_field1, label=\"field 1 mean\")\nax_stat.fill_between(\n    t1,\n    ndvi_mean_field1 - ndvi_std_field1,\n    ndvi_mean_field1 + ndvi_std_field1,\n    alpha=0.3,\n    label=\"field 1 stDev\",\n)\nax_stat.plot(t2, ndvi_mean_field2, label=\"field 2 mean\")\nax_stat.fill_between(\n    t2,\n    ndvi_mean_field2 - ndvi_std_field2,\n    ndvi_mean_field2 + ndvi_std_field2,\n    alpha=0.3,\n    label=\"field 2 stDev\",\n)\nax_stat.tick_params(axis=\"x\", labelrotation=30, labelsize=12)\nax_stat.tick_params(axis=\"y\", labelsize=12)\nax_stat.set_xlabel(\"Date\", size=15)\nax_stat.set_ylabel(\"NDVI/unitless\", size=15)\nax_stat.legend(loc=\"lower right\", prop={\"size\": 12})\nax_stat.set_title(\"NDVI time series\", fontsize=20)\nfor label in ax_stat.get_xticklabels()[1::2]:\n    label.set_visible(False)"
  },
  {
    "objectID": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#summary",
    "href": "notebook-samples/sentinelhub/introduction_to_SH_APIs.html#summary",
    "title": "First Steps in accessing Satellite Imagery on Copernicus Data Space Ecosystem with Sentinel Hub APIs",
    "section": "Summary",
    "text": "Summary\nSo what have we learnt in this notebook?\n\nHow to quickly access satellite imagery though Sentinel Hub using Process API.\nVisualising NDVI derived from the satellite imagery\nUsing Statistical API to produce NDVI time series for single and multiple fields.\n\nThis concludes this notebook on working with Sentinel Hub APIs to access data from the Copernicus Data Space Ecosystem. For more information you can check out the Sentinel Hub API Documentation and the Sentinel Hub Python package documentation too."
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "",
    "text": "Sentinel 2 data is one of the most popular satellite datasets, but it does come with challenges. Cloud-free mosaics have to be constructed often in order to get analysis-ready data. Accessing a lot of data through tiles takes a long time, and getting the data into a format it can be easily analysed in with common Python tools can be a challenge.\nIn this notebook, we will show how this whole process of getting analysis-ready data into Python can be sped up by using the Copernicus Dataspace Ecosystem and Sentinel Hub APIs. This is being presented by running through a basic deforestation monitoring use-case. The notebook uses the popular xarray Python library to handle the multidimensional data.\nWhat we show in this notebook:"
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#prerequisites",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#prerequisites",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA Copernicus Dataspace Ecosystem account\nBasic understanding of the Sentinel Hub Processing API (Introductory Notebook available here)\n\n\nimport getpass\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport requests\nimport matplotlib.colors as mcolors\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\nfrom ipyleaflet import GeoJSON, Map, basemaps\nfrom sentinelhub import (\n    CRS,\n    BBox,\n    DataCollection,\n    MimeType,\n    SentinelHubDownloadClient,\n    SentinelHubRequest,\n    SHConfig,\n)\nfrom sklearn.metrics import accuracy_score\n\n\nCredentials\nTo obtain your client_id & client_secret, you need to navigate to your Dashboard. In the User Settings, you can create a new OAuth client to generate these credentials. More detailed instructions can be found on the corresponding documentation page.\nNow that you have your client_id & client_secret, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found here. Following these instructions, you can create a profile specifically for using the package to access Copernicus Data Space Ecosystem data collections. This is useful as changes to the config class in your notebook are usually only temporary and by saving the configuration to your profile, you don’t have to generate new credentials or overwrite/change the default profile every time you run or write a new Jupyter Notebook.\nIf you are using the Sentinel Hub Python package for the Copernicus Data Space Ecosystem for the first time, you should create a profile specifically for the Copernicus Data Space Ecosystem. You can do this in the following cell:\n\n# Only run this cell if you have not created a configuration.\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n# config.save(\"cdse\")\n\nHowever, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing profile_name.\n\n# config = SHConfig(\"profile-name\")"
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#area-of-interest",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#area-of-interest",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Area of Interest",
    "text": "Area of Interest\nFirst, we define an area of interest. In this case the area of interest is in the Harz Mountains in Germany since we are aware of substantial forest dieback in recent years.\nThe resolution is defined in the units of the coordinate reference system. Because we want to define units in meters, we also need to define the bounding box coordinates in a CRS using meters. We use EPSG:3035 in this case. This CRS is only available for Europe, outside of Europe we could use EPSG:3857 or UTM Zones.\nYou can also explore the area of interest in the Copernicus Browser here.\n\n\n\nDeforestation in Harz Mountain as seen from Sentinel-2, June 2023\n\n\n\n# Desired resolution of our data\nresolution = (100, 100)\nbbox_coords = [10.633501, 51.611195, 10.787234, 51.698098]\nepsg = 3035\n# Convert to 3035 to get crs with meters as units\nbbox = BBox(bbox_coords, CRS(4326)).transform(epsg)\n\n\nx, y = bbox.transform(4326).middle\n\n# Add OSM background\noverview_map = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=(y, x), zoom=10)\n\n# Add geojson data\ngeo_json = GeoJSON(data=bbox.transform(4326).geojson)\noverview_map.add_layer(geo_json)\n\n# Display\noverview_map"
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#data-access",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#data-access",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Data Access",
    "text": "Data Access\nNext, we define our evalscript. The evalscript is a piece of JavaScript code that tells the Copernicus Dataspace Ecosystem how to process the pixels you request before they are delivered to you.\nThis makes it a very powerful tool to perform pixel-based calculations in the cloud. For inspiration on what can be done in an evalscript, there is an extensive online resource of community-created evalscripts called custom-scripts. In this example, we want to calculate cloud-free mosaics. This is a perfect application for an evalscript, as you do not have to download the data needed to generate the mosaic, but all calculations are done on the server and only the final cloud-free mosaic is delivered.\nSo let’s go over how this is done.\nThe evalscript needs to define two functions, setup() and evaluatePixel(). First, let’s look at the setup function:\nfunction setup() {\n    return {\n        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n        output: {\n            bands: 5,\n            sampleType: \"INT16\"\n        },\n        mosaicking: \"ORBIT\"\n    }\n}\nHere we specify which bands we want to request. In this case, we get the bands needed to calculate the NDVI and to display a True Color Image. We also define how our output should be structured, and define the output as a 5-band image with the INT16 data type.\nFinally, we specify the mosaicking parameter. This determines how the pixel values are returned to us. - mosaicking: \"SIMPLE\" returns only a single pixel, either from the most recent, the least recent or the least cloudy Sentinel 2 tile.\n\nmosaicking: \"ORBIT\" returns all pixels of unique orbits for the entire time series as a list. We use this to obtain all possible values from which we can create the cloud-free mosaic.\n\nNext let’s take a look at the evaluatePixel() function. This is the function where the actual calculation is defined:\nfunction evaluatePixel(samples) {\n    var valid = samples.filter(validate);\n    if (valid.length &gt; 0 ) {\n        let cloudless = {\n            b08: getFirstQuartileValue(valid.map(s =&gt; s.B08)),\n            b04: getFirstQuartileValue(valid.map(s =&gt; s.B04)),\n            b03: getFirstQuartileValue(valid.map(s =&gt; s.B03)),\n            b02: getFirstQuartileValue(valid.map(s =&gt; s.B02)),\n        }\n        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n        // This applies a scale factor so the data can be saved as an int\n        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v =&gt; v*10000);\n        return scale\n    }\n    // If there isn't enough data, return NODATA\n    return [-32768, -32768, -32768, -32768]\n}\nThe way we construct the cloud free mosaic is by first filtering all the available acquisitions to only include the ones which contain clear data with samples.filter(validate);. Then we sort the array and get the value at the first quartile of the array. Getting the first quartile instead of the mean or median further reduces the risk that we select a cloudy pixel.\nFinally, we calculate the NDVI using the cloud-free values and return all the desired values as an array.\n\nevalscript_cloudless = \"\"\"\n//VERSION=3\nfunction setup() {\n    return {\n        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n        output: {\n            bands: 4,\n            sampleType: \"INT16\"\n        },\n        mosaicking: \"ORBIT\"\n    }\n}\n\nfunction getFirstQuartileValue(values) {\n    values.sort((a,b) =&gt; a-b);\n    return getFirstQuartile(values);\n}\n\nfunction getFirstQuartile(sortedValues) {\n    var index = Math.floor(sortedValues.length / 4);\n    return sortedValues[index];\n}\n\nfunction validate(sample) {\n    // Define codes as invalid:\n    const invalid = [\n        0, // NO_DATA\n        1, // SATURATED_DEFECTIVE\n        3, // CLOUD_SHADOW\n        7, // CLOUD_LOW_PROBA\n        8, // CLOUD_MEDIUM_PROBA\n        9, // CLOUD_HIGH_PROBA\n        10 // THIN_CIRRUS\n    ]\n    return !invalid.includes(sample.SCL)\n}\n\nfunction evaluatePixel(samples) {\n    var valid = samples.filter(validate);\n    if (valid.length &gt; 0 ) {\n        let cloudless = {\n            b08: getFirstQuartileValue(valid.map(s =&gt; s.B08)),\n            b04: getFirstQuartileValue(valid.map(s =&gt; s.B04)),\n            b03: getFirstQuartileValue(valid.map(s =&gt; s.B03)),\n            b02: getFirstQuartileValue(valid.map(s =&gt; s.B02)),\n        }\n        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n        // This applies a scale factor so the data can be saved as an int\n        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v =&gt; v*10000);\n        return scale\n    }\n    // If there isn't enough data, return NODATA\n    return [-32768, -32768, -32768, -32768]\n}\n\"\"\"\n\nWe have defined how the pixels should be handled. However, we still need to define some other parameters to get a full request.\nWe need to define which data we want to use and the timeframe of the data.\nThis is what we are doing in the next cell. Here, we also start building our time series. To see changes over the years, we want to get cloud-free mosaics for the same 3 months over the years. We do this by defining the three months (June-August) in the interval_of_interest() function. Then we define a function get_request(), which will build the request to the Sentinel Hub API on the Copernicus Data Space Ecosystem.\nIn this SentinelHubRequest, we define the input data, the timeframe, the output type (TIFF), the bounding box, the resolution and where to save the data.\nWe define this as a function because we want to make several requests with the changing years being the only input.\n\ndef interval_of_interest(year):\n    return (datetime(year, 6, 1), datetime(year, 9, 1))\n\n\ndef get_request(year):\n    time_interval = interval_of_interest(year)\n    return SentinelHubRequest(\n        evalscript=evalscript_cloudless,\n        input_data=[\n            SentinelHubRequest.input_data(\n                data_collection=DataCollection.SENTINEL2_L2A.define_from(\n                    \"s2\", service_url=config.sh_base_url\n                ),\n                time_interval=time_interval,\n            )\n        ],\n        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n        bbox=bbox,\n        resolution=resolution,\n        config=config,\n        data_folder=\"./data\",\n    )\n\nThis cell now creates a request for each of the years, from 2018 to 2023.\n\n# create a dictionary of requests\nsh_requests = {}\nfor year in range(2018, 2024):\n    sh_requests[year] = get_request(year)\n\nsh_requests\n\n{2018: &lt;sentinelhub.api.process.SentinelHubRequest at 0x217d27aa4d0&gt;,\n 2019: &lt;sentinelhub.api.process.SentinelHubRequest at 0x217d2951dd0&gt;,\n 2020: &lt;sentinelhub.api.process.SentinelHubRequest at 0x217d29520d0&gt;,\n 2021: &lt;sentinelhub.api.process.SentinelHubRequest at 0x217d29523d0&gt;,\n 2022: &lt;sentinelhub.api.process.SentinelHubRequest at 0x217d29526d0&gt;,\n 2023: &lt;sentinelhub.api.process.SentinelHubRequest at 0x217d2952c90&gt;}\n\n\nThe next step is to download the data. This is done with the utility function SentinelHubDownloadClient. It downloads a list of requests in parallel, greatly improving the download speed. Before we can do that, we need to change the format of the requests slightly, which is done in the variable list_of_requests.\n\nlist_of_requests = [request.download_list[0] for request in sh_requests.values()]\n\n# download data with multiple threads\ndata = SentinelHubDownloadClient(config=config).download(\n    list_of_requests, max_threads=5\n)\n\nThe output of the requests do not provide any information about which year the data is from, so we rename the output of each request to the year of the data it represents.\n\ndef request_output_path(request):\n    # Gets the full path to the output from a request\n    return Path(request.data_folder, request.get_filename_list()[0])\n\n\n# Moves and renames the files to the root directory of results\nfor year, request in sh_requests.items():\n    request_output_path(request).rename(f\"./data/{year}.tif\")"
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#read-data-with-xarray",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#read-data-with-xarray",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Read data with xarray",
    "text": "Read data with xarray\nNow we can load the data into xarray. We use rioxarray, an extension for xarray, to load multiple tiffs into a single xarray dataset. xarray is a scalable tool for analysing multidimensional data in Python. This makes xarray ideal for analysing time series data.\nThe different files correspond to the time dimension, but xarray does not know which file is which time step. Therefore, we add a pre-processing step in which we parse out the year from the filename and add it as the time dimension for that file.\nThe warnings in the output can be safely ignored.\n\ndef add_time_dim(xda):\n    # This pre-processes the file to add the correct\n    # year from the filename as the time dimension\n    year = int(Path(xda.encoding[\"source\"]).stem)\n    return xda.expand_dims(year=[year])\n\n\ntiff_paths = Path(\"./data\").glob(\"*.tif\")\nds_s2 = xr.open_mfdataset(\n    tiff_paths,\n    engine=\"rasterio\",\n    preprocess=add_time_dim,\n    band_as_variable=True,\n)\nds_s2 = ds_s2.rename(\n    {\n        \"band_1\": \"R\",\n        \"band_2\": \"G\",\n        \"band_3\": \"B\",\n        \"band_4\": \"NDVI\",\n    }\n)\nds_s2 = ds_s2 / 10000\n\nWe can use the resultatnt xarray to plot the RGB data as a true color image:\n\n# Get RGB data for a year\nplot_year = 2018\ntrue_color = ds_s2.sel(year=plot_year)[[\"R\", \"G\", \"B\"]].to_array()\n# Divide by scale factor and apply gamma to brighten image\n(true_color * 4).plot.imshow()\nplt.title(f\"True Color {plot_year}\");\n\n\n\n\nWe now have an xarray dataset with 3 coordinates: year, x and y, as well as the data variables returned by the evalscript as data variables in the dataset.\n\nds_s2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (year: 6, x: 105, y: 98)\nCoordinates:\n  * year         (year) int64 2018 2019 2020 2021 2022 2023\n  * x            (x) float64 4.365e+06 4.365e+06 ... 4.375e+06 4.375e+06\n  * y            (y) float64 3.177e+06 3.177e+06 ... 3.167e+06 3.167e+06\n    spatial_ref  int32 0\nData variables:\n    R            (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    G            (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    B            (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    NDVI         (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1xarray.DatasetDimensions:year: 6x: 105y: 98Coordinates: (4)year(year)int642018 2019 2020 2021 2022 2023array([2018, 2019, 2020, 2021, 2022, 2023], dtype=int64)x(x)float644.365e+06 4.365e+06 ... 4.375e+06array([4364933.008071, 4365033.431633, 4365133.855196, 4365234.278759,\n       4365334.702322, 4365435.125885, 4365535.549448, 4365635.973011,\n       4365736.396574, 4365836.820137, 4365937.2437  , 4366037.667263,\n       4366138.090825, 4366238.514388, 4366338.937951, 4366439.361514,\n       4366539.785077, 4366640.20864 , 4366740.632203, 4366841.055766,\n       4366941.479329, 4367041.902892, 4367142.326455, 4367242.750017,\n       4367343.17358 , 4367443.597143, 4367544.020706, 4367644.444269,\n       4367744.867832, 4367845.291395, 4367945.714958, 4368046.138521,\n       4368146.562084, 4368246.985647, 4368347.409209, 4368447.832772,\n       4368548.256335, 4368648.679898, 4368749.103461, 4368849.527024,\n       4368949.950587, 4369050.37415 , 4369150.797713, 4369251.221276,\n       4369351.644839, 4369452.068401, 4369552.491964, 4369652.915527,\n       4369753.33909 , 4369853.762653, 4369954.186216, 4370054.609779,\n       4370155.033342, 4370255.456905, 4370355.880468, 4370456.304031,\n       4370556.727593, 4370657.151156, 4370757.574719, 4370857.998282,\n       4370958.421845, 4371058.845408, 4371159.268971, 4371259.692534,\n       4371360.116097, 4371460.53966 , 4371560.963223, 4371661.386785,\n       4371761.810348, 4371862.233911, 4371962.657474, 4372063.081037,\n       4372163.5046  , 4372263.928163, 4372364.351726, 4372464.775289,\n       4372565.198852, 4372665.622415, 4372766.045977, 4372866.46954 ,\n       4372966.893103, 4373067.316666, 4373167.740229, 4373268.163792,\n       4373368.587355, 4373469.010918, 4373569.434481, 4373669.858044,\n       4373770.281607, 4373870.705169, 4373971.128732, 4374071.552295,\n       4374171.975858, 4374272.399421, 4374372.822984, 4374473.246547,\n       4374573.67011 , 4374674.093673, 4374774.517236, 4374874.940799,\n       4374975.364361, 4375075.787924, 4375176.211487, 4375276.63505 ,\n       4375377.058613])y(y)float643.177e+06 3.177e+06 ... 3.167e+06array([3176652.710129, 3176552.99504 , 3176453.27995 , 3176353.564861,\n       3176253.849772, 3176154.134682, 3176054.419593, 3175954.704504,\n       3175854.989414, 3175755.274325, 3175655.559236, 3175555.844146,\n       3175456.129057, 3175356.413968, 3175256.698878, 3175156.983789,\n       3175057.2687  , 3174957.55361 , 3174857.838521, 3174758.123432,\n       3174658.408342, 3174558.693253, 3174458.978164, 3174359.263074,\n       3174259.547985, 3174159.832896, 3174060.117806, 3173960.402717,\n       3173860.687628, 3173760.972538, 3173661.257449, 3173561.542359,\n       3173461.82727 , 3173362.112181, 3173262.397091, 3173162.682002,\n       3173062.966913, 3172963.251823, 3172863.536734, 3172763.821645,\n       3172664.106555, 3172564.391466, 3172464.676377, 3172364.961287,\n       3172265.246198, 3172165.531109, 3172065.816019, 3171966.10093 ,\n       3171866.385841, 3171766.670751, 3171666.955662, 3171567.240573,\n       3171467.525483, 3171367.810394, 3171268.095305, 3171168.380215,\n       3171068.665126, 3170968.950037, 3170869.234947, 3170769.519858,\n       3170669.804769, 3170570.089679, 3170470.37459 , 3170370.659501,\n       3170270.944411, 3170171.229322, 3170071.514233, 3169971.799143,\n       3169872.084054, 3169772.368965, 3169672.653875, 3169572.938786,\n       3169473.223697, 3169373.508607, 3169273.793518, 3169174.078429,\n       3169074.363339, 3168974.64825 , 3168874.933161, 3168775.218071,\n       3168675.502982, 3168575.787893, 3168476.072803, 3168376.357714,\n       3168276.642625, 3168176.927535, 3168077.212446, 3167977.497357,\n       3167877.782267, 3167778.067178, 3167678.352089, 3167578.636999,\n       3167478.92191 , 3167379.206821, 3167279.491731, 3167179.776642,\n       3167080.061553, 3166980.346463])spatial_ref()int320crs_wkt :PROJCS[\"ETRS89-extended / LAEA Europe\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"latitude_of_center\",52],PARAMETER[\"longitude_of_center\",10],PARAMETER[\"false_easting\",4321000],PARAMETER[\"false_northing\",3210000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Northing\",NORTH],AXIS[\"Easting\",EAST],AUTHORITY[\"EPSG\",\"3035\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :ETRS89horizontal_datum_name :European Terrestrial Reference System 1989projected_crs_name :ETRS89-extended / LAEA Europegrid_mapping_name :lambert_azimuthal_equal_arealatitude_of_projection_origin :52.0longitude_of_projection_origin :10.0false_easting :4321000.0false_northing :3210000.0spatial_ref :PROJCS[\"ETRS89-extended / LAEA Europe\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"latitude_of_center\",52],PARAMETER[\"longitude_of_center\",10],PARAMETER[\"false_easting\",4321000],PARAMETER[\"false_northing\",3210000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Northing\",NORTH],AXIS[\"Easting\",EAST],AUTHORITY[\"EPSG\",\"3035\"]]GeoTransform :4364882.796289107 100.42356290888219 0.0 3176702.5676735905 0.0 -99.71508933638925array(0)Data variables: (4)R(year, y, x)float32dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nG\n\n\n(year, y, x)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB\n\n\n(year, y, x)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nNDVI\n\n\n(year, y, x)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)yearPandasIndexPandasIndex(Int64Index([2018, 2019, 2020, 2021, 2022, 2023], dtype='int64', name='year'))xPandasIndexPandasIndex(Float64Index([4364933.008070561,  4365033.43163347, 4365133.855196379,\n              4365234.278759288, 4365334.702322196, 4365435.125885106,\n              4365535.549448014, 4365635.973010924, 4365736.396573832,\n              4365836.820136741,\n              ...\n              4374473.246546905, 4374573.670109814, 4374674.093672723,\n              4374774.517235631, 4374874.940798541, 4374975.364361449,\n              4375075.787924359, 4375176.211487267, 4375276.635050176,\n              4375377.058613085],\n             dtype='float64', name='x', length=105))yPandasIndexPandasIndex(Float64Index([3176652.7101289225,  3176552.995039586, 3176453.2799502495,\n              3176353.5648609134,  3176253.849771577, 3176154.1346822404,\n               3176054.419592904,  3175954.704503568, 3175854.9894142314,\n               3175755.274324895, 3175655.5592355584, 3175555.8441462223,\n               3175456.129056886, 3175356.4139675493,  3175256.698878213,\n               3175156.983788877, 3175057.2686995403,  3174957.553610204,\n              3174857.8385208673, 3174758.1234315312, 3174658.4083421947,\n              3174558.6932528582, 3174458.9781635217, 3174359.2630741857,\n               3174259.547984849, 3174159.8328955127,  3174060.117806176,\n                3173960.40271684, 3173860.6876275036,  3173760.972538167,\n              3173661.2574488306, 3173561.5423594946,  3173461.827270158,\n              3173362.1121808216,  3173262.397091485,  3173162.682002149,\n              3173062.9669128126,  3172963.251823476, 3172863.5367341395,\n              3172763.8216448035,  3172664.106555467, 3172564.3914661305,\n               3172464.676376794,  3172364.961287458, 3172265.2461981215,\n               3172165.531108785, 3172065.8160194485, 3171966.1009301124,\n               3171866.385840776, 3171766.6707514394,  3171666.955662103,\n              3171567.2405727664, 3171467.5254834304,  3171367.810394094,\n              3171268.0953047574,  3171168.380215421,  3171068.665126085,\n              3170968.9500367483,  3170869.234947412, 3170769.5198580753,\n              3170669.8047687393, 3170570.0896794028, 3170470.3745900663,\n              3170370.6595007298, 3170270.9444113937,  3170171.229322057,\n              3170071.5142327207,  3169971.799143384,  3169872.084054048,\n              3169772.3689647117,  3169672.653875375, 3169572.9387860387,\n              3169473.2236967026,  3169373.508607366, 3169273.7935180296,\n               3169174.078428693,  3169074.363339357, 3168974.6482500206,\n               3168874.933160684, 3168775.2180713476, 3168675.5029820115,\n               3168575.787892675, 3168476.0728033385,  3168376.357714002,\n               3168276.642624666, 3168176.9275353295,  3168077.212445993,\n              3167977.4973566565, 3167877.7822673204,  3167778.067177984,\n              3167678.3520886474,  3167578.636999311,  3167478.921909975,\n              3167379.2068206384,  3167279.491731302, 3167179.7766419654,\n              3167080.0615526293,  3166980.346463293],\n             dtype='float64', name='y'))Attributes: (4)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1\n\n\nWe can also similarly plot the NDVI values:\n\nds_s2.NDVI.plot(cmap=\"PRGn\", x=\"x\", y=\"y\", col=\"year\", col_wrap=3);"
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#analysis",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#analysis",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Analysis",
    "text": "Analysis\nFor analysis the first step is to classify pixels as forest. In our case we will just do a simple thresholding classification where we classify everything above a certain threshold as forest. This isn’t the best approach for classifying forest, since agricultural areas can also easily reach very high NDVI values. A better approach would be to classify based on the temporal signature of the pixel.\nHowever, for this basic analysis, we stick to the simple thresholding approach.\nIn this case we classify everything above an NDVI of 0.7 as forest. This calculated forest mask is then saved to a new Data Variable in the xarray dataset:\n\nds_s2[\"FOREST\"] = ds_s2.NDVI &gt; 0.7\n\nWith this forest mask we can already do a quick preliminary analysis to plot the total forest area over the years.\nTo do this we sum up the pixels along the x and y coordinate but not along the time coordinate. This will leave us with one value per year representing the number of pixels classified as forest. We can then calculate the forest area by multiplying the number of forest pixels by the resolution.\n\ndef to_km2(dataarray, resolution):\n    # Calculate forest area\n    return dataarray * np.prod(list(resolution)) / 1e6\n\n\nforest_pixels = ds_s2.FOREST.sum([\"x\", \"y\"])\nforest_area_km2 = to_km2(forest_pixels, resolution)\nforest_area_km2.plot()\nplt.title(\"Forest Cover\")\nplt.ylabel(\"Forest Cover [km²]\")\nplt.ylim(0);\n\n\n\n\nWe can see that the total forest area in this AOI decreased from around 80 km² in 2018 to only around 50 km² in 2023.\nThe next step is to make change maps from year to year. To do this we basically take the difference of the forest mask of a year with its previous year.\nThis will result in 0 value where there has been no change, -1 where forest was lost and +1 where forest was gained.\n\n# Make change maps of forest loss and forest gain compared to previous year\n\n# 0 - 0 = No Change: 0\n# 1 - 1 = No Change: 0\n# 1 - 0 = Forest Gain: 1\n# 0 - 1 = Forest Loss: -1\n\n# Define custom colors and labels\ncolors = [\"darkred\", \"white\", \"darkblue\"]\nlabels = [\"Forest Loss\", \"No Change\", \"Forest Gain\"]\n\n# Create a colormap and normalize it\ncmap = mcolors.ListedColormap(colors)\nnorm = plt.Normalize(-1, 1)  # Adjust the range based on your data\n\nplot_year = 2022\nds_s2[\"CHANGE\"] = ds_s2.FOREST.astype(int).diff(\"year\", label=\"upper\")\nds_s2.CHANGE.sel(year=plot_year).plot(cmap=cmap, norm=norm, add_colorbar=False)\n\n# Create a legend with string labels\nlegend_patches = [\n    mpatches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n]\nplt.legend(handles=legend_patches, loc=\"lower left\")\nplt.title(f\"Forest Change Map {plot_year}\");\n\n\n\n\nHere, we can see the spatial distribution of areas affected by forest loss. In the displayed change from 2021 to 2022, most of the forest loss happened in the northern part of the study area, while the southern part lost comparatively less forest.\nTo get a feel for the loss per year, we can cumulatively sum up the lost areas over the years. This should basically follow the same trends as the earlier plot of total forest area.\n\n# Forest Loss per Year\nforest_loss = (ds_s2.CHANGE == -1).sum([\"x\", \"y\"])\nforest_loss_km2 = to_km2(forest_loss, resolution)\nforest_loss_km2.cumsum().plot()\nplt.title(\"Cumulative Forest Loss\")\nplt.ylabel(\"Forest Loss [km²]\");\n\n\n\n\nWe can see that there have been two years with particularly large amounts of lost forest area. From 2019-2020 and with by far the most lost area between 2021 and 2022."
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#validation",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#validation",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Validation",
    "text": "Validation\nFinally, we want to see how accurate our data is compared to the widely used Hansen Global Forest Change data. In a real scientific scenario, we would use Ground Truth data to assess the accuracy of our classification. In this case we use the Global Forest Change data in place of Ground Truth data, just to show how an accuracy assessment can be done. The assessment we are doing only shows how accurately we replicate the Global Forest Change data, however we will not know if our product is more or less accurate. For a more accurate assessment, actual Ground Truth data is required.\nFirst we download the Global Forest Change Data here and open it using xarray.\n\ndata_path = Path(\"./data/\")\ndata_path.mkdir(parents=True, exist_ok=True)\nhansen_filename = \"Hansen_GFC-2022-v1.10_lossyear_60N_010E.tif\"\ncomp_data = data_path / hansen_filename\n\nwith comp_data.open(\"wb\") as fs:\n    hansen_data = requests.get(\n        f\"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/{hansen_filename}\"\n    )\n    fs.write(hansen_data.content)\n\n\n# Open the file\nground_truth = (\n    xr.open_dataarray(comp_data, engine=\"rasterio\")\n    .rio.clip_box(*bbox_coords)\n    .rio.reproject(epsg)\n    .sel(band=1)\n    .where(lambda gt: gt &lt; 100, 0)  # fill no-data (values over 100) with 0\n)\nground_truth.plot(levels=range(25), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\nplt.title(\"Global Forest Watch Data\");\n\n\n\n\nThe data shows in which year forest was first lost. To compare with our own data, we need to add the data to our dataset. To do this the data needs to have the same coordinates. This can be achieved with .interp_like(). This function interpolates the data to match up the coordinates of another dataset.\nIn this case we chose the interpolation method nearest since it is categorical data.\n\nds_s2[\"GROUND_TRUTH\"] = ground_truth.interp_like(ds_s2, method=\"nearest\").astype(int)\nds_s2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (year: 6, x: 105, y: 98)\nCoordinates:\n  * year          (year) int64 2018 2019 2020 2021 2022 2023\n  * x             (x) float64 4.365e+06 4.365e+06 ... 4.375e+06 4.375e+06\n  * y             (y) float64 3.177e+06 3.177e+06 ... 3.167e+06 3.167e+06\n    spatial_ref   int32 0\n    band          int32 1\nData variables:\n    R             (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    G             (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    B             (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    NDVI          (year, y, x) float32 dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    FOREST        (year, y, x) bool dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n    CHANGE        (year, y, x) float64 dask.array&lt;chunksize=(2, 9, 105), meta=np.ndarray&gt;\n    GROUND_TRUTH  (y, x) int32 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1xarray.DatasetDimensions:year: 6x: 105y: 98Coordinates: (5)year(year)int642018 2019 2020 2021 2022 2023array([2018, 2019, 2020, 2021, 2022, 2023], dtype=int64)x(x)float644.365e+06 4.365e+06 ... 4.375e+06array([4364933.008071, 4365033.431633, 4365133.855196, 4365234.278759,\n       4365334.702322, 4365435.125885, 4365535.549448, 4365635.973011,\n       4365736.396574, 4365836.820137, 4365937.2437  , 4366037.667263,\n       4366138.090825, 4366238.514388, 4366338.937951, 4366439.361514,\n       4366539.785077, 4366640.20864 , 4366740.632203, 4366841.055766,\n       4366941.479329, 4367041.902892, 4367142.326455, 4367242.750017,\n       4367343.17358 , 4367443.597143, 4367544.020706, 4367644.444269,\n       4367744.867832, 4367845.291395, 4367945.714958, 4368046.138521,\n       4368146.562084, 4368246.985647, 4368347.409209, 4368447.832772,\n       4368548.256335, 4368648.679898, 4368749.103461, 4368849.527024,\n       4368949.950587, 4369050.37415 , 4369150.797713, 4369251.221276,\n       4369351.644839, 4369452.068401, 4369552.491964, 4369652.915527,\n       4369753.33909 , 4369853.762653, 4369954.186216, 4370054.609779,\n       4370155.033342, 4370255.456905, 4370355.880468, 4370456.304031,\n       4370556.727593, 4370657.151156, 4370757.574719, 4370857.998282,\n       4370958.421845, 4371058.845408, 4371159.268971, 4371259.692534,\n       4371360.116097, 4371460.53966 , 4371560.963223, 4371661.386785,\n       4371761.810348, 4371862.233911, 4371962.657474, 4372063.081037,\n       4372163.5046  , 4372263.928163, 4372364.351726, 4372464.775289,\n       4372565.198852, 4372665.622415, 4372766.045977, 4372866.46954 ,\n       4372966.893103, 4373067.316666, 4373167.740229, 4373268.163792,\n       4373368.587355, 4373469.010918, 4373569.434481, 4373669.858044,\n       4373770.281607, 4373870.705169, 4373971.128732, 4374071.552295,\n       4374171.975858, 4374272.399421, 4374372.822984, 4374473.246547,\n       4374573.67011 , 4374674.093673, 4374774.517236, 4374874.940799,\n       4374975.364361, 4375075.787924, 4375176.211487, 4375276.63505 ,\n       4375377.058613])y(y)float643.177e+06 3.177e+06 ... 3.167e+06array([3176652.710129, 3176552.99504 , 3176453.27995 , 3176353.564861,\n       3176253.849772, 3176154.134682, 3176054.419593, 3175954.704504,\n       3175854.989414, 3175755.274325, 3175655.559236, 3175555.844146,\n       3175456.129057, 3175356.413968, 3175256.698878, 3175156.983789,\n       3175057.2687  , 3174957.55361 , 3174857.838521, 3174758.123432,\n       3174658.408342, 3174558.693253, 3174458.978164, 3174359.263074,\n       3174259.547985, 3174159.832896, 3174060.117806, 3173960.402717,\n       3173860.687628, 3173760.972538, 3173661.257449, 3173561.542359,\n       3173461.82727 , 3173362.112181, 3173262.397091, 3173162.682002,\n       3173062.966913, 3172963.251823, 3172863.536734, 3172763.821645,\n       3172664.106555, 3172564.391466, 3172464.676377, 3172364.961287,\n       3172265.246198, 3172165.531109, 3172065.816019, 3171966.10093 ,\n       3171866.385841, 3171766.670751, 3171666.955662, 3171567.240573,\n       3171467.525483, 3171367.810394, 3171268.095305, 3171168.380215,\n       3171068.665126, 3170968.950037, 3170869.234947, 3170769.519858,\n       3170669.804769, 3170570.089679, 3170470.37459 , 3170370.659501,\n       3170270.944411, 3170171.229322, 3170071.514233, 3169971.799143,\n       3169872.084054, 3169772.368965, 3169672.653875, 3169572.938786,\n       3169473.223697, 3169373.508607, 3169273.793518, 3169174.078429,\n       3169074.363339, 3168974.64825 , 3168874.933161, 3168775.218071,\n       3168675.502982, 3168575.787893, 3168476.072803, 3168376.357714,\n       3168276.642625, 3168176.927535, 3168077.212446, 3167977.497357,\n       3167877.782267, 3167778.067178, 3167678.352089, 3167578.636999,\n       3167478.92191 , 3167379.206821, 3167279.491731, 3167179.776642,\n       3167080.061553, 3166980.346463])spatial_ref()int320crs_wkt :PROJCS[\"ETRS89-extended / LAEA Europe\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"latitude_of_center\",52],PARAMETER[\"longitude_of_center\",10],PARAMETER[\"false_easting\",4321000],PARAMETER[\"false_northing\",3210000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Northing\",NORTH],AXIS[\"Easting\",EAST],AUTHORITY[\"EPSG\",\"3035\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :ETRS89horizontal_datum_name :European Terrestrial Reference System 1989projected_crs_name :ETRS89-extended / LAEA Europegrid_mapping_name :lambert_azimuthal_equal_arealatitude_of_projection_origin :52.0longitude_of_projection_origin :10.0false_easting :4321000.0false_northing :3210000.0spatial_ref :PROJCS[\"ETRS89-extended / LAEA Europe\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"latitude_of_center\",52],PARAMETER[\"longitude_of_center\",10],PARAMETER[\"false_easting\",4321000],PARAMETER[\"false_northing\",3210000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Northing\",NORTH],AXIS[\"Easting\",EAST],AUTHORITY[\"EPSG\",\"3035\"]]GeoTransform :4364882.796289107 100.42356290888219 0.0 3176702.5676735905 0.0 -99.71508933638925array(0)band()int321array(1)Data variables: (7)R(year, y, x)float32dask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nG\n\n\n(year, y, x)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nB\n\n\n(year, y, x)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nNDVI\n\n\n(year, y, x)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n241.17 kiB\n3.69 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 20 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nFOREST\n\n\n(year, y, x)\n\n\nbool\n\n\ndask.array&lt;chunksize=(1, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.29 kiB\n0.92 kiB\n\n\nShape\n(6, 98, 105)\n(1, 9, 105)\n\n\nDask graph\n66 chunks in 21 graph layers\n\n\nData type\nbool numpy.ndarray\n\n\n\n\n\n\n\n\n\nCHANGE\n\n\n(year, y, x)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(2, 9, 105), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n482.34 kiB\n14.77 kiB\n\n\nShape\n(6, 98, 105)\n(2, 9, 105)\n\n\nDask graph\n55 chunks in 37 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\nGROUND_TRUTH(y, x)int320 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0AREA_OR_POINT :AreaLAYER_TYPE :athematiclong_name :Layer_1_FillValue :3.402823466e+38array([[ 0,  0,  0, ...,  0, 20,  7],\n       [ 0,  0,  0, ...,  0,  0, 21],\n       [ 0, 22, 21, ...,  0,  0,  7],\n       ...,\n       [ 0,  0,  0, ...,  0,  0,  0],\n       [ 0,  0, 20, ...,  0,  0,  0],\n       [ 0,  0,  0, ...,  0,  0,  0]])Indexes: (3)yearPandasIndexPandasIndex(Int64Index([2018, 2019, 2020, 2021, 2022, 2023], dtype='int64', name='year'))xPandasIndexPandasIndex(Float64Index([4364933.008070561,  4365033.43163347, 4365133.855196379,\n              4365234.278759288, 4365334.702322196, 4365435.125885106,\n              4365535.549448014, 4365635.973010924, 4365736.396573832,\n              4365836.820136741,\n              ...\n              4374473.246546905, 4374573.670109814, 4374674.093672723,\n              4374774.517235631, 4374874.940798541, 4374975.364361449,\n              4375075.787924359, 4375176.211487267, 4375276.635050176,\n              4375377.058613085],\n             dtype='float64', name='x', length=105))yPandasIndexPandasIndex(Float64Index([3176652.7101289225,  3176552.995039586, 3176453.2799502495,\n              3176353.5648609134,  3176253.849771577, 3176154.1346822404,\n               3176054.419592904,  3175954.704503568, 3175854.9894142314,\n               3175755.274324895, 3175655.5592355584, 3175555.8441462223,\n               3175456.129056886, 3175356.4139675493,  3175256.698878213,\n               3175156.983788877, 3175057.2686995403,  3174957.553610204,\n              3174857.8385208673, 3174758.1234315312, 3174658.4083421947,\n              3174558.6932528582, 3174458.9781635217, 3174359.2630741857,\n               3174259.547984849, 3174159.8328955127,  3174060.117806176,\n                3173960.40271684, 3173860.6876275036,  3173760.972538167,\n              3173661.2574488306, 3173561.5423594946,  3173461.827270158,\n              3173362.1121808216,  3173262.397091485,  3173162.682002149,\n              3173062.9669128126,  3172963.251823476, 3172863.5367341395,\n              3172763.8216448035,  3172664.106555467, 3172564.3914661305,\n               3172464.676376794,  3172364.961287458, 3172265.2461981215,\n               3172165.531108785, 3172065.8160194485, 3171966.1009301124,\n               3171866.385840776, 3171766.6707514394,  3171666.955662103,\n              3171567.2405727664, 3171467.5254834304,  3171367.810394094,\n              3171268.0953047574,  3171168.380215421,  3171068.665126085,\n              3170968.9500367483,  3170869.234947412, 3170769.5198580753,\n              3170669.8047687393, 3170570.0896794028, 3170470.3745900663,\n              3170370.6595007298, 3170270.9444113937,  3170171.229322057,\n              3170071.5142327207,  3169971.799143384,  3169872.084054048,\n              3169772.3689647117,  3169672.653875375, 3169572.9387860387,\n              3169473.2236967026,  3169373.508607366, 3169273.7935180296,\n               3169174.078428693,  3169074.363339357, 3168974.6482500206,\n               3168874.933160684, 3168775.2180713476, 3168675.5029820115,\n               3168575.787892675, 3168476.0728033385,  3168376.357714002,\n               3168276.642624666, 3168176.9275353295,  3168077.212445993,\n              3167977.4973566565, 3167877.7822673204,  3167778.067177984,\n              3167678.3520886474,  3167578.636999311,  3167478.921909975,\n              3167379.2068206384,  3167279.491731302, 3167179.7766419654,\n              3167080.0615526293,  3166980.346463293],\n             dtype='float64', name='y'))Attributes: (4)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1\n\n\nThe ground truth data saves the year when deforestation was first detected for a pixel in a single raster. To do this, it encodes the year of forest loss as an integer, giving the year. So, an integer 21 means the pixel was first detected as deforested in 2021, whereas a value of 0 means that deforestation was never detected.\nCurrently our classification saves the deforestation detection in multiple rasters, one for each year. To get our data into a format that is similar to our comparison data we need to convert our rasters for each time step into a single one.\nTo do this we first assign all pixels which were detected as deforestation (CHANGE == -1) to the year in which the deforestation was detected (lost_year). Then we compute over our time-series the first occurence of deforestation (equivalent to the first non-zero value) per pixel. This is then saved in a new data variable.\n\n# convert lost forest (-1) into the year it was lost\nlost_year = (ds_s2.CHANGE == -1) * ds_s2.year % 100\nfirst_nonzero = (lost_year != 0).argmax(axis=0).compute()\nds_s2[\"LOST_YEAR\"] = lost_year[first_nonzero]\nds_s2.LOST_YEAR.plot(levels=range(25), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\nplt.title(\"Classification Forest Loss Year\");\n\n\n\n\nComparing this visually to the Global Forest Watch data, allows us to do some initial quality assessment. We can see definite differences between the two datasets. The Global Forest Watch data has much more clearly defined borders. In general, our classification seems to overestimate deforestation. However, the general pattern of forest loss is the same in both. Most of the deforestation is in the north of the study area, with less forest loss in the south.\nThere are a few reasons for those differences. The main difference has to be in our much more simple approach to forest classification and change detection. It is expected that our approach will lead to large amounts of commission errors since changes are only confirmed using a single observation. It however can also lead to a lot of omission errors since the NDVI thresholding might classify highly productive non-forest areas as forest due to their high NDVI values.\nHowever, there are also some systematic differences. Our algorithm looks at differences between the middle of the years, which means that some changes can happen at the end of the growing year which will be detected first in the next year whereas the Global Forest Watch dataset will detect it in the correct (earlier) year.\n\nds_s2.GROUND_TRUTH.plot(levels=range(25), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\nplt.title(\"Global Forest Watch - Interpolated\");\n\n\n\n\nFinally, we can also calculate an accuracy score. This is a score from 0-1, where values close to 0.5 basically mean that the classification is random, and values close to 1 mean that most of the values of our comparison data and classification data match.\nFirst, we look at the overall accuracy of forest loss over the entire period from 2018 to 2023.\n\nscore = accuracy_score(\n    (ds_s2.LOST_YEAR &gt; 18).values.ravel(), (ds_s2.GROUND_TRUTH &gt; 18).values.ravel()\n)\nprint(f\"The overall accuracy of forest loss detection is {score:.2f}.\")\n\nThe overall accuracy of forest loss detection is 0.77.\n\n\nAs expected from the visual interpretation, with an accuracy of 0.77, our product differs quite a lot compared to the Global Forest Watch data. From this we do not know for sure that our product is less accurate compared to the actual forest loss patterns observed on the ground. We only know that it is different to the Global Forest Watch product. It might be more or less accurate.\nHowever, because of the simplicity of our algorithm, it is safe to assume that our output is less accurate."
  },
  {
    "objectID": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#summary",
    "href": "notebook-samples/sentinelhub/deforestation_monitoring_with_xarray.html#summary",
    "title": "Deforestation Monitoring using Sentinel 2 and xarray",
    "section": "Summary",
    "text": "Summary\nThis notebook showed how to efficiently access data stored on the Copernicus Dataspace Ecosystem using the Sentinel Hub APIs. This includes generating cloud-free mosaics and calculating spectral indices in the cloud.\nIt also showed how to import this data using xarray and carry out a basic multi-temporal detection of forest loss.\nThis notebook should serve as a starting point for your own analysis using the powerful Python Data Analysis ecosystem and leveraging the Copernicus Data Space Ecosystem APIs for quick satellite data access."
  },
  {
    "objectID": "notebook-samples/sentinelhub/ice_monitoring.html",
    "href": "notebook-samples/sentinelhub/ice_monitoring.html",
    "title": "Classification of Ice and Open Water in Nizhnesvirsky Lower Bay using Sentinel-1 IW Product",
    "section": "",
    "text": "The Nizhnesvirsky lower bay is a vital part of a river system characterized by a dynamic freeze-thaw cycle. Understanding the extent of ice coverage and open water in this region is crucial for various applications, including navigation, ecosystem monitoring, and climate research. In this Jupyter notebook, we will leverage satellite imagery from the Sentinel-1 Interferometric Wide (IW) product, along with remote sensing principles, to classify ice and open water over a period of 8 months, from September 2022 to May 2023.\nThe Sentinel-1 satellite offers reliable and frequent radar imaging capabilities, making it ideal for monitoring ice and water bodies. The Sentinel-1 IW product provides Synthetic Aperture Radar (SAR) data, which is unaffected by weather conditions, daylight, or cloud cover. SAR data measures the backscattered electromagnetic waves, allowing us to distinguish between different surface types.\nFirst, let us import all the necessary libraries.\n\nimport datetime\nimport getpass\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nimport numpy as np\nimport geopandas as gpd\nimport json\nfrom ipyleaflet import Map, GeoJSON, basemaps\n\nfrom sentinelhub import (\n    SHConfig,\n    Geometry,\n    DataCollection,\n    MimeType,\n    SentinelHubDownloadClient,\n    SentinelHubRequest,\n    bbox_to_dimensions,\n)\n\n\nCredentials\nCredentials for Sentinel Hub services (client_id & client_secret) can be obtained in your Dashboard. In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant documentation page.\nNow that you have your client_id & client_secret, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found here. Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won’t need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook.\nIf you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:\n\n# Only run this cell if you have not created a configuration.\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n# config.save(\"cdse\")\n\nHowever, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing profile_name.\n\n# config = SHConfig(\"profile_name\")\n\n\n\nDefining the Bounds\nWe can define the bounds of the area of interest by considering a geojson file of the part of the river Svir.\n\nSHAPE_PATH = \"./data/Nizhnesvirsky_lower_bay.geojson\"\nriver_gdf = gpd.read_file(SHAPE_PATH)\n\n# Convert to EPSG 3035\nriver_gdf = river_gdf.to_crs(\"EPSG:3035\")\n# Geometry of an entire area\nresolution = 20\n\ndata = json.load(open(SHAPE_PATH, \"r\"))\n\n# Set center and zoom level\ncenter = [60.78, 33.68]\nzoom = 12\n\n# Add OSM background\nm = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=center, zoom=zoom)\n\n# Add geojson data\ngeo_json = GeoJSON(data=data)\nm.add_layer(geo_json)\n\n# Display\nm\n\nHere, we can split the entire time period into 24 slots so that we can get 2 acquisitions every month to capture the freeze-thaw dynamics adequately.\n\nstart = datetime.datetime(2022, 9, 1)\nend = datetime.datetime(2023, 4, 30)\nn_chunks = 17\ntdelta = (end - start) / n_chunks\nedges = [(start + i * tdelta).date().isoformat() for i in range(n_chunks)]\nslots = [(edges[i], edges[i + 1]) for i in range(len(edges) - 1)]\n\nprint(\"Monthly time windows:\\n\")\nfor slot in slots:\n    print(slot)\n\nMonthly time windows:\n\n('2022-09-01', '2022-09-15')\n('2022-09-15', '2022-09-29')\n('2022-09-29', '2022-10-13')\n('2022-10-13', '2022-10-27')\n('2022-10-27', '2022-11-10')\n('2022-11-10', '2022-11-25')\n('2022-11-25', '2022-12-09')\n('2022-12-09', '2022-12-23')\n('2022-12-23', '2023-01-06')\n('2023-01-06', '2023-01-20')\n('2023-01-20', '2023-02-03')\n('2023-02-03', '2023-02-18')\n('2023-02-18', '2023-03-04')\n('2023-03-04', '2023-03-18')\n('2023-03-18', '2023-04-01')\n('2023-04-01', '2023-04-15')\n\n\n\n\nRequesting Sentinel-1 (SAR) Data\nWe now obtain Sentinel-1 IW SAR images covering the Nizhnesvirsky lower bay in the above mentioned time periods. An evalscript is used to describe the processing that needs to be performed on the SAR imagery. This script defines the input and output expected in the setup() function and evaluatePixel() function defines operations applied at the pixel level.\nNote 1: The default output sample type is ‘AUTO’. In this case all the values between 0 to 1 are stretched to (0,255) and the values lying outside (0,1) are clipped.\nThe SAR imagery needs to be preprocessed before it can be used. The following pre-processing techniques must be performed to mitigate any noise and errors. - Radiometric calibration to normalize the SAR image intensities (by setting the backscatter coefficient to SIGMA0_ELLPSOID).    - Apply LEE speckle filtering techniques to reduce noise and enhance the visual quality of the images (Window size is set to 3x3).    - Apply geometric correction to ensure geometric accuracy and spatial alignment between the images (by applying COPERNICUS DEM).\nThe images are filtered according to the ascending orbit and the resolution is set to HIGH (20m x 20m)\nNote 2: Here, we need to be careful about the units. THe units of geometry with WGS84 are degrees. To get a 20m resolution, the geometry must be transformed to EPSG:3035.\n\nevalscript_sar = \"\"\"\n  function setup() {\n    return {\n      input: [\"VV\", \"dataMask\"],\n      output: { bands: 2, sampleType: \"FLOAT32\"}                  // Refer to Note 1\n    }\n  }\n\n\n// visualizes decibels from -20 to +10\nfunction toDb(linear) {\n  var log = 10 * Math.log(linear) / Math.LN10\n  return Math.max(0, (log + 20) / 30)\n}\n\nfunction evaluatePixel(sample) {\n  var VV = sample.VV;\n\n  return [toDb(VV),sample.dataMask];\n}\n\"\"\"\n\n\ndef get_sar_request(time_interval):\n    return SentinelHubRequest(\n        evalscript=evalscript_sar,\n        input_data=[\n            SentinelHubRequest.input_data(\n                data_collection=DataCollection.SENTINEL1_IW.define_from(\n                    \"s1iw\", service_url=config.sh_base_url\n                ),\n                time_interval=time_interval,\n                other_args={\n                    \"dataFilter\": {\n                        \"resolution\": \"HIGH\",\n                        \"mosaickingOrder\": \"mostRecent\",\n                        \"orbitDirection\": \"ASCENDING\",\n                    },\n                    \"processing\": {\n                        \"backCoeff\": \"SIGMA0_ELLIPSOID\",\n                        \"orthorectify\": True,\n                        \"demInstance\": \"COPERNICUS\",\n                        \"speckleFilter\": {\n                            \"type\": \"LEE\",\n                            \"windowSizeX\": 3,\n                            \"windowSizeY\": 3,\n                        },\n                    },\n                },\n            )\n        ],\n        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n        geometry=Geometry(river_gdf.geometry.values[0], crs=river_gdf.crs),\n        resolution=[20, 20],  # Refer to Note 2\n        config=config,\n        data_folder=\"./results\",\n    )\n\nWe can create a list of all the requests for each of the time slots and run them together.\n\n# create a list of requests\nlist_of_requests = [get_sar_request(slot) for slot in slots]\nlist_of_requests = [request.download_list[0] for request in list_of_requests]\n\n# download data with multiple threads\ndata = SentinelHubDownloadClient(config=config).download(\n    list_of_requests, max_threads=5\n)\n\nBelow are a series of maps that show the SAR imagery in VV mode. This means that the electromagnetic waves are transmitted and received by the Radar on board Sentinel-1 are both oriented to the vertical plane.\n\nncols = 4\nnrows = 4\naspect_ratio = 1.5\nsubplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\n\nfig, axs = plt.subplots(\n    ncols=ncols,\n    nrows=nrows,\n    figsize=(2 * ncols * aspect_ratio, 5 * nrows),\n    subplot_kw=subplot_kw,\n)\n\nfor idx, image in enumerate(data):\n    ax = axs[idx // ncols][idx % ncols]\n    ax.imshow(np.clip((image[:, :, 0]) * 3.5 / 255, 0, 1))\n    ax.set_title(f\"{slots[idx][0]}  -  {slots[idx][1]}\", fontsize=10)\n\nplt.tight_layout()\n\n\n\n\n\n\nEstimating a threshold\nIf we look at the images for all time slots, we can see that the pixels in the river become brighter as the winter months approach. This is because the water acts like a mirror and very little backscatter reaches the sensor. However, if there is a structure or disturbance in the water (in this case ice), the electromagnetic waves are scattered in all directions and the chance of detecting this backscatter is higher (Reference). It is possible that not all bright pixels are ice, which can be verified by in-situ measurements.\nTo determine what a good threshold would be to differentiate the water pixels from ice, we can plot the distribution of pixel values within the entire range (0,255).\n\nncols = 4\nnrows = 4\naspect_ratio = 15 / 10\nsubplot_kw = {\"xticks\": range(0, 255, 25), \"yticks\": [], \"frame_on\": True}\n\nfig, axs = plt.subplots(\n    ncols=ncols,\n    nrows=nrows,\n    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n    subplot_kw=subplot_kw,\n)\n\nfor idx, image in enumerate(data):\n    histogram, bin_edges = np.histogram(\n        2.5 * image[:, :, 0], bins=50, range=(0.0000001, 1)\n    )\n    ax = axs[idx // ncols][idx % ncols]\n    ax.plot(bin_edges[0:-1], histogram)\n    ax.set_xlabel(\"VV value\")\n    ax.set_ylabel(\"Number of Pixels (K)\")\n    ax.set_xlim((0, 1))\n    ax.set_ylim((0, 500))\n    ax.set_title(f\"{slots[idx][0]}  -  {slots[idx][1]}\", fontsize=10)\n\nplt.tight_layout()\n\n\n\n\nConsidering the historic weather information of the region and interpreting the SAR imagery, we can see that the appearance of ice in the river Svir started at the end of November and continued on till mid-March. By looking at the distribution plots above, we can set a threshold at a pixel value 50.\nThe evalscript below creates a visualisation of the ice mask directly after processing the SAR data and checking for the threshold. If the pixel value is more than 50/255, then it is classified at an icy pixel and the iceMask = 1.\n\nevalscript_mask = \"\"\"\n  function setup() {\n    return {\n      input: [\"VV\", \"dataMask\"],\n      output: { bands: 5 , sampleType: \"UINT8\"}\n    }\n  }\n\n// visualizes decibels from -20 to +10\nfunction toDb(linear) {\n  var log = 10 * Math.log(linear) / Math.LN10\n  return Math.max(0, (log + 20) / 30)\n}\n\nfunction evaluatePixel(sample) {\n  var VV = sample.VV;\n  var iceMask = 0;\n  VVdB = toDb(VV);\n  if (VVdB &gt;-0.001 && VVdB &lt; 0.2) {\n    iceMask = 0;\n    return [0, 0, 255, sample.dataMask, iceMask]; // Water mask\n  } else if (VVdB &gt; 0.2 && VVdB &lt;1) {\n    iceMask = 1;\n    return [0, 255, 255,sample.dataMask, iceMask];\n  } else {\n  iceMask = 0;\n  return [0,0,0,sample.dataMask,iceMask];\n  }\n}\n\"\"\"\n\n\ndef get_ice_mask_request(time_interval):\n    return SentinelHubRequest(\n        evalscript=evalscript_mask,\n        input_data=[\n            SentinelHubRequest.input_data(\n                data_collection=DataCollection.SENTINEL1_IW.define_from(\n                    \"s1iw\", service_url=config.sh_base_url\n                ),\n                time_interval=time_interval,\n                other_args={\n                    \"dataFilter\": {\n                        \"resolution\": \"HIGH\",\n                        \"mosaickingOrder\": \"mostRecent\",\n                        \"orbitDirection\": \"ASCENDING\",\n                    },\n                    \"processing\": {\n                        \"backCoeff\": \"SIGMA0_ELLIPSOID\",\n                        \"orthorectify\": True,\n                        \"demInstance\": \"COPERNICUS\",\n                        \"speckleFilter\": {\n                            \"type\": \"LEE\",\n                            \"windowSizeX\": 3,\n                            \"windowSizeY\": 3,\n                        },\n                    },\n                },\n            )\n        ],\n        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n        geometry=Geometry(river_gdf.geometry.values[0], crs=river_gdf.crs),\n        resolution=[20, 20],\n        config=config,\n        data_folder=\"./results_mask\",\n    )\n\n\n# create a list of requests\nlist_of_requests = [get_ice_mask_request(slot) for slot in slots]\nlist_of_requests = [request.download_list[0] for request in list_of_requests]\n\n# download data with multiple threads\nmask_data = SentinelHubDownloadClient(config=config).download(\n    list_of_requests, max_threads=5\n)\n\nThe mask_data maps can be plotted as below to visualize the ice pixels and open water pixels.\n\nncols = 4\nnrows = 4\naspect_ratio = 1131 / 1819\nsubplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\nlegend_elements = [\n    Patch(facecolor=\"cyan\", edgecolor=\"c\", label=\"Ice\"),\n    Patch(facecolor=\"blue\", edgecolor=\"b\", label=\"Water\"),\n]\nfig, axs = plt.subplots(\n    ncols=ncols,\n    nrows=nrows,\n    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n    subplot_kw=subplot_kw,\n)\n\nfor idx, image in enumerate(mask_data):\n    ax = axs[idx // ncols][idx % ncols]\n    ax.imshow(np.clip((image[:, :, :3]), 0, 255))\n    ax.set_title(f\"{slots[idx][0]}  -  {slots[idx][1]}\", fontsize=10)\n    ax.legend(handles=legend_elements, loc=\"lower right\")\n\nplt.tight_layout()\n\n\n\n\n\n\nCreating a Timeseries of the area covered by ice\nNext, we can claculate the area covered by ice by considering the number of pixels classified as ice and multiplying with the initial resolution of the downloaded image, which is 20m x 20m in our case. This is done by counting the number of pixels that have the value 1 in the iceMask band.\n\ndef count_ice_pixels(image):\n    ice_mask = image[:, :, 4]\n\n    # Count the number of blue pixels\n    ice_pixel_count = np.sum(ice_mask)\n\n    # print(ice_pixel_count)\n\n    return ice_pixel_count\n\n\nresolution_s1 = 20 * 20  # meters\narea_covered_ice = []\nfor idx, image in enumerate(mask_data):\n    # Count the number of icy pixels\n    ice_pixels_count = count_ice_pixels(image)\n    area_covered_ice.append(ice_pixels_count * resolution_s1 / 1000000)\n\nThe calculated area can be plotted over time to determine the months with peak ice cover over the river.\n\nxlabels = [\n    \"September\",\n    \"October\",\n    \"November\",\n    \"December\",\n    \"January\",\n    \"February\",\n    \"March\",\n    \"April\",\n]\nx = range(len(slots))\nplt.plot(range(len(slots)), area_covered_ice)\nplt.title(\"Time Series of area covered by ice\")\nplt.xticks(np.arange(0, 16, step=2), xlabels, rotation=30, ha=\"center\")\nplt.xlabel(\"Time slots\")\nplt.ylabel(\"Area covered by ice (in $km^2$)\")\nplt.show()\n\n\n\n\nThis confirms the hypothesis of the ice formation beginning in mid-to-end of November and the amount of ice cover increasing as the winter progressed. We can also notice that once the ice starts breaking up in mid March, the ice clears out very quickly.\n\n\nSummary\n\nThe classification results will provide temporal information about the freeze-thaw cycle in the Nizhnesvirsky lower bay.\nBy analyzing the classified maps, we can observe the progression of ice formation, ice breakup, and the duration of open water periods.\nQuantitative analysis of ice coverage and open water duration can be derived from the classified maps, aiding in the assessment of seasonal changes and long-term trends."
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html",
    "title": "Sentinel Hub Process API",
    "section": "",
    "text": "In this example notebook we show how to use Sentinel Hub Process API to download satellite imagery. We describe how to use various parameters and configurations to obtain either processed products or raw band data. For more information about the service please check the official service documentation."
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-1-true-color-png-on-a-specific-date",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-1-true-color-png-on-a-specific-date",
    "title": "Sentinel Hub Process API",
    "section": "Example 1: True color (PNG) on a specific date",
    "text": "Example 1: True color (PNG) on a specific date\nWe build the request according to the API Reference, using the SentinelHubRequest class. Each Process API request also needs an evalscript.\nThe information that we specify in the SentinelHubRequest object is:\n\nan evalscript,\na list of input data collections with time interval,\na format of the response,\na bounding box and it’s size (size or resolution).\n\nThe evalscript in the example is used to select the appropriate bands. We return the RGB (B04, B03, B02) Sentinel-2 L1C bands.\nThe image from Jun 12th 2020 is downloaded. Without any additional parameters in the evalscript, the downloaded data will correspond to reflectance values in UINT8 format (values in 0-255 range).\n\nevalscript_true_color = \"\"\"\n    //VERSION=3\n\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B02\", \"B03\", \"B04\"]\n            }],\n            output: {\n                bands: 3\n            }\n        };\n    }\n\n    function evaluatePixel(sample) {\n        return [sample.B04, sample.B03, sample.B02];\n    }\n\"\"\"\n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L1C.define_from(\n                \"s2l1c\", service_url=config.sh_base_url\n            ),\n            time_interval=(\"2020-06-12\", \"2020-06-13\"),\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=betsiboka_bbox,\n    size=betsiboka_size,\n    config=config,\n)\n\n\ntrue_color_imgs = request_true_color.get_data()\n\nThe method get_data() will always return a list of length 1 with the available image from the requested time interval in the form of numpy arrays.\n\nprint(\n    f\"Returned data is of type = {type(true_color_imgs)} and length {len(true_color_imgs)}.\"\n)\nprint(\n    f\"Single element in the list is of type {type(true_color_imgs[-1])} and has shape {true_color_imgs[-1].shape}\"\n)\n\n\nimage = true_color_imgs[0]\nprint(f\"Image type: {image.dtype}\")\n\n# plot function\n# factor 1/255 to scale between 0-1\n# factor 3.5 to increase brightness\nplot_image(image, factor=3.5 / 255, clip_range=(0, 1))"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-2-true-color-mosaic-of-least-cloudy-acquisitions",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-2-true-color-mosaic-of-least-cloudy-acquisitions",
    "title": "Sentinel Hub Process API",
    "section": "Example 2: True color mosaic of least cloudy acquisitions",
    "text": "Example 2: True color mosaic of least cloudy acquisitions\nThe SentinelHubRequest automatically creates a mosaic from all available images in the given time interval. By default, the mostRecent mosaicking order is used. More information available here.\nIn this example we will provide a month long interval, order the images w.r.t. the cloud coverage on the tile level (leastCC parameter), and mosaic them in the specified order.\n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L1C.define_from(\n                \"s2l1c\", service_url=config.sh_base_url\n            ),\n            time_interval=(\"2020-06-01\", \"2020-06-30\"),\n            mosaicking_order=MosaickingOrder.LEAST_CC,\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=betsiboka_bbox,\n    size=betsiboka_size,\n    config=config,\n)\n\n\nplot_image(request_true_color.get_data()[0], factor=3.5 / 255, clip_range=(0, 1))"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-3-all-sentinel-2s-raw-band-values",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-3-all-sentinel-2s-raw-band-values",
    "title": "Sentinel Hub Process API",
    "section": "Example 3: All Sentinel-2’s raw band values",
    "text": "Example 3: All Sentinel-2’s raw band values\nNow let’s define an evalscript which will return all Sentinel-2 spectral bands with raw values.\nIn this example we are downloading already quite a big chunk of data, so optimization of the request is not out of the question. Downloading raw digital numbers in the INT16 format instead of reflectances in the FLOAT32 format means that much less data is downloaded, which results in a faster download and a smaller usage of SH processing units.\nIn order to achieve this, we have to set the input units in the evalscript to DN (digital numbers) and the output sampleType argument to INT16. Additionally, we can’t pack all Sentinel-2’s 13 bands into a PNG image, so we have to set the output image type to the TIFF format via MimeType.TIFF in the request.\nThe digital numbers are in the range from 0-10000, so we have to scale the downloaded data appropriately.\n\nevalscript_all_bands = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B10\",\"B11\",\"B12\"],\n                units: \"DN\"\n            }],\n            output: {\n                bands: 13,\n                sampleType: \"INT16\"\n            }\n        };\n    }\n\n    function evaluatePixel(sample) {\n        return [sample.B01,\n                sample.B02,\n                sample.B03,\n                sample.B04,\n                sample.B05,\n                sample.B06,\n                sample.B07,\n                sample.B08,\n                sample.B8A,\n                sample.B09,\n                sample.B10,\n                sample.B11,\n                sample.B12];\n    }\n\"\"\"\n\nrequest_all_bands = SentinelHubRequest(\n    evalscript=evalscript_all_bands,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L1C.define_from(\n                \"s2l1c\", service_url=config.sh_base_url\n            ),\n            time_interval=(\"2020-06-01\", \"2020-06-30\"),\n            mosaicking_order=MosaickingOrder.LEAST_CC,\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=betsiboka_bbox,\n    size=betsiboka_size,\n    config=config,\n)\n\n\nall_bands_response = request_all_bands.get_data()\n\n\n# Image showing the SWIR band B12\n# Factor 1/1e4 due to the DN band values in the range 0-10000\n# Factor 3.5 to increase the brightness\nplot_image(all_bands_response[0][:, :, 12], factor=3.5 / 1e4, vmax=1)\n\n\n# From raw bands we can also construct a False-Color image\n# False color image is (B03, B04, B08)\nplot_image(all_bands_response[0][:, :, [2, 3, 7]], factor=3.5 / 1e4, clip_range=(0, 1))"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-4-save-downloaded-data-to-disk-and-read-it-from-disk",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-4-save-downloaded-data-to-disk-and-read-it-from-disk",
    "title": "Sentinel Hub Process API",
    "section": "Example 4: Save downloaded data to disk and read it from disk",
    "text": "Example 4: Save downloaded data to disk and read it from disk\nAll downloaded data can be saved to disk and later read from it. Simply specify the location on disk where data should be saved (or loaded from) via the data_folder argument of the request’s constructor. When executing the request’s get_data method, set the argument save_data to True.\nThis also means that in all the future requests for data, the request will first check the provided location if the data is already there, unless you explicitly demand to redownload the data.\n\nrequest_all_bands = SentinelHubRequest(\n    data_folder=\"test_dir\",\n    evalscript=evalscript_all_bands,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L1C.define_from(\n                \"s2l1c\", service_url=config.sh_base_url\n            ),\n            time_interval=(\"2020-06-01\", \"2020-06-30\"),\n            mosaicking_order=MosaickingOrder.LEAST_CC,\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=betsiboka_bbox,\n    size=betsiboka_size,\n    config=config,\n)\n\n\n%%time\nall_bands_img = request_all_bands.get_data(save_data=True)\n\n\nprint(\n    \"The output directory has been created and a tiff file with all 13 bands was saved into the following structure:\\n\"\n)\n\nfor folder, _, filenames in os.walk(request_all_bands.data_folder):\n    for filename in filenames:\n        print(os.path.join(folder, filename))\n\n\n%%time\n# try to re-download the data\nall_bands_img_from_disk = request_all_bands.get_data()\n\n\n%%time\n# force the redownload\nall_bands_img_redownload = request_all_bands.get_data(redownload=True)\n\n\nExample 4.1: Save downloaded data directly to disk\nThe get_data method returns a list of numpy arrays and can save the downloaded data to disk, as we have seen in the previous example. Sometimes it is convenient to just save the data directly to disk. You can do that by using save_data method instead.\n\n%%time\nrequest_all_bands.save_data()\n\n\nprint(\n    \"The output directory has been created and a tiff file with all 13 bands was saved into the following structure:\\n\"\n)\n\nfor folder, _, filenames in os.walk(request_all_bands.data_folder):\n    for filename in filenames:\n        print(os.path.join(folder, filename))"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-5-other-data-collections",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-5-other-data-collections",
    "title": "Sentinel Hub Process API",
    "section": "Example 5: Other Data Collections",
    "text": "Example 5: Other Data Collections\nThe sentinelhub-py package supports various data collections. The example below is shown for one of them, but the process is the same for all of them.\n\nNote:\nFor more examples and information check the documentation about Sentinel Hub data collections.\n\n\nprint(\"Supported DataCollections:\\n\")\nfor collection in DataCollection.get_available_collections():\n    print(collection)\n\nFor this example let’s download the digital elevation model data (DEM). The process is similar as before, we just provide the evalscript and create the request. More data on the DEM data collection is available here. DEM values are in meters and can be negative for areas which lie below sea level, so it is recommended to set the output format in your evalscript to FLOAT32.\n\nevalscript_dem = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\"DEM\"],\n    output:{\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.FLOAT32\n    }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [sample.DEM]\n}\n\"\"\"\n\n\ndem_request = SentinelHubRequest(\n    evalscript=evalscript_dem,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.DEM.define_from(\n                \"dem\", service_url=config.sh_base_url\n            ),\n            time_interval=(\"2020-06-12\", \"2020-06-13\"),\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=betsiboka_bbox,\n    size=betsiboka_size,\n    config=config,\n)\n\n\ndem_data = dem_request.get_data()\n\n\n# Plot DEM map\n# vmin = 0; cutoff at sea level (0 m)\n# vmax = 120; cutoff at high values (120 m)\nplot_image(dem_data[0], factor=1.0, cmap=plt.cm.Greys_r, vmin=0, vmax=120)"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-6-multi-response-request-type",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-6-multi-response-request-type",
    "title": "Sentinel Hub Process API",
    "section": "Example 6 : Multi-response request type",
    "text": "Example 6 : Multi-response request type\nProcess API enables downloading multiple files in one response, packed together in a TAR archive.\nWe will get the same image as before, download in the form of digital numbers (DN) as a UINT16 TIFF file. Along with the image we will download the inputMetadata which contains the normalization factor value in a JSON format.\nAfter the download we will be able to convert the INT16 digital numbers to get the FLOAT32 reflectances.\n\nevalscript = \"\"\"\n    //VERSION=3\n\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B02\", \"B03\", \"B04\"],\n                units: \"DN\"\n            }],\n            output: {\n                bands: 3,\n                sampleType: \"INT16\"\n            }\n        };\n    }\n\n    function updateOutputMetadata(scenes, inputMetadata, outputMetadata) {\n        outputMetadata.userData = { \"norm_factor\":  inputMetadata.normalizationFactor }\n    }\n\n    function evaluatePixel(sample) {\n        return [sample.B04, sample.B03, sample.B02];\n    }\n\"\"\"\n\nrequest_multitype = SentinelHubRequest(\n    evalscript=evalscript,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L1C.define_from(\n                \"s2l1c\", service_url=config.sh_base_url\n            ),\n            time_interval=(\"2020-06-01\", \"2020-06-30\"),\n            mosaicking_order=MosaickingOrder.LEAST_CC,\n        )\n    ],\n    responses=[\n        SentinelHubRequest.output_response(\"default\", MimeType.TIFF),\n        SentinelHubRequest.output_response(\"userdata\", MimeType.JSON),\n    ],\n    bbox=betsiboka_bbox,\n    size=betsiboka_size,\n    config=config,\n)\n\n\n# print out information\nmulti_data = request_multitype.get_data()[0]\nmulti_data.keys()\n\n\n# normalize image\nimg = multi_data[\"default.tif\"]\nnorm_factor = multi_data[\"userdata.json\"][\"norm_factor\"]\n\nimg_float32 = img * norm_factor\n\n\nplot_image(img_float32, factor=3.5, clip_range=(0, 1))"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-7-raw-dictionary-request",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-7-raw-dictionary-request",
    "title": "Sentinel Hub Process API",
    "section": "Example 7 : Raw dictionary request",
    "text": "Example 7 : Raw dictionary request\nAll requests so far were built with some helper functions. We can also construct a raw dictionary as defined in the API Reference, without these helper functions, so we have full control over building the request body.\n\nrequest_raw_dict = {\n    \"input\": {\n        \"bounds\": {\n            \"properties\": {\"crs\": betsiboka_bbox.crs.opengis_string},\n            \"bbox\": list(betsiboka_bbox),\n        },\n        \"data\": [\n            {\n                \"type\": \"S2L1C\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": \"2020-06-01T00:00:00Z\",\n                        \"to\": \"2020-06-30T00:00:00Z\",\n                    },\n                    \"mosaickingOrder\": \"leastCC\",\n                },\n            }\n        ],\n    },\n    \"output\": {\n        \"width\": betsiboka_size[0],\n        \"height\": betsiboka_size[1],\n        \"responses\": [\n            {\"identifier\": \"default\", \"format\": {\"type\": MimeType.TIFF.get_string()}}\n        ],\n    },\n    \"evalscript\": evalscript_true_color,\n}\n\n\n# create request\ndownload_request = DownloadRequest(\n    request_type=\"POST\",\n    url=\"https://sh.dataspace.copernicus.eu/api/v1/process\",\n    post_values=request_raw_dict,\n    data_type=MimeType.TIFF,\n    headers={\"content-type\": \"application/json\"},\n    use_session=True,\n)\n\n# execute request\nclient = SentinelHubDownloadClient(config=config)\nimg = client.download(download_request)\n\n\nplot_image(img, factor=3.5 / 255, clip_range=(0, 1))"
  },
  {
    "objectID": "notebook-samples/sentinelhub/data_download_process_request.html#example-8-multiple-timestamps-data",
    "href": "notebook-samples/sentinelhub/data_download_process_request.html#example-8-multiple-timestamps-data",
    "title": "Sentinel Hub Process API",
    "section": "Example 8 : Multiple timestamps data",
    "text": "Example 8 : Multiple timestamps data\nIt is possible to construct some logic in order to return data for multiple timestamps. By defining the time_interval parameter and some logic of splitting it, it is possible to create an SH reques per each “time slot” and then download the data from all the requests with the SentinelHubDownloadClient in sentinelhub-py. In this example we will create least cloudy monthly images for the year 2019.\nHowever, this is already a functionality built on top of this SH API package. We have extended the support for such usage in our package eo-learn. We recommend to use eo-learn for more complex cases where you need multiple timestamps or high-resolution data for larger areas.\n\nstart = datetime.datetime(2019, 1, 1)\nend = datetime.datetime(2019, 12, 31)\nn_chunks = 13\ntdelta = (end - start) / n_chunks\nedges = [(start + i * tdelta).date().isoformat() for i in range(n_chunks)]\nslots = [(edges[i], edges[i + 1]) for i in range(len(edges) - 1)]\n\nprint(\"Monthly time windows:\\n\")\nfor slot in slots:\n    print(slot)\n\n\ndef get_true_color_request(time_interval):\n    return SentinelHubRequest(\n        evalscript=evalscript_true_color,\n        input_data=[\n            SentinelHubRequest.input_data(\n                data_collection=DataCollection.SENTINEL2_L1C.define_from(\n                    \"s2l1c\", service_url=config.sh_base_url\n                ),\n                time_interval=time_interval,\n                mosaicking_order=MosaickingOrder.LEAST_CC,\n            )\n        ],\n        responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n        bbox=betsiboka_bbox,\n        size=betsiboka_size,\n        config=config,\n    )\n\n\n# create a list of requests\nlist_of_requests = [get_true_color_request(slot) for slot in slots]\nlist_of_requests = [request.download_list[0] for request in list_of_requests]\n\n# download data with multiple threads\ndata = SentinelHubDownloadClient(config=config).download(\n    list_of_requests, max_threads=5\n)\n\n\n# some stuff for pretty plots\nncols = 4\nnrows = 3\naspect_ratio = betsiboka_size[0] / betsiboka_size[1]\nsubplot_kw = {\"xticks\": [], \"yticks\": [], \"frame_on\": False}\n\nfig, axs = plt.subplots(\n    ncols=ncols,\n    nrows=nrows,\n    figsize=(5 * ncols * aspect_ratio, 5 * nrows),\n    subplot_kw=subplot_kw,\n)\n\nfor idx, image in enumerate(data):\n    ax = axs[idx // ncols][idx % ncols]\n    ax.imshow(np.clip(image * 2.5 / 255, 0, 1))\n    ax.set_title(f\"{slots[idx][0]}  -  {slots[idx][1]}\", fontsize=10)\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebook-samples/sentinelhub/air_pollution_statistics.html",
    "href": "notebook-samples/sentinelhub/air_pollution_statistics.html",
    "title": "Comparing statistics of NO2 pollution for European cities",
    "section": "",
    "text": "from pathlib import Path\nimport getpass\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport rasterio\nimport rasterio.plot\nfrom rasterio import features\n\nfrom sentinelhub import (\n    SHConfig,\n    CRS,\n    BBox,\n    DataCollection,\n    DownloadRequest,\n    MimeType,\n    MosaickingOrder,\n    SentinelHubDownloadClient,\n    SentinelHubStatisticalDownloadClient,\n    SentinelHubRequest,\n    bbox_to_dimensions,\n    SentinelHubStatistical,\n    Geometry,\n    parse_time,\n)"
  },
  {
    "objectID": "notebook-samples/sentinelhub/air_pollution_statistics.html#outline",
    "href": "notebook-samples/sentinelhub/air_pollution_statistics.html#outline",
    "title": "Comparing statistics of NO2 pollution for European cities",
    "section": "Outline",
    "text": "Outline\nThis notebook analyses air pollution in Europe using the TROPOMI sensor on the Sentinel 5P satellite. This notebook aims to provide data to answer the following questions:\n\nWhat is the spatial distribution of NO2 concentration in Europe\nHow does the NO2 concentration vary over a year\nWhich European capitals are most affected by NO2 emissions"
  },
  {
    "objectID": "notebook-samples/sentinelhub/air_pollution_statistics.html#used-tools-and-features",
    "href": "notebook-samples/sentinelhub/air_pollution_statistics.html#used-tools-and-features",
    "title": "Comparing statistics of NO2 pollution for European cities",
    "section": "Used tools and features",
    "text": "Used tools and features\nTo carry out these analyses we will cover a few different concepts and features available on the Copernicus Dataspace Ecosystem:\n\nDownloading of Raw data using custom resolutions and bounding boxes\nCalculation of monthly mosaics on the fly in the cloud\nDirect access to timeseries data for geometries through the statistical API\n\n\nCredentials\nCredentials for Sentinel Hub services (client_id & client_secret) can be obtained in your Dashboard. In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant documentation page.\nNow that you have your client_id & client_secret, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found here. Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won’t need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook.\nIf you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:\n\n# Only run this cell if you have not created a configuration.\n\nconfig = SHConfig()\n# config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n# config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\nconfig.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nconfig.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n# config.save(\"cdse\")\n\nHowever, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing profile_name.\n\n# config = SHConfig(\"profile_name\")\n\n\n\nAnalysing Spatial Distribution\nLet’s first get an overview of our study area, which is most of mainland Europe. To get this overview we first define an evalscript. An evalscript is a piece of javascript code which specifies how each pixel should be handled. For the first one we just define the input band that we want to look at, which is NO2 and return that band immediately, without carrying out any more calculations before the data is returned to us.\nFor more information on evalscripts have a look at the documentation.\n//VERSION=3\nfunction setup() {\n   return {\n    input: [\"NO2\"], // This specifies the bands that are looked at\n    output: { \n      bands: 1,\n      // This specifies in which data type the values will be returned\n      sampleType: \"FLOAT32\"\n    },\n    // Will make a simple mosaic, taking the most recent tiles to fill the bounding box\n    mosaicking: \"SIMPLE\"\n  };\n}\n\nfunction evaluatePixel(samples) {\n    // Here we could do more calculations which are applied to each pixel, \n    // but for now let's just return the value \n   return [samples.NO2] \n}\n\n# We also need to define the evalscript as a Python variable\nevalscript_raw = \"\"\"\n//VERSION=3\nfunction setup() {\n   return {\n    input: [\"NO2\"], // This specifies the bands that are looked at\n    output: { \n      bands: 1,\n      // This specifies in which data type the values will be returned\n      sampleType: \"FLOAT32\"\n    },\n    // Will make a simple mosaic, taking the most recent tiles to fill the bounding box\n    mosaicking: \"SIMPLE\"\n  };\n}\n\nfunction evaluatePixel(samples) {\n    // Here we could do more calculations which are applied to each pixel, \n    // but for now let's just return the value \n   return [samples.NO2] \n}\n\"\"\"\n\nWith the evalscript we can now make a request for data.\nThe request will take care of a lot of things for us. It will return our the data in our specified resolution and bounding box, for our specified time range and it will automatically mosaic multiple tiles together to fill the entire bounding box.\n\nbbox_europe = BBox([-12.30, 34.59, 32.52, 63.15], crs=CRS.WGS84).transform(CRS(3857))\n# This is defining the data we will use.\n# You can list all available data collections with `DataCollection.get_available_collections()`.\ndata_5p = DataCollection.SENTINEL5P.define_from(\"5p\", service_url=config.sh_base_url)\n\nrequest_raw = SentinelHubRequest(\n    evalscript=evalscript_raw,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=data_5p,\n            time_interval=(\"2023-01-01\", \"2023-05-26\"),\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=bbox_europe,\n    # Resolution is defined in units of the bbox crs! Be careful with WGS84 since this will be in degrees!\n    # Since we have defined our bounding box in Web mercator the resolution is in meters.\n    resolution=(5500, 3500),\n    config=config,\n    data_folder=\"./data\",  # We save the data in a specified folder\n)\n\nAfter we’ve defined the request, we can get the data:\n\nraw_data = request_raw.get_data(save_data=True)\n\nNow we define a function which plots the data of the request together with the borders of the European countries, taken from the natural earth dataset.\n\ncountries = (\n    gpd.read_file(\"./data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\")\n    .to_crs(3857)\n    .cx[bbox_europe.min_x : bbox_europe.max_x, bbox_europe.min_y : bbox_europe.max_y]\n    .reset_index(drop=True)\n)\ncountries = countries[[\"ADMIN\", \"geometry\"]]\n\n\ndef plot_request(request, bbox):\n    image_path = Path(request.data_folder) / request.get_filename_list()[0]\n    with rasterio.open(image_path) as raster:\n        fig, ax = plt.subplots(figsize=(10, 10))\n        ax.set_xlim([bbox.min_x, bbox.max_x])\n        ax.set_ylim([bbox.min_y, bbox.max_y])\n        rasterio.plot.show(raster, ax=ax)\n        countries.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")\n\n\nplot_request(request_raw, bbox_europe)\n\n\n\n\nWe can see here that even though the data is already mosaiced together, filling the entire bounding box, we do not have data everywhere since Tropomi does have a bunch of missing data per acquisition depending on atmospheric conditions.\nWith this image we can already see some patterns but let’s try to get a more representative image and take the mean of the NO2 values over an entire month to get a more complete picture.\nTo do this we do not have to download all of the data for an entire month, instead we can extend our evalscript so that the mean value for a month is calculated in the cloud for us. Doing it like this saves us a bunch of time downloading all images. So let’s have a look at the updated evalscript:\nThe most important thing that changed is that we now changed the mosaicking input to ORBIT. This gives us all acquisitions for a time series to calculate values from. In the input we also have added dataMask as a band. This will tell us, if the NO2 band has data or not. We are using this to remove acquisitions without data from our calculation.\nIn our evaluatePixel function we have added two more steps. The first one is to filter out all acquisitions which do not have data with the isClear() function. After we have filtered the time series we can calculate the mean of all values using the sum() function and the length of the clear timeseries.\nIn the end we return the mean value we have calculated.\n//VERSION=3\nfunction setup() {\n    return {\n        input: [\"NO2\", \"dataMask\"],\n        output: {\n            bands: 1,\n            sampleType: \"FLOAT32\",\n        },\n        mosaicking: \"ORBIT\"\n    };\n}\n\nfunction isClear(sample) {\n    return sample.dataMask == 1;\n}\n\nfunction sum(array) {\n    let sum = 0;\n    for (let i = 0; i &lt; array.length; i++) {\n        sum += array[i].NO2;\n    }\n    return sum;\n}\n\nfunction evaluatePixel(samples) {\n    const clearTs = samples.filter(isClear)\n    const mean = sum(clearTs) / clearTs.length\n    return [mean]\n}\n\nevalscript_mean_mosaic = \"\"\"\n//VERSION=3\nfunction setup() {\n    return {\n        input: [\"NO2\", \"dataMask\"],\n        output: {\n            bands: 1,\n            sampleType: \"FLOAT32\",\n        },\n        mosaicking: \"ORBIT\"\n    };\n}\n\nfunction isClear(sample) {\n    return sample.dataMask == 1;\n}\n\nfunction sum(array) {\n    let sum = 0;\n    for (let i = 0; i &lt; array.length; i++) {\n        sum += array[i].NO2;\n    }\n    return sum;\n}\n\nfunction evaluatePixel(samples) {\n    const clearTs = samples.filter(isClear)\n    const mean = sum(clearTs) / clearTs.length\n    return [mean]\n}\n\"\"\"\n\n\nrequest_monthly = SentinelHubRequest(\n    evalscript=evalscript_mean_mosaic,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=data_5p,\n            time_interval=(\"2022-12-01\", \"2023-01-01\"),\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n    bbox=bbox_europe,\n    resolution=(5000, 3500),\n    config=config,\n    data_folder=\"./data\",\n)\n\nWe now made a request for an entire month of data, for December of 2022. Other than that nothing much changed in the request.\n\nmean_data = request_monthly.get_data(save_data=True)\n\n\nplot_request(request_monthly, bbox_europe)\n\n\n\n\nThis looks much better, since it is Winter the northern latitudes and the alps still do not have much data due to snow cover and low sun angles, however most of the rest of Europe is now covered by data.\nWe can clearly see NO2 hot spots around developed areas, like the Po Valley in Italy and the Ruhr area in Germany. You can also clearly make out the effect of some cities, like Istanbul and Madrid.\nIn general the distribution of values is around the lower end though. Only few pixels have much higher values.\n\nplt.stairs(*np.histogram(mean_data[0], range=(0, 0.00015), bins=25), fill=True);\n\n\n\n\n\n\nAnalysing European countries\nNow let’s look at the distribution of values per country to see which countries had the highest average NO2 values in the month.\nTo do this we rasterize all countries in our area of interest. We do this so that we can select all array values which are covered by a certain country.\n\ncountries[\"ID\"] = countries.index\n\nimage_path = Path(request_monthly.data_folder) / request_monthly.get_filename_list()[0]\nwith rasterio.open(image_path) as src:\n    affine = src.transform\n# convert gpd Dataframe to format accepted by rasterize\ngeo_iter = list(countries[[\"geometry\", \"ID\"]].itertuples(index=False, name=None))\n# This call is converting the array into a raster with the same size as our NO2 raster\ncountry_array = features.rasterize(\n    geo_iter, transform=affine, out_shape=mean_data[0].shape, fill=-1\n)\n\nNow we define two helper functions which get all NO2 values in a country and another function which calcuates the mean of those values.\n\ndef get_array(country_id):\n    return mean_data[0][country_array == country_id]\n\n\ndef get_mean(country_id):\n    return np.nanmean(get_array(country_id))\n\nThis function is then applied to the countries dataframe, to fill a new column mean which holds the mean NO2 values per country.\n\ncountries[\"mean\"] = countries.apply(lambda x: get_mean(x[\"ID\"]), axis=1)\n\n/tmp/ipykernel_5069/2776634825.py:5: RuntimeWarning: Mean of empty slice\n  return np.nanmean(get_array(country_id))\n\n\nWe can then sort by that mean value and have a look at the countries with the highest mean.\n\nsorted_df = countries.sort_values(\"mean\", ascending=False)\nsorted_df.head(10)\n\n\n\n\n\n\n\n\nADMIN\ngeometry\nID\nmean\n\n\n\n\n20\nNetherlands\nMULTIPOLYGON (((667242.940 6577267.510, 655945...\n20\n0.000108\n\n\n48\nBelgium\nPOLYGON ((470455.301 6689943.442, 479173.878 6...\n48\n0.000099\n\n\n40\nEstonia\nMULTIPOLYGON (((3044805.494 7868837.787, 30419...\n40\n0.000077\n\n\n36\nGermany\nMULTIPOLYGON (((1060209.439 6028063.653, 10408...\n36\n0.000065\n\n\n27\nLuxembourg\nPOLYGON ((680886.100 6467256.661, 679972.933 6...\n27\n0.000063\n\n\n32\nItaly\nMULTIPOLYGON (((781584.581 5768463.580, 785443...\n32\n0.000054\n\n\n14\nSan Marino\nPOLYGON ((1389852.107 5450198.738, 1383296.868...\n14\n0.000049\n\n\n30\nLatvia\nPOLYGON ((2960380.966 7492512.846, 2954738.894...\n30\n0.000049\n\n\n1\nJersey\nPOLYGON ((-224715.351 6314191.162, -223742.393...\n1\n0.000047\n\n\n24\nMonaco\nPOLYGON ((828069.165 5426902.987, 821285.634 5...\n24\n0.000047\n\n\n\n\n\n\n\nFor the 5 countries with the highest mean we are then plotting a boxplot of NO2 values.\n\n# get the country ids with the 5 highest mean values\nn_countries = 5\ncountry_ids = list(sorted_df[\"ID\"][:n_countries])\ncountry_names = list(sorted_df[\"ADMIN\"][:n_countries])\n\n\nax = sns.boxplot(data=[get_array(country_id) for country_id in country_ids])\nax.set_xticklabels(country_names);\n\n\n\n\nFrom this we can see that even though the mean of values in Germany is the 4th lowest, of the 5 countries it has the absolute highest values. We can also see that Belgium and Germany both have qutie the large variance in NO2 values, with some areas of low NO2 concentration and some areas quite high concentrations.\n\n\nAnalysing EU Capitals\nNow we want to take a look at EU capitals specifically. For this more focused analysis we want to analyse time series data. To do this we are taking advantage of another API capability, the Statistical API.\nEven for the previous analysis of European countries, if we were not interested at all in the spatial distribution of data and only interested in statistics for certain geometries, the statistical API would have been the perfect fit. It removes the need to download a lot of data to calculate statistics for areas. Instead it does all of the calculation of statistics like mean, max, min and standard deviation in the cloud and in the end only sends those values.\nAnother capability of the API is the easy chunking in regular intervals, which we will be using to make the time series.\nBut let’s first import the EU capitals:\n\n# load capitals\ncapitals = gpd.read_file(\"./data/eu_capitals.geojson\")\n\nThe evalscript for the Statistical API is quite similar to the evalscript for the Processing API. However for the statistical API to work we need to add one more output named dataMask. This provides a binary mask for the API, telling it which pixels should be included in the statistical request.\nOther than that not much changed compared to our previous request.\n\nevalscript_stat = \"\"\"\n//VERSION=3\nfunction setup() {\n    return {\n        input: [\"NO2\", \"dataMask\"],\n        output: [{ \n          id: \"default\",\n          bands: [\"NO2\"],\n          sampleType: \"FLOAT32\" \n        },\n        { \n          id: \"dataMask\",\n          bands: 1,\n        }],\n        mosaicking: \"ORBIT\"\n    };\n}\n\nfunction isClear(sample) {\n    return sample.dataMask == 1;\n}\n\nfunction sum(array) {\n    let sum = 0;\n    for (let i = 0; i &lt; array.length; i++) {\n        sum += array[i].NO2;\n    }\n    return sum;\n}\n\nfunction evaluatePixel(samples) {\n    const clearTs = samples.filter(isClear)\n    const mean = sum(clearTs) / clearTs.length\n    return {default: [mean], dataMask: [clearTs.length]}\n}\n\"\"\"\n\nNow we define the Statistical API request, for that we first define an aggregation. Here we define the time range we are interesed in. In this case it is one year of data, all of 2022. We then define the aggregation interval, this defines how many days are aggregated. Since Sentinel 5P has a very high revisit rate we can define a temporal resolution of one day. However we could just as easily make a time series of weekly or monthly values just by changing the aggregation interval to P1W or P1M respectively.\nThe size is set to 1 by 1 pixel since Sentinel 5P pixels are quite large and we are only intersted in point data for the capitals.\n\naggregation = SentinelHubStatistical.aggregation(\n    evalscript=evalscript_stat,\n    time_interval=(\"2022-01-01\", \"2023-01-01\"),\n    aggregation_interval=\"P1D\",\n    size=(1, 1),\n)\n\ninput_data = SentinelHubStatistical.input_data(\n    DataCollection.SENTINEL5P.define_from(\"5p\", service_url=config.sh_base_url)\n)\n\nrequests = []\n\nWe then create one request for each capital city. Instead of doing it like this we could also use the Batch Statistical API which is designed to calculate statistics for many polygons efficiently. Batch Statistical API will also be available on the Copernicus Browser later in 2023.\nThis list of requests is then downloaded in parallel\n\nfor geo_shape in capitals.geometry.values:\n    request = SentinelHubStatistical(\n        aggregation=aggregation,\n        input_data=[input_data],\n        geometry=Geometry(geo_shape, crs=CRS(capitals.crs)),\n        config=config,\n    )\n    requests.append(request)\n\ndownload_requests = [request.download_list[0] for request in requests]\nclient = SentinelHubStatisticalDownloadClient(config=config)\npollution_stats = client.download(download_requests, max_threads=5, show_progress=True)\n\nThis is a helper function to convert the output of the statistical API to a pandas dataframe.\n\ndef stats_to_df(stats_data):\n    \"\"\"Transform Statistical API response into a pandas.DataFrame\"\"\"\n    df_data = []\n\n    for single_data in stats_data[\"data\"]:\n        df_entry = {}\n        is_valid_entry = True\n\n        df_entry[\"interval_from\"] = parse_time(single_data[\"interval\"][\"from\"]).date()\n        df_entry[\"interval_to\"] = parse_time(single_data[\"interval\"][\"to\"]).date()\n\n        for output_name, output_data in single_data[\"outputs\"].items():\n            for band_name, band_values in output_data[\"bands\"].items():\n                band_stats = band_values[\"stats\"]\n                if band_stats[\"sampleCount\"] == band_stats[\"noDataCount\"]:\n                    is_valid_entry = False\n                    break\n\n                for stat_name, value in band_stats.items():\n                    col_name = f\"{output_name}_{band_name}_{stat_name}\"\n                    if stat_name == \"percentiles\":\n                        for perc, perc_val in value.items():\n                            perc_col_name = f\"{col_name}_{perc}\"\n                            df_entry[perc_col_name] = perc_val\n                    else:\n                        df_entry[col_name] = value\n\n        if is_valid_entry:\n            df_data.append(df_entry)\n\n    return pd.DataFrame(df_data)\n\nHere we build the dataframe from the request output.\n\nno2_dfs = [stats_to_df(polygon_stats) for polygon_stats in pollution_stats]\n\nfor df, capital in zip(no2_dfs, capitals[\"name\"].values):\n    df[\"name\"] = capital\n\nno2_df = pd.concat(no2_dfs)\nno2_df[\"month\"] = no2_df[\"interval_from\"].astype(\"datetime64[ns]\").dt.month\nno2_df.to_csv(\"./data/no2_capitals_timeseries.csv\")\nno2_df\n\n\n\n\n\n\n\n\ninterval_from\ninterval_to\ndefault_NO2_min\ndefault_NO2_max\ndefault_NO2_mean\ndefault_NO2_stDev\ndefault_NO2_sampleCount\ndefault_NO2_noDataCount\nname\n\n\n\n\n0\n2022-03-22\n2022-03-23\n0.000047\n0.000047\n0.000047\n0.0\n1\n0\nVilnius\n\n\n1\n2022-04-17\n2022-04-18\n0.000020\n0.000020\n0.000020\n0.0\n1\n0\nVilnius\n\n\n2\n2022-04-18\n2022-04-19\n0.000018\n0.000018\n0.000018\n0.0\n1\n0\nVilnius\n\n\n3\n2022-04-26\n2022-04-27\n0.000039\n0.000039\n0.000039\n0.0\n1\n0\nVilnius\n\n\n4\n2022-04-27\n2022-04-28\n0.000035\n0.000035\n0.000035\n0.0\n1\n0\nVilnius\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n170\n2022-12-25\n2022-12-26\n0.000103\n0.000103\n0.000103\n0.0\n1\n0\nVienna\n\n\n171\n2022-12-27\n2022-12-28\n0.000034\n0.000034\n0.000034\n0.0\n1\n0\nVienna\n\n\n172\n2022-12-28\n2022-12-29\n0.000047\n0.000047\n0.000047\n0.0\n1\n0\nVienna\n\n\n173\n2022-12-30\n2022-12-31\n0.000163\n0.000163\n0.000163\n0.0\n1\n0\nVienna\n\n\n174\n2022-12-31\n2023-01-01\n0.000015\n0.000015\n0.000015\n0.0\n1\n0\nVienna\n\n\n\n\n2870 rows × 9 columns\n\n\n\nWith this dataframe we can now do analysis. To give an example we are looking at the time series for a few different capital cities.\n\nsel_capitals = [\"Tallinn\", \"Berlin\", \"Rome\", \"Madrid\"]\nsns.lineplot(\n    data=no2_df.loc[no2_df[\"name\"].isin(sel_capitals)],\n    x=\"month\",\n    y=\"default_NO2_mean\",\n    hue=\"name\",\n);\n\n\n\n\nThis shows us that for most cities the NO2 concentration is seasonal, with higher values in winter than in Summer. Madrid, Berlin and Rome all share quite similar temporal patterns. Tallinn on the other hand has much lower NO2 concentration throughout.\nWith this data now acquired many different types of analysis can be carried out. This showed the advantage of statistical API, since you don’t have to download entire tiles even if you are only interested in the value of a single pixel. This makes data access much more efficient, allowing you to get started with analysis much quicker."
  },
  {
    "objectID": "notebook-samples/geo/odata_basics.html",
    "href": "notebook-samples/geo/odata_basics.html",
    "title": "How to query CDSE Catalogue and download products",
    "section": "",
    "text": "The following example shows how to interact with the catalogue and download EO data products for further processing. We will search for cloudless Sentinel-2 L1C products over Warsaw and create an RGB true color image form one of the products found in the catalogue.\n\nImport necessary Python modules\n\n# HTTP requests\nimport requests\n\n# JSON parser\nimport json\n\n# XML parser\nimport xml.etree.ElementTree as ET\n\n# system modules\nimport os\nimport re\nimport sys\nimport random\n\n# data manipulation\nimport pandas as pd\nimport numpy as np\n\n# image manipulation\nimport rasterio\nimport matplotlib.pyplot as plt\nimport matplotlib.image\nfrom rasterio.windows import Window\n\n# file manipulation\nfrom pathlib import Path\n\n\nQuery the catalogue and get a list of products matching the search parameters\nRefer to https://documentation.dataspace.copernicus.eu/APIs/OData.html#query-collection-of-products\n\n# base URL of the product catalogue\ncatalogue_odata_url = \"https://catalogue.dataspace.copernicus.eu/odata/v1\"\n\n# search parameters\ncollection_name = \"SENTINEL-2\"\nproduct_type = \"S2MSI1C\"\nmax_cloud_cover = 1\naoi = \"POLYGON((20.888443 52.169721,21.124649 52.169721,21.124649 52.271099,20.888443 52.271099,20.888443 52.169721))\"\nsearch_period_start = \"2023-06-01T00:00:00.000Z\"\nsearch_period_end = \"2023-06-10T00:00:00.000Z\"\n\n\n\nBuild and check the search query\n\nsearch_query = f\"{catalogue_odata_url}/Products?$filter=Collection/Name eq '{collection_name}' and Attributes/OData.CSC.StringAttribute/any(att:att/Name eq 'productType' and att/OData.CSC.StringAttribute/Value eq '{product_type}') and OData.CSC.Intersects(area=geography'SRID=4326;{aoi}') and ContentDate/Start gt {search_period_start} and ContentDate/Start lt {search_period_end}\"\n\nprint(f\"\"\"\\n{search_query.replace(' ', \"%20\")}\\n\"\"\")\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20'SENTINEL-2'%20and%20Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20'productType'%20and%20att/OData.CSC.StringAttribute/Value%20eq%20'S2MSI1C')%20and%20OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((20.888443%2052.169721,21.124649%2052.169721,21.124649%2052.271099,20.888443%2052.271099,20.888443%2052.169721))')%20and%20ContentDate/Start%20gt%202023-06-01T00:00:00.000Z%20and%20ContentDate/Start%20lt%202023-06-10T00:00:00.000Z\n\n\n\n\n\nRun the query and display the results\n\nresponse = requests.get(search_query).json()\nresult = pd.DataFrame.from_dict(response[\"value\"])\n\n# print first 3 results\nresult.head(3)\n\n\n\n\n\n\n\n\n@odata.mediaContentType\nId\nName\nContentType\nContentLength\nOriginDate\nPublicationDate\nModificationDate\nOnline\nEvictionDate\nS3Path\nChecksum\nContentDate\nFootprint\nGeoFootprint\n\n\n\n\n0\napplication/octet-stream\nadc9ef40-4231-446e-8265-65d85e07d743\nS2A_MSIL1C_20230606T095031_N0509_R079_T34UED_2...\napplication/octet-stream\n628718136\n2023-06-06T14:45:40.192Z\n2023-06-06T14:55:34.488Z\n2023-06-07T01:31:17.153Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2023/06/06/S2A_MSIL...\n[{'Value': '84f6c8ca3a8dfd09b8ebdcfa9b061631',...\n{'Start': '2023-06-06T09:50:31.025Z', 'End': '...\ngeography'SRID=4326;POLYGON ((22.4422542865043...\n{'type': 'Polygon', 'coordinates': [[[22.44225...\n\n\n1\napplication/octet-stream\n7954a18c-5585-4880-a9c4-ef3bc9d8c6c6\nS2B_MSIL1C_20230601T094549_N0509_R079_T34UEC_2...\napplication/octet-stream\n270482371\n2023-06-01T13:42:40.429Z\n2023-06-01T13:48:45.988Z\n2023-06-01T13:55:23.363Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2023/06/01/S2B_MSIL...\n[{'Value': 'de63c6f5614f01cc2b3e3265ab755fc8',...\n{'Start': '2023-06-01T09:45:49.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((21.370763876953 ...\n{'type': 'Polygon', 'coordinates': [[[21.37076...\n\n\n2\napplication/octet-stream\na0e9b43e-0638-4212-aee8-fb5cae2dffdf\nS2B_MSIL1C_20230601T094549_N0509_R079_T34UDC_2...\napplication/octet-stream\n334240926\n2023-06-01T13:50:41.383Z\n2023-06-01T14:00:19.601Z\n2023-06-01T14:02:33.827Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2023/06/01/S2B_MSIL...\n[{'Value': '5a1c9e1f6c40c2f8e13409d209d91581',...\n{'Start': '2023-06-01T09:45:49.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((21.1416714490104...\n{'type': 'Polygon', 'coordinates': [[[21.14167...\n\n\n\n\n\n\n\n\n\nAdd filtering by cloud coverage and repeat the query\n\nsearch_query = f\"{search_query} and Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and att/OData.CSC.DoubleAttribute/Value le {max_cloud_cover})\"\nprint(f\"\"\"\\n{search_query.replace(' ', \"%20\")}\\n\"\"\")\n\nresponse = requests.get(search_query).json()\nresult = pd.DataFrame.from_dict(response[\"value\"])\n\n# Print the first 3 results\nresult.head(3)\n\n\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name%20eq%20'SENTINEL-2'%20and%20Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20'productType'%20and%20att/OData.CSC.StringAttribute/Value%20eq%20'S2MSI1C')%20and%20OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((20.888443%2052.169721,21.124649%2052.169721,21.124649%2052.271099,20.888443%2052.271099,20.888443%2052.169721))')%20and%20ContentDate/Start%20gt%202023-06-01T00:00:00.000Z%20and%20ContentDate/Start%20lt%202023-06-10T00:00:00.000Z%20and%20Attributes/OData.CSC.DoubleAttribute/any(att:att/Name%20eq%20'cloudCover'%20and%20att/OData.CSC.DoubleAttribute/Value%20le%201)\n\n\n\n\n\n\n\n\n\n\n@odata.mediaContentType\nId\nName\nContentType\nContentLength\nOriginDate\nPublicationDate\nModificationDate\nOnline\nEvictionDate\nS3Path\nChecksum\nContentDate\nFootprint\nGeoFootprint\n\n\n\n\n0\napplication/octet-stream\n7954a18c-5585-4880-a9c4-ef3bc9d8c6c6\nS2B_MSIL1C_20230601T094549_N0509_R079_T34UEC_2...\napplication/octet-stream\n270482371\n2023-06-01T13:42:40.429Z\n2023-06-01T13:48:45.988Z\n2023-06-01T13:55:23.363Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2023/06/01/S2B_MSIL...\n[{'Value': 'de63c6f5614f01cc2b3e3265ab755fc8',...\n{'Start': '2023-06-01T09:45:49.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((21.370763876953 ...\n{'type': 'Polygon', 'coordinates': [[[21.37076...\n\n\n1\napplication/octet-stream\na0e9b43e-0638-4212-aee8-fb5cae2dffdf\nS2B_MSIL1C_20230601T094549_N0509_R079_T34UDC_2...\napplication/octet-stream\n334240926\n2023-06-01T13:50:41.383Z\n2023-06-01T14:00:19.601Z\n2023-06-01T14:02:33.827Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2023/06/01/S2B_MSIL...\n[{'Value': '5a1c9e1f6c40c2f8e13409d209d91581',...\n{'Start': '2023-06-01T09:45:49.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((21.1416714490104...\n{'type': 'Polygon', 'coordinates': [[[21.14167...\n\n\n2\napplication/octet-stream\nbadf9949-313d-464a-869e-4c5add6eab5e\nS2B_MSIL1C_20230601T094549_N0509_R079_T34UED_2...\napplication/octet-stream\n633935426\n2023-06-01T13:54:41.204Z\n2023-06-01T14:05:13.794Z\n2023-06-01T14:05:31.433Z\nTrue\n\n/eodata/Sentinel-2/MSI/L1C/2023/06/01/S2B_MSIL...\n[{'Value': '76289c4e705a458eabbba4be5bb21780',...\n{'Start': '2023-06-01T09:45:49.024Z', 'End': '...\ngeography'SRID=4326;POLYGON ((22.4484108895815...\n{'type': 'Polygon', 'coordinates': [[[22.44841...\n\n\n\n\n\n\n\n\n\nAuthenticate your account to download files\n\n# Provide CDSE account credentials - replace with your own data\nimport os\n\nusername = os.environ[\"CDSE_USERNAME\"]\npassword = os.environ[\"CDSE_PASSWORD\"]\n\n# Get authentication token\nimport certifi\n\nauth_server_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\ndata = {\n    \"client_id\": \"cdse-public\",\n    \"grant_type\": \"password\",\n    \"username\": username,\n    \"password\": password,\n}\n\nresponse = requests.post(auth_server_url, data=data, verify=True, allow_redirects=False)\naccess_token = json.loads(response.text)[\"access_token\"]\n\n\n\nSelect the product and establish authenticated session\n\n# Select identifier of the first product\nproduct_identifier = result.iloc[0, 1]\nproduct_name = result.iloc[0, 2]\n\n# Establish session\nsession = requests.Session()\nsession.headers[\"Authorization\"] = f\"Bearer {access_token}\"\n\n\n\nGet manifest file\n\n# Nodes() method lets us traverse the directory tree and retrieve single file from the product\nurl = f\"{catalogue_odata_url}/Products({product_identifier})/Nodes({product_name})/Nodes(MTD_MSIL1C.xml)/$value\"\nresponse = session.get(url, allow_redirects=False)\nwhile response.status_code in (301, 302, 303, 307):\n    url = response.headers[\"Location\"]\n    response = session.get(url, allow_redirects=False)\n\nfile = session.get(url, verify=False, allow_redirects=True)\n\n# Save the product in home directory\noutfile = Path.home() / \"MTD_MSIL1C.xml\"\noutfile.write_bytes(file.content)\n\n45819\n\n\n\n\nParse manifest file and get bands location\n\n# Pass the path of the xml document\ntree = ET.parse(str(outfile))\n# get the parent tag\nroot = tree.getroot()\n\n# Get the location of individual bands in Sentinel-2 granule\nband_location = []\nband_location.append(f\"{product_name}/{root[0][0][12][0][0][1].text}.jp2\".split(\"/\"))\nband_location.append(f\"{product_name}/{root[0][0][12][0][0][2].text}.jp2\".split(\"/\"))\nband_location.append(f\"{product_name}/{root[0][0][12][0][0][3].text}.jp2\".split(\"/\"))\n\n\n\nDownload bands\n\n# Build the url for each file using Nodes() method\nbands = []\nfor band_file in band_location:\n    url = f\"{catalogue_odata_url}/Products({product_identifier})/Nodes({product_name})/Nodes({band_file[1]})/Nodes({band_file[2]})/Nodes({band_file[3]})/Nodes({band_file[4]})/$value\"\n    response = session.get(url, allow_redirects=False)\n    while response.status_code in (301, 302, 303, 307):\n        url = response.headers[\"Location\"]\n        response = session.get(url, allow_redirects=False)\n    file = session.get(url, verify=False, allow_redirects=True)\n    # Save the product in home directory\n    outfile = Path.home() / band_file[4]\n    outfile.write_bytes(file.content)\n    bands.append(str(outfile))\n    print(\"Saved:\", band_file[4])\n\nSaved: T34UEC_20230601T094549_B02.jp2\nSaved: T34UEC_20230601T094549_B03.jp2\nSaved: T34UEC_20230601T094549_B04.jp2\n\n\n\n\nPrepare cropped patch\n\n%matplotlib inline\n\n# Crop the images to random 1000x1000 patch\nxsize, ysize = 1000, 1000\nxoff, yoff, xmax, ymax = 0, 0, 0, 0\nn = 2\n\nfor band_file in bands:\n    full_band = rasterio.open(band_file, driver=\"JP2OpenJPEG\")\n    if xmax == 0:\n        xmin, xmax = 0, full_band.width - xsize\n    if ymax == 0:\n        ymin, ymax = 0, full_band.height - ysize\n    if xoff == 0:\n        xoff, yoff = random.randint(xmin, xmax), random.randint(ymin, ymax)\n    window = Window(xoff, yoff, xsize, ysize)\n    transform = full_band.window_transform(window)\n    profile = full_band.profile\n    crs = full_band.crs\n    profile.update({\"height\": xsize, \"width\": ysize, \"transform\": transform})\n    with rasterio.open(\n        f\"{Path.home()}/patch_band_{n}.jp2\", \"w\", **profile\n    ) as patch_band:\n        # Read the data from the window and write it to the output raster\n        patch_band.write(full_band.read(window=window))\n    print(f\"Patch for band {n} created\")\n    n += 1\n\nPatch for band 2 created\nPatch for band 3 created\nPatch for band 4 created\n\n\n\n\nGenerate true color image\n\n# Read the patch files\nband2 = rasterio.open(f\"{Path.home()}/patch_band_2.jp2\", driver=\"JP2OpenJPEG\")  # blue\nband3 = rasterio.open(f\"{Path.home()}/patch_band_3.jp2\", driver=\"JP2OpenJPEG\")  # green\nband4 = rasterio.open(f\"{Path.home()}/patch_band_4.jp2\", driver=\"JP2OpenJPEG\")  # red\n\nred = band4.read(1)\ngreen = band3.read(1)\nblue = band2.read(1)\n\n# Normalize the pixel values and apply gain\ngain = 2\nred_n = np.clip(red * gain / 10000, 0, 1)\ngreen_n = np.clip(green * gain / 10000, 0, 1)\nblue_n = np.clip(blue * gain / 10000, 0, 1)\n\n# Create composite image\nrgb_composite_n = np.dstack((red_n, green_n, blue_n))\n\n# Display image\nplt.imshow(rgb_composite_n)\n\n# Save image to file\nmatplotlib.image.imsave(f\"{Path.home()}/Sentinel2_true_color.jpeg\", rgb_composite_n)\nprint(\"Saved as:\", outfile)\n\nSaved as: /home/jovyan/T34UEC_20230601T094549_B04.jp2\n\n\n\n\n\n\n\n\n\n\nExplore the CDSE Documentation to learn more about the available APIs and services:\n\n\nhttps://documentation.dataspace.copernicus.eu/Home.html"
  },
  {
    "objectID": "notebook-samples/openeo/UDF.html",
    "href": "notebook-samples/openeo/UDF.html",
    "title": "User-Defined Functions (UDF) in openEO",
    "section": "",
    "text": "While openEO supports a wide range of pre-defined processes and allows to build more complex user-defined processes from them, you sometimes need operations or algorithms that are not (yet) available or standardized as openEO process. User-Defined Functions (UDF) is an openEO feature (through the run_udf process) that aims to fill that gap by allowing a user to express (a part of) an algorithm as a Python/R/… script to be run back-end side.\nThough several types of algorithms can be used as UDF applications, in this notebook, we showcase a simple example of how to work with UDF using the openEO Python Client library.\n\n# estabish connection to the backend and authenticate it\nimport openeo\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n\n# load collection\n\ncube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    bands=[\"B04\", \"B03\", \"B02\"],\n    temporal_extent=(\"2022-05-01\", \"2022-05-30\"),\n    spatial_extent={\n        \"west\": 5.05,\n        \"south\": 51.21,\n        \"east\": 5.1,\n        \"north\": 51.23,\n        \"crs\": \"EPSG:4326\",\n    },\n    max_cloud_cover=50,\n)\n\ncube = cube.reduce_dimension(dimension=\"t\", reducer=\"max\")\ncube\n\n\n    \n    \n        \n    \n    \n\n\nHere the UDF code shown in the following cell does the actual value rescaling.\n\n# Build a UDF object from an inline string with Python source code.\nudf = openeo.UDF(\n    \"\"\"\nfrom openeo.udf import XarrayDataCube\n\ndef apply_datacube(cube: XarrayDataCube, context: dict) -&gt; XarrayDataCube:\n    array = cube.get_array()\n    array.values = 0.0001 * array.values\n    return cube\n\"\"\"\n)\n\nUser can also load their UDF from a seperate file using openeo.UDF.from_file('my_udf.py') and apply it.\n\n# Apply the UDF to a cube.\nrescaled_cube = cube.apply(process=udf)\n\n\nrescaled_cube.download(\"rescale_s2.tiff\")\n\n\nVisualize the result\n\nimport rasterio\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import exposure\n\nimg = rasterio.open(\"rescale_s2.tiff\").read()\n\n\ndef normalizeimg(data):\n    data = data.astype(float)\n    for i in range(data.shape[2]):\n        p2, p98 = np.percentile(data[:, :, i], (2, 98))\n        data[:, :, i] = exposure.rescale_intensity(data[:, :, i], in_range=(p2, p98))\n    return data\n\n\nfig, ax = plt.subplots(figsize=(6, 2), dpi=150)\nax.imshow(normalizeimg(np.moveaxis(img, 0, -1)))\n\nax.set_title(\"Rescaled Image\")\n\n# Adjusting the spacing between subplots\nplt.tight_layout()\n\n# Display the figure\nplt.show()"
  },
  {
    "objectID": "notebook-samples/openeo/NDVI_Timeseries.html",
    "href": "notebook-samples/openeo/NDVI_Timeseries.html",
    "title": "How to create an NDVI Time series using openEO",
    "section": "",
    "text": "This notebook presents an application case, that demonstrates how to display the NDVI (Normalized Difference Vegetation Index) timeseries for specific fields. The case study showcases the process of selecting the fields and generating average NDVI timeseries data for analysis and visualization."
  },
  {
    "objectID": "notebook-samples/openeo/NDVI_Timeseries.html#setup",
    "href": "notebook-samples/openeo/NDVI_Timeseries.html#setup",
    "title": "How to create an NDVI Time series using openEO",
    "section": "Setup",
    "text": "Setup\n\nimport json\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.signal\nimport numpy as np\n\nimport openeo\n\nEstablish an authenticated connection to Copernicus Data Space Ecosystem openEO back-end.\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\nconnection.authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n&lt;Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.1/' with OidcBearerAuth&gt;"
  },
  {
    "objectID": "notebook-samples/openeo/NDVI_Timeseries.html#basic-ndvi-timeseries",
    "href": "notebook-samples/openeo/NDVI_Timeseries.html#basic-ndvi-timeseries",
    "title": "How to create an NDVI Time series using openEO",
    "section": "Basic NDVI Timeseries",
    "text": "Basic NDVI Timeseries\nWe want to calculate the NDVI values in a couple of fields in a time window of a couple of months.\nFor simplicty, we load the field geometries as an inline GeoJSON feature collection:\n\nfields = json.loads(\n    \"\"\"{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[5.055945487931457, 51.222709834076504], [5.064972484168688, 51.221122565090525], [5.064972484168688, 51.221122565090525], [5.067474954083448, 51.218249806779134], [5.064827929485983, 51.21689628072789], [5.05917785594747, 51.217191909908095], [5.053553857094518, 51.21807492332223], [5.055945487931457, 51.222709834076504]]]}}, \n        {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[5.063345886679116, 51.23087606640057], [5.06604742694687, 51.22886710731809], [5.070627820472246, 51.22874440121892], [5.068403609708207, 51.22657208381529], [5.064823257492447, 51.22676051738515], [5.064892324615199, 51.2283032878514], [5.063641745941974, 51.2285757299238], [5.062340811262595, 51.227722351687945], [5.06076005158084, 51.228042312276536], [5.063345886679116, 51.23087606640057]]]}},\n        {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[5.07163184674986, 51.23481147556147], [5.076706025697324, 51.23317590781036], [5.077828303041866, 51.233226237184724], [5.078024733866917, 51.23263978271262], [5.080771081607657, 51.23259097170763], [5.083734842574312, 51.23530464074437], [5.080957826735458, 51.23646091560258], [5.079752631651647, 51.23519531038643], [5.077238400183506, 51.23490534677628], [5.072856439300575, 51.23593546777778], [5.07163184674986, 51.23481147556147]]]}}, \n        {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[5.083897244679042, 51.23510639883143], [5.081302408741335, 51.232922477780846], [5.082963802194108, 51.233146058575876], [5.084497702305552, 51.232672717580655], [5.085732850338428, 51.2340852086282], [5.083897244679042, 51.23510639883143]]]}}\n    ]}\n\"\"\"\n)\n\nLoad the “B04” (red) and “B08” (NIR) brands from the SENTINEL2_L2A collection for the desired time window:\n\ns2cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2020-06-01\", \"2020-10-01\"],\n    bands=[\"B04\", \"B08\"],\n)\n\nNote here that it is not necessary to specify a spatial extent in load_collection, because we will provide the field geometries in a later step. The openEO back-end will automatically limit the effective data loading to the necessary bounding box.\nCalculate the NDVI from the red and NIR bands:\n\nred = s2cube.band(\"B04\")\nnir = s2cube.band(\"B08\")\nndvi = (nir - red) / (nir + red)\n\nWith the DataCube.aggregate_spatial() method , we can calculate the mean NDVI for each of the fields.\n\ntimeseries = ndvi.aggregate_spatial(geometries=fields, reducer=\"mean\")\n\nWe now execute this as a batch job and download the timeseries in CSV format.\n\njob = timeseries.execute_batch(out_format=\"CSV\", title=\"NDVI timeseries\")\n\n0:00:00 Job 'j-273d672147ec4603af1bb2b045ff3a3b': send 'start'\n0:00:12 Job 'j-273d672147ec4603af1bb2b045ff3a3b': created (progress N/A)\n0:00:17 Job 'j-273d672147ec4603af1bb2b045ff3a3b': created (progress N/A)\n0:00:23 Job 'j-273d672147ec4603af1bb2b045ff3a3b': created (progress N/A)\n0:00:31 Job 'j-273d672147ec4603af1bb2b045ff3a3b': created (progress N/A)\n0:00:41 Job 'j-273d672147ec4603af1bb2b045ff3a3b': created (progress N/A)\n0:00:53 Job 'j-273d672147ec4603af1bb2b045ff3a3b': created (progress N/A)\n0:01:09 Job 'j-273d672147ec4603af1bb2b045ff3a3b': running (progress N/A)\n0:01:28 Job 'j-273d672147ec4603af1bb2b045ff3a3b': running (progress N/A)\n0:01:52 Job 'j-273d672147ec4603af1bb2b045ff3a3b': running (progress N/A)\n0:02:22 Job 'j-273d672147ec4603af1bb2b045ff3a3b': running (progress N/A)\n0:03:00 Job 'j-273d672147ec4603af1bb2b045ff3a3b': running (progress N/A)\n0:03:46 Job 'j-273d672147ec4603af1bb2b045ff3a3b': finished (progress N/A)\n\n\nDownload the timeseries CSV:\n\njob.get_results().download_file(\"ndvi-results/timeseries-basic.csv\")\npd.read_csv(\"ndvi-results/timeseries-basic.csv\", index_col=0).head()\n\n\n\n\n\n\n\n\nfeature_index\navg(band_0)\n\n\ndate\n\n\n\n\n\n\n2020-07-06T00:00:00.000Z\n1\n-0.035662\n\n\n2020-07-06T00:00:00.000Z\n2\n-0.036050\n\n\n2020-07-06T00:00:00.000Z\n0\n-0.020222\n\n\n2020-07-06T00:00:00.000Z\n3\n-0.035913\n\n\n2020-08-08T00:00:00.000Z\n2\n0.651458\n\n\n\n\n\n\n\nCreate a quick plot helper to visualize the NDVI data:\n\ndef plot_timeseries(filename, figsize=(6, 3)):\n    df = pd.read_csv(filename, index_col=0)\n    df.index = pd.to_datetime(df.index)\n\n    fig, ax = plt.subplots(figsize=figsize, dpi=90)\n    df.groupby(\"feature_index\")[\"avg(band_0)\"].plot(marker=\"o\", ax=ax)\n    ax.set_title(filename.split(\"/\")[-1])\n    ax.set_ylabel(\"NDVI\")\n    ax.set_ylim(0, 1)\n    ax.legend(title=\"parcel id\", loc=\"lower left\", ncol=2)\n\n\nplot_timeseries(\"ndvi-results/timeseries-basic.csv\")"
  },
  {
    "objectID": "notebook-samples/openeo/NDVI_Timeseries.html#cloud-masking-in-ndvi",
    "href": "notebook-samples/openeo/NDVI_Timeseries.html#cloud-masking-in-ndvi",
    "title": "How to create an NDVI Time series using openEO",
    "section": "Cloud Masking in NDVI",
    "text": "Cloud Masking in NDVI\nThe result above is a good start, but there is room for improvement to get smoother NDVI profiles.\nThere are quite some outliers because we didn’t filter out cloudy observations or pixels. We can use the “SCL” (scene classification) band from the “SENTINEL2_L2A” collection to focus on non-cloud pixels. Let’s load an “SENTINEL2_L2A” data cube again, with the additional “SCL” band, and calculate the NDVI like before:\n\ns2cube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    temporal_extent=[\"2020-06-01\", \"2020-10-01\"],\n    bands=[\"B04\", \"B08\", \"SCL\"],\n)\nred = s2cube.band(\"B04\")\nnir = s2cube.band(\"B08\")\nndvi = (nir - red) / (nir + red)\n\nNow, from the “SCL” band we will build a mask to remove everything that is not class 4 (“vegetation”) or 5 (“not vegetated”). This mask data cube is 1 for pixels which we want to remove (clouds, cloud shadow, …) and 0 for pixels we want to keep.\n\nscl = s2cube.band(\"SCL\")\nmask = ~((scl == 4) | (scl == 5))\n\nThis mask is however a bit noisy (because of imperfect classification) and to be a bit conservative we also want to expand it a bit to exclude extra pixels at cloud edges. We can do this morphological operation by using a convolution with a gaussian kernel and applying a threshold to get a binary mask again.\n\n# 2D gaussian kernel\ng = scipy.signal.windows.gaussian(11, std=1.6)\nkernel = np.outer(g, g)\nkernel = kernel / kernel.sum()\n\n# Morphological dilation of mask: convolution + threshold\nmask = mask.apply_kernel(kernel)\nmask = mask &gt; 0.1\n\nNow, we mask the NDVI data cube and do aggregation again:\n\nndvi_masked = ndvi.mask(mask)\ntimeseries_masked = ndvi_masked.aggregate_spatial(geometries=fields, reducer=\"mean\")\n\n\njob = timeseries_masked.execute_batch(out_format=\"CSV\", title=\"Maked NDVI timeseries\")\n\n0:00:00 Job 'j-7b1b93be7e624a408e7e91691676170b': send 'start'\n0:00:12 Job 'j-7b1b93be7e624a408e7e91691676170b': created (progress N/A)\n0:00:17 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:00:23 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:00:32 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:00:42 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:00:54 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:01:10 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:01:29 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:01:53 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:02:24 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:03:01 Job 'j-7b1b93be7e624a408e7e91691676170b': running (progress N/A)\n0:03:48 Job 'j-7b1b93be7e624a408e7e91691676170b': finished (progress N/A)\n\n\n\njob.get_results().download_file(\"ndvi-results/timeseries-masked.csv\")\n\nPosixPath('ndvi-results/timeseries-masked.csv')\n\n\n\nplot_timeseries(\"ndvi-results/timeseries-masked.csv\")"
  },
  {
    "objectID": "notebook-samples/openeo/NDVI_Timeseries.html#timeseries-smoothing",
    "href": "notebook-samples/openeo/NDVI_Timeseries.html#timeseries-smoothing",
    "title": "How to create an NDVI Time series using openEO",
    "section": "Timeseries Smoothing",
    "text": "Timeseries Smoothing\nAs final step in this demonstration we will add a openEO user-defined function (UDF) to the processing. A UDF allows to submit a snippet of, for example, Python code to be executed on the data at the backend side. In this case we’ll define a UDF to:\n\ninterpolate missing values (due to cloud filtering)\napply a Savitzky-Golay filter for temporal smoothing of the timeseries (using scipy.signal.savgol_filter)\n\nWe can load a UDF from an external file, but here we’ll load it as an inline snippet:\n\nudf = openeo.UDF(\n    \"\"\"\nfrom scipy.signal import savgol_filter\nfrom openeo.udf import XarrayDataCube\n\ndef apply_datacube(cube: XarrayDataCube, context: dict) -&gt; XarrayDataCube:\n    array = cube.get_array()\n    filled = array.interpolate_na(dim='t')\n    smoothed_array = savgol_filter(filled.values, 5, 2, axis=0)\n    return DataCube(xarray.DataArray(smoothed_array, dims=array. dims,coords=array.coords))\n\"\"\"\n)\n\nWe apply this UDF along the time dimension of the ndvi_masked cube we created in the previous step:\n\nndvi_smoothed = ndvi_masked.apply_dimension(code=udf, dimension=\"t\")\n\nNow, aggregate this again per field and get the time series.\n\ntimeseries_smoothed = ndvi_smoothed.aggregate_spatial(geometries=fields, reducer=\"mean\")\n\n\njob = timeseries_smoothed.execute_batch(\n    out_format=\"CSV\", title=\"Smoothed NDVI timeseries\"\n)\n\n0:00:00 Job 'j-465abc149b04431cbe9d4ea21a029c90': send 'start'\n0:00:11 Job 'j-465abc149b04431cbe9d4ea21a029c90': created (progress N/A)\n0:00:17 Job 'j-465abc149b04431cbe9d4ea21a029c90': created (progress N/A)\n0:00:23 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:00:31 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:00:41 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:00:53 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:01:09 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:01:28 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:01:52 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:02:22 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:03:00 Job 'j-465abc149b04431cbe9d4ea21a029c90': running (progress N/A)\n0:03:46 Job 'j-465abc149b04431cbe9d4ea21a029c90': finished (progress N/A)\n\n\n\njob.get_results().download_file(\"ndvi-results/timeseries-smoothed.csv\")\n\nPosixPath('ndvi-results/timeseries-smoothed.csv')\n\n\n\nplot_timeseries(\"ndvi-results/timeseries-smoothed.csv\")"
  },
  {
    "objectID": "notebook-samples/openeo/basics.html",
    "href": "notebook-samples/openeo/basics.html",
    "title": "openEO Basics: Discovery of Collections and Processes",
    "section": "",
    "text": "Import the openeo package and connect to the Copernicus Data Space Ecosystem openEO back-end.\n\nimport openeo\n\n\nconnection = openeo.connect(\n    url=\"openeo.dataspace.copernicus.eu\",\n)"
  },
  {
    "objectID": "notebook-samples/openeo/basics.html#setup",
    "href": "notebook-samples/openeo/basics.html#setup",
    "title": "openEO Basics: Discovery of Collections and Processes",
    "section": "",
    "text": "Import the openeo package and connect to the Copernicus Data Space Ecosystem openEO back-end.\n\nimport openeo\n\n\nconnection = openeo.connect(\n    url=\"openeo.dataspace.copernicus.eu\",\n)"
  },
  {
    "objectID": "notebook-samples/openeo/basics.html#collections",
    "href": "notebook-samples/openeo/basics.html#collections",
    "title": "openEO Basics: Discovery of Collections and Processes",
    "section": "Collections",
    "text": "Collections\nList all available collection ids:\n\nprint(connection.list_collection_ids())\n\n['SENTINEL3_OLCI_L1B', 'SENTINEL3_SLSTR', 'SENTINEL_5P_L2', 'SENTINEL2_L1C', 'SENTINEL2_L2A', 'SENTINEL1_GRD', 'COPERNICUS_30']\n\n\nGet detailed information about a collection\n\nconnection.describe_collection(\"SENTINEL2_L2A\")"
  },
  {
    "objectID": "notebook-samples/openeo/basics.html#processes",
    "href": "notebook-samples/openeo/basics.html#processes",
    "title": "openEO Basics: Discovery of Collections and Processes",
    "section": "Processes",
    "text": "Processes\nList all available processes:\n\nconnection.list_processes()\n\n\n\n    \n    \n        \n    \n    \n\n\nInspect one process in more detail\n\nconnection.describe_process(\"add\")"
  },
  {
    "objectID": "notebook-samples/openeo/Radar_ARD.html",
    "href": "notebook-samples/openeo/Radar_ARD.html",
    "title": "Radar - Sentinel-1: ARD SAR Backscatter",
    "section": "",
    "text": "For certain use cases, the readily available preprocessed data collections in the openEO back-ends are not sufficient or inappropriately preprocessed. openEO supports some processes to address very common preprocessing scenarios:\nThese processes also offer a number of parameters to customize the processing.\nHowever, please note that these operations can be computationally expensive, so they certainly affect the overall processing time and cost of your final algorithm. Hence, make sure to make an informed decision when using these methods.\nIn this notebook has been duplicated from an existing sample processing pipeline for Radar ARD on the openEO platform. In this instance, we aim to showcase it with the Copernicus Data Space Ecosystem backend."
  },
  {
    "objectID": "notebook-samples/openeo/Radar_ARD.html#setup",
    "href": "notebook-samples/openeo/Radar_ARD.html#setup",
    "title": "Radar - Sentinel-1: ARD SAR Backscatter",
    "section": "Setup",
    "text": "Setup\nImport the openeo package and connect to the Copernicus Data Space Ecosystem openEO back-end.\n\nimport openeo\n\n\nbackend = \"openeo.dataspace.copernicus.eu\"\nconn = openeo.connect(backend).authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n\nOn-demand SAR Backscatter\nData from synthetic aperture radar (SAR) sensors requires significant preprocessing to be calibrated and normalized. This is referred to as backscatter computation and is provided in the openEO by the sar_backscatter process.\nThe radiometric correction coefficient used in this example is sigma0-ellipsoid."
  },
  {
    "objectID": "notebook-samples/openeo/Radar_ARD.html#specify-area-of-interest-temporal-extent-polarization",
    "href": "notebook-samples/openeo/Radar_ARD.html#specify-area-of-interest-temporal-extent-polarization",
    "title": "Radar - Sentinel-1: ARD SAR Backscatter",
    "section": "Specify area of interest, temporal extent, polarization",
    "text": "Specify area of interest, temporal extent, polarization\n\nspatial_extent = {\n    \"west\": 11.293602,\n    \"east\": 11.382866,\n    \"south\": 46.460163,\n    \"north\": 46.514768,\n    \"crs\": \"EPSG:4326\",\n}\n\ns1 = conn.load_collection(\n    \"SENTINEL1_GRD\",\n    spatial_extent=spatial_extent,\n    bands=[\"VV\", \"VH\"],\n    temporal_extent=[\"2021-01-01\", \"2021-01-08\"],\n    properties={\"sat:orbit_state\": lambda od: od == \"ASCENDING\"},\n)"
  },
  {
    "objectID": "notebook-samples/openeo/Radar_ARD.html#apply-openeo-processes",
    "href": "notebook-samples/openeo/Radar_ARD.html#apply-openeo-processes",
    "title": "Radar - Sentinel-1: ARD SAR Backscatter",
    "section": "Apply openEO processes",
    "text": "Apply openEO processes\nHere we apply both the SAR backscattering processes on the datacube and then convert it from linear to dB scale.\n\ns1_scatter = s1.sar_backscatter(\n    coefficient=\"sigma0-ellipsoid\", elevation_model=\"COPERNICUS_30\"\n)\ns1bs = s1_scatter.apply(lambda x: 10 * x.log(base=10))"
  },
  {
    "objectID": "notebook-samples/openeo/Radar_ARD.html#execution",
    "href": "notebook-samples/openeo/Radar_ARD.html#execution",
    "title": "Radar - Sentinel-1: ARD SAR Backscatter",
    "section": "Execution",
    "text": "Execution\nSince our area of interest is small, a direct request is preferred. Nevertheless, please note that this approach will not return the JSON metadata. If you want JSON metadata along with the result you can choose the Batch job-based method too.\nNote that this step automatically adds the save_result process at the end based on the output format we choose.\n\n%time s1bs.download(\"sar_bs.nc\")\n\nCPU times: user 13.6 ms, sys: 22.7 ms, total: 36.3 ms\nWall time: 28 s"
  },
  {
    "objectID": "notebook-samples/openeo/Radar_ARD.html#output-visualization",
    "href": "notebook-samples/openeo/Radar_ARD.html#output-visualization",
    "title": "Radar - Sentinel-1: ARD SAR Backscatter",
    "section": "Output visualization",
    "text": "Output visualization\n\nimport xarray as xr\n\nS1_ard = xr.open_dataset(\"sar_bs.nc\")\nS1_ard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (t: 1, x: 704, y: 628)\nCoordinates:\n  * t        (t) datetime64[ns] 2021-01-03\n  * x        (x) float64 6.759e+05 6.76e+05 6.76e+05 ... 6.83e+05 6.83e+05\n  * y        (y) float64 5.154e+06 5.154e+06 5.154e+06 ... 5.148e+06 5.148e+06\nData variables:\n    crs      |S1 ...\n    VV       (t, y, x) float32 ...\n    VH       (t, y, x) float32 ...\nAttributes:\n    Conventions:  CF-1.9\n    institution:  openEO platformxarray.DatasetDimensions:t: 1x: 704y: 628Coordinates: (3)t(t)datetime64[ns]2021-01-03standard_name :tlong_name :taxis :Tarray(['2021-01-03T00:00:00.000000000'], dtype='datetime64[ns]')x(x)float646.759e+05 6.76e+05 ... 6.83e+05standard_name :projection_x_coordinatelong_name :x coordinate of projectionunits :marray([675945., 675955., 675965., ..., 682955., 682965., 682975.])y(y)float645.154e+06 5.154e+06 ... 5.148e+06standard_name :projection_y_coordinatelong_name :y coordinate of projectionunits :marray([5154005., 5153995., 5153985., ..., 5147755., 5147745., 5147735.])Data variables: (3)crs()|S1...crs_wkt :PROJCS[\"WGS 84 / UTM zone 32N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 9.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32632\"]]spatial_ref :PROJCS[\"WGS 84 / UTM zone 32N\", GEOGCS[\"WGS 84\", DATUM[\"World Geodetic System 1984\", SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]], AUTHORITY[\"EPSG\",\"6326\"]], PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]], UNIT[\"degree\", 0.017453292519943295], AXIS[\"Geodetic longitude\", EAST], AXIS[\"Geodetic latitude\", NORTH], AUTHORITY[\"EPSG\",\"4326\"]], PROJECTION[\"Transverse_Mercator\", AUTHORITY[\"EPSG\",\"9807\"]], PARAMETER[\"central_meridian\", 9.0], PARAMETER[\"latitude_of_origin\", 0.0], PARAMETER[\"scale_factor\", 0.9996], PARAMETER[\"false_easting\", 500000.0], PARAMETER[\"false_northing\", 0.0], UNIT[\"m\", 1.0], AXIS[\"Easting\", EAST], AXIS[\"Northing\", NORTH], AUTHORITY[\"EPSG\",\"32632\"]][1 values with dtype=|S1]VV(t, y, x)float32...long_name :VVunits :grid_mapping :crs[442112 values with dtype=float32]VH(t, y, x)float32...long_name :VHunits :grid_mapping :crs[442112 values with dtype=float32]Indexes: (3)tPandasIndexPandasIndex(DatetimeIndex(['2021-01-03'], dtype='datetime64[ns]', name='t', freq=None))xPandasIndexPandasIndex(Index([675945.0, 675955.0, 675965.0, 675975.0, 675985.0, 675995.0, 676005.0,\n       676015.0, 676025.0, 676035.0,\n       ...\n       682885.0, 682895.0, 682905.0, 682915.0, 682925.0, 682935.0, 682945.0,\n       682955.0, 682965.0, 682975.0],\n      dtype='float64', name='x', length=704))yPandasIndexPandasIndex(Index([5154005.0, 5153995.0, 5153985.0, 5153975.0, 5153965.0, 5153955.0,\n       5153945.0, 5153935.0, 5153925.0, 5153915.0,\n       ...\n       5147825.0, 5147815.0, 5147805.0, 5147795.0, 5147785.0, 5147775.0,\n       5147765.0, 5147755.0, 5147745.0, 5147735.0],\n      dtype='float64', name='y', length=628))Attributes: (2)Conventions :CF-1.9institution :openEO platform\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 20))\nax1.imshow(S1_ard.VV[0].values, cmap=\"Greys_r\", vmin=-30, vmax=30)\nax1.set_title(\"VV sigma0\")\nax2.imshow(S1_ard.VH[0].values, cmap=\"Greys_r\", vmin=-30, vmax=30)\nax2.set_title(\"VH sigma0\")\nplt.show()"
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html",
    "href": "notebook-samples/openeo/Batch_job.html",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "",
    "text": "Most of the simple, basic openEO usage examples show synchronous execution of process graphs: you submit a process graph with a HTTP request and receive the result as direct response of that same request. This is only feasible if the processing doesn’t take too long (a couple of minutes at most).\nFor the heavier work, covering large regions of interest, long time series, more intensive processing, etc, you have to use batch jobs.\nThis notebook shows how to programmatically create and interact with batch job using the openEO Python client library."
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html#set-up",
    "href": "notebook-samples/openeo/Batch_job.html#set-up",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "Set up",
    "text": "Set up\nImport openeo package and establish an authenticated connection to Copernicus Data Space Ecosystem openEO back-end.\n\nimport openeo\n\n\nconnection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\nconnection.authenticate_oidc()\n\nAuthenticated using refresh token.\n\n\n&lt;Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.1/' with OidcBearerAuth&gt;"
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html#build-data-cube",
    "href": "notebook-samples/openeo/Batch_job.html#build-data-cube",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "Build data cube",
    "text": "Build data cube\nStart with a simple data cube: small spatiotemporal slice of SENTINEL2_L2A data:\n\ncube = connection.load_collection(\n    \"SENTINEL2_L2A\",\n    bands=[\"B04\", \"B03\", \"B02\"],\n    temporal_extent=(\"2022-05-01\", \"2022-05-30\"),\n    spatial_extent={\n        \"west\": 3.202609,\n        \"south\": 51.189474,\n        \"east\": 3.254708,\n        \"north\": 51.204641,\n        \"crs\": \"EPSG:4326\",\n    },\n    max_cloud_cover=50,\n)\n\nSet up output format to be GeoTIFF:\n\ncube = cube.save_result(format=\"GTiff\")"
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html#run-as-batch-job",
    "href": "notebook-samples/openeo/Batch_job.html#run-as-batch-job",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "Run as Batch Job",
    "text": "Run as Batch Job\nThe easiest way to run our processing as a batch job is using the execute_batch() helper, which takes care of creating a batch job, starting it, and keep polling its status until it’s finished (or failed).\nWhile not necessary, it is recommended to give your batch job a descriptive title so it’s easier to identify in your job listing.\n\njob = cube.execute_batch(title=\"Slice of S2 data\")\n\n0:00:00 Job 'j-cc569e261f4a4dce83e592b0f3425985': send 'start'\n0:00:11 Job 'j-cc569e261f4a4dce83e592b0f3425985': created (progress N/A)\n0:00:16 Job 'j-cc569e261f4a4dce83e592b0f3425985': created (progress N/A)\n0:00:23 Job 'j-cc569e261f4a4dce83e592b0f3425985': created (progress N/A)\n0:00:31 Job 'j-cc569e261f4a4dce83e592b0f3425985': created (progress N/A)\n0:00:45 Job 'j-cc569e261f4a4dce83e592b0f3425985': running (progress N/A)\n0:00:58 Job 'j-cc569e261f4a4dce83e592b0f3425985': running (progress N/A)\n0:01:14 Job 'j-cc569e261f4a4dce83e592b0f3425985': running (progress N/A)\n0:01:33 Job 'j-cc569e261f4a4dce83e592b0f3425985': running (progress N/A)\n0:01:57 Job 'j-cc569e261f4a4dce83e592b0f3425985': finished (progress N/A)\n\n\nIf you need a bit more control over the lifetime of a batch job, you can do each step manually, e.g.  - create a job with job = cube.create_job() - start a job with job.start_job() - wait until job.status() reaches \"finished\""
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html#inspecting-a-job",
    "href": "notebook-samples/openeo/Batch_job.html#inspecting-a-job",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "Inspecting a Job",
    "text": "Inspecting a Job\nA batch job on a back-end is fully identified by its job id. In case of the job we created above:\n\njob.job_id\n\n'j-cc569e261f4a4dce83e592b0f3425985'\n\n\nIt’s recommended to properly take note of the batch job id. It allows you to “reconnect” to your job (using connection.job(job_id)) on the back-end, even if it was created at another time, by another script/notebook or even with another openEO client.\nA batch job typically takes some time to finish, and you can check its status with the status() method.\n\njob.status()\n\n'finished'\n\n\nBatch job logs can be fetched with job.logs(). If you prefer a graphical, web-based interactive environment to manage and monitor your batch jobs, feel free to switch to an openEO web editor like openeo.dataspace.copernicus.eu at any time.\n\njob.logs()"
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html#fetch-batch-job-results",
    "href": "notebook-samples/openeo/Batch_job.html#fetch-batch-job-results",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "Fetch Batch Job Results",
    "text": "Fetch Batch Job Results\nThe result of a finished batch job consists of several elements: - a STAC-compatible description (metadata) of the batch job results - one or more output files (e.g. multiple GeoTIFF or netCDF assets)\nYou can get a handle to these results with get_results():\n\nresults = job.get_results()\nresults\n\n\n    \n    \n        \n    \n    \n\n\nIn the general case, when you have one or more result files (also called “assets”), the easiest option to download them is using download_files() (plural) where you just specify a download folder (otherwise the current working directory will be used by default).\n\nresults.download_files(\"output/batch_job\")\n\n[PosixPath('output/batch_job/openEO_2022-05-08Z.tif'),\n PosixPath('output/batch_job/openEO_2022-05-15Z.tif'),\n PosixPath('output/batch_job/openEO_2022-05-18Z.tif'),\n PosixPath('output/batch_job/openEO_2022-05-28Z.tif'),\n PosixPath('output/batch_job/job-results.json')]"
  },
  {
    "objectID": "notebook-samples/openeo/Batch_job.html#visualize-the-result",
    "href": "notebook-samples/openeo/Batch_job.html#visualize-the-result",
    "title": "Using openEO Batch Jobs To Run Large and Heavy Workflows",
    "section": "Visualize the result",
    "text": "Visualize the result\n\nimport pathlib\nimport rasterio\nimport matplotlib.pyplot as plt\n\n\nfig, axes = plt.subplots(figsize=(6, 4), nrows=2, ncols=2, dpi=90)\nfor i, path in enumerate(sorted(pathlib.Path(\"output/batch_job/\").glob(\"*tif\"))[:4]):\n    data = rasterio.open(path).read()\n    ax = axes[i // 2, i % 2]\n    ax.imshow((data.transpose(1, 2, 0) / 3000).clip(0, 1))\n    ax.set_title(path.name)"
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "In comparison to the existing legacy Copernicus Data Hub, what will be the free services other than stac/cog?\n\n\nCompared to the previously existed Copernicus Data Hub, there will be additional APIs - OGC interfaces (WMS, WMTS, WCS), OpenEO, Sentinel Hub API, S3, and others. Please refer to the Roadmap for more information on the timing of these interfaces.\n\n\n\n\nIs there an end user document that describes specific data products and specific services that are available?\n\n\nThe user level details for every service and dataset will be provided in this documentation. In accordance with each and every service and dataset embedded into the ecosystem, this documentation will be updated.\n\n\n\n\nHow long is the project timescale in total?\n\n\nThe time scale of the project is 6 years starting from late 2022 (i.e. to the end of 2028) with an optional extension up to 4 years (i.e. 2032).\n\n\n\n\nCan anyone outside from Europe have free access to any data?\n\n\nYes, data and services will be available to users worldwide and according to the EU Policy. You can visit Terms and Conditions for more information.\n\n\n\n\nIs there any difference between EU users and non-EU users?\n\n\nThere is no difference between EU users and non-EU users. That said, there will be a continuity of the accounts with higher throughput, managed by ESA (i.e. Copernicus Services, International Partners, etc.).\n\n\n\n\nWhich users are qualified for higher tier accounts?\n\nThere are three types of higher tier accounts:\n\n\nCopernicus Services - this account type is meant for European Union institutions and bodies. Organisations working on Copernicus Services are as well included in this tier.\n\n\nCopernicus Collaborative Users - organisations from Copernicus Participating States with an agreement with ESA and European Commission.\n\n\nCopernicus International Users - international partners with an agreement with ESA and European Commission.\n\n\nIf your organisation fits the above-mentioned profile, do submit a request for an account upgrade. If you are not sure about it, do submit a request anyway and it will be evaluated. Please note that Copernicus Data Space Ecosystem monitoring requirements include reporting of aggregated consumption on account level, shared within project stakeholder group. We would also suggest you check the following content:\n\n\nInformation about quotas for specific services\n\n\nThe Ecosystem developers, where users can get access to extended quotas under commercial terms.\n\n\n\n\n\nWhere can I find a forum for support and get more information about the available services and products?\n\n\nYou can find the Copernicus Data Space Ecosystem forum here. Select your category of interest and you might find your question already answered. If not, ask and our team or other users will help you find a solution.\n\n\n\n\nWhere can I find Terms of Use and Privacy Policy?\n\n\nHere are the links to both:\n\n\n\nTerms of Use\n\n\nPrivacy Policy\n\n\n\n\n\nHow can I stay updated and subscribe to your newsletter?\n\n\nYou can stay updated in various ways: subscribe to our Copernicus Data Space Ecosystem monthly newsletter, use our RSS feeds, subscribe to our YouTube channel, and follow us on social media. See this page for more information and links.\n\n\n\n\nWhat license are the products released under?\n\n\nSee Terms and Conditions of the Copernicus Data Space Ecosystem, in particular chapter 3 for detailed information."
  },
  {
    "objectID": "FAQ.html#general",
    "href": "FAQ.html#general",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "In comparison to the existing legacy Copernicus Data Hub, what will be the free services other than stac/cog?\n\n\nCompared to the previously existed Copernicus Data Hub, there will be additional APIs - OGC interfaces (WMS, WMTS, WCS), OpenEO, Sentinel Hub API, S3, and others. Please refer to the Roadmap for more information on the timing of these interfaces.\n\n\n\n\nIs there an end user document that describes specific data products and specific services that are available?\n\n\nThe user level details for every service and dataset will be provided in this documentation. In accordance with each and every service and dataset embedded into the ecosystem, this documentation will be updated.\n\n\n\n\nHow long is the project timescale in total?\n\n\nThe time scale of the project is 6 years starting from late 2022 (i.e. to the end of 2028) with an optional extension up to 4 years (i.e. 2032).\n\n\n\n\nCan anyone outside from Europe have free access to any data?\n\n\nYes, data and services will be available to users worldwide and according to the EU Policy. You can visit Terms and Conditions for more information.\n\n\n\n\nIs there any difference between EU users and non-EU users?\n\n\nThere is no difference between EU users and non-EU users. That said, there will be a continuity of the accounts with higher throughput, managed by ESA (i.e. Copernicus Services, International Partners, etc.).\n\n\n\n\nWhich users are qualified for higher tier accounts?\n\nThere are three types of higher tier accounts:\n\n\nCopernicus Services - this account type is meant for European Union institutions and bodies. Organisations working on Copernicus Services are as well included in this tier.\n\n\nCopernicus Collaborative Users - organisations from Copernicus Participating States with an agreement with ESA and European Commission.\n\n\nCopernicus International Users - international partners with an agreement with ESA and European Commission.\n\n\nIf your organisation fits the above-mentioned profile, do submit a request for an account upgrade. If you are not sure about it, do submit a request anyway and it will be evaluated. Please note that Copernicus Data Space Ecosystem monitoring requirements include reporting of aggregated consumption on account level, shared within project stakeholder group. We would also suggest you check the following content:\n\n\nInformation about quotas for specific services\n\n\nThe Ecosystem developers, where users can get access to extended quotas under commercial terms.\n\n\n\n\n\nWhere can I find a forum for support and get more information about the available services and products?\n\n\nYou can find the Copernicus Data Space Ecosystem forum here. Select your category of interest and you might find your question already answered. If not, ask and our team or other users will help you find a solution.\n\n\n\n\nWhere can I find Terms of Use and Privacy Policy?\n\n\nHere are the links to both:\n\n\n\nTerms of Use\n\n\nPrivacy Policy\n\n\n\n\n\nHow can I stay updated and subscribe to your newsletter?\n\n\nYou can stay updated in various ways: subscribe to our Copernicus Data Space Ecosystem monthly newsletter, use our RSS feeds, subscribe to our YouTube channel, and follow us on social media. See this page for more information and links.\n\n\n\n\nWhat license are the products released under?\n\n\nSee Terms and Conditions of the Copernicus Data Space Ecosystem, in particular chapter 3 for detailed information."
  },
  {
    "objectID": "FAQ.html#data",
    "href": "FAQ.html#data",
    "title": "Frequently Asked Questions",
    "section": "Data",
    "text": "Data\n\n\nWhat data will be offered online and what is the timeline for the following months?\n\n\nFor the details on the data offer and timing, please refer to the Roadmap\n\n\n\n\nIs there a page that indicates anomalies with the datasets?\n\n\nThe Copernicus Sentinel Operations Dashboard provides details of events over the past three months that have impact on the completeness of the data production, such as planned calibration activities, manoeuvres, or anomalies. The information of which data is affected is included.\n\n\n\n\nWith regard to cloud native formats/interfaces, will the data also be available in the original data formats (e.g. for data downloading)?\n\n\nYes, data will also be available in original data formats (i.e. .SAFE).\n\n\n\n\nWill data, such as Sentinel-2, be processed to a consistent version?\n\n\nThe Sentinel-2 data will be available at the latest processing baseline. And with the reprocessing of Sentinel-2 happening in parallel (out of scope of this project), these will become available on this service as well. For more information on Sentinel-2 reprocessing campaign, please follow Copernicus SENTINEL-2 Collection-1 Availability Status\n\n\n\n\nIs it possible to download a subset of data corresponding to an AOI, instead of the whole image?\n\n\nYes, it is possible to download a subset of data, by using the dedicated APIs, i.e. Sentinel Hub, OpenEO.\n\n\n\n\nAre there data offered in the Cloud Optimized Geotiffs (COG) format?\n\n\nYes, Sentinel-1 GRD Level 1 data will be available in COG format.\n\n\n\n\nWhen Online data is mentioned, does that mean the data are not on tape?\n\n\nThe Online data (or Immediate Access Data (IAD)) are not on the tapes. Online data is the type of data where in its metadata it reads “Online”: true. These data can be downloaded directly from the Copernicus Data Space Ecosystem. On the other hand, Offline data (“Online”: false or Deferred Available Data(DAD)) first needs to be ordered via the Data Workspace and only after that step, it can be downloaded.\n\n\n\n\nCan we download the data acquired by all Sentinel missions (1, 2, 3, 5P, 6) and the other satellites (e.g. Meteosat) via the new interface? Considering some missions are not managed by ESA, but by EUMETSAT for example.\n\n\nThe Roadmap shows how the Copernicus Data Space Ecosystem will be continously enhanced in terms of available data.\n\n\n\n\nWill the new interface offer EO ready-to-use products or just L0 and L1 data?\n\n\nUp to L3 products will be available. Please see the Roadmap.\n\n\n\n\nWhat is the highest resolution SAR data available in Copernicus Data Space Ecosystem?\n\n\nThe Sentinel-1 SAR achieves a spatial resolution of approximately 5 by 20 m. More information can be found here.\n\n\n\n\nIs it possible to acquire compressed data?\n\n\nCopernicus Data Space Ecosystem provides access to EO data in zip format without compression(i.e. zero compression applied).\n\n\n\n\nWhich EO data is available?\n\n\nTo learn more about the available EO data, visit our page.\n\n\n\n\nWhere can I browse various data collections?\n\n\nTo browse various data, go to Copernicus Browser. Read about how to use it here.\n\n\n\n\nHow soon after the acquisition is data available?\n\n\nWe put a lot of effort into making data available as soon as possible. The expected timeliness is:\n\n\n\n24 hours after sensing for Sentinel-1 and Sentinel-2\n\n\n3 hours after sensing for Sentinel-3 and Sentinel-5P\n\n\n\nCheck the timeliness metrics in the Dashboard.\n\n\n\n\nWhere can I get a list of the available imagery dates for a specific area?\n\n\nThe best way to get information about all the available scenes in a specific area is to use our Catalog service API, which will give you detailed geospatial information for each tile, that you can control by specifying fields, limits, and other properties. For this specific use case, it is most useful to use the distinct parameter with date, which will return a list of available dates for your requested BBOX and time range. To make this Catalog API request, import the following CURL request to Postman. You will need to authenticate the request with a token.\ncurl -X POST 'sh.dataspace.copernicus.eu/api/v1/catalog/search' \\\n-header 'Authorization: Bearer &lt;your access token&gt;' \\\n-header 'Content-Type: application/json' \\\n-data-raw '{\n   \"bbox\": [13,45,14,46],\n   \"datetime\": \"2019-12-10T00:00:00Z/2020-12-15T00:00:00Z\",\n   \"collections\": [\"sentinel-1-grd\"],\n   \"limit\": 50,\n   \"distinct\": \"date\"\n}'\n\nIn the curl request above, the first line is the URL for the request, which you only need to change if you’re using data collections on different deployments. The first header is where you add your access token; if you authenticate the collection or the request in Postman, the token will be automatically added. The second header is where you specify the desired output, which in this case, is a JSON file. In data-raw, you will specify the bbox, time range (datetime), data collection (collections), limit (the upper limit of the number of results), and with the line “distinct”:“date”, you will limit your results to only include information on the acquisition date. Consult the Catalog API reference to learn more about the available parameters.\n\n\nThe result for the CURL request above will look like this:\n\n\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        \"2019-12-10\",\n        \"2019-12-11\",\n        \"2019-12-12\",\n        \"2019-12-15\"\n    ],\n    \"links\": [\n        {\n            \"href\": \"http://sh.dataspace.copernicus.eu/api/v1/catalog/search\",\n            \"rel\": \"self\",\n            \"type\": \"application/json\"\n        }\n    ],\n    \"context\": {\n        \"limit\": 50,\n        \"returned\": 4\n    }\n}\n\nAt the beginning of the result, you can see all the available dates listed for your request.\n\n\nIt’s also possible to search for available scenes using an OGC WFS request, which might be easier to use but offers less search control. To make a WFS request, you will need to add your INSTANCE_ID, specify the bbox, time range (TIME), data collection (TYPENAMES), coordinate system (srsName), and request type. An example WFS request would look like this:\n\n\nhttps://sh.dataspace.copernicus.eu/ogc/wfs/INSTANCE_ID?REQUEST=GetFeature&srsName=EPSG:3857&TYPENAMES=DSS2&BBOX=3238005,5039853,3244050,5045897&TIME=2019-02-11/2019-02-12\n\nYou can test WFS examples with different parameters and inspect the results instantly here.\n\n\nNote that you can also use the MAXCC parameter (maximum cloud coverage) in this call to filter cloudless data. Use the FEATURE_OFFSET parameter to control the starting point within the returned features and MAXFEATURES parameter - the maximum number of parameters of features to be returned by a single request.\n\n\nAs a result, you will get a list of all the available scenes for the chosen location in JSON format (or XML if set so). Some of the dates may be duplicated if there are two scenes available in the area. Simply ignore these duplications.\n\n\n\n\nHow are the values calculated within Sentinel Hub and how are they returned as an output?\n\n\nIn Sentinel Hub, the calculated and output values depend on what users specify in their evalscripts (or custom scripts). By calculated values we are referring to the values that are returned from the evaluatePixel() function or from a simple script. Output values are values returned from Sentinel Hub, after the calculated values go through formatting defined by sampleType. In the evalscript, calculated and output values are controlled by:\n\n\n\nIn the setup() function, the requested bands and units define what values are used as input for the calculation (in simple scripts, default units are used). For example, if Sentinel–2 band B04 is requested in REFLECTANCE, the input values will be in the range 0–1. If Sentinel-2 band B04 is requested in DN (digital numbers), the input values for the calculation will be in the range 0–10000. Typical value ranges can be found in our data documentation, chapter Units for each data collection.\n\n\nThe evaluatePixel() function defines the actual calculation (in simple scripts, the entire script is its equivalent). Sentinel Hub uses double precision for all calculations and rounds only the final calculated values before they are output.\n\n\nThe value of the sampleType parameter in the setup() function defines the format of the output values. Possible values are AUTO, UINT8, UINT16, and FLOAT32. See our sampleType documentation for more details. When the sampleType is not specified (e.g., in simple scripts), the default value AUTO will be used. sampleType.AUTO takes calculated values from the interval 0–1 and stretches them to 0–255. If your calculated values are not in the range 0–1, make sure you either scale them to this range in the evaluatePixel() function or specify another sampleType.\n\n\nExample 1: NDVI\n\nIn this example, we want to output values of the NDVI index, calculated based on Sentinel-2 data. Our evaluatePixel() function is:\n\n\n    function evaluatePixel(sample) {\n    let NDVI =  (sample.B08 - sample.B04) / (sample.B08 + sample.B04);\n    return [NDVI];\n}\n\nThe requested units in this example do not have any influence on the calculated values of the NDVI. The output values returned by Sentinel Hub for different sampleType values are:\n\n\n\n\n\n\n\n\n\n\n\nCalculated Value\nsampleType.AUTO\nsampleType.UINT8\nsampleType.UINT16\nsampleType.FLOAT32\n\n\n\n\n-1\n0\n0\n0\n-1\n\n\n0\n0\n0\n0\n0\n\n\n0.25\n64\n0\n0\n0.25\n\n\n1\n255\n1\n1\n1\n\n\n\n\nUse sampleType:“FLOAT32” to return full floating -1 to 1 values. See the example here\n\n\nIf you do not need values but a visualization, you can use sampleType:“AUTO”, but make sure to either:\n\n\nMap the NDVI values to the 0–1 interval in the evaluatePixel() function, e.g.:\n\n           \nfunction evaluatePixel(sample) {\n    let NDVI =  (sample.B08 - sample.B04) / (sample.B08 + sample.B04);\n    return [(NDVI + 1) / 2]\n}\n\nUse a visualizer or a color visualization function, e.g. valueInterpolate.\n\n\n Example 2: Sentinel-2 band B04\n\nIn this example, we want to output raw values of Sentinel-2 band 4. Our evaluatePixel() function looks like this:\n\n    function evaluatePixel(sample) {\n    return [sample.B04]\n}\n\nIf we request units: “REFLECTANCE”, the output values returned by Sentinel Hub for different sampleTypes values are:\n\n\n\n\n\n\n\n\n\n\n\nCalculated Value\nsampleType.AUTO\nsampleType.UINT8\nsampleType.UINT16\nsampleType.FLOAT32\n\n\n\n\n0\n0\n0\n0\n0\n\n\n0.25\n64\n0\n0\n0.25\n\n\n0.5\n128\n1\n1\n0.5\n\n\n1\n255\n1\n1\n1\n\n\n1.05\n255\n1\n1\n1.05\n\n\n\n\nIf we request units: “DN”, the output values returned by Sentinel Hub for different sampleTypes values are:\n\n\n\n\n\n\n\n\n\n\n\nCalculated Value\nsampleType.AUTO\nsampleType.UINT8\nsampleType.UINT16\nsampleType.FLOAT32\n\n\n\n\n0\n0\n0\n0\n0\n\n\n2500\n255\n255\n2500\n2500\n\n\n5000\n255\n255\n5000\n5000\n\n\n10000\n255\n255\n10000\n10000\n\n\n10500\n255\n255\n10500\n10500\n\n\n\n Example 3: Brightness Temperature Bands\n\nHere we output a Sentinel-3 SLSTR band F1 with typical values between 250–320 representing brightness temperature in Kelvin. The evaluatePixel() function is:\n\nfunction evaluatePixel(sample) {\n    return [sample.F1]\n}\n\nThe output values returned by Sentinel Hub for different sampleTypes values are:\n\n\n\n\n\n\n\n\n\n\n\nCalculated Value\nsampleType.AUTO\nsampleType.UINT8\nsampleType.UINT16\nsampleType.FLOAT32\n\n\n\n\n250\n255\n250\n250\n250\n\n\n255\n255\n255\n255\n255\n\n\n275.3\n255\n255\n275\n275.3\n\n\n320\n255\n255\n320\n320\n\n\n\n\nUse sampleType:“FLOAT32” to return original values. If integer values are still acceptable for your application, use sampleType:“UINT16”.\n\n\nIf you do not need values but a visualization, you can use sampleType:“AUTO”, but make sure to either:\n\n\nMap the values to the 0–1 interval in the evaluatePixel() function, e.g.:\n\n\nfunction evaluatePixel(sample) {\n    return [sample.F1 / 320]\n}\n\nUse a visualizer or a color visualization function, e.g. valueInterpolate.\n\n\n\n\nHow can I get images in higher resolution?\n\n\nOpen data are always served in their full resolution. Each of the collections has its own maximum resolution, which you can check in our documentation (see here for Sentinel-2). What you get for Sentinel-2 in Copernicus Browser is always a full resolution image. If you go to the effects in Copernicus Browser and enable NEAREST upscaling and downscaling, you will see individual pixels displayed, and you can even measure them to confirm that the pixel resolution indeed matches the maximum resolution of the sensor.\n\n\nIf you’re using the processing API, you can get full resolution by specifying the maximum resolution the collection offers using the resx/resy parameter. See the full resolution example for Sentinel-2.\n\n\n\n\nWhy are some Sentinel images missing?\n\n\nThere can be several reasons for missing data - for example, there could be an error in the processing chain of the Copernicus ground segment, or it could be debris avoidance. Sometimes there are also problems with the satellite itself (e.g., Sentinel-1B failed completely in December 2021 and the data has been unavailable ever since). If there is an interruption in satellite operation, this is logged in ESA’s Events Viewer. If the data cannot be found there, we recommend that you address the question to ESA.\n\n\n\n\nWhy are some areas white?\n\n\nSome areas never have cloud coverage below 20%. This is why in Copernicus Browser, where we have default settings for 30% of cloud coverage, for some areas it seems there is no satellite imagery available.\n\n\n\nHowever, if we turn the slider for maximum cloud coverage to 100%, we can see data available.\n\n\n\nIn the cloudless quarterly mosaic, we use as a background, areas without a cloud-free pixel during the time frame of interest are also displayed in white. For cold season composites, some areas may have had only one cloud-free satellite overpass but during a period of snow cover, therefore patterns of white swaths might reflect Sentinel-2 coverage patterns.\n\n\n\n\nWhy are the images without contrast?\n\n\nWe would like to serve data as it is, without uncontrolled changes, because it is almost impossible to set color balance to one fitting all places in the world and all groups of users. You can still tweak contrast in several ways.\n\n\n\n\nRaw picture\n\n\n\nYou can tweak “Gain” (brightness) to automatically equalize the image. Click on the “Show effects and advanced options” link at the bottom on the left.\n\n\n\n\nGain adjusted to desired values.\n\n\n\nYou can tweak “Gamma” (contrast) as well.\n\n\n\n\nGamma adjusted to desired values.\n\n\n\nUse the atmospheric correction by using the S2L2A atmospherically corrected data instead of S2L1C. As you can see on the image below, atmospheric correction increases the contrast, as it corrects for the effects of the atmosphere.\n\n\n\n\nAtmospheric correction with S-2 L2A\n\n\n\n\n\nHow to get data in ‘native resolution’ in WGS84?\n\n\nRecommendation: Whenever possible, you should work with the data in the same coordinate system as they are produced in, i.e., UTM (in the same zone!) for Sentinel-2. This will prevent the inaccuracy resulting from reprojections, rounding, etc.\n\n\nHowever, if you really want to do it, we recommend the following approach:\n\n\n\nlatitude resolution = real world ground distance resolution / 111226.26\n\n\nlongitude resolution = real world ground distance resolution / 111226.26 / cos(lat)\n\nlat = the latitude of the point at which the conversion needs to take place\n\n\nExample:\n\n\nYou are at (lat, lon) = (45, 10) and want to know how many degrees 10m is:\n\n\n\nlatitude resolution = 10 / 111226.26 ~= 0.000090\n\n\nlongitude resolution = 10 / 111226.26 / cos (45 degrees) ~= 0.000127\n\n\n\nNotes:\n\n\n\nThe coordinate systems are complicated, even more so if you go towards the poles.\n\n\nThe above formulas hold for real world distances; projected distances (web mercator, UTM, etc.) are not the same thing and can be complicated to convert. That said, the UTM distance is usually quite close to the real-world distance, so the formula for converting between UTM and WGS84 should give reasonable results.\n\n\ncos(lat) should be performed with the latitude in radians, not degrees, so it’s actually cos(latDegrees * PI / 180)."
  },
  {
    "objectID": "FAQ.html#services",
    "href": "FAQ.html#services",
    "title": "Frequently Asked Questions",
    "section": "Services",
    "text": "Services\n\n\nAre the free offering and commercial offering integrated to facilitate the transfer of the users from free to commercial?\n\n\nYes, there will be a common user identity, which will allow registered users to seamlessly transfer between two mentioned systems.\n\n\n\n\nWhen we develop an EO ready-to-use product, could we integrate it into the interface and ask the payment from clients?\n\n\nYes, commercial services can be built on top, similar to the Copernicus Open Licence.\n\n\n\n\nCan users come with wish-list on services?\n\n\nYes, users can come up with suggestions to improve or expand the service portfolio. Users can post their improvement suggestions to Community Forum for further evaluation.\n\n\n\n\nIs there any limitation on the maximum number of downloads at a time?\n\n\nYes, there will be quotas and limitations for different user types. For example, the number of concurrent connections limit for the Copernicus General user type is 4, whereas for the Copernicus Services type, it is 20. Please refer to the Quotas and Limitations section of our documentation for more information regarding quotas and limitations apply to the Copernicus General user type.\n\n\n\n\nIs it possible to download Sentinel-2 data for a large area at a high resolution in the Copernicus Browser?\n\n\nDepending on your use, we suggest to use the high-res print (via the high-res print tab) where you will get large areas in a high resolution (the data is though not georeferenced). If you need georeferenced data, split your area in several smaller images that you download or choose a bit lower resolution to stay within the limits of 2500px.\n\n\n\n\nWhere can I find more information regarding the quotas and limits for accessing data and using the services through your platform?\n\n\nPlease refer to the Quotas and Limitations section of our documentation for more information.\n\n\n\n\nWhich entities are qualified to get higher quotas in scope of “Copernicus Services” group and how can one ask for it?\n\nThe following users and initiatives are qualified for the increased quota:\n\n\nInstitutions and organisations developing or operating Copernicus Services\n\n\nEuropean institutions and Bodies set up under the EU Treaties.\n\n\nEach institution or project can have several individuals’ user accounts, which belong to the same organisation and share the account’s quota (i.e. if one of the organisation users will consume full monthly project quota, the rest will not be able to use it further either). Project owners will be able to add further users to this organisation themselves. Organisations can choose the account type based on their preference, i.e. download-optimised or processing-optimized\n\n\nDownload\n\n\nProcessing - small\n\n\nProcessing - medium\n\n\nProcessing - large\n\n\nIn order to request for increased quota, fill out this form and provide the following information:\n\n\naccount’s e-mail (you have to register before submitting this request, do use your organisation’s e-mail domain)\n\n\norganisation name (including department, if relevant)\n\n\nproject name (i.e. Copernicus Services contract reference, project reference, etc.)\n\n\nproject start date\n\n\nplanned project end date\n\n\ninstitutional contact (i.e. contract officer at European Commission)\n\n\npreferred type of the account (“Download”, “Processing-small”, “Processing-medium”, “Processing-large”)\n\n\nnotes\n\n\nPlease note that Copernicus Data Space Ecosystem monitoring requirements include reporting of aggregated consumption on account level, shared within project stakeholder group.\n\n\n\nWhat does a “processing unit” mean?\n\n\nA definition of a processing unit is available in our documentation. The rules with examples of how to calculate the number of processing units for a request are also provided.\n\n\n\n\nWhat are the general rules of “processing units” and “requests” in the scope of Sentinel Hub?\n\n\nThe processing units and requests included in your Sentinel Hub plan are reset on the first day of each month. The unused processing units and requests are not carried over to the next month. If the usage exceeds the assigned quota, the user will be moved to a slower interface. Such a user will still be able to access and download products but at a reduced speed.\n\n\nVisit our webpage for more details about the processing units and rate limiting.\n\n\nMore about the quotas and limitations is also available here.\n\n\n\n\nWhat can I do if I run out of download quota, processing units, or requests in the middle of the month?\n\n\nEvery user account is set a limited quota to guarantee fair sharing of free tier resources within all users of the Copernicus Data Space Ecosystem.\n\n\nThe total volume of downloads from the Copernicus Data Space Ecosystem for each user account is summed up for the last 30 days. Downloaded volume is checked on an hourly basis, and if it exceeds the assigned quota, the user will be moved to a slower interface. Such a user will still be able to access and download products but at a reduced speed. More about the quotas and limitations is also available here.\n\n\nIf you run out of quota, we recommend exploring some of the commercial services integrated with CDSE.\n\n\nFor eligible users (i.e., Copernicus Services users, European institutions, etc.), we recommend submitting a request for a Copernicus Service account. The form to change your Copernicus user type is available here.\n\n\n\n\nWhich are typical conversions between processing units and square kilometers?\n\n\n\n\n\n\n\n\n\n\nMission\nProduct\nResolution [m]\nkm² for one PU\n\n\n\n\nSentinel-2\nTrue color\n10\n26.2\n\n\n\nNDVI\n10\n39.3\n\n\n\nAll 13 bands\n10\n6.0\n\n\nSentinel-1\nVV gamma0 ortho\n20\n157.3\n\n\nSentinel-3 OCI\nTrue color\n300\n23,593.0\n\n\n\nOTCI\n300\n35,389.4\n\n\n\n\n\n\nHow can I know how many processing units I need and how to calculate the consumption of my requests?\n\n\nEach request is worth a certain amount of processing units. If we only focus on rate limiting, the quota you need depends on how many processing units (PU) and requests per minute or month you intend to use. This in turn depends on how large your images are in pixels, how many of them you order, how many bands you order, the format used and whether some more complex processing options are enabled.\n\n\nSee documentation on how processing units are defined and calculated here. The following are detailed calculation examples for Sentinel-1, exploring three different scenarios, including a simple 1 image request, many small requests, long time ranges and large area processing.\n\n\nExample 1\n\n\n1 request that outputs an image 2000 x 2000 px large, with 2 bands ordered, 16-bit tiff format and orthorectification applied, would result in 20.34 PUs used. See the multiplication factors and the calculation below:\n\n\n\n\n\n\n\n\n\nParameter\nYour Parameters\nMultiplication Factor\n\n\n\n\nOutput image size (width x height)\n2000 x 2000 = 4,000,000\n/ (512 x 512)\n\n\nNumber of input bands\n2\n/ 3\n\n\nOutput format\n16-bit INT/UINT\nx1\n\n\nNumber of data samples per pixel\n1\nx1\n\n\nOrthorectification\nYes\nx2\n\n\n\n\nThe calculation: (4,000,000 / 262,144) x (2 / 3) x 1 x 1 x 2 = 20.34 PU per request;\n\n\nIf you wanted to get 500 images (e.g. for time series), you would need to create 500 separate processing requests. If parameters for each image are equal as above, we can simply multiply our calculation with 500 and see that 500 similar requests would use about 10,172 PU in sum.\n\n\nThe calculation: 20.34 PU x 500 = ~10,172 PU per 500 requests\n\n\nIf your output image were smaller, e.g. 500 x 500 px large, the cost of a single request would be 1.27 PU. Multiplied by 500, that’s just 635.78 PU in total, so much less. On the other hand, if you wanted an image with quite a large output of 5000 x 5000 px, a request would cost you 127.15 PU, and 500 of these would cost ~63,578 PU. However, processing API is limited to output images with width and height up to 2500 px, so setting it to 5000 would result in an error. To output images this large, you would need a Copernicus Service account or a commercial Sentinel Hub account and bucket on CREODIAS to use batch processing API. You will notice that batch processing divides your PU cost by 3, as it’s only a third of the price of processing API. So your 500 requests would cost 63,578 / 3 = 21,192 PU.\n\n\nExample 2\n\n\nIf you wanted to create a single request that uses 500 scenes in a multi-temporal script, where you e.g. calculate max NDVI over several years, using up 500 acquisitions, your request would also use 10,172 PU, just like our request for 2000 px x 2000 px output in Example 1 (given that all the other parameters are the same as in Example 1).\n\n\n\n\n\n\n\n\n\nParameter\nYour Parameters\nMultiplication Factor\n\n\n\n\nOutput image size (width x height)\n2000 x 2000 = 4,000,000\n/ (512 x 512)\n\n\nNumber of input bands\n2\n/ 3\n\n\nOutput format\n16-bit INT/UINT\nx1\n\n\nNumber of data samples per pixel\n500\nx500\n\n\nOrthorectification\nYes\nx2\n\n\n\n\nThe calculation: (4,000,000 / 262,144) x (2 / 3) x 1 x 500 x 2 = 10,172 PU\n\n\nThe issue here is that you would use more than 2,000 PU in a single minute, as it’s just one request, and use this heavy isn’t supported by any package. Each package specifies how many PUs and requests you can use per month, as well as per minute. Your request would time out or fail. If you’re interested in very large time ranges, it’s best to use batch processing API.\n\n\nExample 3\n\n\nLet’s suppose you want to order an orthorectified 16-bit Sentinel-1 image with 2 bands for the whole Australia in full resolution, which let’s say is covered with a 4000 x 4000 km bounding box, which equals to 4 million meters x 4 million meters. As Sentinel-1 resolution is 10 meters, we know that our output image will have to be 400,000 pixels x 400,000 pixels large.\n\n\n\n\n\n\n\n\n\nParameter\nYour Parameters\nMultiplication Factor\n\n\n\n\nOutput image size (width x height)\n400,000 x 400,000 = 160,000,000,000\n/ (512 x 512)\n\n\nNumber of input bands\n2\n/ 3\n\n\nOutput format\n16-bit INT/UINT\nx1\n\n\nNumber of data samples per pixel\n1\nx1\n\n\nOrthorectification\nYes\nx2\n\n\n\n\nThe calculation: (160,000,000,000 / 262,144) x (2 / 3) x 1 x 1 x 2 = 813,802 PU.\n\n\nProcessing API is limited to 2500 pixels for output width and height, so you would have to run this request with batch processing API. To use batch processing API, you would need a Copernicus Service account. As batch processing is cheaper, your batch processing request would use 813,802 / 3 = 271,267 PU. And because batch processing is asynchronous and takes a while to ingest the tiles, you won’t be stopped by per minute rate limiting for a single request."
  },
  {
    "objectID": "FAQ.html#registration-and-authentication",
    "href": "FAQ.html#registration-and-authentication",
    "title": "Frequently Asked Questions",
    "section": "Registration and authentication",
    "text": "Registration and authentication\n\n\nI’m having troubles with registering, what can I do?\n\n\nIf you are having troubles with Recaptcha or receiving the initial confirmation email while registering, please send an e-mail to help-login@dataspace.copernicus.eu for direct support.\n\n\n\n\nI’m having problems when I try to login or submit a request in the Help Center. How can I solve it?\n\n\nThis may be due to your browser settings. Please make sure Enhanced Tracking Protection is turned off and in the Privacy&Security section of your browser, make sure the option Standard (not Strict) is selected. If these doesn’t solve your issue. please contact our support team via Submit a request."
  },
  {
    "objectID": "FAQ.html#apis",
    "href": "FAQ.html#apis",
    "title": "Frequently Asked Questions",
    "section": "APIs",
    "text": "APIs\n\n\nWhere can I find detailed information regarding the duration of Access tokens and Refresh tokens?\n\n\nThe access token is valid for 10 minutes, after 10 minutes, it expires. When Access Token expires, it must  -either be refreshed by using the Refresh Token, -or be re-generated. The refresh token is valid for 60 minutes and can be used multiple times within that 60 minutes. The returned access token is again valid for 10 mins. Please refer to the Quotas and Limitations section of our documentation for more information. Also please refer to the OData section of our documentation for more information on Access and Refresh Tokens.\n\n\n\n\nWill Long Term Archive (LTA) process be discontinued when all archived data become Online?\n\n\nThere will still be services available for so called “deferred data access” : data collections that are not commonly used. That said, all of the most relevant collections will be available Online. The Roadmap shows how the Copernicus Data Space Ecosystem will be continously upgraded and how more data become available.\n\n\n\n\nWill the platform use STAC standards?\n\n\nYes, STAC Product Catalog is already available. However there may be issues with using the current version with generic STAC libraries. Our dedicated teams are actively working on its development to ensure a seamless experience for all users. Nevertheless it already supports basic product search.\n\n\n\n\nAny plan to offer the Pangeo platform for a “pythonist”?\n\n\nThis is currently not in the offer or roadmap.\n\n\n\n\nWhich one amongst the 4 catalog APIs (OData, STAC, OpenSearch, Sentinel Hub catalogue) is updated first when new products are published?\n\n\nOpenSearch, OData and STAC catalog APIs all use the same backend database. Sentinel Hub catalog API contains a subset of the collections, hence it works only for the ones that have been imported to Sentinel Hub. Therefore there is no first updated one.\n\n\n\n\nWhat is the limitation of the number of requests that I can do at the time?\n\n\nFor concurrent requests and other limitations please refer to the Quotas and Limitations section of our documentation for more information as the limits are different for each user type.\n\n\n\n\nDo you have to authenticate for requesting through OpenSearch API?\n\n\nThere’s no need to use any user or authentication when you want to search. User authentication is required for downloading products.\n\n\n\n\nCan we use the Sentinel Hub bucket and fetch the products based on the id we obtained from OpenSearch API?\n\n\nYes, you can use Sentinel Hub bucket in addition to some programming tools by providing product ID obtained by using OpenSearch API or OData of the Copernicus Data Space Ecosystem.\n\n\n\n\nMessage:“This request was rejected due to a violation”\n\n\nWhen implementing UDF in openEO that fetch files within it from a different source, an error is encountered. Failed to parse API error response: [403] ‘{“status”:“error”,“data”:{“message”:“This request was rejected due to a violation. Please consult with your administrator, and provide this reference ID: 12308030347893165864”}}’ Please contact our support team via Submit a request. Make sure to provide the reference ID from the error message.\n\n\n\nS3 API\n\n\nIs a STAC catalog planned ? Will the data be accessible on cloud object storage (S3)?\n\n\nSTAC Catalog API is indeed planned. The data is already available over S3 as well.\n\n\n\n\nHow do I generate S3 access and secret keys?\n\n\nPlease refer to the Access to EO Data via S3 page for the guidance on generating S3 access and secret keys.\n\n\n\n\nCan I connect directly to the S3 bucket using AWS S3 commands with the S3 keys provided or do I have to use “s3cmd” to download images?\n\n\nYes, you can connect to S3 bucket using AWS S3 connection. However some functionality may not be supported. It is recommended to use the ‘s3cmd’ command to download products.\n\n\n\n\nWhat is the benefit of fetching imagery from Copernicus Data Space S3 bucket?\n\n\nDownloading products via S3 is faster as it delivers products as a .zip archive, skipping the need of zipper.\n\n\n\n\nOn which region resides the Copernicus Data Space S3 bucket?\n\n\nThe repo is located in Warsaw/Poland.\n\n\n\n\nHow can I search for the product in S3 bucket?\n\n\nSearching via ID or product name in the OpenSearch or OData will give the S3 path to the product in response.\n\n\n\n\nBring Your Own Data\n\n\nHow can I best prepare my data for Sentinel Hub processing?\n\n\nTo prepare data, check out our documentation here.\n\n\n\n\nIs my data copied somewhere else?\n\n\nSentinel Hub does not keep any copies of your own data. During the process of ingestion, we will read each file once to establish the index. The index will be stored on our side until you decide to remove the data from Sentinel Hub. Whenever a request comes, we will read the relevant parts of the file to provide results, keeping it in memory. Results are streamed to the user, and memory contents are discarded.\n\n\nIn no step of this process is any data stored as a file, not even temporarily. Once you decide to remove the data, we will delete all information we have about it.\n\n\n\n\nWMS, OGC and API\n\n\nWhere can I get the INSTANCE ID?\n\nFind your INSTANCE ID in the CDSE Dashboard - Configuration Utility. See example below:\n\n\n\n\nWhich parameters are available in OGC requests?\n\n\nThe list of the available standard OGC URL parameters is available here and the advanced URL parameters are available here.\n\n\n\n\nHow can I get point values from the services?\n\n\nIn case you would like to get point values at a specific location (e.g., reflectance, NDVI, etc.), you can use the GetFeatureInfo request. This is often labeled as “Identify features” or “feature info” in various GIS applications.\n\n\nHere is an example of such a request:\n\n\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&I=0&J=0&QUERY_LAYERS=NDVI&INFO_FORMAT=application/json&BBOX=38.55105530425345,-7.883667078518689,38.55269383803389,-7.885252872445627&CRS=EPSG:4326&MAXCC=100&WIDTH=1&HEIGHT=1&TIME=2017-03-16\n\n\nCheck also the Statistical API for additional information.\n\n\n\n\nHow can I get all the images from an area of interest in a desired time period?\n\n\nUse Sentinel Hub WFS request and retrieve all relevant geometries for a given bounding box and time frame. From the response, gather the unique dates. For each date, construct a WCS request to retrieve the image.\n\n\nSee the code example below:\n\n// using Sentinel Hub OGC web services - https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/OGC.html\n// config\nconst CDSE_SH_INSTANCE_ID = '&lt;INSTANCE-ID&gt;';\nconst layerName = '1_NATURAL_COLOR';\nconst timeFrom = '2015-01-01';\nconst timeTo = '2017-04-20';\nconst bbox = '-410925.4640611076,4891969.810251283,-391357.58482010243,4911537.689492286';\nconst maxFeatures = 100; // 100 is max\n\n// typenames parameter depends on the collection that is used in the layer\n// https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/OGC/WFS.html#typenames\nlet wfsUrl = `https://sh.dataspace.copernicus.eu/ogc/wfs/${CDSE_SH_INSTANCE_ID}?service=WFS&version=2.0.0&request=GetFeature&time=${timeFrom}/${timeTo}/P1D&typenames=DSS2&maxfeatures=${maxFeatures}&srsname=EPSG:3857&bbox=${bbox}&outputformat=application/json`;\n\nasync function getGeometries() {\n  // retrieving\n  // relevant geometries/images in bbox in timespan [timeFrom, timeTo]\n  // Sentinel Hub - WFS request - https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/OGC/WFS.html#wfs-request\n  try {\nlet response = await fetch(wfsUrl);\nlet data = await response.json();\nreturn data;\n  } catch (e) {\nthrow new Error('There was an error fetching the list of geometries from WFS service.\\nDid you substitute your SENTINEL_HUB_INSTANCE_ID ? ');\n  }\n}\n\nfunction parseDatesFromGeometries(geometries) {\n  // parsing\n  // relevant geometries -&gt; all relevant dates\n\n  if (geometries.features === undefined) {\ngeometries.features = [];\n  }\n  const dates = geometries.features.map(value =&gt; value.properties.date);\n  return Array.from(new Set(dates)); // return unique dates\n}\n\nfunction generateWcsUrlsFromDates(dates) {\n  // mapping\n  // dates -&gt; image url\n  // images available via WCS request - https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/OGC/WCS.html#wcs-request\n\n  const imageUrls = [];\n  dates.forEach(date =&gt; {\nconst niceName = encodeURIComponent(`${layerName} from ${date}.tiff`);\nconst size = 512;\nconst imageUrl = `https://sh.dataspace.copernicus.eu/ogc/wcs/${CDSE_SH_INSTANCE_ID}?service=WCS&version=1.1.2&request=GetCoverage&time=${date}&coverage=${layerName}&nicename=${niceName}&bbox=${bbox}&width=${size}&height=${size}}`;\nimageUrls.push(imageUrl);\n  });\n  return imageUrls;\n}\n\nfunction shout(value) {\n  console.log('Images', value);\n}\n\nasync function getImages() {\n  const geometries = await getGeometries();\n  const dates = parseDatesFromGeometries(geometries);\n  const imageUrls = generateWcsUrlsFromDates(dates);\n  shout(imageUrls);\n}\n\ngetImages();\n\n\n\nIf we do not specify the time range in the endpoint URL, can we assume that the images returned by the WMS service are the latest acquired?\n\n\nImages are ordered by “Mosaic order” priority (see Configuration Utility, can be set for each layer; you can also set this in a parameter). You can choose either to have the most recent on top or least cloud coverage. You should also consider the “maximum cloud coverage” parameter. In case you want to get the most recent images acquired, you should set maximum cloud coverage to 100% and priority to “most recent”.\n\n\n\n\n\nWhy is the result different when I am using WMS or WCS, when the coordinate system is EPSG:4326?\n\n\nOGC consortium specifications for WMS, WFS, WCS, … services define the coordinate axis order. Older OGC service specifications assumed “X, Y” order for all coordinate reference systems (CRSs), even for WGS84 (EPSG:4326), while newer OGC service specifications obey the axis orders defined by the CRS’s, not assuming “X, Y” order anymore.\n\n\nOur services conform to the standards definitions. The WGS84 axis order is thus version dependent:\n\n\nWMS:\n\nversion 1.1.1: longitude, latitude\nversion 1.3.0: latitude, longitude\n\nWFS:\n\nversion 1.0.0: longitude, latitude\nversion 2.0.0: latitude, longitude\n\nWCS:\n\nversion 1.0.0: longitude, latitude\n\n\n\nThe user should always request a specific version of OGC services by providing an explicit “VERSION” parameter in the URL.\n\n\n\n\nCan I access Sentinel Hub using HTTP or HTTPS?\n\n\nAll Sentinel Hub services are available using both HTTP and HTTPS protocols.\n\n\n\n\nWhy am I getting 429 errors?\n\n\nTo prevent overloading the system and to improve user experience, we have a rate limiting system in place. Find more info about it here. In case you need a higher limit, contact support.\n\n\n\n\nIs it possible to integrate the API in public-facing applications without revealing the security credentials?\n\n\nThis is not facilitated by us at the moment; you will have to implement your own authentication service."
  },
  {
    "objectID": "FAQ.html#documentation",
    "href": "FAQ.html#documentation",
    "title": "Frequently Asked Questions",
    "section": "Documentation",
    "text": "Documentation\n\n\n\nWhich distribution channels will be available for high-throughput data access? Does the public side have user tiers, or is high-throughput data transfer (such as EODATA ) only a paid service?\n\n\n\nAll distribution options (i.e. OData, S3, Sentinel Hub,..) will be constrained with user quotas, which includes both bandwidth limitation, as well as monthly data transfer limits. Please refer to the Quotas and Limitations section of our documentation for more information.\n\n\n\n\nWhere can I find more information regarding the cost of the “extra” services?\n\n\nPricing will be published soon.\n\n\n\n\nIt is indicated that “For those interested in processing, there will be scalable cloud resources available, optimized for EO tasks”. Does this refer to the current CreoDIAS resources, or something completely new that hasn’t been addressed yet?\n\n\nThis December advertisement of the Copernicus Data Space Ecosystem indicates that scalable cloud resources will be part of the commercial offering and can be obtained at CREODIAS in first instance. ICT-wise, there will be two options, including Open Telekom Cloud.\n\n\n\n\nAre there tutorials (online & physical meetings) to use the new interface?\n\n\nTutorials will be added to the documentation in due time explaining the usage of the different interfaces. We will also be present on different conferences explaining the service & ecosystem."
  },
  {
    "objectID": "FAQ.html#copernicus-browser",
    "href": "FAQ.html#copernicus-browser",
    "title": "Frequently Asked Questions",
    "section": "Copernicus Browser",
    "text": "Copernicus Browser\n\n\nWhere can I learn more about the Copernicus Browser?\n\n\nVisit our documentation page for details about the Browser.\n\n\n\n\nHow can I find the exact location by knowing the lat/lon coordinates?\n\n\nSimply write coordinates in the “Search places” tool in “lat,lon” form, e.g.: 46.0246,14.5367 And click enter.\n\n\n\n\nHow can I make long time-lapses?\n\n\nAs Copernicus Browser timelapse functionality has a limit of processing up to 300 images at once, we need to do some post-processing, to be able to create longer timelapses. First, use command prompt to convert .gif timelapses from Copernicus Browser into .mp4 files and then merge them together. For example, if we want to create a five-years long timelapse from daily Sentinel-3 data, six timelapses, each 300 days long, are required. You should first download your smaller timelapses from Copernicus Browser. After downloading the gifs from Copernicus Browser, we should transform them into .mp4 files. To do so, we first need to download and install the ffmpeg program for making videos. Follow this tutorial to install ffmpeg. To test if ffmpeg is installed, go to CMD and type: ffmpeg -version. To transform a gif into mp4, first navigate to the folder where your gif files are stored, then open your command prompt and write in the following code:\n\nffmpeg -i GIF1.gif -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" GIF1vid.mp4\nThe only part of the code you need to change is the name of your gif at the start and the name of the output .mp4 video at the end. You need to transform all your .gif time-lapses into separate .mp4 files:\nffmpeg -i GIF1.gif -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" GIF1vid.mp4&lt;br&gt;\nffmpeg -i GIF2.gif -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" GIF2vid.mp4&lt;br&gt;\nffmpeg -i GIF3.gif -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" GIF3vid.mp4\nFinally, merge all the .mp4 files into a final .mp4 file with the following code:\nffmpeg -i GIF1vid.mp4 -i GIF2vid.mp4 -i GIF3vid.mp4 -filter_complex \"[0:0] [1:0] [2:0] concat=n=3:v=1:a=0\" FinalVideo.mp4\nIn this code, we specify the output name of the final .mp4 timelapse and list the .mp4 files we want to merge. We preface the .mp4 files with -i. Then we need to add as many arrays as there are input .mp4 videos; [0:0] for the first one, [0:1] for the second one, [0:3] for the third one and so on. The n=3 indicates how many input .mp4 files there are. For 6 input .mp4 files, the code would look like this:\nffmpeg -i GIF1vid.mp4 -i GIF2vid.mp4 -i GIF3vid.mp4 -i GIF4vid.mp4 -i GIF5vid.mp4 -i GIF6vid.mp4 - filter_complex \"[0:0][1:0][2:0][3:0][4:0][5:0] concat=n=6:v=1:a=0\" FinalVideo.mp4\n\n\n\nHow can I configure my layers for statistical information in Copernicus Browser?\n\n\nIn Copernicus Browser, you can view two kinds of statistical information for your imagery: a line chart of values through time for either a point or a polygon, and a histogram of values (see the chapter Statistical Analysis on our Copernicus Browser page). To enable statistical features, an evalscript supporting statistical API should be used in your layers. Because statistics operate on the indicator values themselves (and not on RGB), you must add an additional output to the evalscript, which returns the value without conversion to color. For example, statistics can be calculated for values of B04 or for an index like NDVI. Additionally, the evalscript should include dataMask to exclude no-data values from the calculation (no-data pixels often have value 0, which could skew the statistics), and it’s also recommended to check for presence of clouds, so we can exclude them as well (cloudy pixels usually have very high values, which could also skew the results). The user can exclude any pixels from the statistics, such as, for example, snow or water.\nThe evalscript should contain the following outputs, with the exact same names:\n\n\ndefault: Mandatory. A visualization output that defines what will be displayed on the map. The visualization can return anything you like, a single value grayscale, an RGB visualization, etc.\nindex: Optional. Returning the raw values of your index, enabling the histogram functionality. Use FLOAT32 format to include decimals and negative values.\neobrowserStats: Mandatory. An output containing both the index values and cloud information, used for line chart generation. Cloud information will allow us to filter the tiles by maximum cloud coverage. Use FLOAT32 format to include decimals and negative values.\ndataMask: Mandatory. A dataMask output to take no-data values into account.\nHere is an example evalscript for NDVI:\n\n//VERSION=3\nfunction setup() {\n   return {\n       input: [\"B04\", \"B08\", \"SCL\",\"dataMask\"], \n       //The four outputs should be setup here in the setup function, and defined under the evaluatePixel function. \n       output: [\n         { id: \"default\", bands: 4 }, \n         { id: \"index\", bands: 1, sampleType: 'FLOAT32' },\n         { id: \"eobrowserStats\", bands: 2, sampleType: 'FLOAT32' }, \n         { id: \"dataMask\", bands: 1 }\n       ]\n     };\n}\n// Optional filtering for clouds using the Sentinel-2A SCL band (values 1, 2, 7, 8, 9, 10, 11 are either clouds or snow)\nfunction isCloud (scl) {\n if ([1, 2, 7, 8, 9, 10, 11].includes(scl))  {\n   return false;\n } \n}\nfunction evaluatePixel(samples) {\n   let val = index(samples.B08, samples.B04); // NDVI calculation\n   let imgVals = null; \n   const indexVal = samples.dataMask === 1 ? val : NaN; //NDVI index without no-data values\n   const cloud = isCloud(samples.SCL) //calling in our cloud filtering functon as a cloud variable\n\n   //Define the RGB NDVI visualization and write it into imgVals\n   if (val&lt;-0.5) imgVals = [0.05,0.05,0.05,samples.dataMask];\n   else if (val&lt;-0.2) imgVals = [0.75,0.75,0.75,samples.dataMask];\n   else if (val&lt;-0.1) imgVals = [0.86,0.86,0.86,samples.dataMask];\n   else if (val&lt;0) imgVals = [0.92,0.92,0.92,samples.dataMask];\n   else if (val&lt;0.025) imgVals = [1,0.98,0.8,samples.dataMask];\n   else if (val&lt;0.05) imgVals = [0.93,0.91,0.71,samples.dataMask];\n   else if (val&lt;0.075) imgVals = [0.87,0.85,0.61,samples.dataMask];\n   else if (val&lt;0.1) imgVals = [0.8,0.78,0.51,samples.dataMask];\n   else if (val&lt;0.125) imgVals = [0.74,0.72,0.42,samples.dataMask];\n   else if (val&lt;0.15) imgVals = [0.69,0.76,0.38,samples.dataMask];\n   else if (val&lt;0.175) imgVals = [0.64,0.8,0.35,samples.dataMask];\n   else if (val&lt;0.2) imgVals = [0.57,0.75,0.32,samples.dataMask];\n   else if (val&lt;0.25) imgVals = [0.5,0.7,0.28,samples.dataMask];\n   else if (val&lt;0.3) imgVals = [0.44,0.64,0.25,samples.dataMask];\n   else if (val&lt;0.35) imgVals = [0.38,0.59,0.21,samples.dataMask];\n   else if (val&lt;0.4) imgVals = [0.31,0.54,0.18,samples.dataMask];\n   else if (val&lt;0.45) imgVals = [0.25,0.49,0.14,samples.dataMask];\n   else if (val&lt;0.5) imgVals = [0.19,0.43,0.11,samples.dataMask];\n   else if (val&lt;0.55) imgVals = [0.13,0.38,0.07,samples.dataMask];\n   else if (val&lt;0.6) imgVals = [0.06,0.33,0.04,samples.dataMask];\n   else imgVals = [0,0.27,0,samples.dataMask];\n\n   // Return the 4 inputs and define content for each one\n   return {\n     default: imgVals,\n     index: [indexVal],     \n     eobrowserStats: [indexVal,cloud?1:0],\n     dataMask: [samples.dataMask]\n   };\n}\nSee the script in Copernicus Browser. The evalscript can be used directly in Copernicus Browser using the custom script function, or setup for the layer within the Configuration Utility. When you create a custom layer, it is possible to select the same layer in Copernicus Browser. Follow the steps below.\n\nLog into Copernicus Browser and select your configuration from the dropdown menu in Configurations in the Visualize tab on the left.\nSelect the desired data source (if all layers in your configuration have the same data source, only one will be available).\nMake sure to set the appropriate date and zoom in to the area where your data is available.\nIf your configuration has more layers, they will be available for selection under the Layers in the Visualize tab on the left.\nWhen you have your layer displayed, draw a point or a polygon on the map and inspect the statistics features."
  },
  {
    "objectID": "FAQ.html#integrate-data-into-gis-and-web-applications",
    "href": "FAQ.html#integrate-data-into-gis-and-web-applications",
    "title": "Frequently Asked Questions",
    "section": "Integrate Data into GIS and Web applications",
    "text": "Integrate Data into GIS and Web applications\n\n\nWhy can’t I see OGC layers displayed in ArcGIS Online?\n\n\nThere are two map viewers currently available: ArcGIS Map Viewer Classic and ArcGIS Map Viewer. You can try both; we noticed that layers are more likely to work in the new ArcGIS Map Viewer. You can also try to use WMTS instead of WMS, as it’s expected to work better.\n\n\nArcGIS also suggests configuring its CORS settings to allow specific domains to communicate with the server, as described in this ArcGIS Online documentation.\n\n\n\n\nHow can I integrate data with AcuGIS?\n\nFollow these steps to integrate your data with AcuGIS:\n\nGo to https://canvas.acugis.com/\n\nSign up and login to Canvas.\nClick on WMS Services in the left menu.\n\n\n\nClick Add WMS Layer.\n\n\n\nEnter the WMS Service information for Sentinel in the URL field.\n\n\n\nGo to Projects and click the Add Project button.\nIn the Project you created, click the Maps button.\n\n\n\nClick the Add Map button.\nIn the left menu of the new map, expand the WMS layer you created in step 4 and toggle the desired layer(s), such as the NDVI layer.\n\n\n\nDefine the Geometry Area:\n\nThere are two methods you can use to define a Geometry Area:\n - A data layer\n - Draw a Polygon or Rectangle area\nWhile there are some simple use-cases in which you might wish to draw an area, in most cases, you will be using a data layer. Below, we are using a data layer of the state of Rhode Island.\n\nClick the settings icon on the NDVI layer you created in step 9 above. Select ‘Geometry’ from the menu as shown below.\n\n\n\nSelect the Geometry Area:\n\nFor a data layer (such as we are using), select Custom as the Geometry Area.\nFor a polygon you have created via the free-hand toolbar, select Polygon.\n\n\n\n\nClick Save and then click the Save button for your map.\nCongratulations! Your map is now ready.\n\n\n\n\n\n\nWhy does an error pop up when I add WMS to ArcMap?\n\n\nIn some cases, ArcMap turns on all layers and calls WMS. Sentinel Hub WMS service does not support visualization of many layers at the same time. We suggest you turn off all layers and turn them on one by one.\n\n\nIf you get the following error:\n\n\n\nTurn off all layers and turn them on one by one:"
  },
  {
    "objectID": "FAQ.html#image-download-and-custom-legends",
    "href": "FAQ.html#image-download-and-custom-legends",
    "title": "Frequently Asked Questions",
    "section": "Image Download and Custom Legends",
    "text": "Image Download and Custom Legends\n\n\nHow can I remove the Sentinel Hub logo from imagery?\n\n\nYou can turn off the logo in the Configuration Utility - just open “Instance configuration” and uncheck “Show logo”.\n\n\n\n\n\nCan I add a legend to a downloaded image?\n\n\nA legend can be added to the exported image by turning on the option “Show legend” when exporting an image.\n\n\n\n\nHow can I define a custom legend?\n\n\nA legend may be defined in the advanced layer editor.\n\n\n\n\nAdd the “legend” object in the parent “styles”. A legend is specified as a JSON object that must have a property named “type” with value “continuous” or “discrete”, depending on a legend type. A continuous legend also needs properties “minPosition” and “maxPosition” of a floating-point type, and an array that contains stop points of a gradient. Each point must have a position of a floating-point type and a color in rgb hexadecimal format. A color can also be specified using the rgb function that accepts percentage values for rgb triplet, just like in CSS language. A point can optionally have a label. Here is a JSON object example of a continuous legend and the legend itself:\n\n{\n \"type\": \"continuous\",\n \"minPosition\": 0.0,\n \"maxPosition\": 1.05,\n \"gradients\": [\n  { \"position\": 0, \"color\": \"rgb(0%,0%,0%)\" },\n  { \"position\": 0.05, \"color\": \"rgb(0%,0%,0%)\", \"label\": \"- 1.0\" },\n  { \"position\": 0.050001, \"color\": \"rgb(75%,75%,75%)\" },\n  { \"position\": 0.1, \"color\": \"rgb(75%,75%,75%)\" },\n  { \"position\": 0.10001, \"color\": \"rgb(86%,86%,86%)\" },\n  { \"position\": 0.15, \"color\": \"rgb(86%,86%,86%)\" },\n  { \"position\": 0.150001, \"color\": \"rgb(100%,100%,88%)\" },\n  { \"position\": 0.2, \"color\": \"rgb(100%,100%,88%)\", \"label\": \"0.0\" },\n  { \"position\": 0.20001, \"color\": \"rgb(100%,98%,80%)\" },\n  { \"position\": 0.25, \"color\": \"rgb(100%,98%,80%)\" },\n  { \"position\": 0.250001, \"color\": \"rgb(93%,91%,71%)\" },\n  { \"position\": 0.3, \"color\": \"rgb(93%,91%,71%)\" },\n  { \"position\": 0.30001, \"color\": \"rgb(87%,85%,61%)\" },\n  { \"position\": 0.35, \"color\": \"rgb(87%,85%,61%)\" },\n  { \"position\": 0.350001, \"color\": \"rgb(80%,78%,51%)\" },\n  { \"position\": 0.4, \"color\": \"rgb(80%,78%,51%)\" },\n  { \"position\": 0.40001, \"color\": \"rgb(74%,72%,42%)\" },\n  { \"position\": 0.45, \"color\": \"rgb(74%,72%,42%)\" },\n  { \"position\": 0.450001, \"color\": \"rgb(69%,76%,38%)\" },\n  { \"position\": 0.5, \"color\": \"rgb(69%,76%,38%)\" },\n  { \"position\": 0.50001, \"color\": \"rgb(64%,80%,35%)\" },\n  { \"position\": 0.55, \"color\": \"rgb(64%,80%,35%)\" },\n  { \"position\": 0.550001, \"color\": \"rgb(57%,75%,32%)\" },\n  { \"position\": 0.6, \"color\": \"rgb(57%,75%,32%)\", \"label\": \"0.2\" },\n  { \"position\": 0.60001, \"color\": \"rgb(50%,70%,28%)\" },\n  { \"position\": 0.65, \"color\": \"rgb(50%,70%,28%)\" },\n  { \"position\": 0.650001, \"color\": \"rgb(44%,64%,25%)\" },\n  { \"position\": 0.7, \"color\": \"rgb(44%,64%,25%)\" },\n  { \"position\": 0.70001, \"color\": \"rgb(38%,59%,21%)\" },\n  { \"position\": 0.75, \"color\": \"rgb(38%,59%,21%)\" },\n  { \"position\": 0.750001, \"color\": \"rgb(31%,54%,18%)\" },\n  { \"position\": 0.8, \"color\": \"rgb(31%,54%,18%)\" },\n  { \"position\": 0.80001, \"color\": \"rgb(25%,49%,14%)\" },\n  { \"position\": 0.85, \"color\": \"rgb(25%,49%,14%)\" },\n  { \"position\": 0.850001, \"color\": \"rgb(19%,43%,11%)\" },\n  { \"position\": 0.9, \"color\": \"rgb(19%,43%,11%)\" },\n  { \"position\": 0.90001, \"color\": \"rgb(13%,38%,7%)\" },\n  { \"position\": 0.95, \"color\": \"rgb(13%,38%,7%)\" },\n  { \"position\": 0.950001, \"color\": \"rgb(6%,33%,4%)\" },\n  { \"position\": 0.990001, \"color\": \"rgb(6%,33%,4%)\" },\n  { \"position\": 1.0, \"color\": \"rgb(0%,27%,0%)\", \"label\": \"0.6\" },\n  { \"position\": 1.05, \"color\": \"rgb(0%,27%,0%)\" }\n ]\n}\n\n\nA discrete legend must have an array of objects that have a color and label. Here is a JSON object example of a discrete legend and the legend itself:\n\n{\n \"type\":\"discrete\",\n \"items\":[\n   {\n    \"color\":\"#000000\",\n    \"label\":\"No Data (Missing data)\"\n   },\n   {\n    \"color\":\"#ff0000\",\n    \"label\":\"Saturated or defective pixel\"\n   },\n   {\n    \"color\":\"#2f2f2f\",\n    \"label\":\"Dark features / Shadows \"\n   },\n   {\n    \"color\":\"#643200\",\n    \"label\":\"Cloud shadows\"\n   },\n   {\n    \"color\":\"#00a000\",\n    \"label\":\"Vegetation\"\n   },\n   {\n    \"color\":\"#ffe65a\",\n    \"label\":\"Not-vegetated\"\n   },\n   {\n    \"color\":\"#0000ff\",\n    \"label\":\"Water\"\n   },\n   {\n    \"color\":\"#808080\",\n    \"label\":\"Unclassified\"\n   },\n   {\n    \"color\":\"#c0c0c0\",\n    \"label\":\"Cloud medium probability\"\n   },\n   {\n    \"color\":\"#ffffff\",\n    \"label\":\"Cloud high probability\"\n   },\n   {\n    \"color\":\"#64c8ff\",\n    \"label\":\"Thin cirrus\"\n   },\n   {\n    \"color\":\"#ff96ff\",\n    \"label\":\"Snow or ice\"\n   }\n ]\n}\n\n\nLegends are accessible at https://sh.dataspace.copernicus.eu/ogc/wms/{instanceId}?service=WMS&request=GetLegendGraphic&layer={layer}&height={height}&width={width}, where height and/or width can be omitted.\n\n\n\n\nHow can I clip the image to a specific polygon?\n\n\nYou can add a parameter GEOMETRY as in this example:\n\n\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?SERVICE=WMS&REQUEST=GetMap&VERSION=1.3.0&LAYERS=TRUE_COLOR&MAXCC=20&WIDTH=640&HEIGHT=640&CRS=EPSG:4326 &BBOX=-7.885,38.540,-7.870,38.560& GEOMETRY=POLYGON(( -7.877244 38.546511,  -7.876377 38.547818,  -7.871950 38.546125,  -7.872611 38.545023,  -7.871241 38.544475,  -7.869831 38.544560,  -7.866011 38.550445,  -7.872323 38.552895,  -7.874112 38.551451,  -7.877110 38.552537,  -7.878791 38.552976,  -7.879413 38.553099,  -7.880600 38.553320,  -7.881314 38.553126,  -7.882678 38.552762,  -7.883951 38.552667,  -7.885064 38.552160,  -7.885370 38.549346,  -7.877244 38.546511))\n\n\nIt is recommended to simplify geometry before passing it as a parameter to avoid exceeding the maximum number of characters in the URL.\n\n\nIf one wants to have a transparent background outside of the clipped geometry, use the FORMAT=image/png parameter in the call and dataMask band in the evalscript as explained here. Do note however that png images are larger in size and will therefore take longer to load.\n\n\n\n\nWhich projection is best for exporting data?\n\n\nProjections depend on the use case, however, since Sentinel-2 data is originally provided in the UTM projection, this is also a good choice for export. For larger areas spanning multiple UTM zones, a more global projection is preferable, such as Web Mercator (EPSG:3857) or WGS84 (EPSG:4326).\n\n\n\n\nWhich formats are available as image or vector outputs?\n\n\nRead about the available output formats here."
  },
  {
    "objectID": "FAQ.html#custom-scripts",
    "href": "FAQ.html#custom-scripts",
    "title": "Frequently Asked Questions",
    "section": "Custom Scripts",
    "text": "Custom Scripts\n\n\nHow to run a custom script in Copernicus Browser?\n\n\nFinding the appropriate satellite scene\n\nSearch for the location of your interest either by scrolling the map with your mouse or entering the location in the search field on the right side of your screen.\nChoose from which satellites you want to receive the data (dropdown menu on the left side of your screen under Data Collections).\nSelect maximum Cloud coverage percentage.\nSelect the time range by either typing the date or selecting the date from the calendar.\n\n&lt;\nIn the Visualize tab, choose the Custom (Create custom visualization) button under the Layers.\n\nOpen the custom script dialog by clicking on the Custom script tab.\n\n\n\n\n\nWhich functions are supported in Custom Scripts?\n\n\nCustom Script integrates Chrome V8 JavaScript engine, so the vast majority of the functions supported there can be used in Custom Script as well. Some of the functions that can be used with our custom scripts are described in our Evalscript V3 documentation.\n\n\n\n\nHow can I input a custom script into the WMS request URL?\n\n\nAdd the EVALSCRIPT parameter to your OGC request URL with the value of the base64 encoded script, as such: evalscript=cmV0dXJuIFtCMDQqMi41LEIwMyoyLjUsQjAyKjIuNV0%3D. More on custom scripts described here.\n\n\nA working WMS request from Copernicus Browser, using a simple custom evaluation script\n\nhttps://browser.dataspace.copernicus.eu/?zoom=10&lat=41.6429&lng=12.8862&themeId=DEFAULT-THEME&visualizationUrl=U2FsdGVkX1%2FdjVgXKNxsgHpvLC0Lk9xYTBFSuIxQlOH6qZOOtL3utpao5rghCRt8AHfhSUB3LFuwAZoDeBviT1Q%2BUoyJky7D6ZDKrnKwOfz3TPX8xluv1RNfpifr0lOU&evalscript=cmV0dXJuIFtCMDgqMi41LEIwNCoyLjUsQjAzKjIuNV07&datasetId=S2_L1C_CDAS&fromTime=2020-05-15T00%3A00%3A00.000Z&toTime=2020-05-15T23%3A59%3A59.999Z&cloudCoverage=30&dateMode=SINGLE#custom-script\n\nNote that the LAYERS parameter should be set as one of the existing layers in the configuration when using a PRESET=CUSTOM with a custom evalscript.\n\n\n\n\nHow do I do raster calculation on bands?\n\n\nRaster calculations are done in custom scripts. You can access custom scripts in Configuration Utility, in each layer under each configuration.\n\n\nTo edit the script, click on the “pencil icon” beside the Data processing option under a selected layer and enter the Custom Script Editor.\n\n\nSee more details here.\n\n\n\nHow to use the script is detailed here.\n\n\nAn example of the NDVI script below and visualized in Copernicus Browser.\n\n\n// NDVI calculation\nlet ndvi = (B08 - B04) / (B08 + B04)  \nreturn [ndvi]\n\n\n\nHow to handle long scripts in Copernicus Browser?\n\n\nScripts in Copernicus Browser are passed to Sentinel Hub service using a “GET request,” which is limited in the number of characters, depending on the browser used, between 1,000 and 4,000.\n\n\nBut there is an easy way to work around this:\n\n\nUpload your script to a publicly accessible website, for example GitHub, like we do in the Custom Script repository. E.g. LAI script.\nSelect “Use URL” and paste the URL to the script in the “Script URL” field.\n\n\n\nClick the “Refresh Evalscript” button.\n\n\n\n\nHow can I observe live changes of custom scripts directly in the Configurator?\n\n\nIn Configuration Utility, it is possible to edit Data Processing of a Layer and observe live changes in the Layer’s Preview before even saving the Layer. This feature can be useful for tweaking custom script parameters. The best way to do this is to:\n\n\n\nDuplicate a Layer by clicking on the “Copy” icon (name the new layer and save it).\n\n\nOpen Preview by clicking the “Preview” link in the original and duplicated Layer for easier comparison.\n\n\nEdit one of them to see changes between scripts by opening the Layer’s “Data Processing” settings by clicking on the “Pencil” button.\n\n\n\n\nAfter the appropriate EO product is chosen, click the button “Copy script to editor” [4] to tweak the script [5]. After setting the custom script [6] you can immediately inspect the changes in the preview window without saving the Layer.\n\n\n\nYou can also choose different EO Products [7] under “Data processing settings”.\n\n\nClicking the “Undo” link will reset the latest custom script changes in the layer.\n\n\n\n\nHow to write complex custom script algorithms?\n\n\nBy employing the dynamically interpreted JavaScript language and providing some specialized functions, you can combine the bands of multispectral satellite data in unprecedented ways. Here is an example of how to tweak the image with a custom script in case of a volcano eruption:\n\n\nreturn [\n  B04 * 2.5 + Math.max(0, B12 - 0.1),\n  B03 * 2.5 + Math.max(0, B11 - 0.1),\n  B02 * 2.5\n];\n\n\n\nEtna volcano eruption dated 16 March 2017. Image combined from true colour image, overlaid with SWIR bands 11 and 12. (view in Copernicus Browser)\n\n\n\nYou can find more examples and learn how to tweak the images for easier detection of Earth surface changes, clouds, snow, shadow, water, etc., in this blog post.\n\n\n\n\nHow to interpolate/blend the index value between two (or more) colors using a custom script?\n\n\nUse the valueInterpolate method:\n\nvalueInterpolate(inputValue, indexArray, outputValueArray)\n\nThe returned value is interpolated between the two consecutive values in outputValueArray (which represent RGB colors normalized to [0,1], e.g., pure red is [1,0,0]) based on the inputValue’s location in the indexArray.\n\n\nNote that indexArray and outputValueArray must be the same size. For example:\n\n\nreturn valueInterpolate(NDVI,    // inputValue\n    [0.0, 0.3, 1.0],        // indexArray\n    [                       // outputValueArray\n      [1, 0, 0],\n      [1, 1, 0],\n      [0.1, 0.3, 0],\n    ])\n\nIf NDVI is 0.2, it will interpolate between [1, 0, 0] and [1, 1, 0] since they are the corresponding colors to the 0.0 and 0.3 indices in the indexArray nearest to NDVI.\n\n\nreturn valueInterpolate(sample.B04,    // inputValue\n    [0, 0.2, 0.4, 0.6, 0.8, 1],      // indexArray\n    [                                // outputValueArray\n      [0, 0, 0],\n      [0.1, 0.2, 0.5],\n      [0.25, 0.4, 0.5],\n      [0.4, 0.6, 0.5],\n      [0.75, 0.8, 0.5],\n      [1, 1, 0.5]\n    ])\n\nIf B04 is 0.25, it will interpolate between [0.1, 0.2, 0.5] and [0.25, 0.4, 0.5] since they are the corresponding colors to the 0.2 and 0.4 indices in the indexArray nearest to B04.\n\n\n\n\nHow to manually configure stretch of the images?\n\n\nYou can use Custom scripts and type in the code along the following lines:\n\n\nfunction stretch(val, min, max) {\n    return (val - min) / (max - min);\n}\n\nreturn [\n  stretch(B04, 0.05, 0.5),\n  stretch(B03, 0.05, 0.5),\n  stretch(B02, 0.05, 0.5)\n]\n\n\n\nWhat is downsampling and upsampling?\n\n\nUpsampling and downsampling define the method used for interpolation of the data on non-natural scales. E.g. The resolution of Sentinel-2 data (R, G, B and NIR bands) is 10 meters but on some occasions, you would want to look at the data with higher scale (e.g. at 1 m pixel resolution) or lower scale (e.g. 1000 m pixel resolution).\n\n\nA default option is “nearest neighbour”, which is best for performance. “Bicubic” is often nicer on higher scales.\n\n\nNote that the data are always exactly the same - it is just the interpolation method.\n\n\nAn example of the NDVI image of the field:\n\n\n\n\n\nWhat is the best way to do cloud filtering at a specific AOI?\n\n\nOften our users would like to get all available dates for specific areas of interest (e.g. agriculture field) together with cloud coverage. The basic cloud coverage meta-data that we provide through our services is based on scene meta-data and is valid for 100 km x 100 km area, which is often not really useful if you are looking at a one-hectare field.\n\n\nWhat we recommend doing in this case is:\n\n\nCreate a new layer configuring it to show cloud data. Make sure you output actual cloud values (e.g. 1=cloud, 0=not cloud) rather than cloud mask as an image (e.g. red=cloud, transparent=no cloud).\nYou can use one of several cloud detection options, such as:\n\nL2A scene classification data\ns2cloudless Python package in combination with Sentinel Hub Python API\nBraaten-Cohen-Yang cloud detector\nHollstein’s cloud detection\n\n\nOr you define some of your own. For this exercise, we will use L2A scene classification. The custom script would look something along the lines:\n\nswitch (SCL) {\n  // No Data (Missing data)    \n  case 0: return [0];\n  // Saturated or defective pixel   \n  case 1: return [0];\n  // Cloud shadows\n  case 3: return [0];\n  // Cloud medium probability\n  case 8: return [1];    \n  // Cloud high probability \n  case 9: return [1];\n  // Thin cirrus \n  case 10: return [1];\n  default : return [0];\n}\n\nUse the Statistical API to query the cloud layer values over the specified time range (e.g. 2017-01-01/2017-12-31) over your area of interest specified as the GEOMETRY parameter.\nThe output of the service will give you an array of all dates along with the mean value of “cloud” for each of the dates. Mean value “0” means there are no clouds, “1” fully covered, “0.7” 70% covered.\n\n\n\n\nHow can I create a custom evaluation of Digital Elevation Model (DEM)?\n\n\nTo avoid problems of negative values in GeoTiff format, we have integrated DEM in a way that:\n\n\nElevation above sea level is stored as it is (e.g., 0-8,848 meters)\nElevation below sea level is stored from 65535 downward (e.g., “-10 meters” is stored as 65525)\n\nTo implement your own visualization, you can write a script along the following lines, e.g., “terrain if sea level rises by 2 meters”:\n\nif (DEM &gt;= 32768) {\n  elevation = DEM - 65535.0;\n} \nelse {\n  elevation = DEM;\n}\nif (elevation &gt; 2.0) {\n  return [0,0,1];\n} \nelse {\n  return [elevation/1000, elevation/1000, elevation/1000];\n}"
  },
  {
    "objectID": "FAQ.html#image-manipulation",
    "href": "FAQ.html#image-manipulation",
    "title": "Frequently Asked Questions",
    "section": "Image Manipulation",
    "text": "Image Manipulation\n\n\nHow can I get actual NDVI values?\n\n\nIf you want to get float values out of the service, you will have to use 32-bit float image type (as uint8 and uint16 types only support integers). A custom script which will return NDVI values for Sentinel-2 data could be:\n\n\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\"B04\", \"B08\"]\n    }],\n    output: {\n      id: \"default\",\n      bands: 1,\n      sampleType: SampleType.FLOAT32\n    }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04);\n  return [ndvi]\n}\n\nYou can save this script in the Configuration Utility, or you can pass it as the EVALSCRIPT parameter. For the latter, an example of the call (which uses Sentinel-2 L2A data) is:\n\n\nhttps://sh.dataspace.copernicus.eu/ogc/wms/&lt;INSTANCE_ID&gt;?service=WMS&request=GetMap&layers=&lt;MY_LAYER&gt;&format=image/tiff&maxcc=20&time=2015-01-01%2F2017-12-05&height=512&width=512&srs=EPSG%3A3857&bbox=1817366.784508351,5882593.696827164,1819812.7694134766,5885039.681732289&EVALSCRIPT=dmFyIG5kdmkgPSAoQjA4LUIwNCkvKEIwOCtCMDQpOw0KcmV0dXJuIFtuZHZpXTs=\n\nWe suggest also checking the FAQ about details of internal calculation.\n\n\n\n\nHow can I make some pixels transparent?\n\n\nIf you want to have pixels transparent (or semi-transparent), the following can be done:\n\n\nformat=image/png (note that PNGs are larger than JPGs, which might affect download speed)\ncustom script output needs to have 4 channels, fourth one being alpha, e.g. “return[1,0,0,0.5]” for semi-transparent red pixel or “return[0,0,0,0]” for full transparency\n\nE.g. if one wants to have a Hollstein’s cloud overview layer shown in a way, that everything except clouds is transparent, you need to change\n\nlet naturalColour = [B04, B03, B02].map(a =&gt; gain * a);\nlet CLEAR  = naturalColour;\nlet SHADOW = naturalColour;\nlet WATER  = [0.1, 0.1, 0.7];\nlet CIRRUS = [0.8, 0.1, 0.1];\nlet CLOUD  = [0.3, 0.3, 1.0];\nlet SNOW   = [1.0, 0.8, 0.4];\n\n// Change to:\n\nlet naturalColour = [0, 0, 0, 0];\nlet CLEAR  = naturalColour;\nlet SHADOW = naturalColour;\nlet WATER  = [0.1, 0.1, 0.7, 1];\nlet CIRRUS = [0.8, 0.1, 0.1, 1];\nlet CLOUD  = [0.3, 0.3, 1.0, 1];\nlet SNOW   = [1.0, 0.8, 0.4, 1];\n\nNote that all other outputs need to be 4-channel ones as well.\n\n\n\n\nHow can I get an image in a desired resolution instead of width/height?\n\n\nInstead of WIDTH and HEIGHT parameters one can use RESX and RESY.\n\n\nE.g. if one adds “RESX=10m&RESY=10m”, the image will be returned in 10m resolution.\n\n\n\n\nHow can I get original reflectance data from the satellite?\n\n\nREFLECTANCE is the physical property of surfaces, equivalent to the ratio of reflected light to incident light (source), with typical values ranging from 0-1. It requires a 32-bit TIFF floating format. To get original reflectance data from the satellite, use the advanced evalscript and set sampleType to FLOAT32. See the evalscript example for a single grayscale band below:\n\n\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\"B04\"]\n    }],\n    output: { \n      bands: 1, \n      sampleType: \"FLOAT32\" \n    }\n  };\n}\nfunction evaluatePixel(samples){\n    return [samples.B04]\n}\n\nTo get the exact original reflectance values, the user must also ensure that the pixels output by Sentinel Hub are at the same position and the same size as in the original data. To do so, the user must ensure to:\n\n\n\nRequest a bounding box, which is aligned with the grid in which satellite data is distributed.\n\n\nRequest the same resolution as in the original data (note that different bands of the same satellite can have different resolutions. To check the resolutions for each band, see our data documentation).\n\n\n\nWe suggest also checking the FAQ about how the values are calculated and returned in Sentinel Hub.\n\n\n\n\nHow can I find the image acquisition date?\n\n\nThere are several options, depending on what you would like to do.\n\n\nOGC services\n\n\nYou can use the WFS service with the same parameters as used in your WMS request (e.g. date, cloud coverage). You will get a list of features representing scenes fitting the criteria.\nYou can configure the layer to show acquisition dates on images returned from WMS service. Go to the layer of your choice and follow the steps:\n\nClick the “Advanced” option in the Layer tab to enter advanced parameters dialogue.\n(optional) You can turn on Help by clicking on “?” top right.\nAdd “additionalData” node with type “Date”.\nClick Save in Advanced parameters dialogue.\nClick Save Layer and layer’s tab.\n\n\n\n\nCatalog API\n\n\nMake a request to Catalog API where you set a distinct parameter to date along with setting bounding box, timespan and collection for which you want to get the acquisition dates (see example).\n\n\n\n\nHow can I access angle data information for Sentinel-2 L2A?\n\n\nGrid of solar zenith angles and grid of solar azimuth angles are accessible by names sunZenithAngles and sunAzimuthAngles, respectively, and can be used in a similar way as band values, e.g.:\n\nreturn [sunAzimuthAngles / 50];\nreturn [sunZenithAngles / 60];\n\nThere are also two grids named viewZenithMean and viewAzimuthMean that contain average viewing incidence angles (zenith and azimuth) across all bands and detectors. They can be used as, e.g.:\n\nreturn [viewAzimuthMean / 70];\nreturn [viewZenithMean / 80];"
  },
  {
    "objectID": "FAQ.html#eo-products",
    "href": "FAQ.html#eo-products",
    "title": "Frequently Asked Questions",
    "section": "EO products",
    "text": "EO products\n\n\nHow can I create EO products?\n\nYou can easily create a new EO product in Configuration Utility. 1. Create a new configuration or open an existing one. 2. Click on “Add new layer”. 3. Enter the title of the layer. 4. Click the “pencil icon” next to the Data Processing option to select a predefined product or enter your processing script.  5. Choose from the available Base Products. 6. Click on the selected product under Visualization Options and the “Copy Script to Editor” button. 7. Edit the selected base script in the Custom script editor and click the “Set Custom Script” button when you are done. 8. Click “Save” under the newly added layer to complete the creation of a new layer. 9. The newly added layer will appear in the list of layers under your configuration. \n\n\n\nHow do I view NDVI?\n\n\nThere are several ways you can view the Normalized Difference Vegetation Index (NDVI):\n\n\nYou can view it directly in Copernicus Browser, which provides the NDVI visualization as one of the default visualizations.\n\n\n\nYou can create a new NDVI layer yourself with custom modifications in Configuration Utility, and view it in Copernicus Browser after logging in with your credentials. Read more about creating a new layer.\n\n\n\nYou can also integrate the NDVI layer with the WMS in your own application. Read more about the WMS request, OGC example requests.\n\n\n\n\nAre EO products being changed through time or is the visualization fixed?\n\n\nWhen the user selects “EO Product template”, she can see the Custom script behind each visualization/processing to make it as transparent as possible on what is happening with the data. We are changing these configurations over time, adding new ones and improving existing ones. For those who want to ensure that processing of their layers is not changed any more, they can simply edit the original Custom script and gain full control over it.\n\n\n\n\nHow can I get S2A scene classification for Sentinel-2?\n\n\nYou can get information from the scene classification layer produced by Sen2Cor, for data where S2A is available. Data can be retrieved by identifier “SCL” (e.g. instead of return [B02]; for blue color one can use return [SCL/10];).\n\n\nData can be then used for e.g. validation of the pixel value, e.g. along the lines:\n\nvar scl = SCL;\nif (scl == 0) { // No Data\nreturn [0, 0, 0]; // black\n} else if (scl == 1) { // Saturated / Defective\nreturn [1, 0, 0.016]; // red\n} else if (scl == 2) { // Dark Area Pixels\nreturn [0.525, 0.525, 0.525]; // gray\n} else if (scl == 3) { // Cloud Shadows\nreturn [0.467, 0.298, 0.043]; // brown\n} else if (scl == 4) { // Vegetation\nreturn [0.063, 0.827, 0.176]; // green\n} else if (scl == 5) { // Bare Soils\nreturn [1, 1, 0.325]; // yellow\n} else if (scl == 6) { // Water\nreturn [0, 0, 1]; // blue\n} else if (scl == 7) { // Clouds low probability / Unclassified\nreturn [0.506, 0.506, 0.506]; // medium gray\n} else if (scl == 8) { // Clouds medium probability\nreturn [0.753, 0.753, 0.753]; // light gray\n} else if (scl == 9) { // Clouds high probability\nreturn [0.949, 0.949, 0.949]; // very light gray\n} else if (scl == 10) { // Cirrus\nreturn [0.733, 0.773, 0.925]; // light blue/purple\n} else if (scl == 11) { // Snow / Ice\nreturn [0.325, 1, 0.980]; // cyan\n} else { // should never happen\nreturn [0,0,0];\n}\n\nPlease note that it makes sense to use this layer only on full resolution as any interpolation based on classification code list will not produce reasonable results. You should also use the NEAREST upsampling/downsampling setting, which you can find in “Effects and advanced options”.\n\n\nAlso please note that this map does not constitute a land cover classification map in a strict sense, its main purpose is to be used internally in Sen2Cor in the atmospheric correction module to distinguish between cloudy pixels, clear pixels and water pixels.\n\n\nAn example of Blinnenhorn, Switzerland (14 October 2017):\n\n\n\n\nSentinel-2 L2A, SCL, acquired with Copernicus Browser\n\n\n\n\n\nSentinel-2 L2A, true color, acquired with Copernicus Browser\n\n\nThis is a prototype feature and can change in future releases."
  },
  {
    "objectID": "AnnualReports.html",
    "href": "AnnualReports.html",
    "title": "Annual Reports",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem (CDSE) proudly announces the publication of its first-ever Annual Report."
  },
  {
    "objectID": "Registration.html",
    "href": "Registration.html",
    "title": "User registration and authentication",
    "section": "",
    "text": "This section provides information on how to register and authenticate on the Copernicus Data Space Ecosystem."
  },
  {
    "objectID": "Registration.html#step-1-registration",
    "href": "Registration.html#step-1-registration",
    "title": "User registration and authentication",
    "section": "Step 1: Registration",
    "text": "Step 1: Registration\nGo to website where you can simply click on “login” with the button located in the top right corner.\n\n\n\n\n\nOn the landing page, locate the “REGISTER” button on the screen’s right side. Clicking on it will lead you to the Copernicus Data Space Ecosystems registration form. Please ensure to complete all mandatory fields (excluding Thematic activity and Purpose of Use), and you have the option to fill in any optional fields. Once you have provided the necessary information, accept the terms and conditions. Additionally, you can accept any other optional consents if applicable. Finally, click the “REGISTER” button to complete the process."
  },
  {
    "objectID": "Registration.html#step-2-e-mail-verification",
    "href": "Registration.html#step-2-e-mail-verification",
    "title": "User registration and authentication",
    "section": "Step 2: E-mail verification",
    "text": "Step 2: E-mail verification\nUpon registration, you will be prompted to verify your email address by receiving a verification email. To complete the process, simply click on the “Verify email address” link when you open the email.\n\nAfter successfully verifying your email, your registration process is complete. You can now log in using your credentials (providing Email and Password).\nIf you have an issue with registering or you want to deregister, please contact us."
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html",
    "href": "Applications/DataSpaceDashboard.html",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "",
    "text": "The Copernicus Data Space Ecosystem Dashboard (hereinafter the Dashboard) is your go-to tool for checking out the Copernicus Data Space Ecosystem in action. It’s a free, user-friendly platform where you can easily see what’s going on with the data space.\nYou can access the Copernicus Data Space Ecosystem Dashboard at: https://dashboard.dataspace.copernicus.eu/"
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html#what-youll-find",
    "href": "Applications/DataSpaceDashboard.html#what-youll-find",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "What You’ll Find",
    "text": "What You’ll Find\n1. Highlights\nThe Dashboard greets users with a snapshot of overall statistics, highlighting key metrics with overall statistics to provide a quick glance at the system’s vital signs.\n2. Main Content\nIn the middle of the screen you will find the main content. The main content changes based on the selected page from the application navigation, so you always get the specific info you need.\n3. Application Navigation and Personalization\nOn the left side, you’ll find easy navigation to explore different parts of the Dashboard. Plus, there are settings to tailor the Dashboard to fit your preferences."
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html#interactive-use",
    "href": "Applications/DataSpaceDashboard.html#interactive-use",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "Interactive Use",
    "text": "Interactive Use\nUsing the Dashboard is easy and interactive, letting you explore more details for each statistic.\n1. Hover for Quick Info\nHover your mouse over any number or chart to get quick descriptions and the last updated info.\n\nFigure 1 Mouse hover on the Total volume of published products – Sentinels\n\nFigure 2 Mouse hover on chart Total volume of published products - Sentinels\n2. Detailed Data\nFor an even closer look, click directly on the chart to explore details. Let’s walk through visualizing the “Total number of published products - Sentinels” per product type as an example:\n\nLocate the pie chart displaying Sentinels missions: Total number of published products - Sentinels\nFocus on the segment corresponding to Sentinel-2.\nClick on the Sentinel-2 segment to reveal detailed data per product type\n\n\n\nPop-up opens with details per product type for the selected mission."
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html#explore-historical-data",
    "href": "Applications/DataSpaceDashboard.html#explore-historical-data",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "Explore Historical Data",
    "text": "Explore Historical Data\nTo view trends over time are available by clicking on the icon  . The chart transforms into a line chart, giving you a historical data overview.\n\nFigure 3 Historical data for Number of Streamlined Data Access requests with time period last 30 days\nIf you want to dive deeper, modify the view with your mouse or chart tools to focus on what matters to you.\n\nFigure 4 Tools for modifying the line chart with historical data\nService Health Exception\nThere is something special on the Service Health page. Each service has its own “Availability Timeline”. Simply click on the icon  next to any service. You will see the availability of that service over time.\n\nFigure 5 Copernicus Browser availability over time"
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html#share-widget",
    "href": "Applications/DataSpaceDashboard.html#share-widget",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "Share Widget",
    "text": "Share Widget\nIf you want to share a statistic within a widget, follow these simple steps:\n\nClick on the widget with the statistic you want to share. This opens a dialog with a URL.\nHover your mouse over the URL, and a pop-up appears with the text “CLICK TO COPY LINK”.\n\n\nFigure 6 Share selected statistic with others\n\nClick on the pop-up and the URL to the selected widget is now copied to your clipboard."
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html#news",
    "href": "Applications/DataSpaceDashboard.html#news",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "News",
    "text": "News\nNews is available by clicking on the icon on the bottom right corner  to open an overview. A quick look reveals the most recent news with an option to read more: “Show all news”."
  },
  {
    "objectID": "Applications/DataSpaceDashboard.html#settings",
    "href": "Applications/DataSpaceDashboard.html#settings",
    "title": "Copernicus Data Space Ecosystem Dashboard",
    "section": "Settings",
    "text": "Settings\nSettings are available next to the News by clicking on the icon on the bottom right corner:\n\nAuto Refresh\nThe statistics on the Dashboard are by default automatically updated without manually refreshing the Dashboard page. If you prefer to keep the current statistics unchanged, simply go to Settings → Auto refresh → toggle the switch.\nShow overview on the page load\nThis setting enables you to turn off the highlights visualization on the Dashboard.\n\nFigure 7 Settings"
  },
  {
    "objectID": "Applications/DataWorkspace.html",
    "href": "Applications/DataWorkspace.html",
    "title": "About Data Workspace",
    "section": "",
    "text": "The Data Workspace is a valuable tool for managing and reviewing Earth observation-related products. This platform enables you to aggregate and review products, which can then be further processed or downloaded for various purposes.\nThe Data Workspace enables management and ordering of satellite products. Offline products can be ordered and their retrieval progress can be monitored in the ‘Processing Status’ section. Online products can be selected for processing with higher-level processors or downloaded.\nWhen the products are selected for processing, you are provided with a list of processors that are capable of processing relevant data types. The processors can be further parameterized to fine-tune the results.\nOnce the order for processing is submitted, the progress can be monitored similarly to orders for product retrieval. The status dashboards also include all orders submitted through the ordering API. The status of the orders can be monitored on the status page, and the orders can be updated while being executed, providing the flexibility to cancel unnecessary tasks.\nYou can familiarise yourself with workspace and access it at https://dataspace.copernicus.eu/workspace/."
  },
  {
    "objectID": "Applications/DataWorkspace.html#adding-products-to-workspace",
    "href": "Applications/DataWorkspace.html#adding-products-to-workspace",
    "title": "About Data Workspace",
    "section": "Adding products to Workspace",
    "text": "Adding products to Workspace\nYou can add products by using the Copernicus Browser.\n\n\n\nBrowser\n\n\nThe Copernicus Browser allows you to search for products using various properties, such as time, location and source.\nAfter you find the product you are interested in, you can add it to workspace by using icon visible under its size.\n\n\n\nAdding to workspace\n\n\nThen it will be visible under My Products tab in the Workspace:\n\n\n\nProduct on workspace\n\n\nWhen you have products listed, you can either download them from here or process them in the Processing Center."
  },
  {
    "objectID": "Applications/DataWorkspace.html#downloading-products-from-workspace",
    "href": "Applications/DataWorkspace.html#downloading-products-from-workspace",
    "title": "About Data Workspace",
    "section": "Downloading products from Workspace",
    "text": "Downloading products from Workspace\nIn order to download products from Workspace panel, first select them from the list in My products tab. Then at the bottom right of the page, click on the Download button.\n\n\n\nWorkspace panel\n\n\nA window displaying downloading process will appear. When status bar will reach 100%, it will switch its state to completed and your product will be saved on your device.\n\n\n\nDownload panel"
  },
  {
    "objectID": "Applications/DataWorkspace.html#ordering-products",
    "href": "Applications/DataWorkspace.html#ordering-products",
    "title": "About Data Workspace",
    "section": "Ordering products",
    "text": "Ordering products\nSome products are not avaliable immediately and appear as offline products. To be able to download such products, you need to order them first.\nTo order a product simply find the product with To order avaliability and then click on the Order offline products button.\n\n\n\nOrder panel\n\n\nName your order and click on the Order button.\n\n\n\nOrder window\n\n\nYou will get a confirmation of your order. You can check its status under the Processing status tab.\n\n\n\nOrder confirmation\n\n\nAfter the processing status is Finished/Completed, the ordered product is available in the catalogue and you can look it up via the API using various searches, for example by name:\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(Name,‘S1A_IW_GRDH_1SDV_20230729T092359_20230729T092424_049636_05F7FC_0A61’)\nDownloading from the catalogue is done in the same way as described in the OData Product Download documentation."
  },
  {
    "objectID": "Applications/DataWorkspace.html#processing-products",
    "href": "Applications/DataWorkspace.html#processing-products",
    "title": "About Data Workspace",
    "section": "Processing products",
    "text": "Processing products\nProcessing products in the Processing center allows user to transform products in a way that they could become useful for certain cases. The method of processing and its outcome is defined for each avaliable processor used for the process.\nTo process the product its avaliability needs to be in Immediate status. Now you can add your product to the Processing center tab.\nCheck the boxes next to the product of your interest and click on the Create processing order button. You can select multiple products.\n\n\n\nWorkspace panel\n\n\nClick on the Confirm button.\n\n\n\nConfirm adding product to procesing center\n\n\nClick Go to Processing center.\n\n\n\nConfirm adding product to procesing center\n\n\nSelect the product and pick a processor from the list used for its processing.\n\n\n\nSelect product for processing\n\n\n\n\n\nSelect processor for processing\n\n\nClick on the Create processing order and the the Continue button.\n\n\n\nPlacing order\n\n\nClick on the Order processing button.\n\n\n\nPlacing order\n\n\nClick on the Submit order button.\n\n\n\nSubmitting order\n\n\nClick on the Go to Processing status button.\n\n\n\nRedirection to procesing status tab\n\n\nYou will be redirected to the Processing Status tab.\n\n\n\nProcessing status tab\n\n\nFrom here you can check ongoing processing orders. Once they will be finished, they shall be transfered to the list of orders under Finished tab.\nAfter the processing status is Finished/Completed, the processed product is available in the catalogue and you can look it up via the API using various searches, for example by name:\nhttps://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(Name,‘S1A_IW_GRDH_1SDV_20230729T092359_20230729T092424_049636_05F7FC_0A61’)\nYou will find both the product on which the product has been processed and the product after processing. Downloading from the catalogue is done in the same way as described in the OData Product Download documentation."
  },
  {
    "objectID": "Applications/PlazaDetails/Strength.html",
    "href": "Applications/PlazaDetails/Strength.html",
    "title": "Credit Consumption",
    "section": "",
    "text": "The following are among the popular services available in OpenEO Algorithm Plaza. Here, we have presented an average credit strength for these services. Please note that the credits can vary depending on the time interval and area of interest.\nThe purpose of this document is to solely provide users with an assumption on how these credits work and their strength for varying services.\n\nNDVI\nThe Normalized Difference Vegetation Index (NDVI) is a key indicator of vegetation health. It is calculated by comparing the difference between near-infrared (which vegetation strongly reflects) and red light (which vegetation absorbs) in satellite imagery. Using the openEO Algorithm Plaza, NDVI service costs 3-5 credits per hectare.\n\n\nNDII\nThe Normalized Difference Infrared Index (NDII) computation offers insight into plant canopies’ water content. Using the openEO Algorithm Plaza, NDII service costs 4-8 credits per hectare.\n\n\nBIOPAR\nThe BIOPAR service provided by the openEO Algorithm Plaza calculates various biophysical parameters for an area defined by a polygon. The result is a GeoTiff file containing the parameter values. Using the openEO Algorithm Plaza, Biomass service costs 20-40 credits per hectare.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThough the required credit for direct download of the output data is two credits for all the services, it has a time limitation of 10 seconds to run the entire process. Otherwise, the download process will fail."
  },
  {
    "objectID": "Applications/PlazaDetails/ExecuteService.html",
    "href": "Applications/PlazaDetails/ExecuteService.html",
    "title": "Execute a service",
    "section": "",
    "text": "Users can use the services offered in the openEO Algorithm Plaza to analyse their data. These services consist of openEO-based User Defined Processes (UDP). Here is a concise overview of the tools used to implement and use these available services:\nUsers have the flexibility to use their preferred tool to execute the services. Each service has a Get Started button within the plaza. This button offers available execution options for the service, such as:"
  },
  {
    "objectID": "Applications/PlazaDetails/ExecuteService.html#online-user-interface",
    "href": "Applications/PlazaDetails/ExecuteService.html#online-user-interface",
    "title": "Execute a service",
    "section": "Online user interface",
    "text": "Online user interface\nA new window opens when a user chooses to run a service in the web editor using the Execute in Web Editor option. Here, users can execute services directly in a web editor by simply providing the required parameters and running them.\nThe full web editor documentation can be found in this section. Moreover, below are some additional resources to help users get started with the web editor:\n\n\n\nopenEO\n\n\n\n\nAccess\n\n\nDocumentation"
  },
  {
    "objectID": "Applications/PlazaDetails/ExecuteService.html#client-libraries",
    "href": "Applications/PlazaDetails/ExecuteService.html#client-libraries",
    "title": "Execute a service",
    "section": "Client libraries",
    "text": "Client libraries\nMost of the openEO Algorithm Plaza services can be executed programmatically using the openEO client libraries. When publishing the service, the developer is requested to provide code examples for running the service using Python, R, or JavaScript. When service users select the See client examples option, they are directed to the provided example.\nFor detailed documentation on how to use the client libraries, please refer to the official in the following links:\n\nJavaScript\nPython\nR\n\nThe following example shows a code sample on how to execute a service through the openEO Python Client.\nimport openeo\n\n# Setup parameters\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [\n                5.179324150085449,\n                51.2498689148547\n            ],\n            [\n                5.178744792938232,\n                51.24672597710759\n            ],\n            [\n                5.185289382934569,\n                51.24504696935156\n            ],\n            [\n                5.18676996231079,\n                51.245342479161295\n            ],\n            [\n                5.187370777130127,\n                51.24918393390799\n            ],\n            [\n                5.179324150085449,\n                51.2498689148547\n            ]\n        ]\n    ]\n}\ndate = '2020-06-01'\n\n# Setup connection with OpenEO\neoconn = openeo.connect(\"https://openeo.dataspace.copernicus.eu\").authenticate_oidc(\"egi\")\n\n# Create a processing graph from the BIOMASS process using an active openEO connection\ntaskmap = eoconn.datacube_from_process(\"taskmap_generate\", namespace=\"https://openeo.dataspace.copernicus.eu/openeo/1.0/processes/u:123456/taskmap_generate\", aoi=aoi,\n                                       date=date)\n\n# Execute the openEO request as a batch job\ntaskmap_job = taskmap.download('task.nc')\nIf any issues are encountered when executing a service, please feel free to raise questions in the forum directly."
  },
  {
    "objectID": "Applications/PlazaDetails/ServiceMaturity.html",
    "href": "Applications/PlazaDetails/ServiceMaturity.html",
    "title": "Service Maturity",
    "section": "",
    "text": "Each service in the openEO algorithm plaza is assigned a maturity level that indicates its qualitative documentation and performance. Currently, five different maturity levels for each service are available. The prototype is the primary and default level, followed by incubating, verifying, validating, and operating as advanced services. These levels are determined solely based on software readiness and user documentation criteria. These criteria are generally designed to ensure that the service meets specific standards and provides customers with a certain level of quality."
  },
  {
    "objectID": "Applications/PlazaDetails/ServiceMaturity.html#level-1-prototype",
    "href": "Applications/PlazaDetails/ServiceMaturity.html#level-1-prototype",
    "title": "Service Maturity",
    "section": "Level 1: Prototype",
    "text": "Level 1: Prototype\nBy default, every published service has a prototype level. It is expected that users consider the following points when publishing a service:\n\nThe service is executable, and basic logging information is supported.\nA possible reference or a general overview of what it tries to implement is provided as service metadata.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose a service meets the criteria for a higher level. In that case, an upgrade can be requested anytime after publication. It’s essential to ensure that all requirements are satisfied before the upgrade can be approved."
  },
  {
    "objectID": "Applications/PlazaDetails/ServiceMaturity.html#level-2-incubating",
    "href": "Applications/PlazaDetails/ServiceMaturity.html#level-2-incubating",
    "title": "Service Maturity",
    "section": "Level 2: Incubating",
    "text": "Level 2: Incubating\nIn addition to the criteria for prototype level, a service needs to meet the following requirements to become an incubating service:\n\nService metadata should include an example along with the expected output format.\nAn approximate assumption on how much user credit is required to execute a service should be provided.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that no added value is associated with services on a prototype or incubating levels. In other words, added value costs are included in services that are either verified, validated or operational."
  },
  {
    "objectID": "Applications/PlazaDetails/ServiceMaturity.html#level-3-verified-or-validated",
    "href": "Applications/PlazaDetails/ServiceMaturity.html#level-3-verified-or-validated",
    "title": "Service Maturity",
    "section": "Level 3: Verified or Validated",
    "text": "Level 3: Verified or Validated\nWhen a service is labelled as either verified or validated, they mark the same level of maturity. Users can expect the same level of performance from them, but the naming difference is due to its irrelevance/relevance to software validation reports as a part of user documentation.\n\nLevel 3a: Verified\n\nA comprehensive functional and integration test should be possible.  \nAdvanced logging should help while debugging.  \nService metadata should include information on detailed descriptions of the services, their parameters and a link to a publication that supports the methodology adopted. An example of the expected outcome from the service should be provided.\nApproximate cost estimation on a larger scale should be presented.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease mention or provide a report to the support team if any constraints/limitations with the published service exist that should be considered.\n\n\n\n\nLevel 3b: Validated\n\nAll the criteria mentioned for the verified level apply to this level. Additionally, the validation report must be provided either as a separate document to the support team or as a non-expiring link."
  },
  {
    "objectID": "Applications/PlazaDetails/ServiceMaturity.html#level-4-operational",
    "href": "Applications/PlazaDetails/ServiceMaturity.html#level-4-operational",
    "title": "Service Maturity",
    "section": "Level 4: Operational",
    "text": "Level 4: Operational\nA highly improved service can only be marked with the highest level of maturity, i.e. operational. It must satisfy the following criteria:\n\nAll the conditions to be either verified or validated should be satisfied.\nThe service has been shown to fit large-scale production and integration in an operating system.\nRules and constraints for estimating resource usage should be provided as a document to the support team.\nThe service lifecycle management policy should be available to end users.\nAn article summarising the process used for the service should be available on a peer-reviewed website or journal or a conference article (There is no limitation to a specific journal, but proof that the article was peer-reviewed should be provided to the openEO algorithm plaza support service)."
  },
  {
    "objectID": "Applications/PlazaDetails/ServiceMaturity.html#requesting-a-change-of-the-maturity-level",
    "href": "Applications/PlazaDetails/ServiceMaturity.html#requesting-a-change-of-the-maturity-level",
    "title": "Service Maturity",
    "section": "Requesting a change of the maturity level",
    "text": "Requesting a change of the maturity level\nBased on the fulfilment of the above criteria, developers can request an upgrade of the service by submitting a ticket at our help center."
  },
  {
    "objectID": "Applications/PlazaDetails/Reporting.html",
    "href": "Applications/PlazaDetails/Reporting.html",
    "title": "Reporting",
    "section": "",
    "text": "This documentation section demonstrates how to use the reporting function within the openEO algorithm plaza. Individuals using the platform can track their usage and see how many credits have been deducted for each service. This guide aims to help users efficiently track and document their credit consumption while keeping an eye on their balance.\nPlease ensure to log in to access the reporting functionality. Once logged in, navigate to the “Reporting” option in the navigation bar. This will take users to the page displayed in the figure below:\n\nOnce a user has used a service, the relevant information will be displayed on this page. The page is divided into two sections: “Personal” and “Service.” The “Personal” section provides a detailed information on:\n\nFiltering Reports: Users can filter the reports based on a specific time interval. This allows them to view the openEO usage and credit deductions within a desired timeframe, providing a more targeted analysis.\nCredit Usage Display: This section will display the credit-usage for both the synchronous as well as the batch job methods. This information helps in understanding how credits are consumed for different job types. Hovering over the credit usage details will provide additional information on the specific service used.\nJobs List: The page includes a comprehensive list of all past jobs and used services. This list provides a comprehensive overview of the services that have been utilized. Each job entry will typically include details such as job ID, the job duration, and any relevant metadata associated with the job.\n\n\n\nUsers can export the report in either a PDF or CSV file format. To do so, click on the “Export” button on the top right corner of the page. This functionality allows for saving and sharing the report for future reference.\n\nNext to the “Personal” section, the “Service” tab is available to monitor and track reports on the services published by the organisation. In the absence of provided services, this tab remains empty, as previously mentioned. However, upon publishing services, the page is populated accordingly. Additionally, a “Service” section is presented, detailing the number of times each service published by the organisation was used."
  },
  {
    "objectID": "Applications/QGIS.html",
    "href": "Applications/QGIS.html",
    "title": "Sentinel Hub QGIS Plugin",
    "section": "",
    "text": "The Sentinel Hub QGIS Plugin allows you to view satellite image data from the Copernicus Data Space Ecosystem or from Sentinel Hub directly within a QGIS workspace. All datasets are available that are part of collections associated with your user, including commercial data within Sentinel Hub subscriptions and Bring Your Own COG datasets in Sentinel Hub and Copernicus Data Space. The current functionality of the QGIS Plugin is for visualization; it does not allow you to perform operations or access properties of the dataset. For individual downloads, we recommend the Copernicus Browser; for downloading multiple datasets for an area and time period of interest in a graphical interface, Request Builder is the optimal tool."
  },
  {
    "objectID": "Applications/QGIS.html#first-step-authentication",
    "href": "Applications/QGIS.html#first-step-authentication",
    "title": "Sentinel Hub QGIS Plugin",
    "section": "First step: authentication",
    "text": "First step: authentication\nBefore starting, you should have an OAuth client prepared in your Copernicus Data Space Sentinel Hub Services Dashboard (or commercial Sentinel Hub Dashboard). This serves as your authentication to these services when you log in.\n\nRegistering OAuth client\nTo register an OAuth client, open the \"User Settings\" tab in your dashboard, then click the Create new button (1) in the \"OAuth client\" section. Give your OAuth client a name (2), set the Client grant type to Client Credentials, and click the Create client button (3). Your client secret will be displayed. Copy the secret value (4) and paste it locally, as it will not be visible after the pop-up window closes! When you are finished, click Close (5). You should now see the newly created OAuth client name and ID (6) in the list of your OAuth clients. With client ID and client secret, you are now ready to request tokens.\n\nYou can install the plugin from the QGIS plugin repository. Select Plugins/Manage and Install Plugins from the main menu in QGIS and use the search box to find the Sentinel Hub plugin, click Install Plugin and you are done. Paste the client ID and secret to the respective fields in the Login tab of the plugin to authenticate - these will be remembered next time you launch QGIS."
  },
  {
    "objectID": "Applications/QGIS.html#creating-a-configuration",
    "href": "Applications/QGIS.html#creating-a-configuration",
    "title": "Sentinel Hub QGIS Plugin",
    "section": "Creating a configuration",
    "text": "Creating a configuration\nOn the left panel of the Copernicus Data Space Ecosystem Sentinel Hub Services dashboard, select Configuration Utility. Here, you will see a list of all configurations you created earlier. You can create a new one with the New Configuration button (1). For a new configuration, you first have to create a name (2), then the option is offered to create the configuration based on one of the existing instances by changing the settings (3). Clicking Create Configuration (4) takes you to the settings of the configuration.\n\nThe Warnings switch decides whether you will see a warning message if you want to show or download an area larger than the limit. Show logo does not affect your QGIS plugin, there is a switch for this on the download panel of the plugin where you can decide.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t switch on Disable OGC requests if you are creating a configuration you want to access in QGIS – the plugin is based on OGC requests!\n\n\nImage quality for visualization can be set using the slider or numerically, and the boundaries of the dataset can be selected in a small map window (Map bounds). Under Advanced Settings, a window opens where you can edit a JSON configuration.\n\nThe New Layer button takes you to the form for setting the data layers in your configuration. Here, you can prepare the dataset you want to view in QGIS. First, add a Name for your layer. Choose an imagery data Source from the Collections available and add a Data Processing evalscript – the pencil icon opens a panel where you can select from predefined evalscripts or edit your own, optionally based on the Custom script repository. By defining a Time range and a Cloud coverage threshold, you can filter the imagery to include, additionally setting the Mosaic order (most recent, first, least cloudy) to your preferences. For one configuration, you can create several layers that will be available as options in QGIS."
  },
  {
    "objectID": "Applications/QGIS.html#use-the-plugin-for-creating-and-updating-a-data-layer",
    "href": "Applications/QGIS.html#use-the-plugin-for-creating-and-updating-a-data-layer",
    "title": "Sentinel Hub QGIS Plugin",
    "section": "Use the Plugin for Creating and updating a data layer",
    "text": "Use the Plugin for Creating and updating a data layer\nOn the Create tab, you can select a Configuration. The configurations available in your dashboard are listed here. These can be used to choose between configurations of different data sources (eg. Sentinel-2 and Sentinel-3) or different evalscript settings.\nSelect Service type based on the data you want to use. You typically will need WMS for this case, since the images are in raster format, but WMTS and WFS services are also available if you want to perform more advanced queries or bring your own areas of interest.\nThe Layer menu allows you to select between the different visualization layers included in your configuration. For the default Sentinel-2 configuration, this menu includes a wide range of visualization options similar to the Copernicus Browser.\n\n\n\n\n\n\nNote\n\n\n\nCRS refers to Coordinate Reference System; this is where you can set the coordinate system of the dataset. For Copernicus Data Space or Sentinel Hub imagery data layers, this should keep the default value of EPSG:3857 .\n\n\n\nIn the Time range bar and the calendar panel, you can choose the start and end date of the period of interest and set an Image Priority order for mosaicking the data layers for this period. Alternatively, if you are interested in images for specific dates, tick Use exact date to disable mosaicking. Calendar dates where an image is available within the selected Cloud Cover ratio for the onscreen area will be highlighted. You can set the Cloud Cover threshold using the slider below the calendar.\nIf you are using mosaicking within a time range, the Image Priority can be set using the dropdown menu to include the most recent, the first or the least cloudy image of the time range in the mosaic for each pixel.\nOnce this is set, you can decide whether to Create a new WMS layer in your QGIS workspace or update an existing one. Take care – the “update existing layer” dropdown is set by default to the selected layer. If you want to look at a different date, click on the date in the calendar and update the layer. If you want to compare, create a new layer for the new date, and you can use QGIS visualization tools such as transparency."
  },
  {
    "objectID": "Applications/QGIS.html#downloading-imagery",
    "href": "Applications/QGIS.html#downloading-imagery",
    "title": "Sentinel Hub QGIS Plugin",
    "section": "Downloading imagery",
    "text": "Downloading imagery\nOn the Download panel, you can download a three-channel RGB rendering of the image on your map window. File format and image resolution can be selected, and optionally a custom bounding box can be added with coordinates."
  }
]